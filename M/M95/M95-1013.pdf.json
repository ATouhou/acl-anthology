{"sections":[{"title":"CRL/NMSU Description of the CRL/NMSU Systems Used for MUC- 6 Jim Cowi e Computing Research Laboratory New Mexico State Universit y jcowie@nmsu.edu (505) 646-518 1 Introductio n This paper discusses the two CRL named entity recognition systems submitted for MUC-6 . The systems are based on entirely different approaches . The first is a data intensive method, whic h uses human generated patterns . The second uses the training data to develop decision trees whic h detect the start and end points of names . Background CRL submitted two systems for the Named Entity task . One of these (Basic) is an improved version of the CRL name recognizer developed in phase one of Tipster[1] . The second (AutoLearn) is a system which learns automatically from training data . The Basic system had approximatel y six man months of work in its original development . Improvements for MUC-6 were carried out by one graduate student (about one man month) . The AutoLearn system was developed by one graduate student specifically for MUC-6 (also one man month) . Availability The Basic system can be accessed for testing through the mail server - ne_tag@crl .nmsu .edu . the subject field of the message should consist of the word \"tag\" . A web interface to the tagger is als o available from CRL's home page . Source code and data files can be ftp'ed from CRL (mai l cable@crl .nmsu .edu for ftp address and access password) . The system has been successfull y tested on Linux running on a PC . Basic NE System The Basic system consists of a pipeline of Unix process . These can be identified as carrying ou t four different types of task . 1. Recognizing names based on character patterns (numbers, dates) . 2. Recognizing names based on pre-stored names . 3. Applying mark-up to potential components of complete names . 4. Recognizing names based on patterns of components . 157 For the most part the system is context free . A few of the patterns used require some additiona l context before a name is recognized . For example an ambiguous human name in isolation may be recognized if it is followed closely by a title . The system consists of suite of C̀' and lex programs . Component units recognized by the system are cities, provinces, countries, company prefixes an d suffixes, company beginning and ending words (Club, Association etc .), unambiguous and ambig - uous human first and last names, titles and human position words . Additional Fix-up Procedure s Final patterns are used to join together units of the same type which are immediately next to eac h other in the text. After all the pattern based procedures have operated on the text a final pass is made to recogniz e abbreviated forms of names . This takes the lists of names found so far and truncates them . (right to left for person and left to right for companies) . These new lists are then used as lists of know n organizations and persons and any occurrences of these in the text are marked . In particular fo r organizations the headline is not processed apart from this last stage . This avoids recognition o f organizations such as \"Leaves Bank\" . The assumption that names mentioned in the heading wil l be repeated in the body of the text holds almost universally . Data Sources The data used in the Basic system is derived from public domain source, university phone lists , the Tipster Gazetteer and government data-bases of company names . Performance The performances of the for the test set and for the walk through article are given in Appendix A . Overall performance was Recall - 85% and Precision - 87% giving an F measure of 85 .8 . Walk through article Performance here was Recall - 63% and Precision - 83% . The main source of error was missing patterns in the system . For example Robert L . James was partially recognized (as L . James), McCann-Erickson was missed as no hyphenated company pat - tern had been added. Once a frequently mentioned name is ignored in its full form the syste m unfortunately misses all abbreviated forms . This article also shows the importance of context i n reliably recognizing some names (e .g . an analyst with PayneWebber) . 158 AutoLearn NE System The AutoLearn system was developed to explore the possibility of using simple learning algorithms to detect specific features in text . An implementation of Quinlan's ID3 Algorithm was use d [2,3] . This algorithm constructs a decision tree which decides whether an element of a collectio n satisfies a property or not . Each element of a collection has a finite number of attributes each o f which may take one of several values . Quinlan's original paper suggests the range of values of th e attributes should be \"small\". In the case of the AutoLearn system the values are every word occurring in the training collection . Collections for Name Recognitio n In order to apply the ID3 algorithm the data needs to be structured into a collection, each membe r of which has specific values for a set of attributes and for each of which it is known whether th e member has a specific property or not . For the name recognition problem the training data wa s converted in tuple of five words . Each tuple was marked as having the start (or end) of a specifi c type of proper name at the middle word of the tuple . This data can be easily generated from the training articles . Thus for the beginning of a person - many differences between Robert L . - 1 differences between Robert L. James 1 between Robert L . James , - 1 etc . Fourteen sets of training data were generated using the 318 development articles supplied fo r MUC-6 . The quality of the tagging is not particularly uniform, but no attempt has been made t o improve this . Generating the decision trees As each word of the training data is read it is hashed and stored in a hash table . Thereafter words are referred to by their hash values . For each of the values of the five attributes (words 1 throug h 5) a count is maintained of the number of times this value contributed to an element holding a proper named occurrence at the middle attribute . The attribute to be tested first is chosen by computing for each value the relative frequency of positive and negative outcomes for this value . This is used to approximate the information content of that attribut e","paragraphs":["- p+log2p+ - p log2p- \\t (EQ 1 )"]},{"title":"The sum of the approximate information contents for each column is calculated and the colum n with the highest value is chosen as the primary decision . Here all the values which always contrib - uted to a positive outcome are used as the primary decision . Values which are always negative are ignored (this is primarily to reduce the size of the data being handled) . New sub-collections are formed with elements containing one value which contributed both to positive or negative outcomes are collected and the tree building process is repeated for each of these new collections . 159 The decision trees thus formed can be output in a readable if somewhat lengthy form . In mos t cases the first choice is the third word in a group taking one of a large number of values . Thereafter a group off fairly impenetrable tests occur. For example for location beginnings - If word 3 is one of the following - Milwaukee Ridgefild Pa ST. . (aroun d 300 more words) then location_beginnin g else if word 3 is Illinois and word 1 is Indiana then location_beginnin g else if word 3 is Northeast and word 1 is ìn' then location_beginnning The printed decision table takes about 5 pages . Running the AutoLearn Syste m A pass through the texts is made for each decision tree (beginning and end) of each named entity . First the hash table of words is read and the corresponding decision tree . The text is then processed in groups of five words . Whenever a positive decision is made a new tag is added to th e output stream . Ideally at this stage the tagging would be done . However, given that we are processing new texts , there are many occasions where an end or a beginning is identified, but the corresponding beginning or end is not. For example a surname may have been seen previously, but not the attache d forename . At this point a heuristic is applied which for every un-matched bracket in the text work s forward or backward until some appropriate point is reached . The actual skipping heuristics need to be different for organizations, persons, locations, dates and numbers . Data Sources The only data source used for the AutoLearn system was the 318 MUC-6 training texts . Performance A high precision was expected from this system . Most of the errors that occur are due to failure s of the bracket insertion heuristic . The overall scores were Recall - 47% and Precision- 81% giving an F measure of 59 .3 . No specific code was inserted to handle numbers or dates . The method was more successful wit h organizations and locations then with persons . More training data is perhaps required to make th e system aware of the spread of examples for human names . Walk through article The performance here was Recall - 36% and Precision - 88% . 160 The major problem here is that the system has not learned a rule which uses \"Mr .\" to identify the word previous to a name. Relationship of Performance to Amount of Training Data . The evaluation texts were processed with decision trees generated using subsets of the MUC- 6 development data . This was done in increasing units of 50 texts . The results are shown in Figure 1 below. Both recall and precision increase with increasing training data . Precision appears to tail off at around 82% . Recall, however, increases (with one exception) steadily over the whole range . score 75 .0 0 70.0 0 65 .0 0 60.00","paragraphs":[".r - 55 .0 0 85 .0 0 x0.00 .m 35 .00 30 .00 Wumbcr of Training Fil m 0.00 \\t 100 .00 \\t 200.00 \\t 300.0 0 FIGURE 1 . Relationship of Performance to Amount of Training"]},{"title":"Future Developments We intend to rebuild the Basic system . One of the principle drawbacks of the system is its sequen - tial application of component tags . In many cases a second tag is not applied because the word o r 5 0.0 0 45.00 40 .00 161 phrase is ambiguous . The correct solution here is to apply all tags in a manner that allows the correct tags to be selected by the pattern processing mechanisms . In addition we plan to improve our collection of patterns . The current version of the system is being made generally available. This , we hope, will provide us with some feedback on patterns and errors in the data files . Some further experiments are also planned with the AutoLearn system . The main drawback with the system is that it doesn't make maximal use of the training data in so far that with small training samples one word may be sufficient to make a decision . This situation can probably b e improved by replacing specific words with a NULL word . This will force he system to develo p rules based more on context . In particular when the system encounters unknown words these will be considered equivalent to the NULL word. We also intend to apply the learning method described here to other NLP tasks such as part o f speech tagging and disambiguation . Reference s [1] Cowie, J .,Guthrie, L., Pustejovsky, J ., Waterman, S ., and Wakao, T., The CRL/Bradneis System us Used for MUC-5 In Proceedings of the Fifth Message Understanding Conference (MUC-5) Baltimore, Ma ., Morgan Kaufmann, 1993 . [2] Quinlan, J .R . Discovering Rules by Induction from Large Collections of Examples .In Expert Systems in the Micro-Electronic Age, ed Michie D ., Edinburgh University Press, 1979 . [3] Quinlan, J .R . Machine Learning : Easily Understood Decision Rules .In Computer Systems that Learn, eds . Weiss S .M . and Kulikowski C .A ., Morgan Kaufmann, 1991 . 162 Appendix","paragraphs":["A - Basic System Scores -------------------------- --------------- ------------ ----------------------- - SLOT \\t POS ACT' COR PAR INC! SPU MIS NONI REC PRE UND OVG ERR SU B <enamex> \\t 925 \\t 8891 841 \\t 0 \\t 01 48 84 \\t 01 91 95 \\t 9 \\t 5 14 \\t 0 type \\t 925 \\t 8891 784 \\t 0 \\t 571 48 84 \\t 01 85 88 \\t 9 \\t 5 19 \\t 7 text \\t 925 \\t 8891 755 \\t 0 \\t 861 48 84 \\t 01 82 85 \\t 9 \\t 5 22 1 0 subtotals \\t 1850 17781 1539 \\t 0 1431 96 168 \\t 01 83 86 \\t 9 \\t 5 21 \\t 8 <timex> \\t 111 \\t 1081 102 \\t 0 \\t 01 \\t 6 \\t 9 \\t 01 92 94 \\t 8 \\t 6 13 \\t 0 type \\t 111 \\t 1081 102 \\t 0 \\t 01 \\t 6 \\t 9 \\t 01 92 94 \\t 8 \\t 6 13 \\t 0 text \\t 111 \\t 1081 \\t 92 \\t 0 \\t 101 \\t 6 \\t 9 \\t 01 83 85 \\t 8 \\t 6 21 1 0 subtotals \\t 222 \\t 2161 194 \\t 0 \\t 101 12 18 \\t 01 87 90 \\t 8 \\t 6 17 \\t 5 <numex> \\t 93 \\t 1021 \\t 91 \\t 0 \\t 01 11 \\t 2 \\t 01 98 89 \\t 2 11 12 \\t 0 type \\t 93 \\t 1021 \\t 91 \\t 0 \\t 01 11 \\t 2 \\t 01 98 89 \\t 2 11 12 \\t 0 text \\t 93 \\t 1021 \\t 88 \\t 0 \\t 31 11 \\t 2 \\t 01 95 86 \\t 2 11 15 \\t 3 subtotals \\t 186 \\t 2041 179 \\t 0 \\t 31 22 \\t 4 \\t 01 96 88 \\t 2 11 14 \\t 2 ALL OBJECTS \\t 2258 21981 1912 \\t 0 1561 130 190 \\t 01 85 87 \\t 8 \\t 6 20 \\t 8 MATCHED ONLY \\t 2068 20681 1912 \\t 0 1561 \\t 0 \\t 0 \\t 01 92 92 \\t 0 \\t 0 \\t 8 \\t","8 -------------------------- --------------- ------------ ----------------------- - P&R \\t 2P&R \\t P&2 R F-MEASURES \\t 85 .82 \\t 86 .52 \\t 85 .1 3 SLOT POS ACTI COR PAR INC' SPU MIS NON' REC PRE UND OVG ERR SUB -------------------------- --------------- ------------ ----------------------- - Enamex : organization \\t 442 \\t 3921 330 \\t 0 \\t 401 22 72 \\t 01 75 84 16 \\t 6 29 1 1 person \\t 373 \\t 3781 353 \\t 0 \\t 91 16 11 \\t 01 95 93 \\t 3 \\t 4 \\t 9 \\t 2 location \\t 110 \\t 1191 101 \\t 0 \\t 81 10 \\t 1 \\t 01 92 85 \\t 1 \\t 8 16 \\t 7 other \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * \\t * Timex : date \\t 111 \\t 1081 102 \\t 0 \\t 01 \\t 6 \\t 9 \\t 01 92 94 \\t 8 \\t 6 13 \\t 0 time \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * \\t * other \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * \\t * Numex : money \\t 76 \\t 771 \\t 74 \\t 0 \\t 01 \\t 3 \\t 2 \\t 01 97 96 \\t 3 \\t 4 \\t 6 \\t 0 percent \\t 17 \\t 251 \\t 17 \\t 0 \\t 01 \\t 8 \\t 0 \\t 01 100 68 \\t 0 32 32 \\t 0 other \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * \\t * * * * DOCUMENT SECTION SCORES * * * SLOT \\t POS ACTT COR PAR INCI SPU MIS NONI REC PRE UND OVG ERR SU B HL \\t 136 \\t 1301 115 \\t 0 \\t 111 \\t 4 10 \\t 01 84 88 \\t 7 \\t 3 18 \\t 9 DD \\t 60 \\t 601 \\t 60 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 100 100 \\t 0 \\t 0 \\t 0 \\t 0 DATELINE \\t 52 \\t 501 \\t 49 \\t 0 \\t 11 \\t 0 \\t 2 \\t 01 94 98 \\t 4 \\t 0 \\t 6 \\t 2 TXT \\t 2010 19581 1688 \\t 0 1441 126 178 \\t 01 84 86 \\t 9 \\t 6 21 \\t 8 163 Appendix A - Autolearn System Score s SLOT POS ACT' COR PAR INCI SPU MIS NON' REC PRE UND OVG ERR SUB <enamex> 915 5141 469 0 01 \\t 45 446 01 51 91 49 9 51 0 type 915 5141 445 0 241 \\t 45 446 01 49 86 49 9 54 5 text 915 5141 371 0 981 \\t 45 446 01 40 72 49 9 61 2 1 subtotals 1830 10281 816 0 1221 \\t 90 892 01 44 79 49 9 58 1 3 <timex> 111 651 59 0 01 \\t 6 52 01 53 91 47 9 50 0 type 111 651 59 0 01 \\t 6 52 01 53 91 47 9 50 0 text 111 651 48 0 111 \\t 6 52 01 43 74 47 9 59 1 9 subtotals 222 1301 107 0 111 \\t 12 104 01 48 82 47 9 54 9 <numex> 93 721 65 0 01 \\t 7 28 01 70 90 30 10 35 0 type 93 721 65 0 01 \\t 7 28 01 70 90 30 10 35 0 text 93 721 63 0 21 \\t 7 28 01 68 88 30 10 37 3 subtotals 186 1441 128 0 21 \\t 14 56 01 69 89 30 10 36 2 ALL OBJECTS 2238 13021 1051 0 1351116 1052 01 47 81 47 9 55 1 1 MATCHED ONLY 1186 11861 1051 0 1351 \\t","0 0 01 89 89 0 0 11 11 -------------------------- --------------- ------------ ----------------------- - \\t P&R \\t 2P&R \\t P&2 R F-MEASURES \\t 59 .38 \\t 70 .57 \\t","51 .2 5 -------------------------- --------------- ------------ ----------------------- - SLOT \\t POS ACTI COR PAR INC' SPU MIS NON' REC PRE UND OVG ERR SUB Enamex : organization \\t 433 \\t 2871 251 \\t 0 \\t 81 28 174 \\t 01 58 87 40 10 46 \\t 3 person \\t 372 \\t 1551 131 \\t 0 \\t 141 10 227 \\t 01 35 84 61 \\t 6 66 1 0 location \\t 110 \\t 721 \\t 63 \\t 0 \\t 21 \\t 7 45 \\t 01 57 88 41 10 46 \\t 3 other \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * \\t * Timex : date \\t 111 \\t 651 \\t 59 \\t 0 \\t 01 \\t 6 52 \\t 01 53 91 47 \\t 9 50 \\t 0 time \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * \\t * other \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * \\t * Numex : money \\t 76 \\t 551 \\t 52 \\t 0 \\t 01 \\t 3 24 \\t 01 68 94 32 \\t 5 34 \\t 0 percent \\t 17 \\t 171 \\t 13 \\t 0 \\t 01 \\t 4 \\t 4 \\t 01 76 76 24 24 38 \\t 0 other \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * * * * DOCUMENT SECTION SCORES * * * SLOT \\t POS ACTI COR PAR INCI SPU MIS NON' REC PRE UND OVG ERR SUB HL \\t 128 \\t 921 \\t 65 \\t 0 \\t 0 71 20 56 \\t 01 51 71 44 22 56 1 0 DD \\t 60 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 60 \\t 01 \\t 0 \\t * 100 \\t * 100 \\t * DATELINE \\t 52 \\t 241 \\t 24 \\t 0 \\t 01 \\t 0 28 \\t 01 46 100 54 \\t 0 54 \\t 0 TXT \\t 1998 11861 962 \\t 0 1281 96 908 \\t 01 48 81 45 \\t 8 54 1 2 16 4"]},{"title":"Appendix A - Basic System Walk-through Score s","paragraphs":["SLOT \\t POS \\t ACTI \\t COR \\t PAR \\t","INCI SPU MIS NONI REC PRE UND OVG ERR SUB -------------------------- --------------- ------------ ----------------------- - <enamex> 66 521 47 0 01 5 19 01 71 90 29 10 34 0 type 66 521 42 0 51 5 19 01 64 81 29 10 41 1 1 text 66 521 42 0 51 5 19 Cl 64 81 29 10 41 1 1 subtotals 132 1041 84 0 101 10 38 01 64 81 29 10 41 1 1 <timex> 6 51 5 0 01 0"]},{"title":"1","paragraphs":["01 83 100 17 0 17 0 type 6 51 5 0 01 0 1 01 83 100 17 0 17 0 text 6 51 5 0 01 0 1 01 83 100 17 0 17 0 subtotals 12 101 10 0 01 0 2 01 83 100 17 0 17 0 <numex> 6 71 6 0 01 1 0 01 100 86 0 14 14 0 type 6 71 6 0 01 1 0 01 100 86 0 14 14 0 text 6 71 6 0 01 1 0 01 100 86 0 14 14 0 subtotals 12 141 12 0 01 2 0 01 100 86 0 14 14 0 ALL OBJECTS 156 1281 106 0 101 12 40 01 68 83 26 9 37 9 MATCHED ONLY 116 1161 106 0 101 0 0 01 91 91 0 0 9 9 P&R \\t","2P& R F-MEASURES P&2R","70 .4 874 .65 79 .3 4","-------------------------- --------------- ----------- - SPU MIS NONI ----------------------- - REC PRE UND OVG ERR SUBSLOT Enamex : POS ACTT COR PAR INC I organization 29 121 5 0 51 2 \\t 19 \\t 01 17 42 66 17 84 5 0 person 35 381 35 0 01 3 \\t 0 \\t 01 100 92 0 8 8 0 location 2 21 2 0 01 0 \\t 0 \\t 01 100 100 0 0 0 0 other 0 01 0 0 01 0 \\t 0 \\t 01 * * * * * * Timex : date 6 51 5 0 01 0 \\t 1 \\t 01 83 100 17 0 17 0 time 0 o l 0 0 o l 0 \\t 0 \\t o l * * * * * * other 0 OI 0 0 O1 0 \\t 0 \\t OI * * * * * * Numex : money 5 61 5 0 01 1 \\t 0 \\t 01 100 83 0 17 17 0 percent 1 11 1 0 01 0 \\t 0 \\t 01 100 100 0 0 0 0 other 0 01 0 0 01 0 \\t 0 \\t 01 * * * * * *"]},{"title":"-------------------------- --------------- ------------ ----------------------- -","paragraphs":["* * * DOCUMENT SECTION SCORES * * * SLOT \\t POS ACTI COR PAR INCI SPU MIS NONI REC PRE UND OVG ERR SU B HL \\t 8 \\t 61 \\t 6 \\t 0 \\t 01 \\t 0 \\t 2 \\t 01 75 100 25 \\t 0 25 \\t 0 DD \\t 2 \\t 21 \\t 2 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 100 100 \\t 0 \\t 0 \\t 0 \\t 0 DATELINE \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t 0 \\t 0 \\t 01 \\t * \\t * \\t * \\t * \\t * TXT \\t 146 \\t 1201 \\t 98 \\t 0 \\t 101 12 38 \\t 01 67 82 26 10 38 \\t 9"]},{"title":"16 5 Appendix A - Autolearn System Walk-through Score s","paragraphs":["SLOT \\t","POS ACT' COR PAR INC' SPU MIS NON' REC PRE UND OVG ERR SUB \\t+\\t+\\t+\\t <enamex> 69 221 20 0 01 2 49 01 29 91 71 9 72 0 type 69 221 20 0 01 2 49 01 29 91 71 9 72 0 text 69 221 18 0 21 2 49 01 26 82 71 9 75 1 0 subtotals 138 441 38 0 21 4 98 01 28 86 71 9 73 5 <timex> 6 51 5 0 01 0 1 01 83 100 17 0 17 0 type 6 51 5 0 01 0 1 01 83 100 17 0 17 0 text 6 51 4 0 11 0 1 01 67 80 17 0 33 2 0 subtotals 12 101 9 0 11 0 2 01 75 90 17 0 25 1 0 <numex> 6 61 6 0 01 0 0 01 100 100 0 0 0 0 type 6 61 6 0 01 0 0 01 100 100 0 0 0 0 text 6 61 5 0 11 0 0 01 83 83 0 0 17 1 7 subtotals 12 121 11 0 11 0 0 01 92 92 0 0 8 8 \\t+\\t+\\t+\\t ALL OBJECTS \\t 162 \\t 661 \\t 58 \\t 0 \\t 41 \\t 4 100 \\t 01 36 88 62 \\t 6 65 \\t 6 MATCHED ONLY \\t 62 \\t 621 \\t 58 \\t 0 \\t 41 \\t 0 \\t 0 \\t 01 94 94 \\t 0 \\t 0 \\t 6 \\t","6","\\t+\\t+\\t+\\t P&R \\t 2P&R \\t P&2 R F-MEASURES \\t 50 .88 \\t 68 .08 \\t","40 .6 2","\\t+","\\t","+","\\t","+","\\t","SLOT \\t POS ACT' COR PAR INC' \\t","SPU MIS NONI REC PRE UND OVG ERR SUB \\t+\\t+\\t+\\t Enamex : organization 32 121 12 0 01 0 20 01 38 100 62 0 62 \\t 0 person 35 71 6 0 01 1 29 01 17 86 83 14 83 \\t 0 location 2 31 2 0 01 1 0 01 100 67 0 33 33 \\t","0 other 0 01 0 0 01 0 0 01 * * * * * \\t+\\t+\\t+ Timex : date 6 51 5 0 01 0 1 01 83 100 17 0 17 \\t 0 time 0 01 0 0 01 0 0 01 * * * * * \\t","* other 0 01 0 0 01 0 0 01 * * * * \\t+\\t+\\t+ Numex : money 5 51 5 0 01 0 0 01 100 100 0 0 0 \\t 0 percent 1 11 1 0 01 0 0 01 100 100 0 0 0 \\t 0 other 0 01 0 0 01 0 0 01 * * * * * \\t","*","\\t+\\t+\\t+\\t","* * * DOCUMENT SECTION SCORES * * *","\\t+","\\t","+","\\t","+","\\t","SLOT \\t","POS ACT' COR PAR INC' SPU MIS NON' REC PRE UND OVG ERR SU B \\t+\\t+\\t+\\t HL 8 21 2 0 01 0 6 01 25 100 75 0 75 \\t 0 DD 2 01 0 0 01 0 2 01 0 * 100 * 100 \\t * DATELINE 0 01 0 0 01 0 0 01 * * * * * TXT 152 641 56 0 41 4 92 01 37 88 60 6 64 \\t 7 16 6"]}]}