{"sections":[{"title":"","paragraphs":["Proceedings of Recent Advances in Natural Language Processing, pages 640–647, Hissar, Bulgaria, 12-14 September 2011."]},{"title":"Linear Transduction Grammars and Zipper Finite-State Transducers Markus Saers and Dekai Wu Human Language Technology Center Dept. of Computer Science and Engineering Hong Kong University of Science and Technology Hong Kong {masaers|dekai}@cs.ust.hk Abstract","paragraphs":["We examine how the recently explored class of linear transductions relates to finite-state models. Linear transductions have been neglected historically, but gainined recent interest in statistical machine translation modeling, due to empirical studies demonstrating that their attractive balance of generative capacity and complexity characteristics lead to improved accuracy and speed in learning alignment and translation models. Such work has until now characterized the class of linear transductions in terms of either (a) linear inversion transduction grammars (LITGs) which are linearized restrictions of inversion transduction grammars or (b) linear transduction grammars (LTGs) which are bilingualized generalizations of linear grammars. In this paper, we offer a new alternative characterization of linear transductions, as relating four finite-state languages to each other. We introduce the devices of zipper finite-state automata (ZFSAs) and zipper finite-state transducers (ZFSTs) in order to construct the bridge between linear transductions and finite-state models."]},{"title":"1 Introduction","paragraphs":["Linear transductions are a long overlooked class of transductions positioned between finite-state transductions and inversion transductions in terms of complexity. In the Aho–Ullman hierarchy, linear transductions are those that can be generated by SDTGs1","of rank 1, but that is about all that is said about them.","1","Syntax-directed transduction grammars or SDTGs (Lewis and Stearns, 1968; Aho and Ullman, 1972) have also been referred to recently in the statistical machine translation subcommunity as synchronous context-free grammars.","Recently, however, linear transduction grammars (LTGs) have been shown to be both effective and efficient for learning word alignments in statistical machine translation models (Saers et al., 2010b; Saers et al., 2010a). LTGs can align words more accurately than FSTs since they allow words to be reordered, and yet alignment and training complexity is two orders of magnitude lower than with ITGs (Wu, 1997).","The added efficiency means that LTGs can be learned directly from parallel corpora rather than relying on external word alignment tools for a priori annotation. The added efficiency does, how-ever, come at a price in expressivity, and it is vital to understand the nature of this trade-off. The automaton/transducer perspective of linear transductions described in this paper offers another vector towards understanding the properties of linear transductions.","Thus far, such work has not characterized the class of linear transductions in terms of finite-state models. Saers et al. (2010b) define linear transductions in terms of linear inversion transduction grammars (LITGs), which are inversion transduction grammars with a linear restriction. Alternatively, Saers et al. (2010a) introduce linear transduction grammars (LTGs), which are the natural bilingual generalization of linear grammars, and show that they define the same class of linear transductions as LITGs.","In this paper, we offer a new alternative characterization of linear transductions based on finite-state models. We will start by giving a definition of LTGs, as the principal mechanism for generating linear transductions (Section 2). As an intermediate step, we will note that linear languages (which are related by linear transductions) can be handled by FSTs under some conditions: we treat linear languages as two finite-state languages dependent on each other, by introducing the device of zipper finite-state automata (Section 3). We then general-640 G = ⟨{S, F } , Σ, ∆, S, R⟩ such that Σ = {b, l, sandwich, t, -} , ∆ = {bacon, bread, lettuce, mayonnaise, tomato} , R =    S → ε/bread F sandwich/bread, F → b/ε F ε/bacon, F → l/lettuce F, F → t/tomato F, F → -/ε, F → -/mayonnaise    (a) Linear transduction grammar","S =⇒ G ε/bread F sandwich/bread =⇒ G b/bread F sandwich/bacon bread =⇒ G b l/bread lettuce F sandwich/bacon bread =⇒ G b l t/bread lettuce tomato F sandwich/bacon bread =⇒ G b l t - sandwich/bread lettuce tomato mayonnaise bacon bread (b) Generation Figure 1: A linear transduction grammar (a) generating a bistring (b). The transduction defined establishes the concept “BLT-sandwich” and the ordered components of its realization: bacon, lettuce and tomato (with optional mayonnaise) sandwiched between two slices of bread. ize this to introduce zipper finite-state transducers, treating linear transductions as relating two linear languages to each other (Section 4). Since linear languages relate two finite-state languages to each other, and linear transductions relate two linear languages to each other, linear transductions can be said to relate four finite-state languages to each other."]},{"title":"2 Linear transduction grammars","paragraphs":["A linear transduction grammar (LTG) is an inversion transduction grammar (ITG) or syntaxdirected transduction grammar (SDTG), of rank 1, which means that any rule may produce at most one nonterminal, eliminating any branching. Figure 1 contains an example of an LTG, and how it generates a bistring. Definition 1 A linear transduction grammar (LTG) over languages L1 and L2 is a tuple: G = ⟨N, Σ, ∆, S, R⟩ where N is a finite nonempty set of nonterminal symbols, Σ is a finite nonempty set of L1 symbols, ∆ is a finite nonempty set of L2 symbols, S ∈ N is the designated start symbol and R is a finite nonempty set of production rules on the forms:","A → a /x B b","/y","A → a /x","where the nonterminals A, B ∈ N and the biter-","minals a","/x , b","/y ∈ Σ∗","× ∆∗",". Definition 2 The rules in an LTG G = ⟨N, Σ, ∆, S, R⟩ define a binary relation =⇒","G","over","(Σ∗","× ∆∗",") (N ∪ (Σ∗ × ∆∗",")) (Σ∗","× ∆∗",") such","that:","a","/w A d","/z =⇒","G","ab","/wx B cd /yz iff A → b","/x B c","/y ∈ R","a","/w A d","/z =⇒","G","abd /wxz iff A → b","/x ∈ R","Note that both the biterminal expressions a /w b /x and ab","/wx can designate the translation between the terminal strings ab and wx. The reflexive transitive closure of this relation can be used to define the transduction generated by an LTG as the set of bistrings that can be generated from the grammar’s start symbol. 641 Definition 3 The transduction generated by the LTG G = ⟨N, Σ, ∆, S, R⟩ is: T (G) = {","⟨a, x⟩∣ ∣ ∣ ∣S ∗ =⇒ G a /x }","∩ (Σ∗ × ∆∗",") Even though no normal form is given in Aho and Ullman (1972) for LTGs or SDTGs of rank 1, it is useful to have such a normal form. In this work we will adopt the following normal form for LTGs. Definition 4 An LTG in normal form is an LTG where the rules are constrained to have one of the forms:","A → a","/x′ B b′ /y′ A → a′","/x B b′","/y′","A → a′","/x′ B b /y′ A → a′","/x′ B b′","/y","A → ε /ε","where A, B ∈ N , a, b ∈ Σ, a′",", b′","∈ Σ ∪ {ε},","x, y ∈ ∆ and x′",", y′","∈ ∆ ∪ {ε}. That is: only rules where at least one terminal symbol is produced together with a nonterminal symbol, and rules where the empty bistring is produced, are allowed. The “primed” symbols are allowed to be the empty string, whereas the others are not. It is possible to construct an LTG in normal form from an arbitrary LTG in the same way that a linear grammar (LG) is normalized. Theorem 1. Grammars of type LTG and type LTG in normal form generate the same class of transductions. Proof. Given an LTG G = ⟨N, Σ, ∆, S, R⟩, we can construct an LTG in normal form G′","= ⟨N ′",", Σ, ∆, S, R′","⟩ that generates the same language. For every rule in R we can produce a series of corresponding rules in R′",". We start by remov-ing useless nonterminals, rules where one nonterminal rewrites into another nonterminal only. This can be done in the same way as for SDTGs, see Aho and Ullman (1972). The rules can then be recursively shortened until they are in normal form.","If the rule A → a","/x B c","/z is not in normal form, it can be rephrased as two rules:","A → a /x 1 B̄ c","/z m","B̄ → a","/x 2 . . . a /x n B c","/z 1 . . . c","/z m−1 where B̄ is a newly created unique nonterminal, n is the length of the biterminal a","/x and m is the length of the biterminal c","/z . The first rule is in normal form by definition. The second rule can be subjected to the same procedure until it is in normal form. Having either a","/x or c","/z be empty does not affect the results of the procedure, and since we started by eliminating useless rules, one of them is guaranteed to be nonempty.","If the rule A → b","/y is not in normal form (mean-","ing that b /y is nonempty), it can be replaced by two rules:","A → b /y 1 B̄","B̄ → b /y 2 . . . b","/y n where B̄ is a newly created unique nonterminal, and n is the length of the biterminal b","/y . The set of nonterminals N ′","is the old set N in union with the set of all nonterminals that were created when R′ was constructed.","Whenever there is a production in G such that:","a","/w A d","/z =⇒","G ab /wx B cd","/yz or","a","/x A c","/z =⇒","G","abc /xyz There is, by construction, a sequence of produc-tions in G′","such that: a /w A d","/z ∗ =⇒ G′ ab /wx B cd","/yz or a /x A c","/z ∗ =⇒ G′","abc /xyz This means that G′","is capable of generating any string that G can generate, giving us the inequality:","L(G) ⊆ L(G′ )","Since the normal form constitutes a restriction, we also know that:","L(G′ ) ⊆ L(G) Which leads us to conclude that:","L(G) = L(G′ ) For statistical machine translation applications, LTGs can be made weighted or stochastic (Saers et al., 2010b; Saers et al., 2010a) in the same way as ITGs Wu (1997). 642"]},{"title":"3 Linear languages revisited","paragraphs":["In this section we will take a look at the connec-tion between linear languages (LLs) and FSTs, and leverage the relationship to define a new type of automaton to handle LLs. The new class of automata is referred to as zipper finite-state automata (ZFSAs), which will replace one-turn pushdown automata (Ginsburg and Spanier, 1966, 1-PDAs) and nondeterministic two-tape automata (Rosenberg, 1967, 2-NDAs) as the principal machine for handling linear languages. This is mainly to facilitate the move into the bilingual domain, and offers nothing substantially new.","Ginsburg and Spanier (1966, Theorem 6.1) show that a linear language can be seen as the input to an FST concatenated with the reverse of its output. Rosenberg (1967, Theorems 9 and 10) shows that any linear grammar can be said to generate the concatenation of the first tape from a 2-NDA with the reverse of the second. Instead of giving the original theorems, we will give two lemmas in the spirit of the previous works. Lemma 2. For every one-restricted finite-state transducer (1-FST) M there is an LG in normal form that generates the language {ab←","|⟨a, b⟩ ∈ T (M )}.2 Proof. Given that M = ⟨Q, Σ, ∆, q0, F, δ⟩ is a 1-FST, we can construct an LG in normal form G = ⟨Q, Σ ∪ ∆, q0, R⟩ where R =","{","q → a q′ b∣ ∣⟨q, a, b, q′","⟩ ∈ δ} ∪ {q → ε|q ∈ F } where q, q′","∈ Q, a ∈ Σ ∪ {ε} and b ∈ ∆ ∪ {ε}. Whenever there is a transition sequence with M such that: ⟨q0, a, b⟩ = ⟨q0, a1 . . . an, b1 . . . bn⟩ ⊢M ⟨q1, a2 . . . an, b2 . . . bn⟩ ⊢∗ M ⟨qn−1, an, bn⟩ ⊢M ⟨qn, ε⟩","where qi ∈ Q, ai ∈ Σ ∪ {ε}, a ∈ Σ∗",", bi ∈ ∆ ∪ {ε}","and b ∈ ∆∗","for all i, and where the state q","n is a member of F , there is, by construction, a deriva-2 Where b←","is used to mean the reverse of b. tion with G such that:","q0 =⇒ G a1q1b1 ∗ =⇒ G a1 . . . anqnbn . . . b1 =⇒ G a1 . . . anbn . . . b1 = ab← Thus: whenever the bistring ⟨a, b⟩ is a member of T (M ), the string ab←","is a member of L(G). By construction, G cannot generate any other strings. We thus conclude that","L(G) = {ab← |⟨a, b⟩ ∈ T (M )} Lemma 3. For every LG in normal form (G), there exists a 1-FST (M ) such that, for all string s ∈ L(G), there exists a partition s = ab←","such that ⟨a, b⟩ ∈ T (M ). Proof. Given that G = ⟨N, Σ, S, R⟩ is an LG in normal form, we can construct a 1-FST M = ⟨N, Σ, Σ, S, F, δ⟩ where: F = {A|A → ε ∈ R} , δ = {⟨A, a, b, B⟩|A → a B b ∈ R} where A, B ∈ N and a, b ∈ Σ ∪ {ε}. Whenever there is a derivation with G such that:","S =⇒ G a1X1b1 ∗ =⇒ G a1 . . . anXnbn . . . b1 =⇒ G a1 . . . anbn . . . b1 = ab← there is, by definition, a sequence of transitions with M such that: ⟨S, a, b⟩ = ⟨S, a1 . . . an, b1 . . . bn⟩ ⊢M ⟨X1, a2 . . . an, b2 . . . bn⟩ ⊢∗ M ⟨Xn−1, an, bn⟩ ⊢M ⟨Xn, ε, ε⟩","(where qi ∈ Q, ai ∈ Σ∪{ε}, a ∈ Σ∗ , bi ∈ ∆∪{ε} and b ∈ ∆∗","for all i) and the state Xn is by definition a member of F . Thus: whenever G generates ab←",", M can recognize ⟨a, b⟩. By construction, M cannot recognize any other bistrings. We thus conclude that","T (M ) = {⟨a, b⟩|ab← ∈ L(R)} 643 Figure 2: A zipper finite-state automata relates the two parts of a string to each other, and define the partitioning of the string at the same time. There is a discrepancy between FSTs and linear languages in that every string in the language has to be partitioned into two strings before the FST can process them. Naturally, the number of ways to partition a string is proportional to its length. Naı̈vely trying all possible partitions would take O(n3",") time (O(n) partitions and O(n2",") time to run the FST on each string pair), which is equal to CFGs. If linear languages are as time-consuming to process as CFLs, we might as well use the more expressive language class. If, however, the partition point could be found as a part of the analysis process rather than conjectured a priori, the process would be faster than CFGs.","It is possible to reinterpret the transition relation defined by an FST such that it reads from both tapes, rather than reads from one and writes to the other. We define this relation as:","⟨q, aα, βb⟩ ⊢M,r ⟨q′ , α, β⟩ iff ⟨q, a, b, q′","⟩ ∈ δ","where q, q′","∈ Q, a ∈ Σ, b ∈ ∆, α ∈ Σ∗","and","β ∈ ∆∗",". Using this interpretation of the FST","M (designated M ′","under this reinterpretation) we","have that:","⟨α, β← ⟩ ∈ T (M ) iff ⟨α, β⟩ ∈ T (M ′",") which, by lemmas 2 and 3, means that the concatenation of α and β over the entire transduction constitutes a linear language. This is the intuition behind zipper finite-state automata. By construct-ing a string γ ∈ (Σ ∪ ∆)∗","such that γ = αβ, we can rewrite the reinterpreted FST relation as:","⟨q, aγb⟩ ⊢M′",",r ⟨q′",", γ⟩ iff ⟨q, a, b, q′","⟩ ∈ δ which define a linear language over (Σ ∪ ∆)∗",". The partitioning of the string is also implicitly defined since the automaton will end up somewhere in the original string, defining the place of partitioning that makes the two parts related (or concluding that they are not, and that the string is not a member of the language defined by the automaton). The attribute “zipper” comes from the visualization, where the control of the automaton slides down two ends of the tape until it reaches the bottom after having drawn all connections between the two parts of the tape—like a zipper (see Figure 2). Again, this is merely a reinterpretation of previous work. The idea of a dedicated automaton to process a single tape containing strings from a linear language with finite control (as opposed to using a stack as the 1-PDAs do, or partitioning the tapes as 2-NDAs strictly speaking have to do) is not new. Nagy (2008) presents 5′","→ 3′","sensing Watson-Crick finite automata which are used to process DNA strings, and Loukanova (2007) presents nondeterministic finite automata to handle linear languages. Our reinterpretation is made to facilitate the transition into the bilingual do-main. Definition 5 A zipper finite-state automaton (ZFSA) is a tuple: M = ⟨Q, Σ, q0, F, δ⟩ where Q is a finite nonempty set of states, Σ is a finite set of symbols, q0 ∈ Q is the start state, F ⊆ Q is a set of accepting states and δ ⊆ Q×Σ∗","×Σ∗","×Q is a finite set of transitions. Transitions define a binary relation over Q × Σ∗ such that:","⟨q, αγβ⟩ ⊢M ⟨q′ , γ⟩ iff ⟨q, α, β, q′","⟩ ∈ δ","where q, q′ ∈ Q and α, β, γ ∈ Σ∗",". Lemma 4. Every FST can be expressed as a ZFSA. Proof. Let M = ⟨Q, Σ, ∆, q0, F, δ⟩ be an FST, and let M ′","= ⟨Q, Σ ∪ ∆, q0, F, δ⟩ be the corresponding ZFSA. The only differences are that M ′ uses the union of the two alphabets that M transduces between, and that the interpretation of the relation defined by δ is different in M and M ′",". Lemma 5. Every ZFSA can be expressed as an FST. Proof. Let M = ⟨Q, Σ, q0, F, δ⟩ be a ZFSA, and let M ′","= ⟨Q, Σ, Σ, q0, F, δ⟩ be the corresponding FST transducing within the same alphabet. The only differences are that M ′","uses two copies of 644 M = ⟨Q, Σ, ∆, qS, { q′} , δ⟩ such that Q = { qS, qF , q′} , Σ = {b, l, sandwich, t, -} , ∆ = {bacon, bread, lettuce, mayonnaise, tomato} , δ =    ⟨qS, ε, bread, sandwich, bread, qF ⟩, ⟨qF , b, ε, ε, bacon, qF ⟩, ⟨qF , l, lettuce, ε, ε, qF ⟩, ⟨qF , t, tomato, ε, ε, qF ⟩, ⟨qF , -, ε, ε, ε, q′","⟩ ⟨qF , -, mayonnaise, ε, ε, q′","⟩    (a) Zipper finite-state transducer ⟨qS, b l t - sandwich, bread lettuce tomato mayonnaise bacon bread⟩ ⊢M ⟨qF , b l t -, lettuce tomato mayonnaise bacon⟩ ⊢M ⟨qF , l t -, lettuce tomato mayonnaise⟩ ⊢M ⟨qF , t -, tomato mayonnaise⟩ ⊢M ⟨qF , -, mayonnaise⟩ ⊢M ⟨q′",", ε, ε⟩ (b) Recognition Figure 3: A zipper finite-state transducer (a) recognizing a bistring (b). This is the same bistring that was generated in Figure 1. the same alphabet (Σ), and that the interpretation of the relation defined by δ is different in M and M ′",". Theorem 6. FSTs in recognition mode are equivalent to ZFSAs. Proof. Follows from Lemmas 4 and 5. Theorem 7. The class of languages recognized by ZFSAs is the class of linear languages. Proof. From Lemmas 2 and 3 we have that FSTs generate linear languages, and from theorem 6 we have that ZFSAs are equivalent to FSTs. To recognize with a ZFSA is as complicated as recognizing with an FST, which can be done in O(n2",") time. Since we are effectively equating a transduction with a language, it is helpful to instead consider this as “finite-state in two dimensions.” For the finite-state transduction, this is easy, since it relates two finite-state languages to each other. For the linear languages it takes a little more to consider them as languages that internally relate one part of every string to the other part of that string. The key point is that they are both relating something that is in some sense finite-state to something else that is also finite-state."]},{"title":"4 Zipper finite-state transducers","paragraphs":["Having condensed a finite-state relation down to a language, we can relate two such languages to each other. This is what zipper finite-state transducers (ZFSTs) do. If linear languages relate one part of every string to the other, linear transductions relate these two parts to the two parts of all the strings in another linear language. There are in all four kinds of entities involved, ⟨a, b⟩ ∈ L1 and ⟨x, y⟩ ∈ L2, and linear transduction have to relate them all to each other. We claim that this is what LTGs do, and in this section we will see that the transducer class for linear languages, ZFSTs, is equivalent to LTGs. An example of a ZFST can be found in Figure 3. Definition 6 A ZFST over languages L1 and L2 is a tuple: M = ⟨Q, Σ, ∆, q0, F, δ⟩ where Q is a finite nonempty set of states, Σ is a finite nonempty set of L1 symbols, ∆ is a 645 finite nonempty set of L2 symbols, q0 ∈ Q is the designated start state, F ⊆ Q is a set of accepting states and:","δ ⊆ Q × Σ∗","× ∆∗ × Σ∗","× ∆∗","× Q is a finite set of transitions. The transitions define a binary relation over Q × Σ∗","× ∆∗","such that:","⟨q, abc, xyz⟩ ⊢M ⟨q′ , b, y⟩ iff ⟨q, a, x, c, z, q′","⟩ ∈ δ","where q, q′","∈ Q, a, b, c ∈ Σ∗","and x, y, z ∈ ∆∗",".","We know that ZFSTs relate linear languages to each other, they are defined to do so, and we conjecture that LTGs relate linear languages to each other. By proving that ZFSTs and LTGs handle the same class of transductions we can assert that LTGs do indeed generate a transduction relation between linear languages. Lemma 8. For every LTG there is a ZFST that recognizes the language generated by the LTG.","Proof. Let G = ⟨N, Σ, ∆, S, R⟩ be an LTG, and","let M = ⟨N ′",", Σ, ∆, S, {S′","}, δ⟩ be the correspond-","ing ZFST where S′","is a unique final state, N ′","=","N ∪ {S′ } and:","δ ={ ⟨A, a, x, c, z, B⟩","∣","∣A → a","/x B c","/z ∈ R } ∪","{","⟨A, b, y, ε, ε, S′ ⟩∣ ∣A → b","/y ∈ R }","where A, B ∈ N , a, b, c ∈ Σ∗","and x, y, z ∈ ∆∗",".","Whenever there is a derivation with G such that:","S =⇒ G a1 /x1 A1 c1","/z1 ∗ =⇒ G","a1","/x1 . . . an /xn An cn","/zn . . . c1","/z1 =⇒ G a1 /x1 . . . an","/xn b /y cn /zn . . . c1","/z1","(where S, Ai ∈ N , ai, bi ∈ Σ∗ and x","i, yi ∈ ∆∗ for all i),3","we have, by construction, a sequence of transitions in M that takes it from an initial configuration with the generated bistring to an accepting configuration: ⟨S, a1 . . . anbcn . . . c1, x1 . . . xnyzn . . . z1⟩ ⊢M ⟨A1, a2 . . . anbcn . . . c2, x2 . . . xnyzn . . . z2⟩ ⊢∗ M ⟨An, b, y⟩ ⊢M ⟨S′",", ε, ε⟩ 3 These i indices are not indicating individual symbols in","a string, but different strings. This means that M can recognize all strings generated by G. By construction, M cannot recognize any other strings. We thus conclude that T (M ) = T (G) Lemma 9. For every ZFST, there is an LTG that generates the transduction recognized by the ZFST. Proof. Let M = ⟨Q, Σ, ∆, q0, F, δ⟩ be a ZFST, and let G = ⟨Q, Σ, ∆, q0, R⟩ be the corresponding LTG where:","R ={","q → a","/x q′ b","/y","∣","∣⟨q, a, x, b, y, q′","⟩ ∈ δ}","∪ { q → ε","/ε ∣ ∣q ∈ F }","where q, q′ ∈ Q, a, b, c ∈ Σ∗","and x, y, z ∈ ∆∗",".","For every bistring that M can recognize: ⟨q0, a1 . . . anbn . . . b1, x1 . . . xnyn . . . y1⟩ ⊢M ⟨q1, a2 . . . anbn . . . b2, x2 . . . xnyn . . . y2⟩ ⊢∗ M ⟨qn, ε, ε⟩","(where qi ∈ Q, ai, bi ∈ Σ∗ and x","i, yi ∈ ∆∗ for all","i,4 and where q","n ∈ F ), we have, by construction, a derivation with G that generates that bistring:","q0 =⇒ G a1 /x1 q1 b1","/y1 ∗ =⇒ G","a1","/x1 . . . an /xn qn bn","/yn . . . b1","/y1 =⇒ G a1 /x1 . . . an","/xn bn /yn . . . b1","/y1 This means that G can generate all strings that M can recognize. By construction, G cannot generate any other strings. We thus conclude that T (G) = T (M ) Theorem 10. The class of transductions generated by LTGs is the same as that recognized by ZFSTs. Proof. Follows from lemmas 8 and 9. 4 Again, these indices do not refer to individual symbols","in a string, but different strings. 646"]},{"title":"5 Conclusion","paragraphs":["We have examined how the class of linear transductions relates to finite-state models. Our analysis complements earlier characterizations of linear transductions in terms of LITGs (linearized restrictions of inversion transduction grammars) and LTGs (bilingualized generalizations of linear grammars). Our new alternative characterization has shown how linear transductions relate four finite-state languages to each other, with the aid of the devices zipper finite-state automata and transducers."]},{"title":"Acknowledgments","paragraphs":["This work was funded by the Defense Advanced Research Projects Agency under GALE Contract Nos. HR0011-06-C-0023 and HR0011-06-C-0023, and the Hong Kong Research Grants Council (RGC) under research grants GRF621008, GRF612806, DAG03/04.EG09, RGC6256/00E, and RGC6083/99E. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. We would also like to thank the four anonymous reviewers, whose feedback made this a better paper."]},{"title":"References","paragraphs":["Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice-Halll, Englewood Cliffs, NJ.","Seymour Ginsburg and Edwin H. Spanier. 1966. Finite-turn pushdown automata. Society for Industrial and Applied Mathematics Journal on Control, 4(3):429–453.","Philip M. Lewis and Richard E. Stearns. 1968. Syntax-directed transduction. Journal of the Association for Computing Machinery, 15(3):465–488.","Roussanka Loukanova. 2007. Linear context free languages. In Cliff Jones, Zhiming Liu, and Jim Woodcock, editors, Theoretical Aspects of Computing – ICTAC 2007, volume 4711 of Lecture Notes in Computer Science, pages 351–365. Springer Berlin/Heidelberg.","Benedek Nagy. 2008. On 5′","→ 3′","sensing Watson– Crick finite automata. In Max Garzon and Hao Yan, editors, DNA Computing, volume 4848 of Lecture Notes in Computer Science, pages 256–262. Springer Berlin/Heidelberg.","Arnold L. Rosenberg. 1967. A machine realization of the linear context-free languages. Information and Control, 10:175–188.","Markus Saers, Joakim Nivre, and Dekai Wu. 2010a. A systematic comparison between inversion transduction grammar and linear transduction grammar for word alignment. In Proceedings of the 4th Workshop on Syntax and Structure in Statistical Transla-tion, pages 10–18, Beijing, China, August. Coling 2010 Organizing Committee.","Markus Saers, Joakim Nivre, and Dekai Wu. 2010b. Word alignment with stochastic bracketing linear inversion transduction grammar. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 341–344, Los Angeles, California, June. Association for Computational Linguistics.","Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403. 647"]}]}