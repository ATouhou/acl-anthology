{"sections":[{"title":"","paragraphs":["Proceedings of Recent Advances in Natural Language Processing, pages 294–301, Hissar, Bulgaria, 7-13 September 2013."]},{"title":"Realization of Common Statistical Methods in Computational Linguistics with Functional Automata Stefan Gerdjikov Faculty of Mathematics and Informatics, Sofia University 5 James Bourchier blvd., 1164 Sofia, Bulgaria st gerdjikov@ abv.bg Petar Mitankin Faculty of Mathematics and Informatics, Sofia University 5 James Bourchier blvd., 1164 Sofia, Bulgaria pmitankin@ fmi.uni-sofia.bg Vladislav Nenchev Faculty of Mathematics and Informatics, Sofia University 5 James Bourchier blvd., 1164 Sofia, Bulgaria lucifer.dev.0@ gmail.com Abstract","paragraphs":["In this paper we present the functional automata as a general framework for representation, training and exploring of various statistical models as LLM’s, HMM’s, CRF’s, etc. Our contribution is a new construction that allows the representation of the derivatives of a function given by a functional automaton. It preserves the natural representation of the functions and the standard product and sum operations of real numbers. In the same time it requires no additional overhead for the standard dynamic programming techniques that yield the computation of a functional value."]},{"title":"1 Introduction","paragraphs":["Statistical models such as n-gram language models (Chen and Goodman, 1996), hidden Markov models (Rabiner, 1989), conditional random fields (Lafferty et al., 2001), log-linear models (Darroch and Ratcliff, 1972) are widely applied in the natural language processing in order to approach various problems, e.g. parsing (Sha and Pereira, 2003), speech recognition (Juang and Rabiner, 1991), statistical machine translation (Brown et al., 1993). Different statistical models perform differently on different tasks. Thus in order to find the best practical solution one might need to try several approaches before getting the desired effect. Disposing on a general framework that allows the flexibility to change the statistical model or/and training scheme would spend much efforts and time.","Focusing on this pragmatical problem, we propose the functional automata as a possible solution. The basic idea is to consider the mathematical expressions of sums and products arising in the statistical models as regular expressions. Thus regarding the functions in these expressions as individual characters, the sums as unions and the products as concatenation, we get the desired correspondence. The relation between a particular statistical model and a functional automaton for its representation is then rather straightforward.","The training of the statistical models is in a way more involved. Most of the approaches require a gradient method that estimates the best model parameters. To this end one needs to have an efficient representation not only of the function used by the model but also of its (partial) derivatives.","To solve similar problem Eisner and Li in-troduce first-order and second-order expectation semirings. In (Jason Eisner, 2002; Zhifei Li and Jason Eisner, 2009) it is shown how derivatives of functions arising in statistical models can be represented. This is achieved by the means of an algebraic construction that: (i) considers pairs of functions (first-order expectation semiring) and quadruples of functions (second-order expectation semiring); (ii) introduces an operation on pairs and quadruples, respectively, of functions that replaces the multiplication and is used to simulate the multiplication of first- and second-order derivatives, respectively. Thus the higher the order of the derivatives in interest, the more complex would be the necessary expectation semiring and the operations that it would require.","In the current paper we propose an alternative approach. It is based on a combinatorial construction that allows preserving both: (i) manipulation with single functions and (ii) the usage of the standard multiplication and addition of real numbers. Thus we get a uniform representation of functions, their first- and higher order derivatives. Our approach requires the same storage as the approach 294 in (Jason Eisner, 2002; Zhifei Li and Jason Eisner, 2009) and enables the same efficiency for the traversal procedures described in (Zhifei Li and Jason Eisner, 2009).","In Section 3 we show that the values of a function represented by an acyclic functional automaton can be efficiently computed by the means of a standard dynamic programming technique. We further describe how to construct functional automata for the partial derivatives of F by given functional automaton representing F . We show in Sections 2 and 6 that such automata can be used for training log-linear models, hidden Markov models and conditional random fields. We only require that the objective function is represented via functional automata. In Section 5 we present a construction of functional automaton for a log-linear model where one of the feature functions uses an n-gram language model (Chen and Goodman, 1996).","In Section 7 we present evaluation of a developed system, based on functional automata, on the tasks of (i) noisy historical text normalization and (ii) OCR postcorrection."]},{"title":"2 Log-linear models","paragraphs":["We consider the task of automatic normalization of Early Modern English texts. In the next two paragraphs we define some notions related to this task. We use them afterwards to formulate typical problems of training and search that can be effectively solved by functional automata.","Given a source text s, say s = theldest sonn hath bin kild, and the goal is to find the most relevant modern English equivalent of s. A candidate generator is an algorithm that for a fixed source word or sequence of words, say sisi+1 . . . si+k, generates finite number of normalization candidates and supplies each normalization candidate, c, with a conditional probability, pcg(c | sisi+1 . . . si+k). Hence we can assume that the candidate generator provides the information in the form of Table 1. In this sense the candidate generator corresponds to the word-to-word or phrase-to-phrase translation tables in statistical machine translation systems (Koehn et al., 2003). From the candidates we construct possible normalization targets: eldest sun hat been kid, the eldest soon has bean killed, the eldest son has been killed etc. For normalization of texts produced by OCR system from noisy","source word set of target candidates theldest {⟨the eldest, 0.75⟩, ⟨eldest, 0.25⟩} sonn {⟨son, 0.92593⟩, ⟨soon, 0.03704⟩, ⟨sun, 0.03704⟩} hath {⟨hat, 0.0088⟩, ⟨hats, 0.0044⟩, ⟨has, 0.9868⟩} bin {⟨bin, 0.1⟩, ⟨been, 0.8⟩, ⟨bean, 0.1⟩} kild {⟨kid, 0.01⟩, ⟨killed, 0.99⟩} Table 1: Source words and their corresponding set of candidates provided by the candidate generator. Each target candidate c for the source word si is associated with a probability pcg(c | si). historical documents the candidate generator could take into account both typical OCR errors and historical spelling variations, (Reffle, 2011) or can use directly automatically extracted spelling variations, for example (Gerdjikov et al., 2013).","A normalization pair is a pair p = ⟨w, c⟩ such that the sequence of target words c is a normalization candidate for the sequence of source words w. We call w left side and c right side of the normalization pair p. The left and the right sides of p are denoted l(p) and r(p) respectively. In our example some of the normalization pairs are ⟨theldest, eldest⟩, ⟨theldest, theeldest⟩, ⟨kild, killed⟩, etc. A normalization alignment from s to t, denoted s → t, is a sequence of normalization pairs p1p2 . . . pk such that s = l(p1)l(p2) . . . l(pk) and t = r(p1)r(p2) . . . r(pk). The i-th normalization pair pi of the alignment s → t is denoted (s → t)i. The length k of the alignment is denoted |s → t|. Thus a possible normalization alignment in our example, from s = theldest sonn hath bin kild to t = eldest sun hat been kid is ⟨theldest, eldest⟩ ⟨sonn, sun⟩⟨hath, hat⟩⟨bin, been⟩⟨kild, kid⟩. We denote with As the set of all normalization alignments from s. Note that As is always finite, because the number of normalization candidates for each sequence sisi+1 . . . si+k of source words is finite.","Problem. Given a training corpus of normalization alignments train a log-linear model that combines the candidate generator with an n-gram statistical language model. Once the model is trained, find a best normalization alignment s → t for a given source s.","Firstly, we consider the case where n = 1, i.e. we have a monogram language model which assigns a nonzero probability plm(ti) to each target word ti. The general case of arbitrary n-gram language model is postponed 295 to Section 5. There are two feature functions: hlm(s → t) = log","∏|t| i=1 plm(ti) and hcg(s → t) = log ∏|s→t|","i=1 pcg[r((s → t)i) | l((s → t)i)]. The probability of a normalization alignment s → t given s is pλ(s → t | s) = exp[λlmhlm(s → t) + λcghcg(s → t)]","∑","s→t′ ∈As exp[λlmhlm(s → t′",") + λcghcg(s → t′",")] , where λ = ⟨λlm, λcg⟩ are the parameters of the model. Training. Assume that we have a training corpus T of N normalization alignments, T = ⟨s(1)","→ t(1)",", s(2)","→ t(2)",", . . . , s(N)","→ t(N)","⟩. The training task is to find parameters λ̂ that optimize the joint probability over the training corpus, λ̂ = argmaxλ ∏N","n=1 pλ(s(n)","→ t(n)","| s(n)",").","Search. Once the parameters λ̂ are fixed, the","problem is to find a best normalization alignment","s → t = argmaxs→t′","∈As pλ̂(s → t′",") for a given","input s.","Introducing es→t(λ) = exp[λlmhlm(s → t) +","λcghcg(s → t)] and Zs(λ) = ∑","s→t′ ∈As es→t′ (λ), (1) we obtain λ̂ = argmaxλ L(λ), where L(λ) =","∑N","n=1[λlmhlm(s(n)","→ t(n)",")+","λcghcg(s(n) → t(n)",") − log Zs(n)(λ)]. (2) To optimize L(λ) we use a gradient method that requires the computation of L(λ), ∂L","∂λcg (λ) and ∂L ∂λlm (λ) by given λ. For i = lm, cg we obtain ∂L ∂λi (λ) = N ∑","n=1[hi(s(n) → t(n)",") − ∂Z s(n) ∂λi (λ) Zs(n)(λ) ].","(3) One possible choice of first order gradient method for the optimization of L is a variant of the conjugate gradient method that converges to the unique maximum of L for each starting point λ0 = ⟨λlm0, λcg0⟩, (Gilbert and Nocedal, 1992)."]},{"title":"3 Functional automata","paragraphs":["The problem we faced in the previous Sec-","tion is how to compute L(λ) and ∂L","∂λi (λ) at a","given point λ. The computation of the terms","λihi(s(n)","→ t(n)",") for i = cg (or i = lm) is","easy since it requires a single multiplication and","|s(n)","→ t(n)","| (or |t(n)","|) additions. However the λ2 1","0 1 2 3","exp(λ1λ3","2) cos(λ2)","sin(λ1) 1 λ2 1+1","Figure 1: Functional automaton representing","the function F (λ1, λ2) = λ2 1 sin(λ1) 1","λ2","1+1","+ λ2","1 cos(λ2) 1 λ2 1+1 + exp(λ1λ3","2) sin(λ1) 1","λ2","1+1 +","exp(λ1λ3","2) cos(λ2) 1","λ2","1+1 term Zs(λ) may require much more efforts. It suffices that each source word si generates two candidates for the expression in Equation 1 to explode in exponential number of summation terms. Computing the derivatives then becomes even harder. In this Section we present a novel efficient solution to these problems. It is based on a compact representation of the mathematical expressions via functional automata.","Imagine, that we have the function F (λ1, λ2) given as an expression: λ2","1 sin(λ1) 1","λ2","1+1 +","λ2","1 cos(λ2) 1 λ2 1+1 + exp(λ1λ3","2) sin(λ1) 1","λ2","1+1 +","exp(λ1λ3","2) cos(λ2) 1","λ2","1+1 . Let us further assume","that we interpret the individual functions λ2","1, cos(λ2), 1","λ2","1+1 , etc, as single symbols. If we further interpret the multiplication of functions as concatenation and the addition as union, then the expression for F (λ1, λ2) given above can be viewed as a regular expression for which a finite state automaton can be compiled, see Figure 1. This is the motivation for the following two definitions: Definition 3.1 Let d be a positive natural number. Functional automaton is a quadruple A = ⟨Q, q0, ∆, T ⟩, where Q is a finite set of states, q0 ∈ Q is a start state, ∆ is a finite multiset of transitions of the form q","W","−→ p where p, q ∈ Q","are states and W : Rd → R is a function and","T ⊆ Q is a set of final states. Definition 3.2 Let A = ⟨Q, q0, ∆, T ⟩ be an acyclic functional automaton (AFA). A path π from p0 to pk in A is a sequence of k ≥ 0 transitions π = p0 W1 −→ p1 W2 −→ p2 . . . pk−1 Wk −→ pk. The label of π is defined as lπ =","∏k","j=1 Wj. If π is empty (k = 0), then lπ = 1. A successful path is a path from q0 to a final state q ∈ T . The function FA : Rd","→ R represented by A is defined as FA =","∑","π is a successful path in A lπ.","Since A is acyclic, the number of successful","paths is finite and FA is well defined. 296 target word the eldest son soon sun probability 0.017 0.00002 0.0003 0.0005 0.0002 target word hat hats has bin probability 0.0001 0.00002 0.002 0.000005 target word been bean kid killed probability 0.003 0.000005 0.00002 0.0001 Table 2: Target words and their language model probabilities.","Classical constructions for union and concatenation of automata (Hopcroft and Ullman, 1979) can be adapted for functional automata. If A is the result of the union (concatenation) of A1 and A2, then FA = FA1 + FA2 (FA = FA1 · FA2).","3.1 Computation of a function FA represented by an AFA A In order to efficiently compute FA(λ) for a given λ = ⟨λ1, λ2, . . . , λn⟩, we use standard dynamic programming. Without loss of generality we assume that A = ⟨Q, q0, ∆, T ⟩ has only one final state and each transition in A belongs to some successful path. Firstly, we sort topologically the states of the automaton A in decreasing order. Let p1, p2, . . . , p|Q| be one such order of the states, i.e. (i) p1 ∈ T is the only one final state, (ii) p|Q| = q0 is the start state and (iii) if there is a transition from pi to pj then j < i. For example for the automaton on Figure 1 we obtain 3, 2, 1, 0. Afterwards for each state pj we compute a value vj in the following way: v1 = 1 and vj+1 =","∑ pj+1 W(λ) −→ pk W (λ) · vk. Eventually FA(λ) = v|Q|. If the computation of W (λ) by given λ takes time O(1) for all label functions W , then the time for the computation of FA(λ) is O(|∆|).","Now we focus on the problem how to compute Zs(λ) at a given point λ, see Equation 1. We illustrate how Zs(λ) can be represented by an AFA, As, on the example from Section 2, s = theldest sonn hath bin kild. Table 1 lists the sets of candidates in modern English for each source word si. Table 2 presents the language model probabilities for each target word. Given this data we represent the possible normalization alignments via an acyclic two-tape automaton, see Figure 2. This automaton can be considered as a string-to-weight transducer (Mohri, 1997) parameterized with λlm and λcg. Specifically, each path from state i − 1 to state i, 1 ≤ i ≤ |s|, corresponds to a target candi-the/","exp[λlm log(0.017)","+λcg log(0.75)] eldest/","exp[λlm log(0.00002)","+λcg log(0.25)]","eldest/","exp[λlm log(0.00002) +λcg log(1)] 0 6 1","son/ exp[λlm log(0.0003) +λcg log(0.92593)]","soon/ exp[λlm log(0.0005) +λcg log(0.03704)]","sun/ exp[λlm log(0.0002) +λcg log(0.03704)] 2","hat/ exp[λlm log(0.0001) +λcg log(0.0088)]","hats/ exp[λlm log(0.00002) +λcg log(0.0044)]","has/ exp[λlm log(0.002) +λcg log(0.9868)] 3","bin/","exp[λlm log(0.000005) +λcg log(0.1)]","been/ exp[λlm log(0.003) +λcg log(0.8)]","bean/","exp[λlm log(0.000005) +λcg log(0.1)] 4","kid/","exp[λlm log(0.00002) +λcg log(0.01)]","killed/ exp[λlm log(0.0001) +λcg log(0.99)] 5 Figure 2: The functional automaton Atheldest sonn hath bin kild is obtained by removing the words from the transition labels. date c for the i-th source word si and has a label exp[λcglog(pcg(c | si)) + λlmlog(plm(c))]. On our example, for i ≥ 2 each such path consists of a single transition, because the candidates are single words. In order to represent the candidate the eldest we use the additional state 6. The transition from 0 to 6 corresponds to the first word the of the candidate and accumulates the whole probability pcg(the eldest | theldest) = 0.75. The transition from 6 to 1 corresponds to the second word eldest of the candidate. It should be clear that removing the target words from the transitions, we obtain the AFA As representing Zs(λ). For each alignment s(n)","→ t(n)","from the training corpus we build a separate functional automaton, like the one on Figure 2, representing Zs(n)(λ). Thus we have N automata that we use to compute L(λ) via Equation (2). 3.2 Computation of partial derivates via AFA Our next goal is to compute the partial derivates ∂L ∂λi (λ). Let us turn back to the function F (λ1, λ2) represented by the automaton on Figure 1. We show how to construct a functional automaton for 297 λ2 1","0 1 2 3","exp(λ1λ3","2) cos(λ2)","sin(λ1) 1 λ2 1+1","0′","1′","2′","3′ 2λ1 λ3 2 exp(λ1λ3","2) cos(λ1) 0 − 2λ1 (λ2 1+1)2 λ2 1 sin(λ1) cos(λ2) 1 λ2 1+1","exp(λ1λ3 2) Figure 3: A functional automaton for the partial derivative of F (λ1, λ2).","∂F","∂λ1 (λ1, λ2). Let G(λ1, λ2) = λ2","1 sin(λ1) 1","λ2","1+1 be","the first of the four summation terms of F . The","partial derivative ∂G","∂λ1 can be written as a sum of three terms:","∂(λ2","1)","∂λ1 sin(λ1) 1","λ2","1+1 , λ2","1","∂(sin(λ1)) ∂λ1 1 λ2 1+1","and λ2 1 sin(λ1)","∂( 1","λ2","1+1 )","∂λ1 . Each of the summation terms differs from the original expression for G(λ1, λ2) in exactly one multiplier whose partial derivative with respect to λ1 is computed. Thus in order to construct a functional automaton for ∂F ∂λ1 we can take two disjoint copies of the original functional automaton, see Figure 3, and set transitions between them in order to reflect the partial derivatives with respect to λ1 of the single multipliers. The general result is presented in the following Proposition:","Proposition 3.3 Let A be an AFA with k states","and t transitions and let A′","= ⟨Q′ , q′","0, ∆′",", T ′","⟩ be","a disjoint copy of A. If the partial derivatives ∂W","∂λi exist for each transition q","W (λ1,λ2,...,λd) −→ p in A,","then B = ⟨Q ∪ Q′ , q0, ∆ ∪ ∆′","∪ {q ∂W ∂λ","i → p′","| q W","→","p ∈ ∆}, T ′","⟩ is an AFA with 2k states, 3t transi-","tions and FB = ∂FA","∂λi . Sketch of proof. We have ∂FA ∂λi =","∑ π is a successful path in A ∂lπ ∂λi = ∑ π = q0 W1 −→ q1 . . . qm−1 Wm −→ qm","is a successful path in A","∑ j π(j,i), where π(j,i) = W1 . . . Wj−1","∂Wj","∂λi Wj+1 . . . Wm. There is a one-to-one correspondence between the successful paths in B and the terms π(j,i) in the above summation.","Let us note that the construction presented in Proposition 3.3 can be iterated i times in order to build a functional automaton with 2i","k states and 3i","t transitions for each i-th order partial derivate of FA. Thus we can build functional automata with 4k states and 9t transitions for ∂2","FA","∂λiλj . This gives the possibility to use some second order gradient method in the training procedure. Note that if the computation of W (λ) for a given λ and all label functions, W , takes constant time, then using functional automata we achieve an O(t)-time computation of both ∂FA","∂λi (λ) and ∂2","FA","∂λiλj (λ)."]},{"title":"4 Search procedure","paragraphs":["By given source sequence s we want to find best alignment s → t = argmaxs→t′","∈As pλ̂(s → t′",") = argmaxs→t′","∈As es→t′(λ̂). For this purpose we use again a standard dynamic programming procedure on the automaton As representing the function Zs(λ), Figure 2. The only difference with the procedure described in Subsection 3.1 is that instead of summation over all transtions from the current state we need to take maximum and to mark a transition that gives the maximum. Finally the successful path of marked transitions represents a best alignment. Actually this procedure corresponds to the backward version of the Viterbi decoding algorithm (Omura, 1967). If the computation of W (λ) by given λ takes time O(1) for all label functions W , then the search procedure is linear in the number of the transitions in the functional automaton."]},{"title":"5 n-gram language models","paragraphs":["In this Section we generalize the constructions of the automaton As from Section 3 and 4 to the case of an arbitrary n-gram language model, n > 1. In this case hlm(s → t) = log","∏|t|","i=1 plm(ti | ti−n+1ti−n+2 . . . ti−1). We construct an automaton representing Zs(λ) as follows. Firstly, we build automaton A1 that represents the function Zs(⟨0, λcg⟩) =","∑","s→t′","∈As exp[λcghcg(s → t′",")]. Each transition in A1 is associated with a target word, see Figure 2. Now we would like to add exp[λlm log(plm(ti | ti−n+1ti−n+2 . . . ti−1))] to the label of each transition associated with ti. However the problem is that there may be multiple sequences of preceding words ti−n+1ti−n+2 . . . ti−1 for one and the same transition. For example for n = 3 on Figure 2 for the transition associated with ti = has from state 2 to state 3 there are three different possible pairs of preceding words ti−2ti−1: eldest son, eldest 298 soon and eldest sun. We overcome this problem of ambiguity by extending A1 = ⟨Q1, q1, ∆1, T1⟩ to equivalent automaton A2 in which for each state the sequence of n − 1 preceding words is uniquely determined. The set of states of A2 is Q2 = {⟨w1w2 . . . wn−1, q⟩ | q ∈ Q1 and w1w2 . . . wn−1 is a sequence of preceding words for q in A1}. The set of transitions of A2 is ∆2 = {⟨w1w2 . . . wn−1, q′","⟩ W","→ ⟨w2 . . . wn−1wn, q′′","⟩ |","transition q′ W → q′′","∈ ∆","1 is associated with wn}. In A2 the transition ⟨w1w2 . . . wn−1, q′","⟩ W","→ ⟨w2 . . . wn−1wn, q′′","⟩ is associated with the word wn. Finally, from A2 we construct functional automaton A3 that represents Zs(⟨λlm, λcg⟩) by adding exp[λlm log(plm(wn | w1w2 . . . wn−1))] to the label of each transition t where wn is the word associated with t.","If m is an upper bound for the number of correction candidates for every sequence sisi+1 . . . si+k, then |Q2| ≤ mn−1","|Q1| and |∆2| ≤ mn−1","|∆1|."]},{"title":"6 Other statistical models","paragraphs":["In this section we apply the technique developed in Sections 3 and 4 to other statistical models.","Conditional random fields. A linear-chain CRF serves to assign a label yi to each the observation xi of a given observation sequence x. We assume that the observations xi belong to a set X and the labels yi belong to a finite set Y . We shall further consider that the probability measure of a linear-chain CRF with |x| states is pλ(y | x) = exp[","∑|x| i=2","∑K j=1 αjfj(yi−1, yi, x, i) +","∑|x| i=1","∑K j=1 βjgj(yi, x, i)] Zx(λ) where |x| = |y|, fj : Y × Y × X∗","× N → R and gj : X∗","× N → R are predefined feature functions, λ = ⟨α1, α2, . . . , αK , β1, β2, . . . , βK ⟩ are parameters and Zx(λ) = ∑","y∈Y |x| exp[∑|x|","i=2","∑K j=1 αjfj(yi−1, yi, x, i)+","∑|x| i=1","∑K j=1 βjgj(yi, x, i)]. The training task","is similar to the one described in Sec-","tion 2. We have a training corpus of N","pairs ⟨x(1)",", y(1)","⟩, ⟨x(2)",", y(2)","⟩, . . . , ⟨x(N)",", y(N)","⟩","and we need to find the parameters","λ̂ = argmaxλ ∏N n=1 pλ(y(n)","| x(n)","). Formulae","very similar to (2) and (3) can be derived. Thus","the main problem is again in the computation of the term Zx(λ). In (Lafferty et al., 2001) Zx(λ) is represented as an entity of a special matrix which is obtained as a product of |x| + 1 matrices of size (|Y | + 2) × (|Y | + 2). The states of an AFA Ax representing Zx(λ) are as follows: a start state s, a final state f and |x| · |Y | “intermediate” states qi,γ, 1 ≤ i ≤ |x|, γ ∈ Y . The transitions are s G → q1,γ for G = exp","∑K j=1 βjgj(γ, x, 1), qi,γ′ F → qi+1,γ′′ for F = exp ∑K j=1[αjfj(γ′",", γ′′",", x, i + 1)+","βjgj(γ′′",", x, i + 1)] and q |x|,γ","1","→ f . Transitions with label 0 can be removed from the automaton. If there are many such transitions this could significantly reduce the time for training.","Hidden Markov models. We adapt the nota-tions and the definitions from (Rabiner, 1989). Let λ = ⟨A, B, π⟩ be the parameters of a HMM with R states S = {S1, S2, . . . , SR} and M distinct observation symbols V = {v1, v2, . . . , vM }, where A = {aSiSj } is a R × R matrix of transition probabilities, B = {bSj (vk)} are the observation symbol probability distributions and π = {πSj } is the initial state distribution. The probability of O1O2 . . . OT is pλ(O1O2 . . . OT ) =∑","q1q2...qT ∈ST c(q1q2 . . . qT ), where c(q1q2 . . . qT ) = πq1bq1(O1)aq1q2bq2(O2) . . . aqT−1qT bqT (OT ).","Given a training set of N observations O(1)",", O(2)",", . . . , O(N)","the optimal parameters λ̂ = argmaxλ ∏N n=1 pλ(O(n)",") have to be determined","under the stohastic constraints","∑ j aSiSj = 1,","∑ k bSj (vk) = 1 and","∑","j πSj = 1. Applying","the method of Lagrange multipliers we obtain a","new function F (λ, α, β, γ) =","∏N n=1 pλ(O(n)",")+","∑ i αi[(∑","j aSiSj ) − 1] +","∑ i βi[(∑","k bSj (vk)) −","1]+ γ[(∑","j πSj ) − 1]. For each training observa-","tion sequence O(n)","with T (n)","symbols the function","pλ(O(n) ) can be represented by an AFA AO(n)","with RT (n)","+ 2 states, R(T (n)","+ 1) transitions","and a single final state as follows. We have the","start state s, the final state f and RT (n)","“in-","termediate” states qt,Si, 1 ≤ t ≤ T (n)",", 1 ≤ i ≤ R. The transitions are s","πS ibS i(O(n)","1 ) −→ q1,Si, qt,Si","aS","iS j bS","j (O(n)","t+1) −→ qt+1,Sj and qT (n)",",Si","1","→ f . The","concatenation of all N automata AO(n) gives","one automaton representing","∏N","n=1 pλ(O(n)","). The union of two automata representing functions F1 and F2 gives an automaton for the function F1 + F2. So using unions and concatenations we obtain one AFA (with a single final state) representing the function F (λ, α, β, γ). We can directly construct 299 functional automata for the partial derivatives of F (first order and if needed second order), see Proposition 3.3. Thus we can use a gradient method to find a local extremum of F ."]},{"title":"7 Evaluation","paragraphs":["In this section we evaluate the quality of a noisy text normalization system that uses the log-linear model presented in Section 2. The system uses a globally convergent variant of the conjugate gradient method, (Gilbert and Nocedal, 1992). The computation of the gradient and the values of the objective function is implemented with functional automata. We test the system on two tasks: (i) OCR-postcorrection of the TREC-5 Confusion Track corpus1","and (ii) normalization of the 1641 Depositions2","- a collection of highly non-standard 17th century documents in Early Modern English, (Sweetnam, 2011), digitized at the Trinity College Dublin.","For the task (i) we use a parallel corpus of 30000 training pairs (s, t), where s is a document produced by an OCR system and t is the corrected variant of s. The 30000 pairs were randomly selected from the TREC-5 corpus that has about 5% error on character level. We use 25000 pairs as a training set and the remaining 5000 pairs serve as a test set. With a heruistic dynamic programming algorithm we automatically converted all these 25000 pairs (s, t) into normalization alignments s → t, see Section 2. We use these alignments to train (a) a candidate generator, (b) smoothed 2-gram language model, to find (c) statistics for the length of the left side of a normalization pair and (d) statistics for normalization pairs with equal left and right sides. Our log-linear model has four feature functions induced by (a), (b), (c) and (d). As a candidate generator we use a variant of the algorithm presented in (Gerdjikov et al., 2013). The word error (WER) rate between s and t in the test set of 5000 pairs is 22.10% and the BLEU (Papineni et al., 2002) is 58.44%. In Table 3 we compare the performace of our log-linear model with four feature functions against a baseline where we use only one feature function, which encodes the candidate generator. Table 3 shows that the combination of the four features reduces more than twice the WER. Precision and recall, obtained on the TREC 5 dataset, for different candidate gener-1 http://trec.nist.gov/pubs/trec5/t5 proceedings.html 2 http://1641.tcd.ie Log-linear model WER BLEU","only candidate generator 6.81% 85.24%","candidate generator + language model","3.27% 92.82% + other features Table 3: Only candidate generator vs. candidate generator + other features. OCR-postcorrection of the TREC-5 corpus. ators can be found in (Mihov et al., 2007; Schulz et al., 2007; Gerdjikov et al., 2013). To test our system on the task of normalization of the 1641 Depositions, we use a corpus of 500 manually created normalization alignments s → t, where s is a document in Early Modern English from the 1641 Depositions and t is the normalization of s in contemporary English. We train our system on 450 documents and test it on the other 50. We use five feature functions: (b), (c) and (d) as above and two language models: (a1) one 2-gram language model trained on part of the normalized training documents and (a2) another 2-gram language model trained on large corpus of documents extracted from the entire Gutenberg English language corpus3",". We obtain WER 5.37% and BLEU 89.34%."]},{"title":"8 Conclusion","paragraphs":["In this paper we considered a general framework for the realization of statistical models. We showed a novel construction proving that the class of functional automata is closed under taking partial derivatives. Thus the functional automata yield efficient training and search procedures using only the usual sum and product operations on real numbers.","We illustrated the power of this mechanism in the cases of CRF’s and HMM’s, LLM’s and n-gram language models. Similar constructions can be applied for the realization of other methods, for example MERT (Och, 2003).","We presented a noisy text normalization system based on functional automata and evaluated its quality."]},{"title":"Acknowledgments","paragraphs":["The research work reported in the paper is supported by the CULTURA project, grant 269973, funded by the FP7 Programme (STReP) and Project BG051PO001-3.3.06-0022/19.03.2012. 3 http://www.gutenberg.org 300"]},{"title":"References","paragraphs":["Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer, 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.","Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL ’96, 310–318.","J. N. Darroch and D. Ratcliff. 1972. Generalized it-erative scaling for log-linear models. The Annals of Mathematical Statistics, 43:1470–1480.","Jason Eisner. 2002. Parameter Estimation for Probabilistic Finite-State Transducers. Proceedings of the 40th annual meeting on Association for Computational Linguistics ACL ’02, 1–8.","Stefan Gerdjikov, Stoyan Mihov, and Vladislav Nenchev. 2013. Extraction of spelling varia-tions from language structure for noisy text correction. Proceedings of the International Conference on Document Analysis and Recognition","Jean Charles Gilbert and Jorge Nocedal. 1992. Global Convergence Properties of Conjugate Gra-dient Methods for Optimization. SIAM Journal on Optimization, 2(1):21–42.","John E. Hopcroft and Jeffrey D. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley Publishing Company.","B. H. Juang and L. R. Rabiner. 1991. Hidden Markov Models for Speech Recognition. Technometrics, 33(3):251–272.","Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, 48–54","John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Label-ing Sequence Data. Proceedings of the Eighteenth International Conference on Machine Learn-ing, ICML ’01, 282–289.","Zhifei Li and Jason Eisner 2009. First- and second-order expectation semirings with applications to minimum-risk training on translation forests. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, 40–51.","S. Mihov, P. Mitankin, A. Gotscharek, U. Reffle, C. Schulz, and K. U. Ringlstetter. 2007. Using automated error profiling of texts for improved selection of correction candidates for garbled tokens. AI 2007: Advances in Artificial Intelligence, 456-465.","Mehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23(2): 269–311.","Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, 160–167.","J. Omura. 1967. On the Viterbi decoding algorithm. IEEE Transactions on Information Theory, 13(2):260–269.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311–318.","Lawrence Rabiner. 1989. A tutorial on HMM and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.","Ulrich Reffle. 2011. Efficiently generating correction suggestions for garbled tokens of historical language. Natural Language Engineering, 17(02):265– 282.","K. U. Schulz, S. Mihov, and P. Mitankin, 2007. Fast selection of small and precise candidate sets from dictionaries for text correction tasks, Proceedings of the International Conference on Document Analysis and Recognition 471-475.","Fei Sha and Fernando Pereira, 2003. Shallow parsing with conditional random fields, Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, 134–141.","Mark S. Sweetnam and Barbara A. Fennell. 2011. Natural language processing and early-modern dirty data: applying IBM Languageware to the 1641 depositions. Literary and Linguistic Computing, 27(1):39–54 301"]}]}