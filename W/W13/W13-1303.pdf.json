{"sections":[{"title":"","paragraphs":["Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ’13), pages 20–28, Atlanta, Georgia, 14 June 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Learning Hierarchical Linguistic Descriptions of Visual Datasets Roni Mittelman","paragraphs":["†"]},{"title":", Min Sun","paragraphs":["‡"]},{"title":", Benjamin Kuipers","paragraphs":["†"]},{"title":", Silvio Savarese","paragraphs":["†"]},{"title":"† Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor ‡ Department of Computer Science and Engineering, University of Washington, Seatle rmittelm,kuipers,silvio@umich.edu, sunmin@cs.washington.edu Abstract","paragraphs":["We propose a method to learn succinct hierarchical linguistic descriptions of visual datasets, which allow for improved navigation efficiency in image collections. Classic exploratory data analysis methods, such as agglomerative hierarchical clustering, only provide a means of obtaining a tree-structured partitioning of the data. This requires the user to go through the images first, in order to reveal the semantic relationship between the different nodes. On the other hand, in this work we propose to learn a hierarchy of linguistic descriptions, referred to as attributes, which allows for a textual description of the semantic content that is captured by the hierarchy. Our approach is based on a generative model, which relates the attribute descriptions associated with each node, and the node assignments of the data instances, in a probabilistic fashion. We furthermore use a nonparametric Bayesian prior, known as the tree-structured stick breaking process, which allows for the structure of the tree to be learned in an unsupervised fashion. We also propose appropriate performance measures, and demonstrate superior performance compared to other hierarchical clustering algorithms."]},{"title":"1 Introduction","paragraphs":["With the abundance of images available both for personal use and in large internet based datasets, such as Flickr and Google Image Search, hierarchies of images are an important tool that allows for convenient browsing and efficient search and retrieval. In-tuitively, desirable hierarchies should capture similarity in a semantic space, i.e. nearby nodes should include categories which are semantically more similar, as compared to nodes which are more distant. Recent works that are concerned with learning image hierarchies (Bart et al., 2011; Sivic et al., 2008), have relied on a bag of visual-words feature space, and therefore have been shown to provide unsatisfactory results with respect to the latter requirement (Li et al., 2010).","A recent trend in visual recognition systems, has been to shift from using a low-level feature based representation to an attribute based feature space, which can capture higher level semantics (Farhadi et al., 2009; Lampert et al., 2009; Parikh & Grauman, 2011; Berg et al., 2011; Ferrari & Zisserman, 2007). Attributes are detectors that are trained using annotation data, to identify particular properties in an instance image. By evaluating these detectors on a query image, one can obtain a linguistic description of the image. Therefore, learning a visual hierarchy based on an attribute representation can allow for an improved semantic grouping, as compared to the previous use of low-level image features.","In this work we wish to utilize an attribute based representation to learn a hierarchical linguistic description of a visual dataset, in which few attributes are associated with each node of the tree. As is illustrated in Figure 1, such an attribute hierarchy is tightly related to a category hierarchy, in which the instances associated with every node are described using all the attributes associated with all the nodes along the path leading up to the root node (the instances in Figure 1 are described by the corresponding photographs). This “duality” between the at-20"]},{"title":"!\"#$%&'()*+#$!'##$,'-.#//$01-2+34$","paragraphs":["%5(6#$ 7#\"(.5#/$ 8(29-:;$$ <#'-9=2<>(.$ /+'*.+*'#$ ,#9<5;$:\"##5$ ?#+<5$8--9$ 8\"##5$ • 1\"<55#2@#/A$B..5*/(-2;$C-(/=$<&'()*+#$/.-'#/3$ Figure 1: Attribute and category hierarchies. tribute and category hierarchies, offers an important advantage when characterizing the dataset to the end-user, since it eliminates the need to visually in-spect the images assigned to each node, in order to reveal the semantic relationship between the categories that are associated with the different nodes. Exploratory data analysis methods for learning hierarchies, such as agglomerative hierarchical clustering (AHC) (Jain & Dubes, 1988 p. 59), only assign instances to different nodes in the tree, whereas our approach learns an attribute hierarchy which is used to assign the instances to the nodes of the tree. The attribute hierarchy provides a linguistic description of a category hierarchy.","We develop a generative model, which we refer to as the attribute tree process (ATP), and which ties together the attribute and category hierarchies in a probabilistic fashion. The tree structure is learned by incorporating a nonparametric Bayesian prior, known as the tree-structured stick breaking process (TSSBP) (Adams et al., 2010), in the probabilistic formulation. An important observation which we make about the attribute hierarchies which are learned using the ATP, is that attributes which are related to more image instances tend to be associated with nodes which are closer to the root, and vice versa, attributes which are associated with fewer instances tend to be associated with leaf nodes. A hierarchical clustering algorithm that is based on the TSSBP for binary feature vectors was developed in (Adams et al., 2010), and is known as the factored Bernoulli likelihood model (FBLM). However, similarly to AHC, it does not produce the attribute hierarchy in which we are interested.","In order to evaluate the ATP quantitatively, we compare its performance to other hierarchical clustering algorithms. If the ground truth of the category hierarchy is available, we propose to use the semantic distance between the categories, that is given by the ground truth hierarchy, to evaluate the degree to which the semantic distance between the categories is preserved by the hierarchical clustering algorithm. If the ground truth is not available, we use two criteria, which as we argue, capture the properties that should be demonstrated by desirable semantic hierarchies. The first is the “purity criterion” (Manning et al., 2009 p. 357) which measures the degree to which each node is occupied by instances from a single class, and the second is the “locality criterion” which we propose, and which measures the degree to which instances from the same class are assigned to nearby nodes in the hierarchy. Our experimental results show that when compared to AHC and FBLM, our approach captures the ground truth semantic distance between the categories more accurately, and without significant dependence on hyper-parameters.","The remaining of this paper is organized as follows. In Sec. 2 we provide background on agglomerative hierarchical clustering, and on the TSSBP, and in Sec. 3 we develop the generative model for the ATP. In Sec. 4 we propose evaluation metrics for the attribute hierarchy, and in Sec. 5 we present the experimental results. Sec. 6 concludes this paper."]},{"title":"2 Background 2.1 Agglomerative hierarchical clustering","paragraphs":["AHC uses a bottom up approach to clustering. In the first iteration, each cluster includes a single instance of the dataset, and at each following iteration, the two clusters which are closest to each other are joined into a single cluster. This requires a distance metric, which measures the distance between clusters, to be defined. The algorithm concludes when the distance between the farthest clusters is smaller than some threshold. 2.2 Tree structured stick breaking process The TSSBP is an infinite mixture model, where each mixture component has one-to-one correspondence with one of the nodes of an infinitely branching and infinitely deep tree. The weights of the infinite mixture model are generated by interleaving two stick-breaking processes (Teh et al., 2006), which allows the number of mixture components to be inferred 21 zi xi N !!0 !1 !2 !3 !1!1 !1!2 !2!1 !3!1 !3!2 (a) TSSBP zi yi N !!0 !1 !2 !3 !1!1 !1!2 !2!1 !3!1 !3!2 ! xi (b) ATP Figure 2: The graphical model representations of the probability distribution functions for the (a) TSSBP, and (b) ATP (ours). The parameter θε is associated with node ε in the tree, and T denotes the parameters {πε}ε∈T from the TSSBP construction, where T is the set of all the nodes in the tree. from the data in a Bayesian fashion. Since each mixture component is associated with a unique node in the infinite tree, this is equivalent to inferring the structure of the tree from the data. Let T denote the infinite set of node indices, and let πε denote the corresponding weight of the mixture component associated with node ε ∈ T , then one can sample a node z in the tree using z ∼ ∑ ε∈T πεδε(z), (1) where δε() denotes a Dirac delta function at ε.","Since the cardinality of the set T is unbounded, sampling from (1) is not trivial, however, an efficient sampling scheme was presented in (Adams et al., 2010). Similarly, an efficient scheme for sampling from the posterior of {πε}ε∈T given the node assignments of all the data instances, was also developed in (Adams et al., 2010). 2.2.1 Factored Bernoulli likelihood model","The FBLM was used in (Adams et al., 2010) to perform hierarchical clustering of color images using binary feature vectors. Let xi ∈ {0, 1}D",", i = 1, . . . , N , denote a set of binary training vectors that are available for learning the hierarchy. The graphical model representation of the probability distribution function is shown in Figure 2a, where for the sake of clarity of the exposition, the tree that is shown here is finite. The parameters θε = [θ (1) ε , . . . , θ (D) ε ]T","satisfy θ(d) ε = θ (d) Pa(ε) + n(d)","ε , d = 1, . . . , D, (2) where n (d) ε ∼ N (0, σ2","), and Pa(ε) denotes the par-","ent of node ε. The indicator variable z is sampled","using (1), and the likelihood of a binary observation","vector x = [x(1) , . . . , x(D)","]T","follows a Bernoulli","distribution whose parameter is a logistic function f (x|θz) = D ∏","d=1(1+ exp {−θ(d) z })−x(d)","× (1 + exp {θ(d) z })−(1−x(d)",") . (3)"]},{"title":"3 The attribute tree process","paragraphs":["In this section, we develop a new generative model that is based on the TSSBP, however unlike the FBLM it also reveals a hierarchy of attributes, which allows for a linguistic description of the image dataset. This is achieved by relating the attribute hierarchy, and the assignment of image instances to nodes, in a probabilistic manner. 3.1 The attribute hierarchy In order to allow for a probabilistic description of the attribute hierarchy, we associate a parameter vector θ = [θ (1) ε , . . . , θ (D) ε ]T","with each node ε ∈ T , where","D denotes the number of attributes. The attributes","y (d) i , d = 1, . . . , D that are associated with a data instance i that is assigned to node ε, are generated using the following scheme:","1. For each ε′","∈ A(ε), draw ξ(d)","ε′",",i ∼ Bernoulli(θ (d) ε′ ), d = 1, . . . , D, 2. Set y (d) i =","⊕ ε′ ∈A(ε) ξ (d) ε′ ,i, d = 1, . . . , D, where ξ (d) ε′ ,i is an auxiliary random variable, ⊕ denotes the logical or operation, and A(ε) denotes the set composed of all the ancestors of node ε. By marginalizing with respect to ξ (d) ε′ ,i,","we obtain a simplified representation: first set 22 h(d) ε = 1 −","∏ ε′ ∈A(ε)(1 − θ (d) ε′ ), and then sample y (d) i ∼ Bernoulli(h(d)","ε ) for every d = 1, . . . , D.","We use the parameters h(d)","ε to define the attribute hierarchy, since they represent the probability of an attribute being associated with an instance assigned to node ε. Furthermore, they satisfy the property that the likelihood of any attribute can only increase when moving deeper into the tree. We can obtain an attribute hierarchy, similar to that in Figure 1, by thresholding h","(d)","ε , and only displaying attributes that have not been detected at any ancestor node.","In order to complete our probabilistic formulation for the attribute hierarchy, we need to specify the prior for the node parameters θε. We use a finite approximation to a hierarchical Beta process (Thibaux & Jordan, 2007; Paisley & Carin, 2009). This choice promotes sparsity, and therefore only few attributes will be associated with each node. Specifically, the parameters at the root node follow θ (d) 0 ∼ Beta(a/D, b(D − 1)/D), d = 1, . . . , D,","(4) and the parameters in the other nodes follow θ(d) ε ∼ Beta(c(d)","θ(d)","Pa(ε), c(d)","(1−θ(d)","Pa(ε))), d = 1, . . . , D,","(5) where Pa(ε) denotes the parent of node ε, and where a, b, and c(d)",", d = 1, . . . , D are positive scalar parameters.","In this work we used a uniform prior for the precision hyper-parameter c(d)","∼ U [l, u] with l = 20, and u = 100. We also used the hyper-parameter values a = 10, and b = 5 (unless otherwise stated). In Section 5.1.1 we demonstrate that the performance of our algorithm depends only weakly on the choice of these parameters. 3.2 Assigning images to nodes In order to assign every image instance to one of the nodes, we combine the attribute hierarchy with the TSSBP. The resulting graphical model representation of the probability distribution function is shown in Figure 2b. For every data instance i, a node zi in the tree is sampled from the TSSBP. The observed attribute vector xi is obtained by sampling y (d) i ∼ Bernoulli(h(d)","zi ), and flipping y","(d)","i with prob-","ability ω, which models the effect of the noisy at-","tribute detectors. By marginalizing over y (d) i , we have that p(x","(d)","i = 1|−) = 1 − ((1 − h(d)","zi )(1 − ω(d)",").","+ h(d) zi ω(d)","). (6) The prior for ω is ω ∼ Beta(ρ0, ρ1), where in this work we used ρ0 = 5, and ρ1 = 20, which promotes smaller values for ω. Our algorithm is highly insensitive to the choice of ρ0, ρ1, as long as they are chosen to promote small values of ω. 3.3 Inference Inference in the ATP is based on Gibbs sampling scheme. In order to sample from the posterior of the node parameter θ (d) ε , we note that","p(θ(d) ε |−) ∝ ∏ ε′ ∈ε∪D(ε)","(1 − ((1 − h(d)","ε′ )(1 − ω(d)",") + h(d)","ε′ ω(d)","))n(1,d)","ε′","× ((1 − h(d)","ε′ )(1 − ω(d)",") + h(d)","ε′ ω(d)",")n(0,d)","ε′ × ∏","ε′′ ∈Ch(ε) Beta(θ","(d)","ε′′ ; c(d) θ(d) ε , c(d)","(1 − θ(d)","ε ))","× Beta(θ(d)","ε ; a(d)","ε , b(d)","ε ), (7) where n (j,d) ε =","∑ i|zi=ε δj(x","(d)","i ) for j = 0, 1, D(ε) denotes the set composed of all the descendants of node ε, Ch(ε) denotes the child nodes of node ε, and a(d) ε = a/D, b(d)","ε = b(D − 1)/D, for ε = 0 (the root node), and for any other node: a","(d)","ε = c(d)","θ(d)","ε , b(d) ε = c(d)","(1 − θ(d)","ε ). The expression in (7) is a highly complicated function of θ","(d)","ε , and therefore we use slice-sampling (Neal, 2000) in order to sample from the posterior. The slice-sampler is very much a “black-box” algorithm, which only requires the log likelihood of (7) and very few parameters, and returns a sample from the posterior. We sample the node parameters using a two-pass approach, starting from the leaf nodes and moving up to the root, and subsequently moving down the tree from the root to the leaves.","In order to sample from ω, we first sample the binary random variables y (d) i using p(y (d) i = j|−) ∝ p(y (d) i = j|−)(δj(x (d) i )(1 − ω(d)",")","+ δ1−j(x(d) i )ω(d)","), j = 0, 1, (8) 23 and then sample ω using","ω(d) |− ∼Beta(","ρ0 + N ∑ i=1 δ1(y (d) i xor x (d) i ), ρ1 + N ∑ i=1 δ0(y (d) i xor x (d) i ))",". (9) Sampling from the posterior of the hyper-parameter c(d)","was also performed using slice sampling. We note that slice sampling each of the parameters θ","(d)","ε for d = 1, . . . , D, and each of c(d)","for d = 1, . . . , D, can be implemented in a parallel fashion. Therefore, the computational bottleneck in the ATP is the number of nodes in the tree, rather than the number of attributes. Sampling from the posterior of the TSSBP parameters is performed using the algorithms developed in (Adams et al., 2010). The parameters of the stick-breaking processes involved in the TSSBP construction are also learned from the data using slice-sampling, by assuming a uniform prior on some interval (as was also performed in (Adams et al., 2010))."]},{"title":"4 Evaluating the attribute hierarchy","paragraphs":["In order to quantify the performance of the attribute hierarchies, we evaluate the performance of the ATP as a hierarchical clustering algorithm. We consider two cases, in the first, the ground truth category hierarchy is available and can be used to compare different hierarchies quantitatively. In the second case, the ground truth is unavailable. 4.1 Using the ground truth category hierarchy The category hierarchy should capture the distance between the categories in a semantic space. For instance, since car and bus are both vehicles, they should be assigned to nodes which are closer, compared to the categories car and sheep, which are semantically less similar. Given the ground truth category hierarchy, we can “measure” the semantic distance between different categories by counting the number of edges that separate any two categories in the graph.","In order to compare the hierarchies learned using different hierarchical clustering algorithms, we propose a criterion which measures the degree to which the semantic distance which a hierarchy assigns to different image instances, diverges from the semantic distance that is given by the ground truth category hierarchy. Let dGT (c1, c2) denote the number of edges separating categories c1 and c2 in the ground truth category hierarchy, and let dH (i, j) denote the number of edges separating instances i and j in a hierarchy that is learned using a hierarchical clustering algorithm. Our proposed criterion, which we refer to as the average edge error (AEE), takes the form","2 N (N − 1) N−1 ∑ i=1 N ∑ j=i+1 |dH (i, j) − dGT (c(i), c(j))|,","(10) where c(i) denotes the category of instance i, and N denotes the number of image instances. 4.2 Without ground truth hierarchy When the ground truth of the category is unavailable, we propose to use the following two criteria in order to evaluate the hierarchies. The first is known as the purity criterion (Manning et al., 2009 p. 357), and the second is the locality criterion which we propose. The purity criterion measures the degree to which each node is occupied by instances from a single class, and takes the form Purity = 1 N ∑ ε∈T Nε ∑ i=1","δc∗ ε (c(i)), (11) where Nε denotes the number of instances in node ε, and c∗","ε is the class which is most frequent in node ε.","The locality criterion measures the degree to which each class is concentrated in few adjacent nodes. Quantitatively we define the category locality for class c as CLc = −","2 (|C| − 1)|C| ∑ i, j ∈ C, i ̸= j dist(εi, εj),","(12) where | · | denotes the cardinality of a set, C = {i|ci = c} where ci is the class associated with instance i, and dist(εi, εj) denotes the number of edges along the path separating nodes εi and εj. The category locality is negative or equal to zero. Values that are closer to 0 indicate that the instances of category 24 c are concentrated in a few adjacent nodes, and negative values indicate that the category instances are more dispersed in the tree. We define the locality as the weighted average of the category locality, where the weights are the category instance frequencies.","We note that each of these objectives can generally be improved on the account of the other: locality can usually be improved by joining nodes (which in general makes purity worse), and purity can usually be improved by splitting nodes (which in general makes locality worse). Therefore, we argue that a desirable hierarchy should offer an acceptable compromise between these two performance measures."]},{"title":"5 Experimental results","paragraphs":["In this section we learn the attribute hierarchy using our proposed ATP algorithm. In order to evaluate the performance we evaluate the ATP as a hierarchical clustering algorithm, and compare it to the FBLM and AHC. We use subsets of the PASCAL VOC2008, and SUN09 datasets, for which attribute annotations are available. We learn hierarchies using the ground truth attribute annotation of the training set, and using the attribute scores obtained for the image instances in the testing set, where the attribute detectors are trained using the training set. We used the FBLM implementation which is available online. Our implementation of the ATP is based the TSSBP implementation which is available online, where we extended it to implement our ATP generative model. We used the AHC implementation available at (Mullner, ), where we used the average distance metric, which is also known as the Unweighted Pair Group Method with Arithmetic Mean (UMPGA) (Murtagh, 1984). 5.1 Object category hierarchy Here we consider the PASCAL VOC 2008 dataset. We use the bounding boxes and attribute annotation data that were collected in (Farhadi et al., 2009), and are available online, along with the low-level image features. Each of the training and testing sets contains over 6000 instances of the object classes: person, bird, cat, cow, dog, horse, sheep, airplane, bicycle, boat, bus, car, motorcycle, train, bottle, chair, dining-table, potted-plant, sofa, and tv/monitor. We used the annotation and features available for the training set, to train the attribute detectors using a linear SVM classifier (Fan et al., 2008). We used 88 attributes, which included 24 attributes in addi-tion to those used in (Farhadi et al., 2009): “pet”, “vehicle”, “alive”, “animal”, and the remaining 20 attributes were identical to the object classes. The annotation for the first 4 additional attributes was inferred from the object classes. In all the experiments presented here, we ran the Markov chain for 10,000 iterations and used the tree and model parameters from the final iteration.","The attribute hierarchies for the PASCAL dataset are shown in Figure 3, when using the annotation for the training set, and when using the attribute scores obtained for the testing set. The hierarchies were obtained by thresholding h","(d)","ε with the threshold parameter 0.7 (this parameter is only used to create the visualization, and it is not used when learning the hierarchies), and only displaying the attributes that are not already associated with an ancestor node. It can be seen that the attribute hierarchies can accurately capture the semantic space that is represented by the 20 categories in the PASCAL dataset. An important observation is that attributes which are associated with more categories, such as alive or vehicle, are assigned to nodes that are closer to the root node, as compared to more specialized attributes such as eye or window.","In order to evaluate the performance of the attribute hierarchies quantitatively, we use the ground truth category hierarchy for the 20 categories in the PASCAL dataset, which is available at (Binder et al., 2012), and is shown in Figure 4. In Figure 5 we show the AEE performance measure, which we discussed in the previous section, for the different hierarchical clustering algorithms which we consider here. It can be seen that for the AHC, the AEE is very sensitive to the threshold parameter, which effectively determines the number of clusters. A poor choice of the parameter can adversely affect the performance significantly. On the other hand, the performance of the ATP and FBLM is significantly less sensitive to the choice of the hyper-parameters, since all the parameters are learned in a Bayesian fashion with weak dependence on the hyper-parameters. This is demonstrated for the ATP in Section 5.1.1. The ATP significantly improves the AEE as com-25","!\"#$%&'%()*+&,%-./&+*)%/&& 0-1%/&)2#+/&1\"*34& 5-(/&6*734/& %8%/&3*()*/&& ,-+./&-(6/&\"%9& :11\"7.%./&%-(/& 6*734/&4-#(/&%8%& ,%-./&3*()*/& )2#+/&1\"*34& !(6/&\"%9& !+#6-\"& ,%-./& ;%3& 5-(/&)+*73/&%8%/& 3*()*/&\"%9/&07((8& <*()*/& )4%%;& ,%-./&%-(/&)+*73/& \"%9/&=**\"& ,%-./& >#(.& <-#\"/&?%-2/&%8%/& 3*()*/&0%-34%(/&\"%9&& ,%-./&3*()*/& 07((8& ,%-./&)+*73/&& 07((8/&4*()%/& 3*()*/&\"%9& @**3A)4*%/& %-(/&%8%& B%4#1\"%& C%3-\"& D#+9/& -#(;\"-+%& EF&>*G8/&.**(/&&","=4%%\"/&&","=#+.*=/&)4#+8&H%3&%+9#+%/& =#+.*=/&(*=I =#+./&=4%%\"/& 3%G3/&4*(#J*+3-\"& 18\"#+.%(& ?*-3&","C%3-\"& D#+.*=/& (*=I=#+.& EF&>*G8/&3(-#+& =#+.*=& EF&>*G8/& 9\"-))/&>7)& C%3-\"/&1-(/& $%4#1\"%& D#+9/& -#(;\"-+%& C%3-\"/& $%4#1\"%& D4%%\"/&>#181\"%/& 4-+.\"%>-()&& ,-+.\"%>-()/& =4%%\"/& 6*3*(>#2%& K4#+8& L4-#(/& 07(+#37(%& >-12& ?*M\"%/&$%(N1-\"& 18\"#+.%(& !\"#$%& O%-0/&;*3/& $%9%3-N*+/& $%(N1-\"& 18\"#+.%(/& ;*M%.I;\"-+3& :11\"7.%./& .#+#+9I3->\"%& K1(%%+/& ;\"-)N1& (a) Using the attribute annotation of the training set. !\"\"#$%&%'","!\"\"#$%&%' ()*+,-./,01' 2\"0&&-1'3#422' 5#,/61'4#.)&1' 7&02,-' 8&4%1'/,02,1' 29.-' :0+1' '#&3' 8&4%1'-,2&1' 64.01';4\"&1'29.-'<401'+,$/61'","&=&1'/,02,' <401'+,$/61'&=&1' /,02,1'64-%1'40+' 8&4%1'4#.)&' <401'2-,$/1' &=&1'/,02,1' 4-.+4#' >&31';,,/? 26,&1';$00='(,02,1'#&3'(,02,1'",";&4/6&0' @&/4#1' )&6.\"#&'","A6&&#'A.-%,B'CD'E,F=1' B6&&#1'26.-=' A.-31'4.07#4-&1' 6,0.G,-/4#'\"=#.-%&0' >&4;1'7,/1'4#.)&1' )&3&/4H,-1' 7,I&%*7#4-/' J,I#&1'' )&0H\"4#'' \"=#.-%&0' (b) Using the attribute scores of the testing set. Figure 3: Attribute hierarchy learned for the PASCAL dataset, using the (a) attribute annotation available for the training set, and (b) attribute scores obtained by applying the attribute detectors to the image instances of the testing set. The largest circle denotes the root node. pared to the FBLM, both for the training and testing sets. We also note, that for the ATP, the AEE obtained for the training set is better than that obtained using the testing set’s attribute scores (training: 1.76, testing: 2.82), which is consistent with our expecta-tion. This is not the case for the FBLM (training: 6.55, testing: 5.63). 5.1.1 Sensitivity to hyper-parameters","In order to validate our claim that the ATP is highly insensitive to the choice of hyper-parameters, we performed experiments with different hyper-parameter values. In Table 1 we compare the performance when using different values for the hyper-parameters a, and b in (4). It can be seen that when comparing to AHC in Figure 5, the ATP is significantly less sensitive to the choice of hyper-parameters. When comparing to the FBLM, even Figure 4: The ground truth category hierarchy, for the 20 categories in the PASCAL dataset. for the the worst choice of a, b the AEE is still significantly better. 5.2 Scene category hierarchy Here we used the SUN09 dataset which is comprised of indoor and outdoor scenes. We use the training 26","2","2.5","3 3.5","4","4.5","50 2 4 6 8 10 12 AHC threshold AEE "," AHC ATP FBLM (a) Training","2","2.5","3 3.5","4","4.5","50 5 10 15 20 25 30 AHC threshold AEE "," AHC ATP FBLM (b) Testing Figure 5: The average edge error (AEE) (10) vs. the AHC threshold parameter, for the hierarchies learned using the (a) training set’s attribute annotations, and (b) attribute detectors applied to the testing set image instances. Smaller values indicate better performance. It can be seen that our ATP algorithm outperforms the FBLM, and unlike the AHC, it is not as sensitive to the choice of the hyper-parameters.","Table 1: Average edge error using the attribute annotation","of the training set, for different hyper-parameters. a b AEE 1 10 1.97 5 5 1.93 10 5 1.76 10 10 1.585 10 20 1.569 and testing sets which were used in (Myung et al., 2012), each containing over 4000 images. The annotation of 111 objects in the training set, and object detector scores for the testing set, are available online. Objects in the scene have the role of attributes in describing the scene. The object classifiers were trained using logistic regression classifiers based on Gist features that were extracted from the training set.","Since the ground truth category hierarchy is un-available for this dataset, we use the locality and purity criteria, which we described in the previous section. We computed both of these measures with respect to the indoor and outdoor categories. Figure 6 shows the locality and purity measures for the training and testing sets. It can be seen that the AHC is very sensitive to the threshold parameter, and can produce unsatisfactory performance for a wide range of parameter values. The FBLM slightly outperforms the ATP with respect to the purity measure, however, its locality is very poor. Therefore, we conclude that the ATP provides an improved compro-","2.5 3","3.5","4−","25 − 20 − 15 − 10 − 5 0 AHC threshold Locality   AHC ATP FBLM (a) Training- Locality","2.5","3","3.5","40.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 AHC threshold Purity   AHC ATP FBLM (b) Training- Purity","2.5 3","3.5","4−","7 − 6 − 5 − 4 − 3 − 2 − 1 0 Locality AHC threshold"," AHC ATP FBLM (c) Testing- Locality","2.5","3","3.5","40.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 AHC threshold Purity "," AHC ATP FBLM (d) Testing- Purity Figure 6: The locality and purity measures vs. the AHC threshold parameter, using the training set’s attribute annotation, and for the testing set’s attribute scores. Larger values indicate better performance. It can be seen that our ATP has significantly better locality, and only slightly worse purity, compared to the FBLM. Furthermore, the performance of the AHC depends significantly on the choice of threshold parameter. mise with respect to the two criteria, which shows that the ATP captures the properties of a desirable hierarchy better than the FBLM."]},{"title":"6 Conclusions","paragraphs":["We developed an algorithm, which we refer to as the attribute tree process (ATP), that uses an attribute based representation to learn a hierarchy of linguistic descriptions, and can be used to describe a visual dataset verbally. In order to quantitatively evaluate the performance of our algorithm, we proposed appropriate performance metrics for the cases where the ground truth category hierarchy is known, and when it is unknown. We compared the ATP’s performance as a hierarchical clustering algorithm to other competing methods, and demonstrated that our method can more accurately capture the ground truth semantic distance between the different categories. Furthermore, we demonstrated that our method has weak sensitivity to the choice of hyper-parameters."]},{"title":"Acknowledgments","paragraphs":["We acknowledge the support of the NSF Grant CPS-0931474. 27"]},{"title":"References","paragraphs":["[Adams et al.2010] R. P. Adams, Z. Ghahramani, and M. I. Jordan. 2010. Tree-Structured Stick Breaking for Hierarchical Data. NIPS.","[Bart et al.2011] E. Bart, and M. Welling, and P. Perona. 2011. Unsupervised organization of Image Collections: Taxonomies and Beyond. IEEE Tran. on PAMI, 33(11):2302–2315.","[Berg et al.2011] T. L. Berg, A. C. Berg and J. Shih. 2010. Automatic Attribute Discovery and Characterization from Noisy Web Data. CVPR.","[Binder et al.2012] A. Binder, K. R. Muller, and M. Kawanabe. 2012. On Taxonomies for Multi-class Image Categorization. International Journal of Computer Vision, 99:281–301.","[Fan et al.2008] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.","[Farhadi et al.2009] A. Farhadi, I. Endres, D. Hoiem, and David Forsyth. 2009. Describing objects by their attributes. CVPR.","[Ferrari & Zisserman2007] V. Ferrari and A. Zisserman. 2010. Learning Visual Attributes. CVPR.","[Jain & Dubes1988 p. 59] A. K. Jain and R. C. Dubes. 1988. Algorithms for Clustering Data. Prentice-Hall, Englewood Cliffs, NJ.","[Lampert et al.2009] C. H. Lampert, H. Nickisch, and S. Harmeling. 2009. Learning to detect unseen object classes by between class attribute transfer. CVPR.","[Li et al.2010] L. J. Li, and C. Wang, and Y. Lim, and D. M. Blei, and L. Fei-Fei. 2010. Building and using a semantivisual image hierarchy. CVPR.","[Manning et al.2009 p. 357] C. D. Manning, P. Raghavan, and H. Schtze. 2009. An Introduc-tion to information retrieval. Available online at http://nlp.stanford.edu/IR-book/.","[Mullner] D. Mulner. fastcluster: Fast hierarchical clustering routines for R and Python. http://math.stanford.edu/ muellner/index.html.","[Murtagh1984] F. Murtagh. 1984. Complexities of Hierarchic Clustering Algorithms: the state of the art. Computational Statistics Quarterly, 1: 101–113.","[Myung et al.2012] M. J. Choi, A. Torralba and A. S. Willsky. 2012. A Tree-Based Context Model for Object Recognition. IEEE Tran. on PAMI, 34(2):240–252.","[Neal2000] R. Neal. 2000. Nonparametric factor analysis with beta process priors. Annals of Statistics, 31:705– 767.","[Paisley & Carin2009] J. W. Paisley, and L. Carin. 2009. Nonparametric factor analysis with beta process priors. ICML.","[Parikh & Grauman2011] P. Devi and G. Kristen. 2011. Interactively building a discriminative vocabulary of nameable attributes. CVPR.","[Sivic et al.2008] J. Sivic, and B. C. Russel, and A. Zisserman, and W. T. Freeman, and A. A. Efros. 2008. Unsupervised Discovery of visual object class hierarchies. CVPR.","[Teh et al.2006] Y. W. Teh and M. I. Jordan and M. J. Beal and D. M. Blei. 2006. Hierarchical Dirichlet Processes. Journal of the American Statistical Association, 101(476):1566–1581.","[Thibaux & Jordan2007] R. Thibaux and M. I. Jordan. 2007. Hierarchical Beta Processes and the Indian Buffet Process. AISTATS. 28"]}]}