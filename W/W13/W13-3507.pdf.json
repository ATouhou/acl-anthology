{"sections":[{"title":"","paragraphs":["Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56–64, Sofia, Bulgaria, August 8-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Spectral Learning of Refinement HMMs Karl Stratos","paragraphs":["1"]},{"title":"Alexander M. Rush","paragraphs":["2"]},{"title":"Shay B. Cohen","paragraphs":["1"]},{"title":"Michael Collins","paragraphs":["1 1"]},{"title":"Department of Computer Science, Columbia University, New-York, NY 10027, USA","paragraphs":["2"]},{"title":"MIT CSAIL, Cambridge, MA, 02139, USA {stratos,scohen,mcollins}@cs.columbia.edu, srush@csail.mit.edu Abstract","paragraphs":["We derive a spectral algorithm for learning the parameters of a refinement HMM. This method is simple, efficient, and can be applied to a wide range of supervised sequence labeling tasks. Like other spectral methods, it avoids the problem of local optima and provides a consistent estimate of the parameters. Our experiments on a phoneme recognition task show that when equipped with informative feature functions, it performs significantly better than a supervised HMM and competitively with EM."]},{"title":"1 Introduction","paragraphs":["Consider the task of supervised sequence labeling. We are given a training set where the j’th training example consists of a sequence of observations x (j) 1 ...x(j)","N paired with a sequence of labels a(j)","1 ...a(j)","N and asked to predict the correct labels on a test set of observations. A common approach is to learn a joint distribution over sequences p(a1 . . . aN , x1 . . . xN ) as a hidden Markov model (HMM). The downside of HMMs is that they assume each label ai is independent of labels before the previous label ai−1. This independence assumption can be limiting, particularly when the label space is small. To relax this assumption we can refine each label ai with a hidden state hi, which is not observed in the training data, and model the joint distribution p(a1 . . . aN , x1 . . . xN , h1 . . . hN ). This refinement HMM (R-HMM), illustrated in figure 1, is able to propagate information forward through the hidden state as well as the label.","Unfortunately, estimating the parameters of an R-HMM is complicated by the unobserved hidden variables. A standard approach is to use the expectation-maximization (EM) algorithm which a1, h1 a2, h2 aN, hN x1 x2 xN (a) a1 a2 aN h1 h2 hN x1 x2 xN (b) Figure 1: (a) An R-HMM chain. (b) An equivalent representation where labels and hidden states are intertwined. has no guarantee of finding the global optimum of its objective function. The problem of local optima prevents EM from yielding statistically consistent parameter estimates: even with very large amounts of data, EM is not guaranteed to estimate parameters which are close to the “correct” model parameters.","In this paper, we derive a spectral algorithm for learning the parameters of R-HMMs. Unlike EM, this technique is guaranteed to find the true parameters of the underlying model under mild conditions on the singular values of the model. The algorithm we derive is simple and efficient, relying on singular value decomposition followed by standard matrix operations.","We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the naı̈ve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs.","We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical 56 success for the R-HMM spectral algorithm. The spectral algorithm performs competitively with EM on a phoneme recognition task, and is more stable with respect to the number of hidden states.","Cohen et al. (2013) present experiments with a parsing algorithm and also demonstrate it is competitive with EM. Our set of experiments comes as an additional piece of evidence that spectral algorithms can function as a viable, efficient and more principled alternative to the EM algorithm."]},{"title":"2 Related Work","paragraphs":["Recently, there has been a surge of interest in spectral methods for learning HMMs (Hsu et al., 2012; Foster et al., 2012; Jaeger, 2000; Siddiqi et al., 2010; Song et al., 2010). Like these previous works, our method produces consistent parameter estimates; however, we estimate parameters for a supervised learning task. Balle et al. (2011) also consider a supervised problem, but our model is quite different since we estimate a joint distribution p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) as opposed to a conditional distribution and use feature functions over both the labels and observations of the training data. These feature functions also go beyond those previously employed in other spectral work (Siddiqi et al., 2010; Song et al., 2010). Experiments show that features of this type are crucial for performance.","Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b).","Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing."]},{"title":"3 The R-HMM Model","paragraphs":["We decribe in this section the notation used throughout the paper and the formal details of R-HMMs. 3.1 Notation We distinguish row vectors from column vectors when such distinction is necessary. We use a superscript ⊤ to denote the transpose operation. We write [n] to denote the set {1, 2, . . . , n} for any integer n ≥ 1. For any vector v ∈ Rm",", diag(v) ∈ Rm×m","is a diagonal matrix with entries v1 . . . vm. For any statement S, we use [[S]] to refer to the indicator function that returns 1 if S is true and 0 otherwise. For a random variable X, we use E[X] to denote its expected value.","A tensor C ∈ Rm×m×m","is a set of m3","values Ci,j,k for i, j, k ∈ [m]. Given a vector v ∈ Rm",", we define C(v) to be the m × m matrix with [C(v)]i,j = ∑","k∈[m] Ci,j,kvk. Given vectors","x, y, z ∈ Rm",", C = xy⊤","z⊤","is an m×m×m tensor","with [C]i,j,k = xiyjzk. 3.2 Definition of an R-HMM An R-HMM is a 7-tuple ⟨l, m, n, π, o, t, f ⟩ for in-tegers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations.","• π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence.","• o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m].","• t(b, h′","|a, h) is the probability of generating b ∈ [l] and h′","∈ [m], given a ∈ [l] and h ∈ [m].","• f (∗|a, h) is the probability of generating the stop symbol ∗, given a ∈ [l] and h ∈ [m]. See figure 1(b) for an illustration. At any time step of a sequence, a label a is associated with a hidden state h. By convention, the end of an R-HMM sequence is signaled by the symbol ∗.","For the subsequent illustration, let N be the length of the sequence we consider. A full sequence consists of labels a1 . . . aN , observations x1 . . . xN , and hidden states h1 . . . hN . The model assumes p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) = π(a1, h1)× N ∏ i=1 o(xi|ai, hi) × N−1 ∏ i=1 t(ai+1, hi+1|ai, hi) × f(∗|aN , hN ) 57","Input: a sequence of observations x1 . . . xN ; operators〈","Cb|a",", C∗|a",", c1","a, ca","x 〉 Output: μ(a, i) for all a ∈ [l] and i ∈ [N] [Forward case]","• α1 a ← c1","a for all a ∈ [l]. • For i = 1 . . . N − 1 αi+1 b ← ∑ a∈[l]","Cb|a (ca x i) × αi","a for all b ∈ [l] [Backward case]","• βN+1","a ← C∗|a (ca x","N ) for all a ∈ [l] • For i = N . . . 1 βi a ← ∑ b∈[l]","βi+1","b × Cb|a (ca x","i) for all a ∈ [l] [Marginals]","• μ(a, i) ← βi a × αi","a for all a ∈ [l], i ∈ [N] Figure 2: The forward-backward algorithm A skeletal sequence consists of labels a1 . . . aN and observations x1 . . . xN without hidden states. Under the model, it has probability p(a1 . . . aN , x1 . . . xN ) = ∑ h1...hN p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) An equivalent definition of an R-HMM is given by organizing the parameters in matrix form. Specifically, an R-HMM has parameters〈 πa",", oa","x, T b|a",", f a〉","where πa ∈ Rm","is a column","vector, oa x is a row vector, T b|a","∈ Rm×m","is a ma-","trix, and f a","∈ Rm","is a row vector, defined for all","a, b ∈ [l] and x ∈ [n]. Their entries are set to","• [πa ] h = π(a, h) for h ∈ [m]","• [oa x]h = o(x|a, h) for h ∈ [m]","• [T b|a","]h′ ,h = t(b, h′","|a, h) for h, h′","∈ [m]","• [f a ]h = f (∗|a, h) for h ∈ [m]"]},{"title":"4 The Forward-Backward Algorithm","paragraphs":["Given an observation sequence x1 . . . xN , we want to infer the associated sequence of labels under an R-HMM. This can be done by computing the marginals of x1 . . . xN μ(a, i) = ∑ a1...aN : ai=a p(a1 . . . aN , x1 . . . xN )","for all labels a ∈ [l] and positions i ∈ [N ]. Then","the most likely label at each position i is given by a∗ i = arg max","a∈[l] μ(a, i) The marginals can be computed using a tensor variant of the forward-backward algorithm, shown in figure 2. The algorithm takes additional quantities 〈","Cb|a",", C∗|a , c1","a, ca","x 〉","called the operators:","• Tensors Cb|a","∈ Rm×m×m","for a, b ∈ [l]","• Tensors C∗|a ∈ R1×m×m","for a ∈ [l]","• Column vectors c1 a ∈ Rm","for a ∈ [l]","• Row vectors ca x ∈ Rm","for a ∈ [l] and x ∈ [n] The following proposition states that these operators can be defined in terms of the R-HMM parameters to guarantee the correctness of the algorithm. Proposition 4.1. Given an R-HMM with parameters 〈","πa",", oa x, T b|a",", f a〉",", for any vector v ∈ Rm","define the operators:","Cb|a","(v) = T b|a","diag(v) c1","a = πa","C∗|a","(v) = f a","diag(v) ca","x = oa","x Then the algorithm in figure 2 correctly computes marginals μ(a, i) under the R-HMM. The proof is an algebraic verification and deferred to the appendix. Note that the running time of the algorithm as written is O(l2","m3","N ).1","Proposition 4.1 can be generalized to the following theorem. This theorem implies that the operators can be linearly transformed by some invertible matrices as long as the transformation leaves the embedded R-HMM parameters intact. This observation is central to the derivation of the spectral algorithm which estimates the linearly transformed operators but not the actual R-HMM parameters. Theorem 4.1. Given an R-HMM with parameters〈 πa",", oa","x, T b|a",", f a〉",", assume that for each a ∈ [l] we","have invertible m × m matrices Ga","and Ha",". For","any vector v ∈ Rm","define the operators:","Cb|a","(v) = Gb","T b|a","diag(vHa",")(Ga",")−1","c1","a = Ga","πa","C∗|a","(v) = f a","diag(vHa",")(Ga",")−1","ca","x = oa","x(Ha",")−1","Then the algorithm in figure 2 correctly computes","marginals μ(a, i) under the R-HMM.","The proof is similar to that of Cohen et al. (2012). 1 We can reduce the complexity to O(l2","m2","N) by pre-","computing the matrices Cb|a","(ca","x) for all a, b ∈ [l] and x ∈ [n] after parameter estimation. 58"]},{"title":"5 Spectral Estimation of R-HMMs","paragraphs":["In this section, we derive a consistent estimator for the operators 〈","Cb|a",", C∗|a , c1","a, ca","x 〉","in theorem 4.1 through the use of singular-value decomposition (SVD) followed by the method of moments.","Section 5.1 describes the decomposition of the R-HMM model into random variables which are used in the final algorithm. Section 5.2 can be skimmed through on the first reading, especially if the reader is familiar with other spectral algorithms. It includes a detailed account of the derivation of the R-HMM algorithm.","For a first reading, note that an R-HMM sequence can be seen as a right-branching L-PCFG tree. Thus, in principle, one can convert a sequence into a tree and run the inside-outside algorithm of Cohen et al. (2012) to learn the parameters of an R-HMM. However, projecting this transformation into the spectral algorithm for L-PCFGs is cumbersome and unintuitive. This is analogous to the case of the Baum-Welch algorithm for HMMs (Rabiner, 1989), which is a special case of the inside-outside algorithm for PCFGs (Lari and Young, 1990). 5.1 Random Variables We first introduce the random variables underlying the approach then describe the operators based on these random variables. From p(a1 . . . aN , x1 . . . xN , h1 . . . hN ), we draw an R-HMM sequence (a1 . . . aN , x1 . . . xN , h1 . . . hN ) and choose a time step i uniformly at random from [N ]. The random variables are then defined as X = xi A1 = ai and A2 = ai+1 (if i = N, A2 = ∗) H1 = hi and H2 = hi+1 F1 = (ai . . . aN , xi . . . xN ) (future) F2 = (ai+1 . . . aN , xi+1 . . . xN ) (skip-future) P = (a1 . . . ai, x1 . . . xi−1) (past) R = (ai, xi) (present) D = (a1 . . . aN , x1 . . . xi−1, xi+1 . . . xN ) (destiny) B = [[i = 1]] Figure 3 shows the relationship between the random variables. They are defined in such a way that the future is independent of the past and the present is independent of the destiny conditioning on the current node’s label and hidden state.","Next, we require a set of feature functions over the random variables.","• φ maps F1, F2 to φ(F1), φ(F2) ∈ Rd1 . a1 ai−1 ai ai+1 aN x1 xi−1 xi xi+1 xN P F1 F2 (a) a1 ai−1 ai ai+1 aN x1 xi−1 xi xi+1 xN D R (b) Figure 3: Given an R-HMM sequence, we define random variables over observed quantities so that conditioning on the current node, (a) the future F1 is independent of the past P and (b) the present R is independent of the density D.","• ψ maps P to ψ(P ) ∈ Rd2 .","• ξ maps R to ξ(R) ∈ Rd3 .","• υ maps D to υ(D) ∈ Rd4 .","We will see that the feature functions should be","chosen to capture the influence of the hidden","states. For instance, they might track the next la-","bel, the previous observation, or important combi-","nations of labels and observations.","Finally, we require projection matrices Φa","∈ Rm×d1","Ψa","∈ Rm×d2 Ξa","∈ Rm×d3","Υa","∈ Rm×d4 defined for all labels a ∈ [l]. These matrices will project the feature vectors of φ, ψ, ξ, and υ from (d1, d2, d3, d4)-dimensional spaces to an m-dimensional space. We refer to this reduced dimensional representation by the following random variables: F 1 = ΦA","1","φ(F 1) (projected future)","F 2 = ΦA 2 φ(F","2) (projected skip-future: if i = N, F 2 = 1)","P = ΨA 1 ψ(P ) (projected past)","R = ΞA 1 ξ(R) (projected present)","D = ΥA 1 υ(D) (projected destiny)","Note that they are all vectors in Rm . 59 5.2 Estimation of the Operators Since F 1, F 2, P , R, and D do not involve hidden variables, the following quantities can be directly estimated from the training data of skeletal sequences. For this reason, they are called observable blocks:","Σa = E[F","1P ⊤ |A1 = a] ∀a ∈ [l]","Λa = E[R D⊤","|A1 = a] ∀a ∈ [l]","Db|a","= E[[[A2 = b]]F 2P ⊤","R⊤","|A1 = a] ∀a, b ∈ [l] da x = E[[[X = x]]D⊤","|A1 = a] ∀a ∈ [l], x ∈ [n] The main result of this paper is that under certain conditions, matrices Σa","and Λa","are invertible and the operators 〈","Cb|a",", C∗|a , c1","a, ca","x 〉","in theorem 4.1 can be expressed in terms of these observable blocks.","Cb|a","(v) = Db|a (v)(Σa",")−1","(1)","C∗|a","(v) = D∗|a (v)(Σa",")−1","(2)","ca","x = da","x(Λa )−1","(3) c1 a = E[[[A1 = a]]F 1|B = 1] (4) To derive this result, we use the following definition to help specify the conditions on the expecta-tions of the feature functions.","Definition. For each a ∈ [l], define matrices","Ia","∈ Rd1×m , J a","∈ Rd2×m",", Ka","∈ Rd3×m",", W a","∈","Rd4×m","by [Ia ]k,h = E[[φ(F1)]k|A1 = a, H1 = h] [J a","]k,h = E[[ψ(P )]k|A1 = a, H1 = h] [Ka","] k,h = E[[ξ(R)]k|A1 = a, H1 = h]","[W a ] k,h = E[[υ(D)]k|A1 = a, H1 = h]","In addition, let Γa","∈ Rm×m","be a diagonal matrix","with [Γa ]h,h = P (H1 = h|A1 = a). We now state the conditions for the correctness of Eq. (1-4). For each label a ∈ [l], we require that","Condition 6.1 Ia",", J a , Ka",", W a","have rank m.","Condition 6.2 [Γa ]h,h > 0 for all h ∈ [m]. The conditions lead to the following proposition. Proposition 5.1. Assume Condition 6.1 and 6.2 hold. For all a ∈ [l], define matrices Ωa 1 = E[φ(F1)ψ(P )⊤","|A 1 = a] ∈ Rd1×d2 Ωa 2 = E[ξ(R)υ(D)⊤","|A 1 = a] ∈ Rd3×d4","Let ua","1 . . . ua","m ∈ Rd1 and va","1 . . . va","m ∈ Rd2","be the","top m left and right singular vectors of Ωa",". Sim-","ilarly, let la","1 . . . la","m ∈ Rd3","and ra","1 . . . ra","m ∈ Rd4","be","the top m left and right singular vectors of Ψa",".","Define projection matrices","Φa","= [ua","1 . . . ua","m]⊤","Ψa","= [va","1 . . . va","m]⊤","Ξa","= [la","1 . . . la","m]⊤","Υa","= [ra","1 . . . ra","m]⊤ Then the following m × m matrices","Ga","= Φa","Ia","Ga","= Ψa","J a","Ha","= Ξa","Ka","Ha","= Υa","W a are invertible.","The proof resembles that of lemma 2 of Hsu et al.","(2012). Finally, we state the main result that shows〈","Cb|a , C∗|a",", c1","a, ca","x 〉","in Eq. (1-4) using the projections from proposition 5.1 satisfy theorem 4.1. A sketch of the proof is deferred to the appendix.","Theorem 5.1. Assume conditions 6.1 and 6.2","hold. Let ⟨Φa",", Ψa",", Ξa",", Υa","⟩ be the projection ma-","trices from proposition 5.1. Then the operators in","Eq. (1-4) satisfy theorem 4.1.","In summary, these results show that with the","proper selection of feature functions, we can con-","struct projection matrices ⟨Φa",", Ψa",", Ξa",", Υa","⟩ to ob-","tain operators 〈","Cb|a",", C∗|a , c1","a, ca","x 〉 which satisfy the conditions of theorem 4.1."]},{"title":"6 The Spectral Estimation Algorithm","paragraphs":["In this section, we give an algorithm to estimate the operators","〈","Cb|a",", C∗|a",", c1","a, ca","x 〉","from samples of skeletal sequences. Suppose the training set consists of M skeletal sequences (a(j)",", x(j)",") for j ∈ [M ]. Then M samples of the random variables can be derived from this training set as follows","• At each j ∈ [M ], choose a position ij uniformly at random from the positions in (a(j)",", x(j)","). Sample the random variables (X, A1, A2, F1, F2, P, R, D, B) using the procedure defined in section 5.1. This process yields M samples","(x(j)",", a(j)","1 , a(j)","2 , f(j)","1 , f(j)","2 , p(j)",", r(j)",", d(j)",", b(j)",") for j ∈ [M] Assuming (a(j)",", x(j)",") are i.i.d. draws from the PMF p(a1 . . . aN , x1 . . . xN ) over skeletal sequences under an R-HMM, the tuples obtained through this process are i.i.d. draws from the joint PMF over (X, A1, A2, F1, F2, P, R, D, B). 60 Input: samples of (X, A1, A2, F1, F2, P, R, D, B); feature functions φ, ψ, ξ, and υ; number of hidden states m Output: estimates","〈","Ĉb|a",", Ĉ∗|a",", ĉ1","a, ĉa","x 〉 of the operators used in algorithm 2 [Singular Value Decomposition] • For each label a ∈ [l], compute empirical estimates of Ωa 1 = E[φ(F1)ψ(P )⊤","|A1 = a] Ωa 2 = E[ξ(R)υ(D)⊤","|A1 = a]","and obtain their singular vectors via an SVD. Use","the top m singular vectors to construct projections〈 Φ̂a , Ψ̂a",", Ξ̂a",", Υ̂a 〉 . [Sample Projection] • Project (d1, d2, d3, d4)-dimensional samples of (φ(F1), φ(F2), ψ(P ), ξ(R), υ(D)) with matrices","〈","Φ̂a , Ψ̂a",", Ξ̂a",", Υ̂a 〉 to obtain m-dimensional samples of (F 1, F 2, P , R, D) [Method of Moments]","• For each a, b ∈ [l] and x ∈ [n], compute empirical estimates 〈","Σ̂a",", Λ̂a , D̂b|a",", d̂a","x 〉 of the observable blocks","Σa = E[F","1P ⊤ |A1 = a]","Λa = E[R D⊤","|A1 = a]","Db|a","= E[[[A2 = b]]F 2P ⊤","R⊤","|A1 = a] da x = E[[[X = x]]D⊤","|A1 = a]","and also ĉ1 a = E[[[A1 = a]]F 1|B = 1]. Finally, set","Ĉb|a","(v) ← D̂b|a","(v)( Σ̂a",")−1","Ĉ∗|a","(v) ← D̂∗|a","(v)( Σ̂a",")−1","ĉa","x ← d̂a","x(Λ̂a",")−1 Figure 4: The spectral estimation algorithm","The algorithm in figure 4 shows how to derive estimates of the observable representations from these samples. It first computes the projection matrices ⟨Φa",", Ψa",", Ξa",", Υa","⟩ for each label a ∈ [l] by computing empirical estimates of Ωa","1 and Ωa","2 in proposition 5.1, calculating their singular vectors via an SVD, and setting the projections in terms of these singular vectors. These projection matrices are then used to project (d1, d2, d3, d4)-","0 5 10 15 20 25 30 hidden states (m) 54.0 54.5 55.0 55.5 56.0 56.5 57.0 57.5 accuracy Spectral EM Figure 5: Accuracy of the spectral algorithm and EM on TIMIT development data for varying numbers of hidden states m. For EM, the highest scor-ing iteration is shown.","dimensional feature vectors ( φ(f (j) 1 ), φ(f","(j)","2 ), ψ(p(j) ), ξ(r(j)","), υ(d(j)",") )","down to m-dimensional vectors (","f (j)","1 , f (j) 2 , p(j)",", r(j)",", d(j) ) for all j ∈ [M ]. It then computes correlation between these vectors in this lower dimensional space to estimate the observable blocks which are used to obtain the operators as in Eq. (1-4). These operators can be used in algorithm 2 to compute marginals.","As in other spectral methods, this estimation algorithm is consistent, i.e., the marginals μ̂(a, i) computed with the estimated operators approach the true marginal values given more data. For details, see Cohen et al. (2012) and Foster et al. (2012)."]},{"title":"7 Experiments","paragraphs":["We apply the spectral algorithm for learning R-HMMs to the task of phoneme recognition. The goal is to predict the correct sequence of phonemes a1 . . . aN for a given a set of speech frames x1 . . . xN . Phoneme recognition is often modeled with a fixed-structure HMM trained with EM, which makes it a natural application for spectral training.","We train and test on the TIMIT corpus of spoken language utterances (Garofolo and others, 1988). The label set consists of l = 39 English phonemes following a standard phoneme set (Lee and Hon, 1989). For training, we use the sx and si utterances of the TIMIT training section made up of 61 φ(F1) ai+1 × xi, ai+1, xi, np(ai . . . aN ) ψ(P ) (ai−1, xi−1), ai−1, xi−1, pp(a1 . . . ai) ξ(R) xi υ(D) ai−1 × xi−1, ai−1, xi−1, pp(a1 . . . ai),","pos(a1 . . . aN )","iy r r r r r r ow ...... pp b m e np Figure 6: The feature templates for phoneme recognition. The simplest features look only at the current label and observation. Other features in-dicate the previous phoneme type used before ai (pp), the next phoneme type used after ai (np), and the relative position (beginning, middle, or end) of ai within the current phoneme (pos). The figure gives a typical segment of the phoneme sequence a1 . . . aN M = 3696 utterances. The parameter estimate is smoothed using the method of Cohen et al. (2013).","Each utterance consists of a speech signal aligned with phoneme labels. As preprocessing, we divide the signal into a sequence of N over-lapping frames, 25ms in length with a 10ms step size. Each frame is converted to a feature representation using MFCC with its first and second derivatives for a total of 39 continuous features. To discretize the problem, we apply vector quantization using euclidean k-means to map each frame into n = 10000 observation classes. After preprocessing, we have 3696 skeletal sequence with a1 . . . aN as the frame-aligned phoneme labels and x1 . . . xN as the observation classes.","For testing, we use the core test portion of TIMIT, consisting of 192 utterances, and for development we use 200 additional utterances. Accuracy is measured by the percentage of frames labeled with the correct phoneme. During inference, we calculate marginals μ for each label at each position i and choose the one with the highest marginal probability, a∗","i = arg maxa∈[l] μ(a, i).","The spectral method requires defining feature functions φ, ψ, ξ, and υ. We use binary-valued feature vectors which we specify through features templates, for instance the template ai × xi corresponds to binary values for each possible label and output pair (ln binary dimensions).","Figure 6 gives the full set of templates. These feature functions are specially for the phoneme labeling task. We note that the HTK baseline explicitly models the position within the current Method Accuracy EM(4) 56.80 EM(24) 56.23 SPECTRAL(24), no np, pp, pos 55.45 SPECTRAL(24), no pos 56.56 SPECTRAL(24) 56.94 Figure 7: Feature ablation experiments on TIMIT development data for the best spectral model (m = 24) with comparisons to the best EM model (m = 4) and EM with m = 24. Method Accuracy UNIGRAM 48.04 HMM 54.08 EM(4) 55.49 SPECTRAL(24) 55.82 HTK 55.70 Figure 8: Performance of baselines and spectral R-HMM on TIMIT test data. Number of hidden states m optimized on development data (see figure 5). The improvement of the spectral method over the EM baseline is significant at the p ≤ 0.05 level (and very close to significant at p ≤ 0.01, with a precise value of p ≤ 0.0104). phoneme as part of the HMM structure. The spectral method is able to encode similar information naturally through the feature functions.","We implement several baseline for phoneme recognition: UNIGRAM chooses the most likely label, arg maxa∈[l] p(a|xi), at each position; HMM is a standard HMM trained with maximumlikelihood estimation; EM(m) is an R-HMM with m hidden states estimated using EM; and SPECTRAL(m) is an R-HMM with m hidden states estimated with the spectral method described in this paper. We also compare to HTK, a fixed-structure HMM with three segments per phoneme estimated using EM with the HTK speech toolkit. See Young et al. (2006) for more details on this method.","An important consideration for both EM and the spectral method is the number of hidden states m in the R-HMM. More states allow for greater label refinement, with the downside of possible overfitting and, in the case of EM, more local optima. To determine the best number of hidden states, we optimize both methods on the development set for a range of m values between 1 to 32. For EM, 62 we run 200 training iterations on each value of m and choose the iteration that scores best on the development set. As the spectral algorithm is noniterative, we only need to evaluate the development set once per m value. Figure 5 shows the development accuracy of the two method as we adjust the value of m. EM accuracy peaks at 4 hidden states and then starts degrading, whereas the spectral method continues to improve until 24 hidden states.","Another important consideration for the spectral method is the feature functions. The analysis suggests that the best feature functions are highly informative of the underlying hidden states. To test this empirically we run spectral estimation with a reduced set of features by ablating the templates indicating adjacent phonemes and relative position. Figure 7 shows that removing these features does have a significant effect on development accuracy. Without either type of feature, development accuracy drops by 1.5%.","We can interpret the effect of the features in a more principled manner. Informative features yield greater singular values for the matrices Ωa","1 and Ωa","2, and these singular values directly affect the sample complexity of the algorithm; see Cohen et al. (2012) for details. In sum, good feature functions lead to well-conditioned Ωa","1 and Ωa","2, which in turn require fewer samples for convergence.","Figure 8 gives the final performance for the baselines and the spectral method on the TIMIT test set. For EM and the spectral method, we use best performing model from the development data, 4 hidden states for EM and 24 for the spectral method. The experiments show that R-HMM models score significantly better than a standard HMM and comparatively to the fixed-structure HMM. In training the R-HMM models, the spectral method performs competitively with EM while avoiding the problems of local optima."]},{"title":"8 Conclusion","paragraphs":["This paper derives a spectral algorithm for the task of supervised sequence labeling using an R-HMM. Unlike EM, the spectral method is guaranteed to provide a consistent estimate of the parameters of the model. In addition, the algorithm is simple to implement, requiring only an SVD of the observed counts and other standard matrix operations. We show empirically that when equipped with informative feature functions, the spectral method performs competitively with EM on the task of phoneme recognition."]},{"title":"Appendix","paragraphs":["Proof of proposition 4.1. At any time step i ∈ [N] in the al-","gorithm in figure 2, for all label a ∈ [l] we have a column","vector αi","a ∈ Rm","and a row vector βi","a ∈ Rm",". The value of","these vectors at each index h ∈ [m] can be verified as","[αi a]h = ∑","a","1...a i,h","1...h","i:","a i=a,h","i=h p(a1 . . . ai, x1 . . . xi−1, h1 . . . hi) [βi a]h =","∑","a","i...a N ,h","i...h","N : a i=a,h","i=h p(ai+1 . . . aN , xi . . . xN , hi+1 . . . hN |ai, hi)","Thus βi aαi","a is a scalar equal to ∑","a","1...a N ,h","1...h","N : a i=a p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) which is the value of the marginal μ(a, i). Proof of theorem 5.1. It can be verified that c1","a = Ga","πa",". For the others, under the conditional independence illustrated in figure 3 we can decompose the observable blocks in terms of the R-HMM parameters and invertible matrices","Σa","= Ga","Γa","(Ga",")⊤","Λa","= Ha","Γa","(Ha",")⊤","Db|a","(v) = Gb","T b|a","diag(vHa )Γa","(Ga",")⊤","D∗|a","(v) = fa","diag(vHa",")Γa (Ga",")⊤","da","x = oa","xΓa","(Ha",")⊤ using techniques similar to those sketched in Cohen et al. (2012). By proposition 5.1, Σa","and Λa","are invertible, and these observable blocks yield the operators that satisfy theorem 4.1 when placed in Eq. (1-3)."]},{"title":"References","paragraphs":["A. Anandkumar, D. P. Foster, D. Hsu, S.M. Kakade, and Y.K. Liu. 2012a. Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. Arxiv preprint arXiv:1204.6703.","A. Anandkumar, D. Hsu, and S.M. Kakade. 2012b. A method of moments for mixture models and hidden markov models. Arxiv preprint arXiv:1203.0683.","B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral learning algorithm for finite state transducers. Machine Learning and Knowledge Discovery in Databases, pages 156–171.","S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.","S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. 2013. Experiments with spectral learning of latent-variable pcfgs. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 63","D. P. Foster, J. Rodu, and L.H. Ungar. 2012. Spectral dimensionality reduction for hmms. Arxiv preprint arXiv:1203.6130.","J. S. Garofolo et al. 1988. Getting started with the darpa timit cd-rom: An acoustic phonetic continuous speech database. National Institute of Standards and Technology (NIST), Gaithersburgh, MD, 107.","D. Hsu, S.M. Kakade, and T. Zhang. 2012. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences.","H. Jaeger. 2000. Observable operator models for discrete stochastic time series. Neural Computation, 12(6):1371– 1398.","K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer speech & language, 4(1):35–56.","K.F. Lee and H.W. Hon. 1989. Speaker-independent phone recognition using hidden markov models. Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(11):1641–1648.","F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. 2012. Spectral learning for non-deterministic dependency parsing. In EACL, pages 409–419.","T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic cfg with latent annotations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 75–82. Association for Computational Linguistics.","A. Parikh, L. Song, and E.P. Xing. 2011. A spectral algorithm for latent tree graphical models. In Proceedings of the 28th International Conference on Machine Learning.","F. Pereira and Y. Schabes. 1992. Inside-outside reestima-tion from partially bracketed corpora. In Proceedings of the 30th annual meeting on Association for Computational Linguistics, pages 128–135. Association for Computational Linguistics.","S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433–440. Association for Computational Linguistics.","Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learning structured models for phone recognition. In Proc. of EMNLP-CoNLL.","L. R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.","S. Siddiqi, B. Boots, and G. J. Gordon. 2010. Reducedrank hidden Markov models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010).","L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola. 2010. Hilbert space embeddings of hidden markov models. In Proceedings of the 27th International Conference on Machine Learning. Citeseer.","S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, XA Liu, G. Moore, J. Odell, D. Ollason, D. Povey, et al. 2006. The htk book (for htk version 3.4). 64"]}]}