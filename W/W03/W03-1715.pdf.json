{"sections":[{"title":"Abductive Explanation-based Learning Improves Parsing Accuracy and Efficiency Oliver Streiter Language and Law, European Academy, Bolzano, Italy ostreiter@eurac.edu Abstract","paragraphs":["Natural language parsing has to be accurate and quick. Explanation-based Learning (EBL) is a technique to speed-up parsing. The accuracy however often declines with EBL. The paper shows that this accuracy loss is not due to the EBL framework as such, but to deductive parsing. Abductive EBL allows extending the deductive closure of the parser. We present a Chinese parser based on abduction. Experiments show improvements in accuracy and efficiency.1"]},{"title":"1 Introduction","paragraphs":["The difficulties of natural language parsing, in general, and of parsing Chinese, in particular, are due to local ambiguities of words and phrases. Extensive linguistic and non-linguistic knowledge is required for their resolution (Chang, 1994; Chen, 1996). Different parsing approaches provide different types of knowledge. Example-based parsing approaches of-fer rich syntagmatic contexts for disambiguation, richer than rule-based approaches do (Yuang et al., 1992). Statistical approaches to parsing acquire mainly paradigmatic knowledge and require larger corpora, c.f. (Carl and Langlais, 2003). Statistical approaches handle unseen events via smoothing. Rule-based approaches use abstract category labels.","1","This research has been carried out within Logos Gaias project, which integrates NLP technologies into a Internet-based natural language learning platform (Streiter et al., 2003). Example-based parsing generalizes examples during compilation time, e.g. (Bod and Kaplan, 1998), or performs a similarity-based fuzzy match during runtime (Zavrel and Daelemans, 1997). Both techniques may be computationally demanding, their effect on parsing however is quite different, c.f. (Streiter, 2002a).","Explanation-based learning (EBL) is a method to speed-up rule-based parsing via the caching of examples. EBL however trades speed for accuracy. For many systems, a small loss in accuracy is acceptable if an order of magnitude less computing time is required. Apart from speed, one generally recognizes that EBL acquires some kind of knowledge from texts. However, what is this knowledge like if it does not help with parsing? Couldn’t a system improve by learning its own output? Can a system learn to parse Chinese by parsing Chinese? The paper sets out to tackle these questions in theory and practice. 1.1 Explanation-based Learning (EBL) Explanation-based learning techniques transform a general problem solver (PS) into a specific and operational PS (Mitchel et al., 1986). The caching of the general PS’s output accounts for this transformation. The PS generates, besides the output, a documentation of the reasoning steps involved (the explanation). This determines which output the system will cache.","The utility problem questions the claim of speeding-up applications (Minton, 1990): Retriev-ing cached solutions in addition to regular processing requires extra time. If retrieval is slow and cached solutions are rarely re-used, the cost-benefit ratio is negative.","The accuracy of the derived PS is generally be-low that of the general PS. This may be due to the EBL framework as such or the deductive base of the PS. Research in abductive EBL (A-EBL) seems to suggest the latter: A-EBL has the potential to acquire new knowledge (Dimopoulos and Kakas, 1996). The relation between knowledge and accuracy however is not a direct and logical one. The U-shaped language learning curves in children exemplifies the indirect relation (Marcus et al., 1992). Wrong regular word forms supplant correct irregular forms when rules are learned. We therefore cannot simply equate automatic knowledge acquisition and accuracy improvement, in particular for complex language tasks. 1.2 EBL and Natural Language Parsing Previous research has applied EBL for the speed-up of large and slow grammars. Sentences are parsed. Then the parse trees are filtered and cached. Subsequent parsing uses the cached trees. A complex HPSG-grammar transforms into tree-structures with instantiated values (Neumann, 1994). One hash table lookup of POS-sequences replaces typedfeature unification. Experiments conducted in EBL-augmented parsing consistently report a speed-up of the parser and a drop in accuracy (Rayner and Samuelsson, 1994; Srinivas and Joshi, 1995).","A loss of information may explain the drop of accuracy. Contextual information, taken into account by the original parser, may be unavailable in the new operational format (Sima’an, 1997), especially if partial, context-dependent solutions are retrieved. In addition, the set of cached parse trees, judged to be ”sure to cache”, is necessarily biased (Streiter, 2002b). Most cached tree structures are short noun phrases. Parsing from biased examples will bias the parsing. A further reason for the loss in accuracy are incorrect parses which leak into the cache. A stricter filter does not solve the problem. It increases the bias in the cache, reduces the size of the cache, and evokes the utility problem.","EBL actually can improve parsing accuracy (Streiter, 2002b) if the grammar does not derive the parses to be cached via deduction but via abduction. The deductive closure2","which cannot increase with EBL from deductive parsing may increase with abductive parsing."]},{"title":"2 A Formal View on Parsing and Learning","paragraphs":["We use the following notation throughout the paper:","⊴◁","▷⊵","(function","applied to ⊵","yields x),","","◁▷⊵","(relation","applied to ⊵","yields x)."," and ","represent tuples and sets respec-","tively. The","prefix denotes the cardinality of a col-","lection, e.g."," ",".","Uppercase variables stand for collections and","lowercase variables for elements. Collections may","contain the anonymous variable","(the variable _ in","PROLOG). Over-braces or under-braces should fa-","cilitate reading: ˂⌃˃⌄˂  ","℧","","⋈◇□","˂  ","℧  .","A theory is  ↝⁓ ","where ⁓ is a set of","rules ⊏ .  and","are two disjoint sets of attributes","⊐","and (e.g."," ⊏ ","   ","). A rule is written as ⊏ ",""," ⊐","","or ⊏","","◁ ▷  ⊐",". A rule specifies the rela-","tion between an observable fact","and an attribute ⊐","assigned to it. is the set of observable data with","each","being a tuple","","","⊐ ↝  .3 ","is the set of data classified according to , with","◇ "," ⊐"," . ,","and ⊐","may have an internal struc-","ture in the form of ordered or unordered collections","of more elementary",",","and ⊐","respectively.","Transferring this notation to the description of","parsing,","is a syntactic formalism and ⁓","a gram-","mar.  is the union of syntax trees and morpho-","syntactic tags.","is a corpus tagged with ",".","cor-","responds to a list of words, phrases or sentences (the","surface strings). ","is a treebank, a cache of parse trees, or a history of explanations."," ","⊐↝","↝ ","  ⊐↝"," (1)","2.1 Parsing:","","◁▷  ","","","","A parser defines a relation between and ","(c.f.","2). Parsing is a relation between and a subset of  (c.f. 3).","","","◁▷","","","","(2) 2 The deductive closure of the set of axioms","is the set","which can be proved from it. 3 The formalization follows (Dimopoulos and Kakas, 1996).","","","◁▷  ","",""," (3)","Simplifying, we can assume that","is defined as","the set of rules, i.e. "," "," ","","⁓",". A spe-","cific parser","is derived by the application of","to the","training material (e.g. ","):  ◁▷","","","",". The set of","possible relations","is",". Elements of","are caching","(no generalization), induction (hypothesis after data","inspection) and abduction (hypothesis during classi-","fication). Equation (5) describes the cycle of gram-","mar learning and grammar application.","  ◁▷","",""," (4)","▷","  ◁▷","","  ↝","","","℧ ","","◁▷  ",""," (5)","2.1.1 Memory-based Parsing","","is based on memory if ▷   ◁▷↝","","",""," ",""," ⊏↝⊏  .","in (6) is the trivial formalization of","caching. Parsing proceeds via recalling","defined","in (7). The cycle of grammar learning and parsing","","","◁ ▷  ","is defined in (8): The training material","yields the parsing output","",".4","  ◁▷","","","⊐ ","",""," ⊐",""," ","℧  (6)","","","◁▷","","",""," ⊐"," (7) parsing","","","℧","","learning from"," ℧","","▷   ◁▷ "," ℧"," ","","⊐  ↝","◁▷","  ","   ⊐","",""," ","℧  (8) 2.1.2 Deduction-based Parsing Let","be a function which replaces one or","more elements of a collection by a named variable","or .","is a deductive inference if ⊏","is obtained","from an induction (a reduction of","with the help of","↝","). The following expressions define induction"," (9), deduction","(10) and the inductive-deductive","cycle","","◁▷","(11): 4 We use subscripts to indicate the identity of variables. The","same subscript of two variables implies the identity of both vari-","ables. Different subscripts imply nothing. The variables may","be identical or not identical. In memory-based parsing, learning","material and parsing output are identical."," ◁⌄▷ ","⊐ ↝ ","  ⊐ ","","","℧   ","","◁▷","","⊐ ↝ ","","","","","℧","  ","  ⊐ "," ","℧   (9)","  ◁▷","","⊐ ↝ ","","","","℧  "," ","⊐ ↝ ","  ⊐  (10)","parsing","","℧","","▷ ◁  "," ℧"," ▷ ","⊐ ↝ ","","","⊐","  ◁▷","","⊐","↝  ","↝"," ⊐ ↝ ","  ⊐  (11) 2.1.3 Abduction-based Parsing Abduction, defined as "," ℧","","▷","◁▷↝","◁▷","","","is a run-","time generalization which is triggered by a concrete","","to be classified. We separate","and","for presen-","tation purpose only.5 The relation","may express a","similarity, a temporal or causal relation. (12) and the","cycle of","","◁▷","(13) define abduction.","","◁▷","  ◁▷","⊏","(12)","parsing","","℧","","learning","from"," ℧","","▷","◁▷ ","⊐"," ↝ ","","","⊐  ","↝","◁ "," ℧"," ▷ ","⊐  ↝ "," "," ⊐ ↝ "," ","℧    ⊐","","(13)","Abduction subsumes reasoning by analogy. Abduction is an analogy, if","describes a similarity. Reasoning from rain to snow is a typical analogy. Reasoning from wet street to rain is an abductive reasoning. For a parsing approach based on analogy c.f. (Lepage, 1999).","5","Abduction is a process of hypothesis generation. Deduc-tion and abduction may work conjointly whenever deductive in-ferences encounter gaps. A deductive inference stops in front of a gap between the premises and a possible conclusion. Abduction creates a new hypothesis, which allows to bridge the gap and to continue the inference. 2.2 Learning:"," ▷↝▷","","","◁▷","","↝","◁▷  ↝ In this section, we formalize EBL. We mechanically substitute","in the definition of EBL by","to show their learning potentials.","A learning system changes internal states, which influence the performance. The internal states of are determined by ","and",". We assume that, for a","given",",","remains identical before and after learn-","ing. Therefore, the comparison of  (before learning) with","  (after learning) reveals the ac-","quired knowledge.","We define EBL in (14). ▷","  ◁","▷","","↝","is the parser","before learning. This parser applies to","and yields","",", formalized as ▷","","","◁ ▷ ","↝","◁","▷  ",". The new","parser is the application of","to the union of  and  ",".","","","  ◁▷","","▷↝▷","  ◁▷","","↝"," ","℧"," ","","◁▷","",""," ","℧  "," ↝ (14)","From two otherwise identical parsers, the parser","with","","⊐    ⊐ ","","not present in the other","has a greater deductive closure. The cardinality of","","⊐    ⊐ ","  ","reflects an empirical knowledge. The empirical knowledge does not allow to conclude something new, but to resolve ambiguities in accordance with observed data, e.g. for a sub-language as shown in (Rayner and Samuelsson, 1994). Both learning techniques have the potential of improving the accuracy.","2.2.1 Learning through Parsing","A substitution of","with","reveals the trans-","formation of  ","to",". We start with caching and","recalling (Equation 15).","","","","","◁▷","","","","","▷","  ◁▷↝"," ","℧ ","","◁▷","",""," ","℧ "," (15)","Parsing","","with the cache of","","yields","",". The de-","ductive closure is not enlarged. Quantitative rela-","tions with respect to","change in ",". If","is not cached","twice, memory-based EBL is idempotent.6 6 Idempotence is the property of an operation that results in","the same state no matter how many times it is executed. EBL with induction and deduction is shown in","(16). Here the subscripts merit special attention:"," ","","⊐ ↝ ","is parsed from ","⊐ ↝ ","  ⊐ ",". This yields ","⊐ ↝   ⊐ ",". In-","tegrating","","into C changes the empirical knowl-","edge with respect to ⊐","and",". If the empirical","knowledge does not influence",", D-EBL is idem-","potent. The deductive closure does not increase as","","⊐    ⊐    .",""," ◁▷","","⊐ ↝ ","  ⊐ ","","▷↝▷","◁▷","","⊐ ↝ ","  ⊐ ","","","℧ ","↝","◁▷","","⊐ ↝"," ↝"," ","℧","","    ",""," ","(16) Abductive EBL (A-EBL) is shown in (17). A-EBL acquires empirical knowledge similarly to D-EBL. In addition, a new"," ⊐    ⊐","","is ac-","quired. This","may differ from","","","with respect","to ⊐","","and/or ⊐",". In the experiments in A-EBL we","reported below, ⊐","","⋈⊐  and ⊐","","⋈⊐","","holds.",""," ◁▷","","⊐ ↝ ",""," ⊐  ","","▷↝▷","◁▷","","⊐ ↝ ","  ⊐ ","","","","℧  "," ","℧ learning","","◁▷","","⊐ ↝ "," ","℧  ↝"," ","℧","","     ","","  (17)","2.2.2 Recursive Rule Application Parsing is a classification task in which ⊐","","is assigned to",". Differently from typical classification tasks in machine learning, natural language parsing requires an open set ",". This is obtained via the recursive application of ⁓",", which unlike non-recursive styles of analysis (Srinivas and Joshi, 1999) yields ","(syntax trees) of any complexity.","Then ↝ is applied to  so that","","◁","▷","can be matched by further rules (c.f. 18). With-","out this reduction, recursive parsing could not go be-","yond memory-based parsing.","⊏","⌄","⌄","⊐ ↝ ","  ","↝","","◁▷","⊏"," ◁▷","","↝","↝",""," "," ⊐ "," ⊐","↝⊏","","◁▷","",""," (18) Figure 1: An explanation produced by OCTOPUS. At the top, the final parse obtained via deductive substitutions. Abductive term identification bridges gaps in the deduction (X","Y). The marker ’?’ is a graphical shortcut for the set of lexemes ","in .","The function ↝","defines an induction and recursive parsing is thus a deduction. Combinations of memory-based and deduction-based parsing are deductions, combinations of abduction-based parsing with any another parsing are abductions.","Macro Learning is the common term for the combination of EBL with recursive deduction (Tadepalli, 1991). A macro ⊏","","","↝","is a rule which yields the same result as a set of rules ⁓","with ⁓","","and ⊏","⌃","","","⁓","does. In terms of a grammar, such macros correspond to redundant phrases, i.e. phrases that are obtained by composing smaller phrases of ⁓",". Macros represent shortcuts for the parser and, possibly, improved likelihood estimate of the composed structure compared to the estimates under independency assumption (Abney, 1996). When the usage of macros excludes certain types of analysis, e.g. by trying to find longest/best matches we can speak of pruning. This is the contribution of D-EBL for parsing."]},{"title":"3 Experiments in EBL 3.1 Experimental purpose and setup","paragraphs":["The aim of the experiments is to verify whether new knowledge is acquired in A-EBL and D-EBL. Secondly, we want to test the influence of new knowledge on parsing accuracy and speed.","The general setup of the experiment is the following. We use a section of a treebank as seed-corpus (    ). We train the seed-corpus to a corpus-based","parser. Using a test-corpus we establish the parsing Figure 2: The main parsing algorithm of OCTOPUS. The parser interleaves memory-based, deductive, and abductive parsing strategies in five steps: Recalling, non-recursive deduction, deduction via chunk substitution, first with lexemes, then without lexemes and finally abduction.  #"]},{"title":"1 recalling from POS (a) and lexeme (i)","paragraphs":["RETURN","IF (","",") #"]},{"title":"2 deduction on the basis of POS (a)","paragraphs":["RETURN","IF (","",") #"]},{"title":"3 deductive, recursive parsing with POS and lexeme","paragraphs":["# Substitutions are defined as in TAGs (Joshi, 2003) IF","(","","↝","","  ","","","","","","","","",""," ) RETURN  ","↝       ","# deduction","↝","","↝","","↝","","   ","","",""," ) #"]},{"title":"4a deductive recursive parsing with lexeme,","paragraphs":["#"]},{"title":"4b compared to abductive parsing","paragraphs":["IF (","","","","↝"," ","","","","","","","","","","","","",""," )","RETURN  ","("," ",", #abduction"," ","↝     "," ","#deduction","↝  ","","","","   ","","",""," )) #"]},{"title":"5 abduction as robust parsing solution","paragraphs":["RETURN "," Figure 3: Abductive parsing with k-nn retrieval and adaptation of retrieved examples."," ","RETURN   ","","","","  ","","",""," ","","","","   ","",""," ","accuracy and speed of the parser ( ⊐ ","⊐","↝","▷","","","◁","▷↝","(recall,precision,f-score,time)). Then, we","parse a large corpus (","","◁","▷   ","","). A","filter criterion that works on the explanation ap-","plies. We train those trees which pass the filter","to the parser (  ◁","▷","","","","","","","","",").","Then the parsing accuracy and speed is tested","against the same training corpus (","⊐","","⊐","","▷","","","◁","▷↝ (recall,precision,f-score,time)).","Sections of the Chinese Sinica Treebank (Huang","et al., 2000) are used as seed-treebank and gold stan-","dard for parsing evaluation. Seed-corpora range be-","tween 1.000 and 20.000 trees. We train them to","the parser OCTOPUS (Streiter, 2002a). This parser","integrates memory- deduction- and abduction-based","parsing in a hierarchy of preferences, starting from","1 memory-based parsing, 2 non-recursive deductive","parsing, 3 recursive deductive parsing and 5 finally","abductive parsing (Fig. 2).","Learning the seed corpora (","","◁▷","↝↝","↝ ↝↝ ",")","results in ↝↝ ","","↝ ↝↝",". For each",""," ↝↝   ↝","↝↝ ",", a POS tagged corpus with","","",""," is parsed, producing the corpora","","","  ↝ ◁","",". The corpus used is a subset of","the 5 Million word Sinica Corpus (Huang and Chen,","1992).","For every","⌄","the parser produces one parse-","tree",""," ⊐","","and an explanation. The explanation has the form of a derivation tree in TAGs, c.f (Joshi, 2003). The deduction and abduction steps are visible in the explanation. Filters apply on the explanation and create sub-corpora that belong to one inference type. The first filter requires the explanation to contain only one non-recursive deduction, i.e. only parsing step 2. As deductive parsing is attempted after memory-based parsing (1),","","","","","holds. A second filter extracts those structures, which","are obtained by parsing step 4a or 5 where only","one POS-labels may be different in the last char-","acters (e.g.  ◁","▷","⊴","⊐",""," ","⊴","⊐  ). The resulting corpora are       ⊵▷","  ↝ ◁","","    ▷","and       ▷","  ↝ ◁","","","   ▷",". 3.2 The Acquired Knowledge We want to know whether or not new knowledge has been acquired and what the nature of this acquired knowledge is. As parsing was not recursive, we can approach the closure by the types of POS-sequences from all trees and their subtrees in a corpus. We contrast this with to the types of lexeme-sequences. The data show that only A-EBL increases the closure. But even when looking at lexemes, i.e. empirical knowledge, the A-EBL acquires richer information than D-EBL does. Figure 4: The number of types of POS-sequences as approximation of the closure with","","",", A-EBL and D-EBL. Below the number of type of LEXEMEsequences.","0","5000 10000 15000","20000","25000","30000","size of seed corpus 0 10000 20000 30000 40000 closure: number of POS−sequences closure with C_seed closure with C_seed + C_A closure with C_seed + C_D","0","","5000 10000","","15000"," 20000","","25000","","30000","","35000","","size of seed corpus  0 10000 20000 30000 40000 50000 60000 70000 number of LEXEME−sequences C_seed C_seed + C_A C_seed + C_D","The representatives of the cached parses is gauged by the percentage of top NPs and VPs (including Ss) as top-nodes. Fig 5 shows the bias of cached parses which is more pronounced with D-EBL than with A-EBL. Figure 5: The proportion of top-NPs and top-VP(S) in abduced and deduced corpora.","0","","5000 10000 ","15000","","20000","","size of seed corpus  0.00 20.00 40.00 60.00 80.00 % top−NP in C_D % top−NP in C_A % top−VP in C_D % top−VP in C_A % top−NP standard % top−VP standard 3.3 Evaluating Parsing The experiments consist in evaluating the parsing accuracy and speed for each  ","      ⊵▷","   "," ↝ ◁","","","   ▷",".","Figure 6: The parsing accuracy with abductive EBL","(","","˃   ) and deductive EBL (","","˃","","",").","0","5000 10000","15000","20000","25000 size of seed corpus 0.68 0.69 0.70 0.71 0.72 0.73 0.74 coverage (f−score) parsing accuracy with C_seed parsing accuracy with C_seed + C_A parsing accuracy with C_seed + C_D","We test the parsing accuracy on 300 untrained and randomly selected sentences using the f-score on unlabeled dependency relations. Fig. 6 shows parsing accuracy depending on the size of the seed-corpus. The graphs show side branches where we introduce the EBL-derived training material. This allows comparing the effect of A-EBL, D-EBL and hand-coded trees (the baseline). Fig. 7 shows the parsing speed in words per second (Processor:1000 MHz, Memory:128 MB) for the same experiments. Rising lines indicate a speed-up in parsing. We have interpolated and smoothed the curves.","Figure 7: The parsing time with A-EBL (","","˃","","",") and D-EBL ("," ˃","","",").","0","5000 10000","15000","20000","25000 size of seed corpus 44.00 46.00 48.00 50.00 52.00 54.00 words per second  parsing speed with C_seed parsing speed with C_seed + C_A parsing speed with C_seed + C_D","The experimental results confirm the drop in parsing accuracy with D-EBL. This fact is consistent across all experiments. With A-EBL, the parsing accuracy increases beyond the level of departure.","The data also show a speed-up in parsing. This speed-up is more pronounced and less data-hungry with A-EBL. Improving accuracy and efficiency are thus not mutually exclusive, at least for A-EBL."]},{"title":"4 Conclusions","paragraphs":["Explanation-based Learning has been used to speed-up natural language parsing. We show that the loss in accuracy results from the deductive basis of parsers, not the EBL framework. D-EBL does not extend the deductive closure and acquires only empirical (disambiguation) knowledge. The accuracy declines due to cached errors, the statistical bias the filters introduce and the usage of shortcuts with limited contextual information.","Alternatively, if the parser uses abduction, the deductive closure of the parser enlarges. This makes accuracy improvements possible - not a logical consequence. In practice, the extended deductive closure compensates for negative factors such as wrong parses or unbalanced distributions in the cache.","On a more abstract level, the paper treats the problem of automatic knowledge acquisition for Chinese NLP. Theory and practice show that abduction-based NLP applications acquire new knowledge and increase accuracy and speed. Future research will maximize the gains."]},{"title":"References","paragraphs":["Steven Abney. 1996. Partial Parsing via Finite-State Cascades. In Proceedings of the ESSLLI ’96 Robust Parsing Workshop.","Rens Bod and Ronald M. Kaplan. 1998. A probabilistic corpus-driven model for lexical-functional analysis. In COLING-ACL’98.","Michael Carl and Philippe Langlais. 2003. Tuning general translation knowledge to a sublanguage. In Proceedings of CLAW 2003, Dublin, Ireland, May, 15-17.","Hsing-Wu Chang. 1994. Word segmentation and sentence parsing in reading Chinese. In Advances in the Study of Chinese Language Processing, National Taiwan University, Taipei.","Keh-Jiann Chen. 1996. A model for robust Chinese parser. Computational Linguistics and Chinese Language, 1(1):183–204.","Yanis Dimopoulos and Antonis Kakas. 1996. Abduction and inductive learning. In L. De Taedt, editor, Advances in Inductive Logic Programming, pages 144– 171. IOS Press.","Chu-Ren Huang and Keh-Jiann Chen. 1992. A Chinese corpus for linguistics research. In COLING’92, Nantes, France.","Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhaoming Gao, and Kuang-Yu Chen. 2000. Sinica treebank: Design criteria, annotation guidelines and on-line interface. In M. Palmer, M. Marcus, A. K. Joshi, and F. Xia, editors, Proceedings of the Second Chinese Language Processing Workshop, Hong Kong, October. ACL.","Aravind K. Joshi. 2003. Tree-adjoining grammars. In R. Mitkov, editor, The Oxford Handbook of Computational Linguistics. Oxford University Press, Oxford.","Yves Lepage. 1999. Open set experiments with direct analysis by analogy. In Proceedings NLPRS’99 (Natural Language Processing Pacific Rim Symposium), pages 363–368, Beijing.","Gary F. Marcus, Steven Pinker, Michael Ullman, Michelle Hollander, John T. Rosen, and Fei Xu. 1992. Overregularization in Language Learning. Monographs of the Society for Research in Child Development, 57 (No. 4, Serial No. 228).","Steven Minton. 1990. Quantitative results concerning the utility problem of explanation-based learning. Artificial Intelligence, 42:363–393.","Tom S. Mitchel, R. Keller, and S. Kedar-Cabelli. 1986. Explanation-based generalization: A unifying view. Machine Learning, 1(1).","Günter Neumann. 1994. Application of explanation-based learning for efficient processing of constraint-based grammars. In The 10th Conference on Artificial Intelligence for Applications, San Antonio, Texas.","Manny Rayner and Christer Samuelsson. 1994. Corpus-based grammar specification for fast analysis. In Spoken Language Translator: First Year Report, SRI Technical Report CRC-043, pg. 41-54.","Khalil Sima’an. 1997. Explanation-based leaning of partial-parsers. In W. Daelemans, A. van den Bosch, and A. Weijters, editors, Workshop Notes of the ECML/ML Workshop on Empirical Learning of Natural Language Processing Tasks, pages 137–146, Prague, Czech Republic, April.","Bangalore Srinivas and Aravind K. Joshi. 1995. Some novel applications of explanation-based learning to parsing lexicalized tree-adjoining grammars. In 33th Annual Meeting of the ACL, Cambridge, MA.","Bangalore Srinivas and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.","Oliver Streiter, Judith Knapp, and Leonhard Voltmer. 2003. Gymn@zilla: A browser-like repository for open learning resources. In ED-Media, World Conference on Educational Multimedia, Hypermedia & Telecommunications, Honolulu, Hawaii, June, 23-28.","Oliver Streiter. 2002a. Abduction, induction and memorizing in corpus-based parsing. In ESSLLI-2002 Workshop on ”Machine Learning Approaches in Computational Linguistics”, pages 73–90, Trento, Italy, August 5-9.","Oliver Streiter. 2002b. Treebank development with deductive and abductive explanation-based learning: Exploratory experiments. In Workshop on Treebanks and Linguistic Theories 2002, Sozopol, Bulgaria, September 20-21.","Prasad Tadepalli. 1991. A formalization of explanation-based macro-operator learning. In IJCAI, Proceedings of the International Joint Conference of Artificial Intelligence, pages 616–622, Sydney, Australia. Morgan Kaufmann.","Chunfa Yuang, Changming Huang, and Shimei Pan. 1992. Knowledge acquisition and Chinese parsing based on corpus. In COLING’92.","Jakub Zavrel and Walter Daelemans. 1997. Memory-based learning: Using similarity for smoothing. In W. Daelemans, A. van den Bosch, and A. Weijters, editors, Workshop Notes of the ECML/ML Workshop on Empirical Learning of Natural Language Processing Tasks, pages 71–84, Prague, Czech Republic, April."]}]}
