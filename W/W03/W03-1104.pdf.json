{"sections":[{"title":"A Differential LSI Method for Document Classification Liang Chen Computer Science Department University of Northern British Columbia Prince George, BC, Canada V2N 4Z9 chenl@unbc.ca Naoyuki Tokuda R & D Center, Sunflare Company Shinjuku-Hirose Bldg., 4-7 Yotsuya Sinjuku-ku, Tokyo, Japan 160-0004 tokuda n@sunflare.co.jp Akira Nagai Advanced Media Network Center Utsunomiya University Utsunomiya, Tochigi, Japan 321-8585 anagai@cc.utsunomiya-u.ac.jp Abstract","paragraphs":["We have developed an effective probabilistic classifier for document classification by introducing the concept of the differential document vectors and DLSI (differential latent semantics index) spaces. A simple posteriori calculation using the intra- and extra-document statistics demonstrates the advantage of the DLSI space-based probabilistic classifier over the popularly used LSI space-based classifierin classification performance."]},{"title":"1 Introduction","paragraphs":["This paper introduces a new efficient supervised document classification procedure, whereby given a number of labeled documents preclassified into a finite number of appropriate clusters in the database, the classifier developed will select and classify any of new documents introduced into an appropriate cluster within the learning stage.","The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000).","In view of the inherent fle xibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Schütze and Silverstein, 1997), the firstproblem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace.","Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Kirby, 1987; Turk and Pentland, 1991), the Latent Semantic Indexing (LSI) method has proved to be a most efficient method for the dimensionality reduction scheme in document analysis and extraction, providing a powerful tool for the classifier (Schütze and Silverstein, 1997) when introduced into document retrieval with a good performance confirmed by empirical studies (Deerwester et al., 1990; Berry et al., 1999; Berry et al., 1995).The LSI method has also demonstrated its efficiency for automated cross-language document retrieval in which no query translation is required (Littman et al., 1998).","In this paper, we will show that exploiting both of the distances to, and the projections onto, the LSI space improves the performance as well as the robustness of the document classifier. To do this, we introduce, as the major vector space, the differential LSI (or DLSI) space which is formed from the differences between normalized intra- and extra-document vectors and normalized centroid vectors of clusters where the intra- and extra-document refers to the documents included within or outside of the given cluster respectively. The new classifiersets up a Baysian posteriori probability function for the differential document vectors based on their projections on DLSI space and their distances to the DLSI space, the document category with a highest probability is then selected. A similar approach is taken by Moghaddam and Pentland for image recognition (Moghaddam and Pentland, 1997; Moghaddam et al., 1998).","We may summarize the specific features introduced into the new document classification scheme based on the concept of the differential document vector and the DLSI vectors:","1. Exploiting the characteristic distance of the differential document vector to the DLSI space and the projection of the differential document onto the DLSI space, which we believe to denote the differences in word usage between the document and a cluster’s centroid vector, the differential document vector is capable of capturing the relation between the particular document and the cluster.","2. A major problem of context sensitive semantic grammar of natural language related to synonymy and polysemy can be dampened by the major space projection method endowed in the LSIs used.","3. A maximum for the posteriori likelihood function making use of the projection of differential document vector onto the DLSI space and the distance to the DLSI space provides a consistent computational scheme in evaluating the degree of reliability of the document belonging to the cluster.","The rest of the paper is arranged as follows: Section 2 will describe the main algorithm for setting up the DLSI-based classifier. A simple example is computed for comparison with the results by the standard LSI based classifier in Section 3. The conclusion is given in Section 4."]},{"title":"2 Main Algorithm 2.1 Basic Concepts","paragraphs":["A term is definedas a word or a phrase that appears at least in two documents. We exclude the so-called stop words such as “a”, “the” , ”of” and so forth. Suppose we select and list the terms that appear in the documents as ",".","For each document","in the collection, we assign","each of the terms with a real vector      , with     , where    is the local weighting of the term ","","in the document indicating","the significance of the term in the document, while"," ","is a global weight of all the documents, which is a parameter indicating the importance of the term in representing the documents. Local weights could be either raw occurrence counts, boolean, or logarithms of occurrence counts. Global ones could be no weighting (uniform), domain specific,or entropy weighting. Both of the local and global weights are thoroughly studied in the literatures (Raghavan and Wong, 1986; Luhn, 1958; van Rijsbergen, 1979; Salton, 1983; Salton, 1988; Lee et al., 1997), and will not be discussed further in this paper. An example will be given below:      and        ","      ","where ","    ,","is the total number of times that term   appears in the collection,  the number of times the term   appears in the document",", and  the number of documents in the collection. The document vector","  ","  ","  can be normalized as      by the following formula:   ","       ","    (1) The normalized centroid vector        "," ","of a cluster can be calculated in terms of the normalized vector as ","  ","  ","    , where  is a mean vector of the member documents in the cluster which are normalized as      ; i.e., "," ","  ","    ",". We can always","take","itself as a normalized vector of the cluster.","A differential document vector is definedas","","  ","where  and ","are normalized document vec-","tors satisfying some criteria as given above.","A differential intra-document vector","is the dif-","ferential document vector definedas","","","  , where   and ","are two normalized document vectors of","same cluster.","A differential extra-document vector","is the","differential document vector defined as","","","  ,","where  and ","are two normalized document vectors of different clusters. The differential term by intra- and extra-document matrices","","and","are respectively defined as a matrix, each column of which comprise a differential intra- and extra- document vector respectively. 2.2 The Posteriori Model","Any differential term by document -by-  matrix","of , say, of rank       ",", whether it is a differential term by intra-document matrix"," or a differential term by extra-document matrix can be decomposed by SVD into a product of three matrices:    , such that  (left singular matrix) and ","(right singular matrix) are an -by-"," and","-by- ","unitary matrices respectively with the","first","columns of U and V being the eigenvectors of","  and "," respectively. Here  is called singular matrix expressed by"," ","diag    ","","),","where  are nonnegtive square roots of eigen values","of ",",","","","for","and","   for",". The diagonal elements of ","are sorted in the decreasing order of magnitude. To obtain a new reduced matrix   , we simply keep the k-by-k","leftmost-upper corner matrix (",") of ",", deleting other terms; we similarly obtain the two new matrices   and  ","by keeping the left most columns of  and  respectively. The product of   ,   and   ","provide a reduced matrix","","of","which ap-","proximately equals to",".","How we choose an appropriate value of",", a re-","duced degree of dimension from the original matrix,","depends on the type of applications. Generally we","choose  ","for"," "," , and the corresponding","is normally smaller for the differential term by intra-document matrix than that for the differential term by extra- document matrix, because the differential term by extra-document matrix normally has more columns than the differential term by intra-document matrix has.","Each of differential document vector","could find a projection on the","dimensional fact space spanned by the","columns of   . The projection can easily be obtained by   ","",".","Noting that the mean","","of the differential intra-","(extra-) document vectors are approximately",", we","may assume that the differential vectors formed fol-","lows a high-dimensional Gaussian distribution so","that the likelihood of any differential vector","will","be given by ",""," ","","   ","","       ","    ","","where",""," ","",""," ","",", and","is the covariance of","the distribution computed from the training set ex-","pressed","     . Since   constitutes the eigenvalues of  , we have    ","   ",", and thus we have   ","     ","  ","","    ","","","","","  ","","",""," , where","","","","",""," ","","  ","    . Because ","is a diagonal matrix,  ","","can be repre-","sented by a simpler form as:","","  ",""," ","        . It is most convenient to estimate it as ","   ","","                     ","     ","where     ","     ","  ","",". In practice,","","(",")","could be estimated by fitting a function (say,   )","to the available  (","), or we could let","","    "," ","since we only need to compare the rela-","tive probability. Because the columns of  are or-thogonal vectors,   ","","","  ","","","could be estimated by    ","  ","    . Thus, the likelihood function   "," could be estimated by  ","  ","","   ","    ","   ","     ","       ","","  ","    ","    ","   "," (2) where","","","   ",", ","","  ","","  ","  ","    ,","          ","  ","",", and","is the rank of matrix",". In","practice, may be chosen as    ","  , and ","may be","substituted for",". Note that in equation (2), the term","    ","","","describes the projection of","onto the DLSI","space, while","",""," approximates the distance from","to DLSI space. When both ","  "," and   ","","are computed, the Baysian posteriori function can be computed as: ","","   "," ",""," ","","  ","  ","  ","     ",""," ","","  ","   where   ","  is set to    where   is the number","of clusters in the database 1 while  ","  is set to       . 2.3 Algorithm","2.3.1 Setting up the DLSI Space-Based","Classifier","1. By preprocessing documents, identify terms either of the word and noun phrase from stop words.","2. Construct the system terms by setting up the term list as well as the global weights.","3. Normalize the document vectors of all the collected documents, as well as the centroid vectors of each cluster.","4. Construct the differential term by intra-document matrix    ","",", such that each of its","column is an differential intra-document vec-","tor2 .","5. Decompose","",", by an SVD algorithm, into"," ","    "," "," (   ","diag       ","  ,","followed by the composition of   ","","","         giving an approximate","","in terms","of an appropriate","",", then evaluate the likeli-","hood function:","","  ","    ","","","","    "," ","       "," ","      ","      ","","","  ","","","      ","","    ","","     ","(3) 1","","can also be set to be an average number of recalls","divided by the number of clusters in the data base if we do not","require that the clusters are non-overlapped 2 For a cluster with","elements, we may include at most ","differential intra-document vectors in","","to avoid the linear","dependency among columns where","","","   ",", ","","  ","","      ","   ,  "," ","   ","","              , and","","is the rank of","matrix","",". In practice,","","may be set to   ,","and  to ","","   ","","  if both   and","are sufficiently large.","6. Construct the term by extra- document matrix      ",", such that each of its column is an","extra- differential document vector.","7. Decompose  , by exploiting the SVD al-","gorithm, into","","    "," "," (   ","diag "," "," ","  , then with a proper","",", de-","finethe","","","","   ","","","","","","to approximate"," . We now definethe likelihood function as,","","  ","","   ","    ","    ","        "," ","  ","","   ","      ","","","  ","","","  ","  ","","   ","","","      (4) where","","","   ","",", ","","  ","","      ","   ,  "," ","   ","","   ","   ","  ","   ,","","is the rank of","matrix","",". In practice,  may be set to"," ,","and ","to ","    ","","  if both  ","and are sufficientlylarge. 8. Definethe posteriori function: ","","   ",""," ","  ","    ",""," ","","  ","   ","  ","     (5)     is set to    where   is the number of clusters in the database and     is set to     ","  .","2.3.2 Automatic Classificationby DLSI Space-Based Classifier","1. A document vector is set up by generating the terms as well as their frequencies of occurrence in the document, so that a normalized document vector","is obtained for the document from equation (1). For each of the clusters of the data base, repeat the procedure of item 2-4 below.","2. Using the document to be classified,construct a differential document vector","","","","",", where ","is the normalized vector giving the center or centroid of the cluster.","3. Calculate the intra-document likelihood function ","  ","",", and calculate the extra- document likelihood function   "," for the document.","4. Calculate the Bayesian posteriori probability function ","","   "," . 5. Select the cluster having a largest ","","   "," as the recall candidate."]},{"title":"3 Examples and Comparison 3.1 Problem Description","paragraphs":["We demonstrate our algorithm by means of numerical examples below. Suppose we have the following 8 documents in the database:   : Algebra and Geometry Education System.  ",": The Software of Computing Machinery.  ",": Analysis and Elements of Geometry.  ",": Introduction to Modern Algebra and Geometry.  ",": Theoretical Analysis in Physics.  ",": Introduction to Elements of Dynamics.  ",": Modern Alumina.  ",": The Foundation of Chemical Science.","And we know in advance that they belong to 4 clusters, namely,      ",",        ,      ","","","and","      ","where  belongs to Computer related field, ","to Mathematics,","","to","Physics, and","","to Chemical Science. We will show,","as an example, below how we will set up the classi-","fierto classify the following new document: ",": “The Elements of Computing Science.” We should note that a conventional matching","method of “common” words does not work in this","example, because the words “compute” and, “sci-","ence” in the new document appear in ","and","","separately, while the word “elements” occur in both"," ","and","","simultaneously, giving no indication on the appropriate candidate of classificationsimply by counting the “common” words among documents. We will now set up the DLSI-based classifierand LSI-based classifierfor this example. First, we can easily set up the document vectors of the database giving the term by document matrix by simply counting the frequency of occurrences; then we could further obtain the normalized form as in Table 1. The document vector for the new document ","is given by:","                                     , and in normalized form by                                                           . 3.2 DLSI Space-Based Classifier The normalized form of the centroid of each cluster is shown in Table 2.","Following the procedure of the previous section, it is easy to construct both the differential term by intra-document matrix and the differential term by extra-document matrix. Let us denote the differential term by intra-document matrix by         ","     "," ",""," ",""," ",""," ",""," ","and the differential term by extra-document matrix by           ","       ","    "," ","respectively.","Note that the","","’s and ","’s can be found in the ma-","trices shown in tables 1 and 2.","Now that we know","","and",", we can de-","compose them into ","    "," "," and","     "," "," by using SVD algorithm, where                              0.25081 0.0449575 -0.157836 -0.428217 0.130941 0.172564 0.143423 0.0844264 -0.240236 0.162075 -0.043428 0.257507 -0.25811 -0.340158 -0.282715 -0.166421 -0.237435 -0.125328 0.439997 -0.15309 0.300435 -0.391284 0.104845 0.193711 0.0851724 0.0449575 -0.157836 0.0549164 0.184643 -0.391284 0.104845 0.531455 -0.25811 -0.340158 -0.282715 -0.166421 0.135018 0.0449575 -0.157836 -0.0904727 0.466072 -0.391284 0.104845 -0.289423 -0.237435 -0.125328 0.439997 -0.15309 0.296578 0.172564 0.143423 -0.398707 -0.124444 0.162075 -0.043428 -0.0802377 -0.25811 -0.340158 -0.282715 -0.166421 -0.237435 -0.125328 0.439997 -0.15309 0.0851724 0.0449575 -0.157836 0.0549164 -0.124444 0.162075 -0.043428 -0.0802377                             ","diag","","","","","  ","","","","","","       0.465291 0.234959 -0.824889 0.218762 -0.425481 -2.12675E-9 1.6628E-9 0.904967 -0.588751 0.733563 -0.196558 -0.276808 0.505809 0.637715 0.530022 0.237812 ","   ","                           0.00466227 -0.162108 0.441095 0.0337051 -0.214681 0.13568 0.0608733 -0.387353 0.0265475 -0.210534 -0.168537 -0.529866 -0.383378 0.047418 -0.195619 0.0771912 0.216445 0.397068 0.108622 0.00918756 0.317607 -0.147782 -0.27922 0.0964353 0.12743 0.0388027 0.150228 -0.240946 0.27444 -0.367204 -0.238827 -0.0825893 -0.383378 0.047418 -0.195619 0.0771912 -0.0385053 -0.38153 0.481487 -0.145319 0.19484 -0.348692 0.0116464 0.371087 0.216445 0.397068 0.108622 0.00918756 -0.337448 -0.0652302 0.351739 -0.112702 0.069715 0.00888817 -0.208929 -0.350841 -0.383378 0.047418 -0.195619 0.0771912 0.216445 0.397068 0.108622 0.00918756 0.12743 0.0388027 0.150228 -0.240946 0.069715 0.00888817 -0.208929 -0.350841                            ","","diag","","","","","","","","","","","","","","","","","","","","","","","","","  ","    0.200663 0.901144 -0.163851 0.347601 -0.285473 -0.0321555 0.746577 0.600078 0.717772 -0.400787 -0.177605 0.540952 -0.60253 -0.162097 -0.619865 0.475868 "," ","We now choose the number","in such a way that","","","   "," remains sufficiently large. Let us choose  ","  ","","","and","","  ","","","to test the","classifier. Now using equations (3), (4) and (5),","we can calculate the ","  "," , ","  "," and finally ","","   "," for each differential document vec-","tor ","","  ","(         ) as shown in Ta-","ble 3. The  having a largest ","","   ","","","","is chosen as the cluster to which the new document  belongs. Because both   ,   are actually quite","small, we may here set "," ","   ","","              ,","and "," ","   ","","   ","   ","      . The last row of Ta-","ble 3 clearly shows that Cluster ",", that is, “Math-","ematics” is the best possibility regardless of the pa-","rameters ","  ","","","or","","  "," ","chosen,","showing the robustness of the computation. 3.3 LSI Space-Based Classifier As we have already explained in Introduction, the LSI based-classifierworks as follows: First, employ an SVD algorithm on the term by document matrix to set up an LSI space, then the classificationis completed within the LSI space.","Using the LSI-based classifier, our experiment show that, it will return","",", namely “Physics”, as the most likely cluster to which the document","belongs. This is obviously a wrong result. 3.4 Conclusion of the Example For this simple example, the DLSI space-based approach findsthe most reasonable cluster for the document “The elements of computing science”, while the LSI approach fails to do so."]},{"title":"4 Conclusion and Remarks","paragraphs":["We have made use of the differential vectors of two normalized vectors rather than the mere scalar cosine of the angle of the two vectors in document classification procedure, providing a more effective means of document classifier. Obviously the concept of differential intra- and extra-document vectors imbeds a richer meaning than the mere scalar measure of cosine, focussing the characteristics of each document wheere the new classifier demonstrates an improved and robust performance in document classification than the LSI-based cosine approach. Our model considers both of the projections and the distances of the differential vectors to the DLSI spaces, improving the adaptability of the conventional LSI-based method to the unique characteristics of the individual documents which is a common weakness of the global projection schemes including the LSI. The simple experiment demonstrates convincingly that the performance of our model outperforms the standard LSI space-based approach. Just as the cross-language ability of LSI, DLSI method should also be able to be used for document classification of docuements in multiple languages. We have tested our method using larger collection of texts, we will give details of the results elsewhere. ."]},{"title":"References","paragraphs":["M. Benkhalifa, A. Bensaid, and A Mouradi. 1999. Text categorization using the semi-supervised fuzzy c-means algorithm. In 18th International Conference of the North American Fuzzy Information Processing Society, pages 561–565.","Michael W. Berry, Susan T. Dumais, and G. W. O’Brien. 1995. Using linear algebra for intelligent information retrieval. SIAM Rev., 37:573–595.","Michael W. Berry, Zlatko Drmac, and Elizabeth R. Jessup. 1999. Matrices, vector spaces, and information retrieval. SIAM Rev., 41(2):335–362. Table 1: The normalized document vectors","  ","","","","",""," Algebra 0.5 0 0 0.5 0 0 0 0 Alumina 0 0 0 0 0 0 0.707106781 0 Analysis 0 0 0.577350269 0 0.577350269 0 0 0 Chemical 0 0 0 0 0 0 0 0.577350269 Compute 0 0.577350269 0 0 0 0 0 0 Dynamics 0 0 0 0 0 0.577350269 0 0 Education 0.5 0 0 0 0 0 0 0 Element 0 0 0.577350269 0 0 0.577350269 0 0 Foundation 0 0 0 0 0 0 0 0.577350269 Geometry 0.5 0 0.577350269 0.5 0 0 0 0 Introduction 0 0 0 0.5 0 0.577350269 0 0 Machine 0 0.577350269 0 0 0 0 0 0 Modern 0 0 0 0.5 0 0 0.707106781 0 Physics 0 0 0 0 0.577350269 0 0 0 Science 0 0 0 0 0 0 0 0.577350269 Software 0 0.577350269 0 0 0 0 0 0 System 0.5 0 0 0 0 0 0 0 Theory 0 0 0 0 0.577350269 0 0 0","Table 2: The normalized cluster centers  "," ","","",""," Algebra 0.353553391 0.311446376 0 0 Alumina 0 0 0 0.5 Analysis 0 0.359627298 0.40824829 0 Chemical 0 0 0 0.40824829 Compute 0.40824829 0 0 0 Dynamics 0 0 0.40824829 0 Education 0.353553391 0 0 0 Element 0 0.359627298 0.40824829 0 Foundation 0 0 0 0.40824829 Geometry 0.353553391 0.671073675 0 0 Introduction 0 0.311446376 0.40824829 0 Machine 0.40824829 0 0 0 Modern 0 0.311446376 0 0.5 Physics 0 0 0.40824829 0 Science 0 0 0 0.40824829 Software 0.40824829 0 0 0 System 0.353553391 0 0 0 Theory 0 0 0.40824829 0","Table 3: Classificationwith DLSI space-based classifier","","",""," ","","","","","","",":","","","  "," ","  ","   ","   ","  "," ","  ","       ","","-0.085338834 -0.565752063 -0.368120678 -0.077139955 -0.085338834 -0.556196907 -0.368120678 -0.077139955","-0.404741071 -0.403958563 -0.213933843 -0.250613624","-0.164331163 0.249931018 0.076118938 0.35416984","","  ","","0.000413135 0.000430473 0.00046034 0.000412671 3.79629E-5 7.03221E-5 3.83428E-5 3.75847E-5","","   ","-0.281162007 0.022628465 -0.326936108 0.807673935 -0.281162007 -0.01964297 -0.326936108 0.807673935 -0.276920807 0.6527666 0.475906836 -0.048681069 -0.753558043 -0.619983845 0.258017361 -0.154837357","   ","","0.002310807 0.002065451 0.002345484 0.003140447 0.003283825 0.001838634 0.001627501 0.002118787","    ","0.056242843 0.064959115 0.061404975 0.041963635 0.003838728 0.012588493 0.007791905 0.005878172","Scott Deerwester, Susan T. Dumais, Grorge W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391– 407.","Jennifer Farkas. 1994. Generating document clusters using thesauri and neural networks. In Canadian Conference on Electrical and Computer Engineering, volume 2, pages 710–713.","H. Hyotyniemi. 1996. Text document classification with self-organizing maps. In STeP ’96 - Genes, Nets and Symbols. Finnish Artificial Intelligence Conference, pages 64–72.","M. Iwayama and T. Tokunaga. 1995. Hierarchical bayesian clustering for automatic text classification. In Proceedings of the Fourteenth International Joint Conference on ArtificialIntelligence, volume 2, pages 1322–1327.","Wai Lam and Kon-Fan Low. 1997. Automatic document classification based on probabilistic reasoning: Model and performance analysis. In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, volume 3, pages 2719–2723.","D. L. Lee, Huei Chuang, and K. Seamons. 1997. Docu-ment ranking and the vector-space model. IEEE Software, 14(2):67–75.","Wei Li, Bob Lee, Franl Krausz, and Kenan Sahin. 1991. Text classification by a neural network. In Proceedings of the Twenty-Third Annual Summer Computer Simulation Conference, pages 313–318.","M. L. Littman, Fan Jiang, and Greg A. Keim. 1998. Learning a language-independent representation for terms from a partially aligned corpus. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 314–322.","H. P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2):159–165, April.","D. Merkl. 1998. Text classification with self-organizing maps: Some lessons learned. Neurocomputing, 21(1-3):61–77.","B. Moghaddam and A. Pentland. 1997. Probabilistic visual learning for object representation. IEEE Trans. Pattern Analysis and Machine Intelligence, 19(7):696– 710.","B. Moghaddam, W. Wahid, and A. Pentland. 1998. Beyond eigenfaces: Probabilistic matching for face recognition. In The 3rd IEEE Int’l Conference on Automatic Face & Gesture Recognition, Nara, Japan, April.","Kamal Nigam, Andrew Kachites MaCcallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using em. Machine Learning, 39(2/3):103–134, May.","V. V. Raghavan and S. K. M. Wong. 1986. A critical analysis of vector space model for information retrieval. Journal of the American Society for Information Science, 37(5):279–87.","Gerard Salton. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.","Gerard Salton. 1988. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513–524.","Hinrich Schütze and Craig Silverstein. 1997. Projections for efficient document clustering. In Proceedings of SIGIR’97, pages 74–81.","L. Sirovich and M. Kirby. 1987. Low-dimensional procedure for the characterization of human faces. Journal of the Optical Society of America A, 4(3):519–524.","Borge Svingen. 1997. Using genetic programming for document classification. In John R. Koza, editor, Late Breaking Papers at the 1997 Genetic Programming Conference, pages 240–245, Stanford University, CA, USA, 13–16 July. Stanford Bookstore.","M. Turk and A. Pentland. 1991. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71–86.","C. J. van Rijsbergen. 1979. Information retrieval. Butterworths."]}]}