{"sections":[{"title":" SpokenDialogueforVirtualAdvisersinasemi-immersiveCommand andControlenvironment DominiqueEstival,MichaelBroughton,AndrewZschorn,ElizabethPronger HumanSystemsIntegrationGroup,CommandandControlDivision DefenceScienceandTechnologyOrganisation POBox1500,EdinburghSA5111 AUSTRALIA {Dominique.Estival,Michael.Broughton,AndrewZschorn}@dsto.defence.gov.au   Abstract","paragraphs":["We present the spoken dialogue system designed and implemented for Virtual Advisers in the FOCAL environment. Its architectureisbasedon:DialogueAgents using propositional attitudes, a Natural Language Understanding component using typed unification grammar, and a commercial speaker-independent speech recognition system. The current application aims to facilitate the multimedia presentation of military planning information in a semi-immersive environment."]},{"title":"1 Introduction","paragraphs":["In this paper, we present the spoken dialogue system implemented for communicating with the virtual advisers (VAs) in the Future Operations Centre Analysis Laboratory (FOCAL) at the Australian Defence Science and Technology Organisation(DSTO).Weareexperimentingwith the use of spoken dialogue with virtual conversational characters to access multi-media information during the conduct of military operations and in particular to facilitate the planningofsuchoperations.","Unlike telephone-based dialogue systems (Estival, 2002), which are mainly created for new commercial applications, dialogue systems for Command and Control applications (Moore et al. 1997) generally seek to simulate the military domain and therefore require an understanding of thatdomain."]},{"title":"2 UsingVirtualAdvisersinFOCAL","paragraphs":["FOCAL was established to \"pioneer a paradigm shiftincommandenvironmentsthroughasuperior use of capability and greater situation awareness\". The facility was designed to experiment with innovativetechnologiestosupportthisgoal,andit hasnowbeenrunningfortwoyears.","FOCAL contains a large-screen, semi-immersive virtual reality environment as its primary display, allowing vast quantities of informationtobedisplayed.OurcurrentVAscan be described as 3-dimensional \"Talking Heads\", i.e. only the head and upper portions of the body arerepresented.Theycandisplayexpression,lipsynchronisation and head movement, along with certain autonomous behaviours such as blinking and gaze (Taplin et al., 2001). These factors all combinetoaddlife-likenesstotheVAsandcreate moreengaginginteractionwithusers.","Presenting information via a Talking Head has been commercially demonstrated by the virtual newscaster “Ananova” (Ananova, 2002). Embodiedcharactersarealsobeingdevelopedand include the PPP (Andre, Rist and Muller, 1998) and Rea (Cassell, 2000). PPP is a cartoon style Personalized Plan-based Presenter that combines pointing, head movements and facial expressions to draw the viewer’s attention to the information being presented. Rea isa virtualreal-estateagent thattakesanactive role inconversation, shenods herheadtoindicateunderstandingofspokeninput, orcanraiseherhandtoindicateadesiretospeak.","Several VAs have been implemented for FOCAL, each having a particular role or knowledge expertise. For example, one adviser  may have specialist knowledge relating to legal issues, another may have information relating to the geography of a region. Each VA has a differentfacialappearance,voiceandmannerisms.","To demonstrate and evaluate the performance of VAs (and of the other FOCAL projects), a fictitious scenario has been developed that incorporates key elements of military planning at the operational level (see section 8). The VAs provide information rich briefs through the combineduseofspokenoutputviaText-to-Speech (TTS)andmultimedia.Relevantquestionscanbe asked at the end of the briefs through the use of spokendialogue."]},{"title":"3 Previousimplementation:Franco","paragraphs":["Asdescribedin(Taplinetal.,2001)thefirstVAin FOCAL, named Franco, was also an animated 3-dimensional \"Talking Head\" model, intended to either deliver prepared information, such as a briefing or slide show, or to interact conversationally with users. To demonstrate the conversational functionality (Broughton et al., 2002), it was implemented with a commercial speaker-dependent automated speech recogniser (ASR),DragonNaturallySpeakingTM.TheNatural Language understanding component was implemented in NatLink (Gould, 2001) and a simpleuser-drivendialoguemanagement,basedon key-word recognition and nesting of dialogue statestoprovidecontext,wasalsoimplementedin Python.","Franco has been successful in demonstrating the proof-of-concept of a VA in the FOCAL environment. Answering spoken questions about specific military assets and platforms, it also permits the display of other types of information such as pictures, animated video clips, tabular information from a database, and location details ondigitalmaps."]},{"title":"4 Improvements","paragraphs":["Although Franco was successful in demonstrating the potential usefulness of a VA in a Command andControlenvironmentforoperationalplanning, it suffers from certain limitations which we are nowaddressinginafollow-upproject.","The first limitation, and the easiest to remedy, was the unnaturalness of the synthetic voice we had given Franco. For greater effectiveness, we had to provide our VA with a more natural voice andwithan Australianaccent.Wechosethenew Australian TTS voice from Rhetorical, developed by Appen (rVoice, 2002). This required making some changes, some of them relatively important, to the interface with the talking head model to achieve lip-synchronisation, but that aspect of the workwillnotbeaddressedinthispaper.","The second limitation was the relative rigidity of the dialogue management strategy we were using. The alternative approach we have developed is to create Dialogue Agents implemented in ATTITUDE. This is described in section6.","The third limitation was due to the speaker-dependent nature of the ASR. While a speaker-dependent ASR allows greater flexibility in the inputto whichtheVAcanrespond,we wantedto develop a system which could not only be demonstrated by thefew people whohave trained the speech recogniser, but where visitors themselvescouldbeparticipantsandcouldinteract with the VA. Switching toa speaker-independent ASR led us to radically modify our Spoken Language Understanding component, and this is describedinsection7.","The new implementation we describe here has allowed us not only to address those three limitations, but also to alter fundamentally the architectureofthesystem,openingupthedialogue managementcomponentstocontrolandinteraction by other tools and agents in the FOCAL environment. The resulting system is now fully modular and provides scalability as well as flexibility.","Thisnewimplementationallowsustofocusour research into dialogue management issue, to investigate the use of ATTITUDE for dialogue management and to experiment with more natural languageinput."]},{"title":"5 Integration","paragraphs":["Communication between the various components ofthesystem(speechrecogniser,dialoguecontrol, virtual adviser control and multimedia display) is now achieved with the CoABS (Control of Agent Based Systems) Grid infrastructure (Global InfoTek,2002).TheCoABSGridwasdesignedto allowalargenumberofheterogeneousprocedural,  object-oriented and agent-based systems to communicate. Using the CoABS Grid as our infrastructure has allowed us to integrate all the components of the dialogue system and it will provideaneasywaytointegrateotheragentsanda variety of input and output devices. Communication between CoABS agents is accomplishedviastringmessages."]},{"title":"6 DialogueManagementwithATTITUDE","paragraphs":["ATTITUDE is a multi-agent architecture developed at DSTO, capable of representing and reasoning both with uncertainty and about multiple alternative scenarios (Lambert, 1999). It is a multi-agent extension of the MetaCon reactive planner developed for control of phased array radars on the Swedish Airborne Early Warning aircraft(LambertandRelbe,1998).ATTITUDEhas some similarities with Prolog and other logic programming languages as well as with AI research on blackboard and multi-agent architectures. Because ATTITUDE was designed specificallytosupporttheprogrammingofreactive systems, it possesses powerful facilities for handling interactions of the internal system entities,bothwitheachotherandwiththeexternal world.","ATTITUDEisveryhigh-level,weakly-typed,and thanks to the agent paradigm, it produces loosely coupled and modularised systems. For these reasons, and because ATTITUDE implements reasoningaboutpropositionalattitudes,itprovides a very attractive framework in which to develop and express dialogue management control strategies. It is worth emphasizing here that ATTITUDEis not merely a notation to represent speech acts or communicative acts between agents, but that it is actually the programming language and environment in which both the agents themselves and the control structure for interaction between the agents are implemented andexecuted.","BecauseATTITUDEhasneverbeenusedforthis purpose before, this is an interesting area of research in itself, and one of the goals of the projecthasbeentoseehowATTITUDEneedstobe extended to implement dialogue management. Further, this allows us to investigate how far attitude programming (see section 6.2) can go towardsexpressingspeechactsandcommunicative acttype.However,wedonotclaimtoemploythe full power of propositional attitudes in our implementation yet. This is another area of researchwhichwearenowexploring.Neitherare we yet at the stage where we could perform automatic detection of utterance type (Wright, 1998) or of dialogue act (Carberry and Lambert, 1999;PrasadandWalker,2002)."]},{"title":"6.1 Propositionalattitudes","paragraphs":["The ATTITUDE programming environment is so named because it utilisespropositional attitude instructions as programming instructions (this has beendubbedattitudeprogramming).Propositional attitudesareallegedmentalstatescharacterisedby propositional attitude expressions, which are the means by which individuals relate their own mentalbehaviourtoothers'. Propositional attitude instructions are of the form shownin(1).  (1)[subject][attitude][propositionalexpression]  In(1):","-[subject]denotestheindividualwhosemental","stateisbeingcharacterised;","- [propositional expression] describes some","propositionalclaimabouttheworld;and","-[attitude]expresses the subject'sdispositional","attitudetowardthatclaimabouttheworld."]},{"title":"6.2 ATTITUDEprogramming","paragraphs":["When software agentMary encounters the propositionalattitudeinstruction\"Freddesire[the door is closed]\",Mary will issue a message to softwareagentFredinstructingFredtodesirethat the door be closed. Similarly, when encountering thepropositionalattitudeinstruction\"Ibelieve[the sky is blue]\",Mary herself willattempt to believe thattheskyisblue.","An important characteristic of ATTITUDE programming is that each propositional attitude instruction either succeeds or fails, possibly with sideeffects,dependinguponwhethertherecipient agentisabletosatisfytheinstructionalrequest.As each propositional attitude instruction either succeeds or fails, the execution path selected through a network of propositional attitude instructions (routine) is determined by the successesandfailuresofthepropositionalattitude  instructionsattemptedalong the way. Thecontrol structure is therefore governed by asemantics of success.","Computational routines for a software agent arise by linking together particular choices of propositional attitudeinstructions.These networks ofpropositionalattitudeinstructionsthenprescribe recipesdefiningthepossiblementalbehaviourofa softwareagent."]},{"title":"6.3 TheATTITUDEDialogueAgents","paragraphs":["We have implemented a number of ATTITUDE DialogueAgents.ThemainagentinourDialogue Management architecture (shown in Figure 1) is theConductor. It is the agent responsible for the flowofinformationbetweentheotheragentsandit manages multi-modal interactions. The other agents, also described further in this section, are theSpeaker, theNLG (Natural Language Generator), theMMP(Multimedia Presenter) and severalIS(InformationSource)agents.Inaddition totheseagents,eachdialoguestate(seesection8) isalsoimplementedasanATTITUDEagent,withits ownsetofroutines. As explained in section 6.2, each ATTITUDE agent’s behaviour is programmed as a set of routines  Figure1.DialoguewithATTITUDE","The interaction between the ATTITUDE DialogueagentsisshowninFigure1,inwhichthe frame around the ATTITUDE agents can be interpretedasrepresentingtheCoABSgrid. SpeakerAgent Whenspeechfromtheuserhasbeendetectedand recognised, the attribute-value pairs for that utterance (see section 7) are sent toSpeaker. Speaker takes that information and produces a correspondingATTITUDEexpression,whichisthen forwardedtoConductor.","The linguistic coverage of the system is determinedbythegrammarswhichareavailableat each dialogue state. For now, the coverage is limited to a set of utterances appropriate for the briefing scenario described in section 8. These were used to define the Regulus1 grammars from whichtheNuancegrammarsarecompiled.Weare nowplanningtomovefromRegulus1toRegulus2, which will allow us to derive dialogue state grammarsfromalargeEnglishgrammarusingthe EBLstrategydescribedin(Rayneretal.,2002b)        "]},{"title":"Conductor","paragraphs":["Thisagentisresponsiblefordialogueflowcontrolandallothe rdialogue agentsmustregisterwithit. ConductorreceivescommunicativeactsfromSpeaker.Forexample: (whquestion(propertymig-29flying-range?value?units)) Thisqueryisforwardedontoallregisteredagents.Conductorchoosesthe mostappropriateresponsereceivedandsendsthistoMMPtopresentthe answer."]},{"title":"Speaker","paragraphs":["Speakerreceivesspeechrecognitionresultsintheform ofattribute-valuepairs,andtranslatestheseinto AttitudeexpressionstosendtoConductor."]},{"title":"InformationSource (IS)","paragraphs":["ThiscategoryofagentseachregisterwithConductorand interfacewithabackgrounddatasource,forexample,a databaseofaircraftproperties. Eachusestheirdatasourcetorespondtoqueriesfrom Conductor."]},{"title":"MultimediaPresenter (MMP)","paragraphs":["ThisagentreceivesalistofexpressionsfromConductoranddirectstheappropriateservicestopresent multimediadatatotheuser.Forexample: ((whanswer(propertymig-29flying-range810nautical-miles))(imagemig-29)) Inthiscase,MMPrequestsanEnglishformofthewhanswerexpressionandsendstheresulttotheTTS application.Similarly,anappropriateapplicationisdirectedt odisplaytherequestedimage."]},{"title":"NaturalLanguageGenerator (NLG)","paragraphs":["ReceivesexpressionsfromMMPandusestemplatestoreturn correspondingEnglishsentences. Englishquestionfromuser Nuance/Regulus Attitudeexpression Query Response Presentationdirectives NLGdirective TTS Englishstring VirtualAdvisorspeaking Englishstring Attribute/Valuepairs Multimediadisplayed  ConductorAgent Conductor takes an ATTITUDE expression from SpeakerandforwardsitontoalltheISagentsthat have registered with it. It then waits for all the responses to come back from those agents, in the formoflistsofexpressions.","Every responseConductor receives is put into its knowledge base, along with some extra information:","-Sender:whichISagentsenttheresponse.","- In-Reply-To: which previous communicative","actthisisaresponseto.","- Strength: whether every expression of the","response is 'strong' (the sender believes it is","either absolute truth or absolute negation) or if","one or more is 'weak' (the sender believes it is","neitherabsolutetruthnorabsolutenegation).","-Bound-State:ifthereareanyfreevariablesin","theresponse,orifitisfullyground.","- Unifiability: whether one or more of the","expressionsintheresponseisofthesameform","asSpeaker’sinitialexpression. The final expression inConductor’s knowledge baseisasshownin(2).  (2)(response?in-reply-to?sender?strength ?bound_state?unifiability?content)"," Given the initial expression fromSpeaker and the replies it receives from the IS agents,Conductor chooses the 'best' response. For example, a response that is strong, fully ground and unifies withSpeaker’s expression is deemed to be more relevant and informative than a response that is weak and contains free variables. Conductor forwardsthisresponsetoMMP. MultimediaPresenter(MMP) MMP iterates through the list of expressions sent byConductor and presents each expression to the user. MMPrecognises classes of expressions and choosesto presentthemusing certain media. For example, some expressions are instructions to changetheVAheadmodel,whileothersaretobe translatedintoEnglishsentencesandspokenbythe VA. For the latter functionMMP usesNLG(see below).","OthermediathroughwhichMMPcanchooseto present the information contained in the expressionsinclude:imageryfromadatabase(e.g. pictures of military platforms, or of strategic locations), video clips, images from weather or radar information sources, virtual video, 3-dimensional virtual battle space maps, textual informationandaudio. NaturalLanguageGenerator(NLG) For now,NLG uses templates to transform ATTITUDEexpressionsintoEnglish. Forexample, the instruction in (3) provides two possible responsesfortheATTITUDEexpressionspecified:1",""," (3)(property?assetoverview?valuetext) whanswerpriority10 ((response1(\"The\"?asset\"isa\"?value\".\")) ((response2(\"Iunderstandthatthe\"?asset\"isa \"?value\".\"))))  WhenNLG is first requested to generate the Englishoutputfortheexpressionin(4.a),intended to be a communicative act of typewhanswer, it uses the template given in (4.b), corresponding to \"response1\"in(3),toproducetheEnglishanswer givenin(4.c).  (4.a)(propertymig-29overview\"Russianmulti-role fighter\"text) b.(\"The\"?asset\"isa\"?value\".\") c.TheMig-29isaRussianmulti-rolefighter.  WhenNLGisrequestedasecondtimeto generate the output for (3), it uses the template in (5.a), corresponding to \"response 2\" in (3), to produce theEnglishanswergivenin(5.b).  (5.a)(\"Iunderstandthatthe\"?asset\"isa\"?value\".\") b.IunderstandthattheMig-29isaRussianmultirolefighter."," ThusNLG cycles throughthelist of templates for appropriateresponses.Prioritiescanalsobegiven to templates, enablingNLG to use general templatestogether with more specificandtailored ones.","It is clear that template-based language generation is too rigid for fully natural dialogues, andweintendtoexploremoreflexibletechniques after we implement a wider coverage English grammar;however,ithassofarbeensufficientfor  1 Variablesaredenotedwith\"?\",whiletextstrings(tobesent tospeechsynthesis,ordisplayedonaslide)arebetween doublequotes,\"\".  our purposes, namely to demonstrate and investigateagent-baseddialoguemanagement. InformationSourceAgent(IS) TheISagents,e.g.a WeatherAgentor aPlatform Capabilities Agent, can answer users' questions, eitherby using their own internal knowledge base orbyaccessingexternalInformationSources,such as a weather information server, or a database of military assets. AllIS agents register with Conductor, and when an expression is sent by Speaker,allISagentstrytorespondtoit."," ByusingtheCoABSGridastheinfrastructureand implementing the agent with ATTITUDE, we leave the architecture extremely flexible and scalable (Kahn and Della Torre Cicalese, 2001). For instance, it is possible to increase the amount of information at the system’s disposal during run-time by launching a newIS agent and by adding sometemplatestoNLG."]},{"title":"6.4 Dialoguedesign","paragraphs":["Fornow, the dialogue is specifiedasa finite state machineandisstillverymuchsystemdirected.In the briefing application (seesection8.1), the VAs first \"push\" the information that needs to be presented, as briefing officers do in a normal briefing.Someoftheinformationisalsopresented using visual aids, such as power point slides and maps for specifying location information. The information to be presented and the media to be usedaredeterminedbytheagentforthatparticular dialoguestate.","The VA then allows users to ask questions to repeat or clarify particular points, or to gain additionalinformation."]},{"title":"7 SpokenLanguageProcessing 7.1 Speaker-independentspeechrecognition","paragraphs":["Asstatedinsection4,oneofthemainmotivations formovingfromaspeaker-dependenttoaspeakerindependentASRwastoallowvisitorsinFOCAL the possibility of using the system themselves, rather than relying on a small set of trained individuals to run demonstrations. We chose to usetheNuanceToolkit(Nuance,2002)forseveral reasons: besides its reliability as a speaker-independent ASR for both telephone and microphone speech, Nuance 8.0 provides Australian-New Zealand English, as well as US andUKEnglish,acousticlanguagemodels.Even more importantly for our purposes, Nuance grammarscanbecompiledfromRegulus,ahigher-level language processing component which has already been used to develop several spoken dialogue systems in different domains (Rayner et al.,2001,RaynerandBouillon,2002)."]},{"title":"7.2 SpokenLanguageUnderstanding","paragraphs":["Following our decision to move from a speaker-dependent to a speaker-independent ASR, we decided to use Regulus to implement our Natural Language Understanding component. Regulus is an Open Source environment which compiles typed unification grammars into context-free grammar language models compatible with the Nuance Toolkit. It is \"written in a Prolog-based feature-value notation and compiles into Nuance GSL grammars.\"(Rayneretal.,2002a).Regulus isalsodescribedindetailin(Rayneretal.,2001).","The main motivation for using Regulus is the usual one of greater efficiency due to the more compact nature of a unification grammar representation compared with a context-free grammar. In addition, using Regulus to define a higherlevelgrammar,weareabletoobtainasour semantic representation a list of attribute-value pairs, and this permits a more sophisticated processingoftheinformationbytheotheragents.","Regulus also allows the development of bidirectional grammars, and we intend to make use ofthisfunctionalityinlaterimplementationsofthe NLG agent. However,for now, the grammars we have developed have been limited to recognition andunderstanding."]},{"title":"8 Currentapplicationimplementation 8.1 Dialoguescenario","paragraphs":["The scenario for the current application was developed by members of the Human Systems Integration (HSI) group and is grounded on their experience with, and observations of, military operational planning. It is based on a fictitious scenario developed for training (the examples givenherehaveallbeenmodified)andexemplifies  theJointMilitaryAppreciationProcess(JMAP)for military planningacrossthe three services (Army, Navy and Air Force). A sub-scenario was chosen for the development of the spoken dialogue with theVAs.2",""]},{"title":"8.2 Dialogueflow","paragraphs":["The structured nature of a military planning task such as this one makes it very easy to partition it intodifferentstages,whichcanthenbemappedto different dialogue states. In our dialogue script, each top-level dialogue state corresponds to a sectionoftheplanningexercise,givenin(6).  (6)Commander'sInitialGuidance","-CDF(ChiefofDefenceForces)Intent","-PlanningGuidance","-Constraints","-Restrictions","-LegalIssues","-CommandandControl  These6topleveldialoguestatesarethenfollowed byanOverallQuestionTime.","The mixed-initiative nature of the system can bemodelledinafinitestatediagram,allowingfor a) briefing-like system ‘pushes’, b) confirmation queriesfromthesystemandc)questionsfromthe user. However, because the system is primarily agent-based, the dialogue can also evolve dynamically.Forinstance,oncethesystemisina ‘question’ state, the dialogue flow then allows users to ask a number of questions, until they are satisfied,and thedialoguecanmove toadifferent state.","Each of the top level dialogue states also corresponds to anIS agent with its own set of ATTITUDE routines. These agents register with Conductor and act as experts in their particular fields (e.g., the Legal Issues adviser). The agents contain knowledge which they use to answer questionsposedtothembyConductor.Allagents have the ability to keep track of which state (or topic)theyarein.ThisallowsnotonlyConductor, but also the other dialogue agents, to distinguish between providing the user with new information orinformationthathasalreadybeenpresented.  2 ThisistheCommander’sinitialguidancetotheTheatre PlanningGroup(TPG),whichispartoftheMissionAnalysis sectionofJMAP."]},{"title":"8.3 KnowledgeRepresentation","paragraphs":["Thecurrentontologydevelopedforthisapplication is only a small part of the larger Knowledge Representationontologytobeusedthroughoutthe whole FOCAL system. For now, we only representtheconceptsneededinoursmalldomain, and their relationships are translated into ATTITUDE statements, allowing agents to draw inferences. For example, if a user can ask the question given in (7.a), it will be translated into the listof attribute valuepairs given in(7.b)and sent toSpeaker.Speaker then translates these attributevaluepairsintothe ATTITUDEexpression in(7.c)andforwardsitontoConductor.  (7.a) Whatdepartmentoverseesnegotiations withunionsandindustry? b. [questionwhatquestion,concept negotiation,attributeoversee,obj1department] c. conductordesire(comm_act(negotiation oversee?department)fromspeakertype whatquestionin-response-tonull)  As described in section 6, whenConductor poses thequestiontotheappropriateagents,theyrespond with the information in their knowledge base or information they can extract from a database. Agentsstoreknowledgeasbelievestatementssuch astheoneshownin(8):  (8) Ibelieve(negotiationoversee“department ofworkplacerelations”)  Thesebelievestatementsarethenunifiedwiththe propositions translated bySpeaker, and if unification is successful, a reply is sent back to Conductor. Finally,Conductorpasses the answer on toNLG to match a template and produce an Englishanswer,forinstance(9).  (9) TheDepartmentofWorkplaceRelations overseesnegotiationswithunionsandindustry.  An agent which has access to a database can also translate a user's question into the relevant databasequerytoobtaintheanswer.Animportant issue under research concerns the automatic derivation of ATTITUDE statements from a preexistingdatabase. "]},{"title":"8.4 SeveraldifferentVAs","paragraphs":["As explained above, each stage of the planning processispresentedtotheuserbyaparticularVA withitsassociatedISagentandtheVAthenallows users to ask further questions. Besides their specialised knowledge, the VAs are differentiated through different head models, different TTS voices(maleorfemale, differentregionalaccents) anddifferentpersonalities.","Onceadialoguestateiscompletedandtheuser has no further questions, the VA for that state sendsamessagetoConductortomovetothenext state. Conductor can then initiate the change in recognition grammar, voice for the next VA and modelforthenextVAhead.","Having several VAs coming on at different stagestopresentdifferentinformationallowsfora VA to be specialised in a particular domain, just as real briefing officers are during a real military planningexercise.","Fornow,weonlydisplayoneVAatatime,but weintendtoexperimentwithhavingmultipleVAs at the same time. The final state of the dialogue flowallowsuserstoaskquestionsaboutanyaspect of the planning process, and questions can be posedtoalltheVAs,soitwouldbenaturalforthe userstoseealltheVAsatthatstage."]},{"title":"8.5 RapidPrototypingandEvaluation","paragraphs":["The key word version developed previously (see Broughton et al., 2002) has been maintained as a rapid prototyping environment for evaluating new scripts and dialogues. It allows new dialogues to be quickly tested by entering suitable key words, sufficient to discriminate one question from another.Thissystemprovesfasterfortestingthan the more precise method of grammar building. Multiple response strings can be generated, providing more naturalness for those interacting with the VAs on a regular basis. By rapidly prototyping questions and responses, we can test the intuitiveness of expected questions and the smoothness and timeliness of responses, particularly when presented combined with multimedia.","The implemented system described here has sofar onlybeen tested with other members of the group,but demonstrationstovisitorsandpotential users will provide a more rigorous form of evaluation on an on-going basis. An evaluation phase for the project is scheduled for 2003-2004, during which time we will have access to more users and will be able to conduct more structured experiments."]},{"title":"9 NaturalInteractionwithVAs","paragraphs":["In addition to the ASR and TTS systems previously discussed, other technologies can be combined into the overall system to increase naturalnessofinteraction,andweareinvestigating speaker recognition as wellasa range ofpointing technologies.","The need for a speaker recognition system has emerged with the move to a speaker independent ASR.WithaspeakerdependentASR,userswould load their individual profile before use, thus enabling the system to know who was using it. With a speaker-independent ASR, a speaker recognition system would allow the VAs to recognise whoistalking tothemandenablethem to address known users by name. We plan to integrate within FOCAL the speaker recognition system which has been developed at DSTO (Roberts, 1998). This system uses statistical modelling techniques and is capable of both speaker identification (recognising users from a database of stored speech profiles) and speaker verification (verifying the identity of a particular user).","We are also proposing to use pointing techniques in combination with the speech and language technologies to build a multimodal system. Multimodal systems were originally demonstrated by Bolts (1980) and research is continuing across varied applications (e.g., Oviatt et al., 2000 and Gibbon et al., 2000). However, unlike systems such as MATCH (Johnston et al., 2002), where the issue is allowing multimodal interaction on portable devices with very small screens, in FOCAL we are concerned with ensuring thatusers getthe full benefit ofthe very large screen and with allowing several users to interact at a distance from the screen. It is also worth mentioning that, unlike the interactive systemdescribedin(Rickeletal.,2002),whichis concernedwithtraininginamilitaryenvironment, we are not trying to simulate a complete virtual worldwithembodiedagents.","However, we propose to include traditional pointingtechnologies,suchasthestandarddesktop  mouse,throughto3-dimensional trackingsystems for gaze, gesture and user tracking. This will involve integrating more complex language understanding, as information will need to be derived from both the user's utterance and from whatisbeingpointedto.Forexample,tointerpret an utterance such as (10) uttered while the user pointstoa locationonamap,weneedtoperform reference resolution on \"this region\", and match thatreferenttotheitembeingpointedat.  (10)Whatdoweknowaboutthisregion? "]},{"title":"10 Conclusion","paragraphs":["We have now implemented in FOCAL the infrastructure needed to perform spoken and multimodaldialogue withseveral VAs. Thisisof interestinitself,asitwillallowustocontinueour research on spoken language understanding and spokendialoguesystemsandalsotoaddressissues of language generation which have for now been left aside. Already we have been able to move from a rigid dialogue control structure, with very constrained input, to a more flexible and scalable control structure allowing real connectivity betweenagents.","Having moved to a speaker-independent ASR, andtakingadvantageoftheopensourcenatureof Regulus, we intend to pursue research issues regarding robust processing of spoken input, such asusinggrammarspecialisationfromacorpusand devisingtechniquesforignoringpartsoftheinput.","We have implemented a dialogue management architecture based on ATTITUDE agents which communicate with each other using propositional attitude expressions. Other agents can now be developed to perform additional functions, in particular to launch the display of other types of informationandtointerpretothertypesofinput.","This will allow us to explore how spoken dialogue with VAs can be combined with other virtual interaction technologies (e.g., gesture, pointing, gaze tracking). In this respect, the next step in our project is the development of a full fledgeMMP agent based on the framework describedin(ColineauandParis,2003).","","However,theworkwehavereportedheremust also be seen as part of the larger research programmeundertakenwithinFOCAL.Fromthis perspective, this work is of interest because it allows other members of the HSI group to pursue research in the usability of new technologies to perform the paradigm shift in command environments. In particular, this project is providing the support for further research into whether this way of presenting information is helpful in an operational command environment. It allows us to devise experiments to explore the crucial issue of trust in the information being presented,andhowtheway theinformationbeing presentedcanaffectthattrust.","Integratingspokendialoguewithplanningtools willalsoallowustoexplorewhetherVAscanhelp inmilitaryoperationplanning,andhowbesttouse thesetools. "]},{"title":"Acknowledgements","paragraphs":["We wish to thank the Chief of C2D, and the Director of Information Sciences Laboratory, for sponsoring and funding this work. We wish to acknowledge the work of Paul Taplin in integrating speech synthesis and lipsynchronisation, and the work of Benjamin Fry from the University of South Australia in developing the Regulus/Nuance grammars. Finallywewishtothanktheothermembersofthe HSIgroupinC2Dfortheirconstantandinvaluable helpwiththeFOCALproject. "]},{"title":"References","paragraphs":["Ananova.2002.http://www.ananova.com.","E. Andre, T. Rist, and J. Muller. 1998. Integrating Reactive and Scripted Behaviours in a Life-Like Presentation Agent,Proceedings of the Second International Conference on Autonomous Agents, 261-268. Appen.2002.http://www.appen.com.au.","R. A.Bolt.1980.\"Put-that-there\":voiceandgestureat the graphics interface. Proceedings of the SIGGRAPH,July,262-270.","Michael Broughton, Oliver Carr, Dominique Estival, Paul Taplin, Steven Wark, Dale Lambert. 2002. \"Conversing with Franco, FOCAL’s Virtual Adviser\". Conversation Characters Workshop, HumanFactors2002,Melbourne,Australia. ","Sandra Carberry and Lynn Lambert. 1999. \"A Process Model for Recognizing Communicative Acts and ModelingNegotiationSubdialogues\".Computational Linguistics.25,1,pp.1-53","Justine Cassell. 2000. Embodied Conversational Interface Agents,CommunicationsoftheACM,Vol. 43,No.4,70-78.","Nathalie Colineau and Cécile Paris. 2003.Framework fortheDesignofIntelligentMultimediaPresentation Systems: An architecture proposal for FOCAL. CMISTechnicalReport03/92,CSIRO,May2003.","Dominique Estival. 2002. \"The Syrinx Spoken Language System\".International Journal ofSpeech Technology.vol.5.no.1.pp.85-96.","Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor. 2002. \"MATCH: an ArchitectureforMultimodal Dialogue Systems\".Proceedingsofthe40thAnnualMeetingof the Association for Computational Linguistics (ACL'02).pp.376-383.Philadelphia..","DafyddGibbon, IngeMertins, RogerK.Moore (Eds.). 2000. Handbook of Multimodal and Spoken Dialogue Systems: Resources, Terminology and ProductEvaluation.KluwerAcademicPublishers.","Global InfoTek Inc. 2002.Control of Agent Based Systems.http://coabs.globalinfotek.com.","Joel Gould. 2001. \"Implementation and Acceptance of NatLink, aPython-Based Macro System forDragon NaturallySpeaking\",The Ninth International Python Conference,March5-8,California","Martha L. Kahn and Cynthia Della Torre Cicalese. 2001. \"CoABS Grid Scalability Experiments\". Proceedings of the Second International Workshop on Infrastructure for Agents, MAS, and Scalable MAS,AutonomousAgents2001Conference.","Dale A. Lambert and Mikael G. Relbe. 1998. \"Reasoning with Tolerance\". 2nd","International Conference on Knowledge-Based Intelligent ElectronicSystems.IEEE.pp.418-427.","DaleA.Lambert.1999.\"AdvisersWithATTITUDEfor Situation Awareness\".Proceedings of the 1999 Workshop on Defence Applications of Signal Processing. pp.113-118, Edited A. Lindsey, B. Moran, J. Schroeder, M. Smith and L. White. LaSalle,Illinois.","Dale A. Lambert. 2003. \"Automating Cognitive Routines\", accepted for publication in the6th"," InternationalConferenceonInformationFusion.","R. Moore, J. Dowding, H. Bratt, J. Gawron, Y. Gorfu, A. Cheyer. 1997. \"CommandTalk: A spoken-language interface for battlefield simulations\". In Proceedings of the Fifth Conference on Applied NaturalLanguageProcessing,pp1-7. Nuance.2002.http://www.nuance.com/.","Oviatt, S., Cohen, P., Wu, L., Vergo, J., Duncan, L., Suhm, B., Bers, J., Holzman, T., Winograd, T., Landay,J.,Larson,J.,Ferro,D.2000.\"Designingthe user interface for multimodal speech and pen-based gesture applications: state-of-the-art systems and future research directions\".Human Computer Interaction.","Rashmi Prasad and Marilyn Walker. 2002. \"Training a Dialogue Act Tagger for Human-Human and Human-ComputerTravelDialogues\".Proceedingsof 3rdSIGDIALWorkshop.Philadelphia.pp.162-173.","Manny Rayner, John Dowding, Beth Ann Hockey. 2001. \"A Baseline method for compiling typed unification grammars into context free language models\". InProceedings of Eurospeech 2001,pp 729-732.Aalborg,Denmark.","Manny Rayner, John Dowding, Beth Ann Hockey. 2002a.\"RegulusDocumentation\".","Manny Rayner, Beth Ann Hockey, John Dowding. 2002b. \"Grammar Specialisation meets Language Modelling\".ICSLP2002.Denver.","Manny Rayner and Pierrette Bouillon. 2002. \"A Flexible Speech to Speech Phrasebook Translator\". Proceedings of the ACL-02 Speech-Speech TranslationWorkshop,pp69-76.","Jeff Rickel, Stacy Marsella, Jonathan Gratch, Randall Hill,DavidTraum,WilliamSwartout.2002.Toward aNewGenerationofVirtualHumansforInteractive Experiences. IEEE Intelligent Systems, 1094-7167, pp.32-38.","William Roberts. 1998. \"Automatic Speaker Recognition Using Statistical Models\".DSTO ResearchReport,DSTO-RR-0131,DSTOElectronics andSurveillanceResearchLaboratory.","rVoice. 2002. Rhetorical Systems, http://www.rhetoricalsystems.com/rvoice.html.","Paul Taplin, Geoffrey Fox, Michael Coleman, Steven Wark, Dale Lambert. 2001. \"Situation Awareness Using a Virtual Adviser\",Talking Head Workshop, OzCHI2001,Fremantle,Australia.","Helen Wright. 1998. \"Automatic utterance type detection using suprasegmental features\". Proceedings of the 5th International Conference on SpokenLanguageProcessing(ICSLP'98).Sydney."]}]}