{"sections":[{"title":"A Patent Document Retrieval System Addressing Both Semantic and Syntactic Properties Liang Chen Computer Science Department University of Northern British Columbia Prince George, BC, Canada V2N 4Z9 chenl@unbc.ca Naoyuki Tokuda Hisahiro Adachi R & D Center, Sunflare Company Shinjuku-Hirose Bldg., 4-7 Yotsuya Sinjuku-ku, Tokyo, Japan 160-0004  tokuda n,adachi h @sunflare.co.jp Abstract","paragraphs":["Combining the principle of Differential Latent Semantic Index (DLSI) (Chen et al., 2001) and the Template Matching Technique (Tokuda and Chen, 2001), we propose a new user queries-based patent document retrieval system by NLP technology. The DLSI method first narrows down the search space of a sought-after patent document by content search and the template matching technique then pins down the documents by exploit-ing the words-based template matching scheme by syntactic search. Compared with the synonymous search scheme by thesaurus dictionaries, the new method results in an improved overall retrieval efficiency of patent documents."]},{"title":"1 Introduction","paragraphs":["Information (document) retrieval systems resort to two classes of approaches; the first makes use of the form-based or words-based approach addressing the exact syntactic properties of documents, while the second makes use of the content-based approach which exploits the semantic connection be-tween documents and queries. While most of commercial systems adopt the form-based approach exploiting the simple string matching algorithm or the weighted matching algorithm, the approach needs a thesaurus dictionary to resolve the synonym-related problem. Some research works have now been underway from the content-based approach focusing the dimension reduction scheme.","The content-based approach is motivated by semantics-based search schemes. Assuming that the content of a document is closely related to the tf-idf of the words used (Zobel and Moffat, 1998), we first represent documents as term vectors. One of the immediate difficulties we encounter in deal-ing with document vector spaces lies in its too high a dimensionality of the vector spaces which is particularly true in document analysis largely due to a large variety of synonyms and polysemic words used in natural language. In image recognition field (Turk and Pentland, 1991; Chen and Tokuda, 2003b), a so-called PCA (principal component analysis) principle has been used successfully in facial recognition problems as a most effective scheme of dimension reduction. The LSI (latent semantic indexing) technique (Berry et al., 1999; Littman et al., 1998) is a counterpart of the PCA in text document processing.","We have recently extended the LSI to a DLSI (differential latent semantic indexing) method (Chen et al., 2001), where in the DLSI scheme, we improve the robustness of the LSI scheme by in-troducing and making use of projections of, interior as well as exterior differential document vectors (see Section 2 for detailed discussions). Our present study shows how we can make use of the characteristics in improving the IR performance in patent document search. In patent retrieval application, we are fortunate because all the patent documents are well structured with very precise, human generated abstracts attached so that two interior and exterior documents are automatically provided, facilitating the application of the DLSI method in developing a patent document retrieval system.","Despite the improved superiority of the DLSI technique over the LSI technique (see Section 2 for detailed discussions), the system still has a problem of instability when used as an NLP-oriented query-based commercial product due to content search’s inherent poor precision and recall rate. A content-based information retrieval system is still far be-yond our research ability to be implemented into a coding system. Some syntactic properties seeking the “form” or ”word” similarity must be introduced if the LSI/DLSI based system can be used with robustness. This is so because we have to resolve some conflicting factors here. The content based IR system tries to search the document in accordance with the similarity of “meaning” of a query, which captures the abstraction of the exact words used. For example, we believe that the LSI/DLSI based system should be able to retrieve a similar set of documents to a query “Information Processing Devices” and “Computing Machinery”, where probably some of documents obtained might not contain even the phrases “Information Processing Devices” or “Computing Machinery”, or even neither of these words at all. Form based systems, on the other hand, have to depend on the exact words used; in other words, unless a “perfect” thesaurus dictionary is used, we may not capture the correct documents. Unfortunately we know of no such complete thesaurus dictionary, and even if there is such a dictionary, the matching or collating method will be still too complex with respect to computing resources.","To solve “form” similarity problems encountered in a DLSI/LSI approach, we introduce the template-automaton method which has been originally developed for the language tutoring system (Tokuda and Chen, 2001). The template method sets up a variety of expected patterns of patent document abstracts whereby we want to match a query against a multitude of template paths by pinning down a path having the highest similarity measure to the query from among the documents preselected by the DLSI method. All we have to do here is to maintain the template structure contain-ing the possible candidates of the abstracts of patent documents in natural language, and maintain the template structures in the database. A DP(dynamic programming) based-template matching method is very efficient in finding a best matched path to a query facilitating the final location of the patent document.","The rest of the paper is organized as follows. The scheme of the DLSI method is introduced in Section 2 while the template structure will be explained in Section 3. The Flow of the entire search process and concluding remarks will be given in Sections 4 &5."]},{"title":"2 Differential Latent Semantic Indexing Method","paragraphs":["A term is defined as a word or a phrase that appears","at least in two documents. We exclude the so-called","stop words such as ”a”, ”the” in English which are","used most frequently in any topics, but remain ir-","relevant to our purpose of document search.","Suppose we select and list the terms that appear","in the documents as","","","","","","","",". For each patent","document in collection, we preprocess it and assign","it with a document vector as","","","","","","","","","",", where","","","","   ","","; here","","denotes the number of times","the term ","appears in an expression of the docu-","ment, and","","denotes the global weight over all the","documents; the weight denotes a parameter indicat-","ing the relative importance of the term in represent-","ing the document abstracts. Local weights could","be either raw occurrence counts, boolean, or loga-","rithms of occurrence count. Global weights could","be no weighting (uniform), domain specific, or en-","tropy weighting. The document vector is normal-","ized as  ","","","","","","","",". Since all the patent docu-","ments are provided with a formal abstract, we sup-","pose the abstracts be equivalent to their documents","in content so that the abstract and the document","should both be retrieved as part of the similar doc-","uments to the query supplied. We will show be-","low how we can set up the DLSI technique lead-","ing to an improved robust scheme below. We have","shown how the shortcoming of a global projection-","based LSI scheme can be improved by making a","best use of differences of two vectors in adapting to","the unique characteristics of each document (Chen","et al., 2001).","A Differential Document Vector is defined as","","","  ","where","","and","","are normalized document","vectors satisfying particular types of documents.","An Exterior Differential Document Vector in par-","ticular is defined as the Differential Document Vec-","tor ","","","","","",",if","","and","","constitute two nor-","malized document vectors of any two different doc-","uments. An Interior Differential Document Vec-","tor is defined by the Differential Document Vector","","   ","","",", where","","and","","constitute two differ-","ent normalized document vectors of the same doc-","ument. The different document vectors of the same","documents may be taken from parts of documents","including abstracts, or may be produced by differ-","ent schemes of summaries, or from the querries.","The Exterior Differential Term-Document Matrix","is defined as a matrix, each column of which is set","to an Exterior Differential Document Vector. The","Interior Differential Term-Document Matrix is de-","fined as a matrix, each column of which comprises","an interior Differential Document Vector. 2.1 Details of a DLSI Model","Any differential term-document matrix, say, of m-","by-n matrix D of rank","","","","","","","","",",","can be decomposed into a product of three ma-","trices, namely","","","","",", such that","and","are an","-by-","and","-by-","unitary matrices respec-","tively, where the first columns of","and","are","the eigenvectors of  and","","","respectively.","","","diag","Æ","","Æ","","","","Æ   , where Æ","","are nonnega-","tive square roots of eigen values of","",", Æ"," ","","for",""," and Æ","","","","for","","",". By convention,","the diagonal elements of S are sorted in decreasing","order of magnitude. To obtain a new reduced ma-","trix","",", we simply keep the","-by-","leftmost-upper","corner matrix","","","","","of",", other terms being","deleted; we similarly obtain the two new matrices","","","and"," by keeping the leftmost","columns of","","and","respectively. The product of","",",","","and","","","","provides a matrix","","which is approximately","equal to . Each of differential document vec-","tor","could find a projection on the","dimensional","differential latent semantic fact space spanned by","the k columns of","",". The projection can easily","be obtained by","","","",". Note that, the mean","","of the","exterior-(interior-)differential document vectors are","approximately 0. Thus, ","   ","",", where is the covariance of the distribution computed from the training set. Assuming that the differential document vectors formed follow a high-dimensional Gaussian distribution, the likelihood of any differential document vector","will be given by","    ","  ","    ",""," ","","","","   ","","","","","","where","","","","","","","   ",". Since Æ","","","are eigenvalues","of","",",wehave  ","","","","","",", and thus","","","","","","","","","  ","","","","","","","","","","where","","  ","","","","","","","  ","","",".","Because","is a diagonal matrix,","","",""," ","","","","    Æ","","",".","It is convenient to estimate the quantity by ","   ","","    ","   Æ","","","          ","","where","   ","","","","   ","Æ","","",".","Because the columns of","are orthonormal vec-","tors,","","","","","","   ","could be estimated by","","","","","","","  ","",". Thus, the likelihood function","","","","","","could be estimated by","","","   ","","   "," ","  ","       Æ       ","   ","   ","","","","","","","",""," Æ ","","","","","","","","",""," (1)","where","","","","","",",","","","","","  ","","","","","","","","","",",","    ","","","","",""," Æ   ,","is the rank of matrix",". In prac-","tical cases, may be approximated by Æ","","","","","",", and"," by",". 2.2 Algorithm 2.2.1 Setting Up Retrieval System 1. Text preprocessing: Identify words and noun phrases as well as stop words. 2. System term construction: Set up the term list as well as the global weights. 3. Set up the document vectors of all the collected documents in normalized form . 4. Construct interior differential term-document matrix","","","","","",", such that each of its column is an","interior differential document vector.","5. Construct an exterior differential term-document","matrix","  ","","",", such that each of its column is an","exterior differential document vector.","6. Decompose","","and","","by","","(singular value","decomposition) algorithm into","","form. Find","proper values of","’s to define the likelihood func-","tions","","","   ","and","","","","","","","as Equition (1).","7.","","","",""," ","","","","","   ","","","","","","","","","","","","","","","","","","","","","","","","","","",""," ","where","","","","","is set to an average number of re-","calls divided by the number of documents in the","data base and","  ","","is set to","","","","","","",". 2.2.2 Patent Document Search","1. A query is treated as a document; a document","vector is set up by generating the terms as well as","their frequency of occurrence, and thus a normal-","ized document vector is obtained for the query .","Each document in the data base are processed by","the procedures in items 2-5 below.","2. Given a query, construct a differential document","vector",".","3. Calculate the interior document likelihood func-","tion","","","","","  , and calculate the exterior document","likelihood function","","","","","","","for the document.","4. Calculate the Bayesian posteriori probability","function","","   ","",".","5. Select those documents whose","","","","","","","exceeds","a given threshold (say, 0.5), or choose N documents","having the first","largest","","","","","","","."]},{"title":"3 Template Structure for Storing Patent Abstracts","paragraphs":["Each patent document is usually provided with an abstract. The abstract can be used for content-based information retrieval by using DLSI method as described above. As we have mentioned before, the content-based information retrieval system by LSI analysis is not robust enough to be directly applicable to a real system. We will use the DLSI method only to narrow down the search space at a first stage of filtering in information retrieval. We will resort to a form based searching strategy to pin down the patent document.","Now that the content-based DLSI search scheme has narrowed down the search space in content, the form based search strategy we now employ need not to pay attention to the synonymous expressions of the searching terms or sentences.","This first stage of filtering is now implemented without going through the tedious process of deal-ing with the synonymous expressions by synonym dictionaries which are hard to develop and to use. Even if we succeeded in treating the synomyms, we also have to realize that the polynonym of a natural language will reduce the advantage of using synonym dictionary further, because two words are synonymous in one situation but might not be so in other situations, depending on context.","In view of lengthy sentences used in patent documents including their abstracts, we want to emphasize that automaton-based template structure is an extremely efficient way of expressing lengthy sentences with their synonymous expressions.","We will demonstrate this point by way of examples below. For a sentence, “There are beautiful parks in Japan across the nation”, we can use a template as of figure 1 where a variety of synonymous expressions are explicitly represented.","The problem here is, how we could get the template for an abstract of patent document? Firstly, we regard the original abstract of patent itself as a simplest template. Then, we register queries into the matched template structures by combining each pair of matched terms into one node. This is illustrated by an example procedure in figures 1-3. The original template of an abstract is indicated by figure 1, but when a query of figure 2, namely, “There are lovely parks across Japan”, is matched to the template of figure 1, the template could be modified to a new structure of figure3.","Suppose that the query sentence is, “There are ugly streets in Japan”. Now although we could locate a matching pattern similar to that of figure 2, we will have to rule it out so that we will not come up with a template which include the above sentence as a path, or part of a path . This mechanism should be established from users’ response. We will explain it in Section 4.1. across all over the country nation in Japan nationwide nationwide in Japan beautiful There are pretty parks Figure 1: Template Example Indicating a set of Semantically Similar Patent Abstracts across all over the countr y nation in Japan nationwide nationwide in Japan beautiful There are pretty parks lovelyThere are parks across Japan Figure 2: Query Template to be matched with Abstract Template across all over the country nation in nationwide nationwide","in Japan beautiful There are pretty parks lovely across Japan Figure 3: Modified Template"]},{"title":"4 The Flow of the Search Process 4.1 The Entire Flow of the Complete Search Process","paragraphs":["Before starting the search process, we should set up the DLSI for all the patent documents. 1. Locate the query in the DLSI space. 2. Find and select those patent documents whose abstracts’ vector space lie in a neighborhood of the query vector space having semantic similarity to sentences of figure 1 by the DLSI matching algorithm. 3. For each of the abstracts obtained by step 4.1, use the template matching algorithm of (Chen and Tokuda, 2003a) to calculate the similarity of the summary and the query, select the documents of which the abstracts have a highest similarity to the query. 4. Show the result to the user. 5. Modify the abstracts in the database by users’ responses."]},{"title":"5 Concluding Remarks","paragraphs":["We have proposed a new IR method for patent documents addressing both semantic and syntactic properties by combining a mixed model of content and form based methods; the first stage of DLSI method narrows down the search space by content and the second template method pins down the document by syntactic search on words. We are able to do so, mainly because the DLSI matching in the first stage captures those documents based on content while the template method can now pin down the patent documents having a highest similarity in form with the query. An experimental verification of the present approach is now underway."]},{"title":"References","paragraphs":["M. W. Berry, Z. Drmac, and E. R. Jessup. 1999. Matrices, vector spaces, and information retrieval. SIAM Rev., 41(2):335–362.","L. Chen and N. Tokuda. 2003a. Bug diagnosis by string matching: Application to ILTS for translation. CALICO Journal, 20(2):227–244.","L. Chen and N. Tokuda. 2003b. Robustness of regional matching scheme over global matching scheme. Artificial Intelligence, 144(1-2):213–232.","L. Chen, N. Tokuda, and A. Nagai. 2001. Probabilistic Information Retrieval Method Based on Differential Latent Semantic Index Space. IEICE Trans. on Information and Systems, E84-D(7):910–914.","M. L. Littman, Fan Jiang, and Greg A. Keim. 1998. Learn-ing a language-independent representation for terms from a partially aligned corpus. In Proceedings of the Fifteenth In-ternational Conference on Machine Learning, pages 314– 322.","N. Tokuda and L. Chen. 2001. An online tutoring system for language translation. IEEE Multimedia, 8(3):46–55.","M. Turk and A. Pentland. 1991. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71–86.","Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. ACM SIGIR FORUM, 32(1):18–34."]}]}