{"sections":[{"title":"Maximum Entropy Models for Named Entity Recognition Oliver Bender  and Franz Josef Och  and Hermann Ney   Lehrstuhl für Informatik VI  Information Sciences Institute Computer Science Department University of Southern California RWTH Aachen - University of Technology Marina del Rey, CA 90292 D-52056 Aachen, Germany och@isi.edu bender,ney @cs.rwth-aachen.de Abstract","paragraphs":["In this paper, we describe a system that applies maximum entropy (ME) models to the task of named entity recognition (NER). Starting with an annotated corpus and a set of features which are easily obtainable for almost any language, we first build a baseline NE recognizer which is then used to extract the named entities and their context information from additional non-annotated data. In turn, these lists are incorporated into the finalrecognizer to further improve the recognition accuracy."]},{"title":"1 Introduction","paragraphs":["In this paper, we present an approach for extracting the","named entities (NE) of natural language inputs which","uses the maximum entropy (ME) framework (Berger et","al., 1996). The objective can be described as follows.","Given a natural input sequence","","","","","  we","choose the NE tag sequence","","  ","",""," ","with the","highest probability among all possible tag sequences:    ","","  ","","","","   ","","The argmax operation denotes the search problem, i.e. the generation of the sequence of named entities. According to the CoNLL-2003 competition, we concentrate on four types of named entities: persons (PER), locations (LOC), organizations (ORG), and names of miscellaneous entities (MISC) that do not belong to the previous three groups, e.g. [PER Clinton] ’s [ORG Ballybunion] fans in-vited to [LOC Chicago] .","Additionally, the task requires the processing of two different languages from which only English was specified before the submission deadline. Therefore, the system described avoids relying on language-dependent knowledge but instead uses a set of features which are easily obtainable for almost any language.","The remainder of the paper is organized as follows: in section 2, we outline the ME framework and specify the features that were used for the experiments. We describe the training and search procedure of our approach. Sec-tion 3 presents experimental details and shows results obtained on the English and German test sets. Finally, section 4 closes with a summary and an outlook for future work."]},{"title":"2 Maximum Entropy Models","paragraphs":["For our approach, we directly factorize the posterior probability and determine the corresponding NE tag for each word of an input sequence. We assume that the decisions only depend on a limited window of ",""," ","","","around the current word","and","on the two predecessor tags. Thus, we obtain the follow-","ing second-order model: ","","","    ","","  ","  "," "," ","    ","  m  "," ","    "," ","   "," ","","A well-founded framework for directly modeling the posterior probability      "," ","   ","","","is maximum en-","tropy (Berger et al., 1996). In this framework, we have","a set of","feature functions  "," ","   ",""," ","  "," ","","  ",". For each feature function",", there exists a","model parameter","",". The posterior probability can then","be modeled as follows: Input Sequence","","","","","Preprocessing  Global Search      ","","","","","","","    "," ","","","","","","","","     ","","","","","","","      ","","","","","","  ","   ","","","     ","","","","",""," ","","","","","Postprocessing  Tag Sequence Figure 1: Architecture of the maximum entropy model approach.    "," "," ","   ","   exp     ","  ","  "," ","  ",""," ","      exp     ","  ","  "," ","    "," ","  "," (1)","The architecture of the ME approach is summarized in Figure 1.","As for the CoNLL-2003 shared task, the data sets often provide additional information like part-of-speech (POS) tags. In order to take advantage of these knowledge sources, our system is able to process several input sequences at the same time. 2.1 Feature Functions We have implemented a set of binary valued feature functions for our system: Lexical features: The words ","","are compared to a vocabulary. Words which are seen less than twice in the training data are mapped onto an ’unknown word’. For-mally, the feature","","     "," ","  ",""," ","  "," ","     ","  ","","   ","  ","will fireif the word","","","matches the vocabulary entry","","and if the prediction for the current NE tag equals",".","     ","denotes the Kronecker-function. Word features: Word characteristics are covered by the word features, which test for:","- Capitalization: These features will fireif","","is capitalized, has an internal capital letter, or is fully capitalized.","- Digits and numbers: ASCII digit strings and number expressions activate these features.","- Pre- and suffixes: If the prefix(suffix)of","equals a given prefix(suffix),these features will fire. Transition features: Transition features model the dependence on the two predecessor tags:","     ","  "," ","  ",""," ","  "," ","","      ","  ","","","     Prior features: The single named entity priors are incorporated by prior features. They just fire for the currently observed NE tag:    "," ","   ",""," ","  "," ","   "," Compound features: Using the feature functions defined so far, we can only specify features that refer to a single word or tag. To enable also word phrases and word/tag combinations, we introduce the following compound features:","","","","","",""," ","","","","","","","  "," ","   ",""," "," ","     ","","   ","","  "," ","   ",""," ","    ","","       ","  ","   Dictionary features: Given a list","of named entities, the dictionary features check whether or not an entry of ","occurs within the current window. Formally,  ","  "," ","  ",""," "," "," entryOccurs     ","   ","  ",""," Respectively, the dictionary features fire if an entry of a context list appears beside or around the current word position","",". 2.2 Feature Selection Feature selection plays a crucial role in the ME framework. In our system, we use simple count-based feature reduction. Given a threshold",", we only include those features that have been observed on the training data at least","times. Although this method does not guarantee to obtain a minimal set of features, it turned out to perform well in practice.","Experiments were carried out with different thresholds. It turned out that for the NER task, a threshold of  for the English data and ","for the German corpus achieved the","best results for all features, except for the prefixand suffix","features, for which a threshold of","(   resp.) yielded best results. 2.3 Training For training purposes, we consider the set of manually annotated and segmented training sentences to form a single long sentence. As training criterion, we use the maximum class posterior probability criterion:   "," ","     ","       "," "," ","  ","","  "," This corresponds to maximizing the likelihood of the ME model. Since the optimization criterion is convex, there is only a single optimum and no convergence problems occur. To train the model parameters ","","we use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972).","In practice, the training procedure tends to result in an overfittedmodel. To avoid overfitting,(Chen and Rosenfeld, 1999) have suggested a smoothing method where a Gaussian prior on the parameters is assumed. Instead of maximizing the probability of the training data, we now maximize the probability of the training data times the prior probability of the model parameters:   "," ","       ","    ","        "," ","   ","  "," where    ","     ","  ","","","      "," This method tries to avoid very large lambda values and avoids that features that occur only once for a specific class get value infinity. Note that there is only one parameter  for all model parameters "," . 2.4 Search In the test phase, the search is performed using the socalled maximum approximation, i.e. the most likely sequence of named entities    ","is chosen among all possible","sequences   : ","","","","","","","","","","","","","    ","",""," ","  ","  ","        "," ","  ","","  "," Therefore, the time-consuming renormalization in Eq. 1 is not needed during search. We run a Viterbi search to find the highest probability sequence (Borthwick et al., 1998)."]},{"title":"3 Experiments","paragraphs":["Experiments were performed on English and German test sets. The English data was derived from the Reuters corpus1","while the German test sets were extracted from the ECI Multilingual Text corpus. The data sets contain to-kens (words and punctuation marks), information about the sentence boundaries, as well as the assigned NE tags. Additionally, a POS tag and a syntactic chunk tag were assigned to each token. On the tag level, we distinguish five tags (the four NE tags mentioned above and a filler tag).","3.1 Incorporating Lists of Names and Non-annotated Data For the English task, extra lists of names were provided, and for both languages, additional non-annotated data was supplied. Hence, the challenge was to findways of incorporating this information. Our system aims at this challenge via the use of dictionary features.","While the provided lists could straightforward be integrated, the raw data was processed in three stages:","1. Given the annotated training data, we used all features except the dictionary ones to build a firstbaseline NE recognizer.","2. Applying this recognizer, the non-annotated data was processed and all named entities plus contexts (up to three words beside the classifiedNE and the two surrounding words) were extracted and stored as additional lists.","3. These lists could again be integrated straightforward. It turned out that a threshold of five yielded best results for both the lists of named entities as well as for the context information. 3.2 Results Table 1 and Table 2 present the results obtained on the development and test sets. For both languages, 1 000 GIS iterations were performed and the Gaussian prior method was applied. Test Set Precision Recall F",""," English devel. 90.01% 88.52% 89.26 English test 84.45% 82.90% 83.67 German devel. 73.60% 57.73% 64.70 German test 76.12% 60.74% 67.57 Table 1: Overall performance of the baseline system on the development and test sets in English and German. 1 The Reuters corpus was kindly provided by Reuters Lim-","ited. 86 87 88 89 0 2 4 6 8 10 F-Measure [%] standard deviation","smoothed no smoothing Figure 2: Results of the baseline system for different smoothing parameters.","As can be derived from table 1, our baseline recognizer clearly outperforms the CoNLL-2003 baseline (e.g. ","","   ","   vs. ","  ","","   ","","). To investigate the contribution of the Gaussian prior method, several experiments were carried out for different standard deviation parameters ",". Figure 2 depicts the obtained F-Measures in comparison to the performance of non-smoothed ME models ( ","         ). The gain in performance is obvious.","By incorporating the information extracted from the non-annotated data our system is further improved. On the German data, the results show a performance degradation. The main reason for this is due to the capitaliza-tion of German nouns. Therefore, refinedlists of proper names are necessary."]},{"title":"4 Summary","paragraphs":["In conclusion, we have presented a system for the task of named entity recognition that uses the maximum entropy framework. We have shown that a baseline system based on an annotated training set can be improved by incorporating additional non-annotated data.","For future investigations, we have to think about a more sophisticated treatment of the additional information. One promising possibility could be to extend our system as follows: apply the baseline recognizer to an-notate the raw data as before, but then use the output to train a new recognizer. The scores of the new system are incorporated as further features and the procedure is iterated until convergence."]},{"title":"References","paragraphs":["A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72, March. English devel. Precision Recall F",""," LOC 93.27% 93.58% 93.42 MISC 88.51% 81.02% 84.60 ORG 84.67% 83.59% 84.13 PER 92.26% 91.91% 92.09 Overall 90.32% 88.86% 89.58 English test Precision Recall F",""," LOC 86.44% 89.81% 88.09 MISC 78.35% 73.22% 75.70 ORG 80.27% 76.16% 78.16 PER 89.77% 87.88% 88.81 Overall 84.68% 83.18% 83.92 German devel. Precision Recall F",""," LOC 72.23% 71.13% 71.67 MISC 66.08% 44.95% 53.51 ORG 71.90% 56.49% 63.27 PER 82.77% 68.59% 75.02 Overall 74.16% 61.16% 67.04 German test Precision Recall F",""," LOC 69.06% 69.66% 69.36 MISC 66.52% 46.27% 54.58 ORG 68.84% 53.17% 60.00 PER 87.91% 75.48% 81.22 Overall 74.82% 63.82% 68.88 Table 2: Results of the finalsystem on the development and test sets in English and German.","A. Borthwick, J. Sterling, E. Agichtein, and R. Grisham. 1998. NYU: Description of the MENE named entity system as used in MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7), 6 pages, Fairfax, VA, April. http://www.itl.nist.gov/iaui/894.02/related projects/muc/.","S. Chen and R. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University, Pittsburgh, PA.","J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470–1480."]}]}