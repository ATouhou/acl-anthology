{"sections":[{"title":"  Incorpora\\b\\tng \\f\\ba\\b\\ts\\b\\tcal Informa\\b\\ton of Lex\\tcal Dependency \\tn\\bo a Rule-Based Parser ∗∗∗∗ ","paragraphs":["Yoon-Hy\\bn\\t Roh\\f Ki-Yo\\bn\\t Lee\\f and Yo\\bn\\t-Gil Kim","","Nat\\bral Lan\\t\\ba\\te Processin\\t Research Team\\f Electronics and Telecomm\\bnications Research Instit\\bte\\f","161 Gajeon\\t-don\\t\\f Y\\bseon\\t-\\t\\b\\f Daejeon\\f 305-350\\f Korea","{yhroh\\f leeky\\f kimyk}@etri.re.kr Abs\\brac\\b. This paper presents a method to incorporate statistical information into a r\\blebased parser to resolve syntactic ambi\\t\\bities. We extract the statistical information from the Penn Treebank\\f and apply the information to the r\\ble-based parser. For the extraction of the statistical information the ta\\t conversion is needed beca\\bse of the disa\\treement of the ta\\ts and the bracketin\\t style. We will show the effect of the ta\\t conversion with experiments. The final res\\blt shows abo\\bt 7% error rate red\\bction in the dependency eval\\bation. We will also show how m\\bch each type of statistical information affects the parsin\\t performance. Keywords: r\\ble-based parsin\\t\\f syntactic ambi\\t\\bity\\f statistical information\\f PCFG\\f lexical dependency \\K \\K ∗ The work reported in this paper was s\\bpported by the IT R&D pro\\tram of MKE\\f “Development of Machine","Translation Technolo\\ty for Korean/Chinese/En\\tlish Spoken Lan\\t\\ba\\te and B\\bsiness Doc\\bments”.  Copyri\\tht 2009 by Yoon-Hy\\bn\\t Roh\\f Ki-Yo\\bn\\t Lee\\f and Yo\\bn\\t-Gil Kim"]},{"title":"1 In\\broduc\\b\\ton","paragraphs":["While it is easy to develop a r\\ble-based parser and to improve the performance in the early developin\\t sta\\te\\f it becomes more and more diffic\\blt to resolve the conflicts between the r\\bles as the n\\bmber of r\\bles increases. Beca\\bse the r\\ble-based parser has the limit in its ability to resolve syntactic ambi\\t\\bity\\f syntactic ambi\\t\\bity is the challen\\tin\\t problem especially for the r\\ble-based parser \\bsin\\t CFG as its \\trammar. One way to solve the problem is lexicalization.","Many recent parsin\\t technolo\\ties have taken statistical approaches as we can \\tet more lin\\t\\bistic data s\\bch as the Penn Treebank (Collins\\f 1999; Collins\\f 2000; Charniak and Johnson\\f 2005). They also show enco\\bra\\tin\\t performance. B\\bt practically the statistical parsin\\t has the efficiency problem and the scalability problem. The scalability problem means the diffic\\blty in incorporatin\\t other types of syntactic information s\\bch as lexical patterns or semantic patterns. Also it is not easy to t\\bne the parser min\\btely with respect to each sentence. So\\f we want to \\bse a PCFG parser as a base parsin\\t system and \\bse statistical information for syntactic ambi\\t\\bity.","There are many researches abo\\bt resolvin\\t syntactic ambi\\t\\bity \\bsin\\t statistical information. The representative case is PP attachment disambi\\t\\bation (Stetina and Na\\tao\\f 1997; Oltean\\b and Moldovan\\f 2005; Foth and Menzel 2006). Most of them simplified the problem into selectin\\t an attachment site between a no\\bn and a verb. However\\f in the real parsin\\t the sit\\bation is more complicated. There can be more attachment sites and the impact of PP attachment on the other part has to be considered. In Foth and Menzel (2006)\\f more comprehensive disambi\\t\\bation method was presented \\bsin\\t Lexical Attraction\\f a sort of m\\bt\\bal information.","Another related research area is dependency parsin\\t technolo\\ty (K\\bdo and Mats\\bmoto\\f 2000; McDonald et al.\\f 2006). B\\bt for the present\\f we want to add statistical information witho\\bt si\\tnificant infl\\bence to the c\\brrent wei\\thtin\\t mechanism for the parsed tree selection. 493 23rd Pacific Asia Conference on Language, Information and Computation, pages 493–500  ","In the next section we introd\\bce o\\br base parsin\\t system\\f and in the section 3\\f we present the way to apply statistical information for syntactic ambi\\t\\bity. In the section 4\\f by analyzin\\t the performance variation accordin\\t to the types of statistical information\\f we improve the efficiency by applyin\\t the statistical information selectively. Then\\f we concl\\bde this paper with several remarks on f\\bt\\bre works."]},{"title":"2 Base Pars\\tng \\fys\\bem","paragraphs":["O\\br parsin\\t system was developed for the \\teneral domain of the web. The parser cond\\bcts bottom-\\bp chart parsin\\t \\bsin\\t ACFG (A\\b\\tmented Context Free Grammar) r\\bles\\f where the r\\bles are constrained on vario\\bs types of syntactic and semantic context\\bal condition.","The parsin\\t r\\bles are initially extracted from the Brown Corp\\bs in the Penn Treebank. Not only have the syntactic ta\\ts and the bracketin\\t style been modified\\f b\\bt also the r\\bles have been revised and enlar\\ted in the co\\brse of extendin\\t the tar\\tet domain of the parser. In the be\\tinnin\\t sta\\te\\f the reason that we didn’t adopt a statistical approach is d\\be to the efficiency and the scalability of the parser. Generally\\f a statistical parser \\bse a h\\b\\te amo\\bnt of parameters and the search space is lar\\te\\f so the parsin\\t speed is relatively low and it is not adeq\\bate for a real time application. In the preliminary test\\f the speed of the statistical parsin\\t is more than 7 times lower than that of the r\\ble-based parsin\\t. Also\\f in the side of scalability\\f \\bs\\bally a practical parser \\bse vario\\bs types of additional knowled\\te for parsin\\t\\f it is diffic\\blt to incorporate other knowled\\te. R\\bles are easy to reco\\tnize and mana\\te.","O\\br parser \\bses lexical patterns and verb s\\bbcate\\torization probability information besides the parsin\\t r\\bles. The r\\bles have many syntactic and semantic feat\\bres and constraints for prohibitin\\t impla\\bsible syntactic str\\bct\\bres and prioritizin\\t the r\\bles. Lexical pattern are handcrafted idiomatic expressions. They incl\\bde at least one lexical item\\f and they are applied to sentences comprisin\\t the specific word. The verb s\\bbcate\\torization probability information is the probability that a certain verb takes a certain s\\bbcate\\torization type. It was also extracted from the Brown corp\\bs in the Penn Treebank.","O\\br parser shows over 90% dependency acc\\bracy1","in the \\teneral domain\\f which is competitive performance in the r\\ble-based parsin\\t. B\\bt the parsin\\t performance is standstill and now we need to resolve syntactic ambi\\t\\bity for additional performance improvement. One way is to \\bse statistical information."]},{"title":"3 Apply\\tng \\f\\ba\\b\\ts\\b\\tcal \\tnforma\\b\\ton","paragraphs":["When we consider what type of statistical information can be \\bsed from the Penn Treebank\\f the inconsistency between o\\br bracketin\\t style and that of Penn Treebank is an obstacle to \\bsin\\t some knowled\\te like lexicalized r\\bles beca\\bse it req\\bire convertin\\t one bracketin\\t style to another. Maybe\\f the best pla\\bsible way to \\bse lexical statistic information is statistical information abo\\bt lexical dependency. So we mainly consider \\bsin\\t lexical dependency information. "]},{"title":"3.1 Lex\\tcal Dependency Informa\\b\\ton","paragraphs":["There are two different dependency models dependin\\t on how the dependency probability is conditioned. One is the bilexical dependency model (Collins\\f 1996) and the other is the \\tenerative model (Collins\\f 1999). In the bilexical model\\f \\tiven two words wi\\f wh\\f the probability that wi is dependent on wh is expressed as follows. "]},{"title":")\\f|(","paragraphs":["h\\b"]},{"title":"\\t\\tDP","paragraphs":["(1) \\K \\K 1 We \\bse o\\br own dependency meas\\bre\\f which will be described later. 494   ","In the \\tenerative model\\f \\tiven a head wh\\f the probability that a dependent wi is \\tenerated is expressed as follows. "]},{"title":")|(","paragraphs":["h\\b"]},{"title":"\\t\\tP","paragraphs":["(2) ","The state-of-the-art in the statistical parsin\\t is the \\tenerative model (Collins\\f 2000; Charniak and Johnson\\f 2005). So\\f first we consider the \\tenerative model. The \\tenerative probability that the i-th dependent (child) di is dependent on a head child h in a chart of a chart parsin\\t is expressed as follows. "]},{"title":"))()\\f()\\f(|)()\\f(()|( \\b\\f\\bsthlht\\fl\\ftPh\\fP","paragraphs":["\\b\\b\\b"]},{"title":"≈","paragraphs":["(3)  where t(x) represents the ta\\t of x\\f l(x) represents the lexical root of x\\f and dist(i) represents the distance feat\\bre between the head child and the i-th dependent in the chart.","The distance feat\\bre capt\\bres how far the dependent is from the head child and it is a f\\bnction of s\\brface strin\\t as in Collins (1999). The problem of \\bsin\\t s\\bch probability is that the \\tenerative probability is too low. This makes it diffic\\blt to apply other type of knowled\\te s\\bch as lexical or semantic pattern or to apply the statistical information selectively. The desirable characteristic of the wei\\tht of lexical dependency is that the wei\\tht has 1 when the head child and its dependent have no preference\\f has a val\\be \\treater than 1 when two words have dependency preference\\f and has a val\\be between 0 and 1 when they have dependency dispreference. For this\\f we normalize the \\tenerative probability by dividin\\t it with the \\tenerative probability \\tiven only the ta\\t of head child. The dependency wei\\tht is expressed as follows.",""]},{"title":"))()\\f(|)()\\f(( ))()\\f()\\f(|)()\\f(( )|( \\b\\f\\bstht\\fl\\ftP \\b\\f\\bsthlht\\fl\\ftP h\\fW","paragraphs":["\\b\\b","\\b\\b \\b"]},{"title":"=","paragraphs":["(4)  Considerin\\t that the r\\ble probability of PCFG reflects only the probability abo\\bt syntactic","ta\\t\\f the wei\\tht can be re\\tarded as reflectin\\t the variation by lexicalization. Also\\f the wei\\tht","can be expressed another way. "]},{"title":"))()\\f(|)()\\f((*))()\\f(|)(( ))()\\f(|)()\\f()\\f(( )|( \\b\\f\\bstht\\fl\\ftP\\b\\f\\bsththlP \\b\\f\\bsththl\\fl\\ftP h\\fW","paragraphs":["\\b\\b","\\b\\b \\b"]},{"title":"=","paragraphs":["(5)  It is the form of the m\\bt\\bal information when P(A) is"]},{"title":"))()\\f(|)(( \\b\\f\\bsththlP","paragraphs":["and P(B) is"]},{"title":"))()\\f(|)()\\f(( \\b\\f\\bstht\\fl\\ftP","paragraphs":["\\b\\b .  The above method s\\bffers from the data sparseness problem which the lexical statistical","approach \\bs\\bally has. The followin\\t back-off method can be \\bsed. "]},{"title":"))()\\f(|)(( ))()\\f()\\f(|)(( )|( \\b\\f\\bstht\\ftP \\b\\f\\bsthlht\\ftP h\\fW","paragraphs":["\\b","\\b \\b"]},{"title":"=","paragraphs":["(6)  Then the total wei\\tht of the r\\ble r applied to a chart is calc\\blated by the followin\\t.  495  "]},{"title":"∏=","paragraphs":["\\b \\b"]},{"title":"h\\fWrPrW )|(*)()(","paragraphs":["(7)","Generally\\f the probability of a parse tree is calc\\blated by m\\bltiplyin\\t all r\\bles applied to the parsed tree like"]},{"title":"∏=","paragraphs":["\\b \\b"]},{"title":"rPTP )()(","paragraphs":[". Likewise the wei\\tht of a parse tree with statistical information is calc\\blated by"]},{"title":"∏=","paragraphs":["\\b \\b"]},{"title":"rWTW )()(","paragraphs":["and the parse tree with the maxim\\bm wei\\tht is selected as the final res\\blt. ","Meanwhile\\f the dependency probability that the i-th dependent di is dependent on a head child h in a chart in the bilexical model is expressed as follows. "]},{"title":"))()\\f()\\f()\\f()\\f(|()\\f|( \\b\\f\\bst\\fl\\fthlhtDP\\fhDP","paragraphs":["\\b\\b\\b"]},{"title":"≈","paragraphs":["(8)  The dependency wei\\tht is expressed as follows. "]},{"title":"))()\\f()\\f(|( ))()\\f()\\f()\\f()\\f(|( )\\f|( \\b\\f\\bst\\fthtDP \\b\\f\\bst\\fl\\fthlhtDP \\fhDW","paragraphs":["\\b","\\b\\b \\b"]},{"title":"=","paragraphs":["(9) When we cannot find the probability"]},{"title":"))()\\f()\\f()\\f()\\f(|( \\b\\f\\bst\\fl\\fthlhtDP","paragraphs":["\\b\\b \\f it can also be backed off as follows. "]},{"title":"))()\\f()\\f(|( ))()\\f()\\f()\\f(|( )\\f|( \\b\\f\\bst\\fthtDP \\b\\f\\bst\\fl\\fthtDP \\fhDW","paragraphs":["\\b","\\b\\b \\b"]},{"title":"=","paragraphs":["(10) "]},{"title":"))()\\f()\\f(|( ))()\\f()\\f()\\f(|( )\\f|( \\b\\f\\bst\\fthtDP \\b\\f\\bst\\fthlhtDP \\fhDW","paragraphs":["\\b","\\b \\b"]},{"title":"=","paragraphs":["(11) ","Act\\bally we cond\\bcted preliminary test abo\\bt the bilexical model and it showed lower performance than the \\tenerative model. Besides\\f it \\tenerates too many parameters beca\\bse the sample space is all combination of two words in a sentence. So we will consider only the \\tenerative model henceforth.",""]},{"title":"3.2 Ex\\brac\\b\\tng Lex\\tcal Dependency Informa\\b\\ton","paragraphs":["We \\bse the Penn Treebank as the lin\\t\\bistic data so\\brce. When we extract the dependency data from the Penn Treebank\\f there are several points to consider.","The first is that the Penn Treebank \\bses some coarse ta\\ts. For example\\f the part of speech (POS) ta\\t “IN” incl\\bdes both prepositions s\\bch as “in” and conj\\bnctions s\\bch as “while”. Also the Treebank does not distin\\t\\bish the “TO” of a preposition and the “TO” of a to-infinitive. Moreover\\f SBAR represents all types of cla\\bses incl\\bdin\\t no\\bn cla\\bses s\\bch as that-cla\\bse\\f adverbial cla\\bses s\\bch as if-cla\\bse\\f and relative cla\\bses s\\bch as which-cla\\bse. For this problem\\f we do not \\bse a POS ta\\t b\\bt the syntactic ta\\t of the parent of the pre-terminal in the syntactic tree.","The second is the problem by the difference of syntactic ta\\ts and bracketin\\t style. O\\br parser basically \\bses the Penn Treebank ta\\ts\\f b\\bt we modified syntactic and POS ta\\ts and modified the bracketin\\t style from the Penn Treebank. For example o\\br parser distin\\t\\bishes adverbial cla\\bses (SBARV) s\\bch as if-cla\\bse from that-cla\\bse (SBAR). So\\f the portin\\t of ta\\ts and str\\bct\\bres is needed . 496  ","Lastly\\f \\bsin\\t syntactic ta\\t eliminates some important information. In case of a verb\\f the syntactic ta\\t “VP”(Verb Phrase) misses the form information of the verb s\\bch as an in\\t-form or an infinitive form. So we distin\\t\\bish them by \\bsin\\t different ta\\ts s\\bch as VPG(present participle VP)\\f VPB(infinitive VP)\\f VPN(past participle VP)\\f etc\\f only in the case that the verb is \\bsed as a dependent. In the case that the verb is \\bsed as a head\\f we do not distin\\t\\bish them.","The overall proced\\bre is as follows:","","All pre-terminals in the parse tree are reco\\tnized. For all the pre-terminals\\f cond\\bct the followin\\t. For example\\f from the below parse tree\\f “He/PRP acc\\bsed/VBD Dow/NNP Jones/NNP of/IN \\bsin\\t/VBG \\bnfair/JJ means/NNS..” is extracted. (SS (S (NP-SBJ (PRP He) ) (VP (VBD acc\\bsed) (NP-1 (NNP Dow) (NNP Jones) ) (PP-CLR (IN of) (`̀ `̀) (S-NOM (NP-SBJ-2 (-NONE- *-1) ) (VP (VBG \\bsin\\t) (NP (JJ \\bnfair) (NNS means) ) ... (. .) ('' '') )) ","word/ta\\t normalization: words are stemmed\\f the words ta\\t\\ted with “CD\\f NNP” are replaced by their ta\\ts\\f and the ta\\ts “VBZ VBD” are replaced by “VBP” for covera\\te\\f etc. (he/PRP acc\\bse/VBP NNP/NNP NNP/NNP of/IN \\bse/VBG \\bnfair/JJ mean/NNS..)","Findin\\t its head in the pre-terminals \\bsin\\t tree str\\bct\\bres. (acc\\bse/VBP! NNP/NNP of/IN)","Ta\\t conversion: As described above\\f some POS ta\\ts or syntactic ta\\ts are converted. (VPG (VBG \\bsin\\t) (NP (JJ \\bnfair) (NNS means) ) (PP-DIR (IN to) (NP (NN sin\\tle-A-3) ))","Co\\bnt all the events of word pairs with the distance feat\\bre. From (acc\\bse/VBP! NNP/NNP of/IN)\\f the followin\\ts are \\tenerated. 000 acc\\bse/VBP! NNP/NNP 100 acc\\bse/VBP! of/IN 100 acc\\bse/VBP","Calc\\blate all the lexical dependency wei\\thts accordin\\t to the form\\bla (4)."]},{"title":")100\\f|\\f( )100\\f\\f|\\f( )100\\f/|/( VBPofINP accuseVBPofINPVBPaccuseINofW =","paragraphs":["There are several ta\\t \\bsa\\te strate\\ties:  Ta\\t1: Usin\\t the POS ta\\ts of the terminal nodes.","From the parse tree (VP (VBG \\bsin\\t) ... )\\f “\\bse/VBG” Ta\\t2: Usin\\t the parent ta\\ts of the terminal nodes. (“\\bse/VP”) Ta\\t3: Ta\\t2 with reflectin\\t the above considerations. (“\\bse/VPG”)  For the experiment\\f we \\bse the standard data division (Collins\\f 1999). The lexical data was","extracted from the section 02-21 of the WSJ corp\\bs. And the section 23 was reserved for the 497  ","eval\\bation and the section 00 is \\bsed as a development set. The total n\\bmber of extracted","dependency information is 60\\f550. Some extracted samples are shown below:  39.9245 89 000 acco\\bnt/VP! for/PP 1.6473 3 000 acco\\bntable/ADJP! PP 19.4185 3 000 acco\\bntable/ADJP! for/PP 1.7168 4 000 accr\\be/VP! PP 0.8807 4 000 acc\\bsation/NP! PP 1.2503 3 000 acc\\bsation/NP! of/PP 110.5855 18 000 acc\\bse/VP! of/PP 3.4336 3 000 acc\\bstom/VP! PP 26.1406 3 000 acc\\bstom/VP! to/PP ","In the above example\\f the word with “!” is a head and the first field is the dependency wei\\tht. For example\\f “110.5855 18 000 acc\\bse/VP! of/PP” means that the dependency wei\\tht that the “of/PP” comes immediately after the head “acc\\bse/VP” is 110.5855."]},{"title":"3.3 Apply\\tng Dependency We\\tgh\\b","paragraphs":["When an inactive chart is \\tenerated in the chart parsin\\t\\f all the dependency wei\\thts between the head child and other children calc\\blated and m\\bltiplied to the total chart wei\\tht. B\\bt we excl\\bde some children which have little dependency ambi\\t\\bity like “the”."]},{"title":"3.4 Evalua\\b\\ton","paragraphs":["The common method to eval\\bate the parsin\\t performance is the way by matchin\\t bracketin\\t. B\\bt that method is affected heavily by the ta\\ts and bracketin\\t style. Moreover\\f o\\br parser \\bses the lexical patterns\\f which does not follow \\bs\\bal reco\\tnition \\bnit of syntactic constit\\bent like “IN -> in reference with”\\f “VB -> provide NP with”. So we \\bse the dependency acc\\bracy between words.","All words in a sentence have their own headword except the headword of the whole sentence. Therefore\\f the performance of dependency acc\\bracy is meas\\bred by obtainin\\t the headwords and matchin\\t them. The \\bs\\bal dependency acc\\bracy incl\\bdes the match of the relation between the head and its dependents (Lin\\f 1998). B\\bt we do not consider the ta\\ts or any relation beca\\bse of the disa\\treement of ta\\ts and bracketin\\t style. O\\br method only discerns whether the dependent is an ar\\t\\bment or not. This makes it possible to distin\\t\\bish whether to-infinitive is \\bsed as an ar\\t\\bment of a verb s\\bch as “want” or not.","Table 1 shows the parsin\\t performance by dependency acc\\bracy.  Table 1: The dependency acc\\bracy of parsin\\t res\\blts.","Labeled Precision Dependency Acc\\bracy Error Rate Red\\bction Collins Model2 (Collins\\f 1999) 88.3% 91.00%","Base parsin\\t system 91.27% Ta\\t1 91.79% 5.95% Ta\\t2 91.72% 5.15% Ta\\t3 91.89% 7.10%","Ta\\t3 with back-off 91.80% 6.07%",""," 498  ","For comparison\\f Table 2 shows all the parsin\\t performance with respect to the ta\\t \\bsa\\te strate\\ty. The Ta\\t3 method shows abo\\bt 7% error rate red\\bction. Contrary to o\\br expectation the Ta\\t3 with back-off shows performance de\\tradation. It seems mainly d\\be to the coarse \\tran\\blarity of syntactic ta\\ts for the back-off model."]},{"title":"4 Analys\\ts on \\bhe effec\\b accord\\tng \\bo Lex\\tcal Dependency Informa\\b\\ton Type","paragraphs":["There are two reasons of analyzin\\t the performance variation of each dependency type. In the parsin\\t\\f the application of the statistical information ca\\bses the efficiency problem. So we want to know what type of statistical information affects the parsin\\t performance most little beca\\bse not all types of statistical information seem to contrib\\bte to the dependency performance. For example the direct object of a verb has little ambi\\t\\bity of dependency. And we do not apply s\\bch type of statistical information.","Also\\f we want to know what type of dependency information we need to b\\bild. B\\bildin\\t the lin\\t\\bistic knowled\\te man\\bally is an expensive task. So we want to know what type of dependency information is most effective to the parsin\\t performance. The dependency type is obtained by cate\\torizin\\t the dependency by the ta\\t pair of the dependency information. We can test the selectional preference stren\\tth abo\\bt each lexical dependency (Brockmann and Lapata\\f 2003)."," where S(v) is a selectional preference stren\\tth of a verb v\\f P(c) is the overall distrib\\btion of classes which the verb takes as the relation r. P(c|v\\fr) is conditional probability. We can \\tet the selectional preference stren\\tth of each dependency type by replacin\\t v\\f r with a head\\f c with the dependents of the head.","B\\bt for now\\f we want to know the direct effect to the parsin\\t performance beca\\bse tho\\b\\th direct objects have hi\\th selectional preference\\f they do not seem to contrib\\bte to the performance enhancement. Table 2 shows the performance variation accordin\\t to dependency type. Table 2: Performance variation accordin\\t to dependency type","Lexical dependency type Dependency Acc\\bracy","Lexical dependency type","Dependency","Acc\\bracy NP! PP 91.54% SINV VP! 91.29% VP! PP 91.39% SS! VPN 91.29% NP! VPF 91.39% PP VP! 91.29% ADJP! PP 91.33% SBAR! VP 91.29% VP! VPF 91.32% NP! WHNP 91.29% VP! SBAR 91.30% VP! S 91.29% NP! VPN 91.30% VP! VPG 91.29% NP NP! 91.29% WHNP! VP 91.29% PP! NP 91.29% VP! SBARV 91.29% NP VP! 91.29% SS! VPB 91.29% VP VP! 91.29% VPB VP! 91.29% SS! VP 91.29% ADJP NP! 91.29% S VP! 91.29% VP! VPN 91.29% NP! NP 91.29% VP! PRT 91.29% VP! VP 91.29% PP! PP 91.29% ADVP VP! 91.29% PP! VPG 91.29% VP! ADJP 91.29% VP! NP 91.28%","ADJP ADJP! 91.29% VP! VPB 91.28% VP! ADVP 91.29% Base 91.27% 499  ","As the Table 2 shows\\f the statistical information types which contrib\\bte to parsin\\t performance is mainly by the attachment of PP\\f to-infinitive(VPF)\\f SBAR\\f past participle VP(VPN)\\f etc."]},{"title":"5 Conclus\\ton","paragraphs":["This paper presented a method to incorporate statistical information into a r\\ble-based parser to resolve syntactic ambi\\t\\bity. We employ a PCFG parser as a base system and \\bse additional lexical knowled\\te for the syntactic disambi\\t\\bation. We extracted the statistical information from Penn Treebank\\f and applied the information to the r\\ble-based parser. The res\\blt shows abo\\bt 7% error red\\bction in the dependency eval\\bation. We also cond\\bcted some analysis abo\\bt how m\\bch each type of statistical information affects the parsin\\t performance\\f th\\bs applyin\\t the statistical information selectively for efficiency. This analysis res\\blt can be \\bsed for b\\bildin\\t additional information for syntactic disambi\\t\\bation.","For the f\\bt\\bre works\\f we need to analyze the sentences whose parsin\\t acc\\bracy is lower than statistical parsin\\t res\\blt and what information need to be reflected. And we sho\\bld analyze why the backed-off method deteriorates the performance. Lastly\\f we plan to add the verb s\\bbcate\\torization type to the condition of the \\tenerative model to cope with data sparseness."]},{"title":"References","paragraphs":["Brockmann\\f C. and M. Lapata. 2003. Eval\\batin\\t and Combinin\\t Approaches to Selectional Preference Acq\\bisition. Procee\\f\\bngs of the Tenth Conference on European Chapter of the Assoc\\bat\\bon for Computat\\bonal L\\bngu\\bst\\bcs.","Charniak\\f E. and M. Johnson. 2005. Coarse-to-fine N-best Parsin\\t and MaxEnt Discriminative Rerankin\\t. Procee\\f\\bngs of 43r\\f Meet\\bng of Assoc\\bat\\bon for Computat\\bonal L\\bngu\\bst\\bcs\\f pp. 173-180.","Collins\\f M. 1996. A New Statistical Parser based on Bi\\tram Lexical Dependencies. Procee\\f\\bngs of ACL’96\\f pp. 184–191.","Collins\\f M. 1999. Hea\\f-Dr\\bven Stat\\bst\\bcal Mo\\fels for Natural Language Pars\\bng. Ph.D. thesis\\f University of Pennsylvania.","Collins\\f M. 2000. Discriminative Rerankin\\t for Nat\\bral Lan\\t\\ba\\te Parsin\\t. Procee\\f\\bngs of the Seventeenth Internat\\bonal Conference on Mach\\bne Learn\\bng\\f pp. 175–182.","Foth\\f K. and W. Menzel. 2006. The benefit of stochastic PP attachment to a r\\ble-based parser. Procee\\f\\bngs of the COLING/ACL\\f pp. 223–230.","K\\bdo\\f T. and Y. Mats\\bmoto. 2000. Japanese Dependency Str\\bct\\bre Analysis Based on S\\bpport Vector Machines. Procee\\f\\bngs of the Jo\\bnt SIGDAT Conference on Emp\\br\\bcal Metho\\fs \\bn Natural Language Process\\bng an\\f Very Large Corpora (EMNLP/VLC)\\f pp. 18–25","Lin\\f D. 1998. A Dependency-based Method for Eval\\batin\\t Broad-covera\\te Parsers. Natural Language Eng\\bneer\\bng\\f 4(2)\\f 97-114.","McDonald\\f R.\\f K. Lerman and F. Pereira. 2006. M\\bltilin\\t\\bal Dependency Analysis with a Two Sta\\te Discriminative Parser. Procee\\f\\bngs of the Tenth Conference on Computat\\bonal Natural Language Learn\\bng.","Oltean\\b\\f M. and D. Moldovan\\f 2005. PP-attachment Disambi\\t\\bation Usin\\t Lar\\te Context. Procee\\f\\bngs of the conference on Human Language Technology an\\f Emp\\br\\bcal Metho\\fs \\bn Natural Language Process\\bng\\f pp. 273-280","Stetina\\f J. and M. Na\\tao. 1997. Corp\\bs Based PP Attachment Ambi\\t\\bity Resol\\btion with a Semantic Dictionary. Procee\\f\\bngs of the F\\bfth Workshop on Very Large Corpora\\f pp. 66– 80. 500"]}]}