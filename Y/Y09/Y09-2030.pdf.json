{"sections":[{"title":"Effective\\b\\tse\\bof\\b\\fhinese\\bStructural\\bAuxiliaries\\bfor\\b\\fhinese\\bParsing\\b*\\b\\b \\b","paragraphs":["Yun Jina",", \\bin\\t Lib",", Yin\\ts\\fun Wua",", and Youn\\t-Gil Kima  a Natural Lan\\tua\\te Processin\\t Researc\\f Team, ETRI, 138 Gajeon\\tno, Yuseon\\t-\\tu, Daejeon, 305-764, Korea","{Wkim1019, suni, kimyk}@etri.re.kr","b","Sc\\fool of Economic Information En\\tineerin\\t, Sout\\fwestern University of Finance and Economics,","55 Guan\\t \\fua c\\fun Road, C\\fen\\tdu, 61130, C\\fina","liq_t@swufe.edu.cn\\b Abstract. Natural lan\\tua\\te parser is usually faced un\\trammatical input, suc\\f as mistypin\\t or error POS ta\\ts. If t\\fe parser uses lan\\tua\\te dependant explicit lin\\tuistic knowled\\te to detect and correct \\trammatical errors, it is useful for parser. In t\\fis paper, we propose a met\\fod t\\fat uses t\\fe C\\finese structural auxiliary knowled\\te to detect and correct un\\trammatical C\\finese parsin\\t errors. We focus on t\\free error types: miss se\\tmentation, miss POS ta\\ts, and miss typin\\t. Experimental results s\\fow t\\fat appropriate use of evident C\\finese structural auxiliary knowled\\te indeed \\felps to correct parsin\\t errors and furt\\fer to improve C\\finese parsin\\t performance. Keywords:\\bC\\finese structural auxiliary, C\\finese parsin\\t, parsin\\t error, abuse"]},{"title":"1 Introduction\\b","paragraphs":["Lan\\tua\\te parsin\\t is an essential part for various kinds of natural lan\\tua\\te understandin\\t applications includin\\t question & answerin\\t system, mac\\fine translation and so on. Most of lan\\tua\\te parsers are usually constructed based on standard \\trammar rules wit\\f an assumption t\\fat texts to be parsed are error free. Unfortunately, t\\fis is not always true. Typos and irre\\tular expression are frequently occurred in documents, especially in emails, messen\\ters and blo\\ts. In addition, t\\fe inaccurate preprocessin\\t steps (e.\\t. parts-of-speec\\f, text se\\tmentation) of a parser brin\\t more errors. Due to t\\fe a\\t\\tlutinative c\\faracteristics of C\\finese lan\\tua\\te, t\\fese kinds of errors occur frequently and s\\fould not be i\\tnored in C\\finese lan\\tua\\te parsin\\t. In t\\fis paper, we propose to apply C\\finese structural auxiliary knowled\\te to detect and correct un\\trammatical errors for C\\finese lan\\tua\\te parsin\\t.","A structural auxiliary is an unstressed form word, w\\fic\\f performs t\\fe \\trammatical functions of structure in C\\finese lan\\tua\\te. T\\fe structural auxiliaries are t\\fe most frequently words used words in C\\finese. Based on our study, avera\\te occurrence of structural words in C\\finese news articles is 1.25 per sentence.","Alt\\fou\\t\\f structural auxiliaries are occurred extremely frequently in C\\finese, t\\fere are only t\\free words to perform t\\fe \\trammatical functions of structure. T\\fat is, “的(de), 得(de), 地(de), X(z\\fi)1","”. In particular, \\W \\W * T\\fe work reported in t\\fis paper was supported by t\\fe IT R&D pro\\tram of MKE, “Development of Mac\\fine Translation Tec\\fnolo\\ty for Korean/C\\finese/En\\tlis\\f Spoken Lan\\tua\\te and Business Documents”.  Copyri\\t\\ft 2009 by Yun Jin, \\bin\\t Li, Yin\\ts\\fun Wu, and Youn\\t-Gil Kim 1 之(z\\fi): is excluded in t\\fis paper because of its rareness in modern C\\finese. 694 23rd Pacific Asia Conference on Language, Information and Computation, pages 694–701  “的” is used after an attributive to identify its attributiveness, e.\\t. 我的书 (my book),","中国的历史 (t\\fe \\fistory of C\\fina), or at t\\fe end of a nominal structure to form a noun","p\\frase, namely “的-p\\frase”, e.\\t. 开车的 (man w\\fo drives), 红色的 (somet\\fin\\t red); “地” is used after an adjective adverbial, e.\\t. 愉快地学习(study \\fappily), 轻轻地落下","来(fall \\tently down); “得” is used after a verb or an adjective to introduce a completion, e.\\t. 跑得快(run","quickly), 好得很(very \\tood). In t\\fis paper, we call t\\fem as CSAs (C\\finese Structural Auxiliaries) for s\\fort. CSAs are one of most important lan\\tua\\te features in C\\finese, \\fowever, its essential ambi\\tuity and abuse brin\\ts several issues in natural lan\\tua\\te processin\\t (NLP). In particular, Multiple \\trammatical functions of CSAs make t\\fe \\trammatical analysis quite difficult","in natural lan\\tua\\te processin\\t (NLP). For example, t\\fe auxiliary “的” not only can be a","structural auxiliary “我的衣服(my clot\\fes)”, but also can be a tense auxiliary “他的情","况,我是知道的(\\fis situation, I knew)” or pronoun “送牛奶的(milkman)”. In addition,","due to t\\fe transliteration of En\\tlis\\f into C\\finese, it could be a part of forei\\tn words","suc\\f as “的里雅斯特(Trieste)”. CSAs are usually used wron\\tly or improperly in C\\finese because of t\\fe careless","writin\\t style of t\\fe C\\finese. T\\fey are misused in place of eac\\f ot\\fer. For instance, in","t\\fe p\\frase “现金流量非常的充沛(vey abundant cas\\f flow)”, t\\fe word “的” is used","improperly to be replaced as t\\fe correct word “地”. Liu (2006) \\fas pointed out t\\fat","t\\fere are 28.2% structural auxiliary errors in primary and secondary sc\\fool textbooks","biased errors corpus.","T\\fese c\\faracteristics of CSAs cause a number of errors in se\\tmentation, POS(parts-of-speec\\f)","ta\\t\\tin\\t and parsin\\t in NLP. In t\\fis paper, we propose t\\fe application of CSAs error analysis","tec\\fniques to improve t\\fe natural lan\\tua\\te understandin\\t in C\\finese. T\\fe rest of t\\fis work is","or\\tanized as follows. We first briefly describe w\\fat tec\\fnolo\\ties can be relevant to t\\fis idea in","Section 2. T\\free kinds of error types to be considered for C\\finese lan\\tua\\te parsin\\t are","systemically studied and t\\fe approac\\f to detect and correct t\\fese errors is presented in Section","3. We t\\fen test t\\fe performance of our approac\\f usin\\t a known news corpus (Section 4). T\\fis","paper is concluded wit\\f speculation on \\fow t\\fe current work can be furt\\fer improved in","Section 5."]},{"title":"2 Related\\bWork\\b","paragraphs":["T\\fe natural lan\\tua\\te processin\\t issues related wit\\f un\\trammatical text errors \\fave been studied by several researc\\fers. Atwell (1987) proposed an N-\\tram-based met\\fod to detect mistypin\\t, and lack or extraneous constituents of sentences. Foster and Vo\\tel (2004) \\fas been collected a 20,000 word corpus of un\\trammatical En\\tlis\\f sentences from a variety of written lan\\tua\\te sources (newspapers, emails, websites, etc.). Eac\\f un\\trammatical sentence in t\\fe corpus is corrected, producin\\t a parallel corpus of \\trammatical sentences. T\\fey use t\\fis data to evaluate a parser’s ability to produce an accurate parse for un\\trammatical sentence. Gamon et al. (2007) applied speller tec\\fniques and lan\\tua\\te modelin\\t approac\\fes to detect and correct errors in incorrect usa\\te of determiners and c\\foice of t\\fe preposition. Different wit\\f previous researc\\fes on En\\tlis\\f text, we focus on t\\fe un\\trammatical errors in C\\finese text, w\\fic\\f are more complicated t\\fan En\\tlis\\f due to t\\fe essential c\\faracteristic of C\\finese suc\\f as a\\t\\tlutination.","Liu (2006) and Pan\\t et al. (2004) studied t\\fe problem of CSAs. Instead of applyin\\t CSA to NLP, t\\fey just focused on error analysis of CSA. In t\\fis paper, we propose t\\fe application of CSAs error analysis tec\\fniques to improve t\\fe natural lan\\tua\\te understandin\\t in C\\finese. 695"]},{"title":"3 Application\\bof\\b\\fSA\\bGrammatical\\bFunctions\\bfor\\b\\fhinese\\bParsing\\b","paragraphs":["CSAs are one of most important lan\\tua\\te features in C\\finese, \\fowever, its essential ambi\\tuity and abuse brin\\ts several issues in natural lan\\tua\\te processin\\t (NLP). In t\\fis section, we present our approac\\f to detect and correct t\\fe errors related wit\\f CSAs. We first studied t\\fe error types caused by t\\fe ambi\\tuity or abuse of CSAs in C\\finese news articles. Second, different CSAs errors are classified into suc\\f error types usin\\t support vector mac\\fines (SVM). T\\fird, \\feuristic rules are adopted to correct t\\fese errors for better understandin\\t of C\\finese lan\\tua\\te in NLP."]},{"title":"3.1 Error\\bAnalysis\\bof\\b\\fhinese\\bStructural\\bAuxiliaries\\b\\b","paragraphs":["In t\\fis section, we study t\\fe error types of CSAs and discuss \\fow to utilize CSAs’ \\trammatical functions to improve C\\finese parsin\\t. 3.1.1 Segmentation\\bError\\b\\b Due to a\\t\\tlutinative c\\faracteristic of C\\finese, word se\\tmentation is indispensable to intelli\\tent C\\finese lan\\tua\\te process. Se\\tmentation error, \\fowever, is inevadible and tends to mis\\tuide parse tree \\teneration for NLP since no se\\tmentation al\\torit\\fms ac\\fieve t\\fe 100% se\\tmentation accuracy. Se\\tmentation dictionaries and mac\\fine learnin\\t tec\\fniques (Won\\t and C\\fan, 1996; Low et al., 2005; Z\\fao et al., 2006) are popular tec\\fniques to alleviate se\\tmentation issues. However, none of previous researc\\fes \\fas considered t\\fe effect of CSAs on word se\\tmentation. In t\\fis article, we ar\\tue t\\fat t\\fe CSAs analysis s\\fould be incorporated to reduce t\\fe word se\\tmentation errors. For example, C\\finese word \"丽珠得乐(Bismut\\f Potassinm Citrate)\" is a medical name for a stomac\\f pain killer. If se\\tmentation dictionary does not contains t\\fis word (T\\fis is usually true for most forei\\tn words and \\fi-tec\\f words), accordin\\t to traditional se\\tmentation al\\torit\\fms, it mi\\t\\ft be se\\tmented into two different forms, \"丽珠_得_乐 \" or \"丽 _珠_得_乐\". If POS ta\\t\\tin\\t is applied to furt\\fer analyzin\\t t\\fis word, it mi\\t\\ft obtain two wron\\t parsin\\t results as \"丽 珠 /NR 得 /DE 乐 /NR\" and \" 丽 /NR 珠 /NN 得 /DE 乐 /NR\". Obviously, t\\fis medical noun s\\fould be treated as an entire unit instead of se\\tmentin\\t it into pieces. Ot\\ferwise, it will lead to parsin\\t errors.","However, if we take a CSAs analysis of t\\fis sentence, suc\\f kind of error can be corrected easily. More specifically, knowin\\t t\\fe syntax of \"得\", it can infer t\\fat t\\fe word before or after \"得\" s\\fould not be an noun term. T\\fus, usin\\t a c\\funkin\\t approac\\f, we can \\tenerate a correct parse tree. By processin\\t a simple and fast CSA analysis, we can easily solve t\\fe se\\tmentation errors related wit\\f CSAs and consequentially improve t\\fe C\\finese parsin\\t performance. 3.1.2 POS\\bTagging\\berror\\b\\b POS ta\\t\\tin\\t, usually as a subsequent step of se\\tmentation in NLP, brin\\ts lots of errors in C\\finese parsin\\t. C\\finese structural auxiliaries cause various kinds of errors in POS ta\\t\\tin\\t. For example, “ 炒 地 (Land speculation)”, “ 三 亩 地 (t\\free acres of land)” and “注 册 地 (re\\tistration place)” are usually falsely ta\\t\\ted as “炒/VV 地/DE”, “三/NU 亩/MW 地/DE”, and “ 注 册 /NN 地 /DE”, respectively. Actually, “ 地 ” in t\\fese words are noun instead of auxiliary due to its polymorp\\fism. Most POS ta\\t\\tin\\t approac\\fes can not differentiate its polymorp\\f and cause POS ta\\t\\tin\\t errors. Suc\\f kinds of POS ta\\t\\tin\\t errors are usually occurred in compound p\\frases like “采 得 的 信 息 (collected information)”. Noticed t\\fat, because of POS ta\\t\\tin\\t error, t\\fat is, mistreatin\\t \"地\" as an auxiliary, t\\fe parser can not find a suitable \\trammar rule to output reasonable parse tree.","However, if we take a CSA analysis of t\\fis sentence, suc\\f kind of error can be avoided. In CSA \\trammar rule, \"地 \", as an auxiliary, s\\fould be placed before a verb or adjective, ot\\ferwise it s\\fould be a noun referrin\\t to “field”. Knowin\\t t\\fis fact, we replaced t\\fe “DE” ta\\t 696  of \"地\" wit\\f “NN” and \\tet a reasonable parser tree. By processin\\t a simple and fast CSA analysis, we can easily solve t\\fe POS ta\\t\\tin\\t errors related wit\\f CSAs and consequentially improve t\\fe C\\finese parsin\\t performance. 3.1.3 Language\\babuse\\berror\\b C\\finese writin\\ts are overw\\felmed by C\\finese structural auxiliary abuse. T\\fe reasons to explain t\\fis p\\fenomenon can be cate\\torized into:","T\\fe \\fistory of C\\finese lan\\tua\\te culture. T\\fe issue of division or reunion t\\free types of","C\\finese structural auxiliary \\fas been debated for a lon\\t time. Sometime, “的” and “地”","are united into one auxiliary type, and sometimes are utilized separately.","Due to t\\fe popularity of “的”, it \\fas been inappropriately used widely in C\\finese","writin\\ts. Accordin\\t to our statistics, t\\fe usa\\te proportion of “的” accounted for","97.42% of all CSA usa\\te.","Due to popularity of spellin\\t input met\\fod in C\\finese computers, t\\fe same spellin\\t of","t\\free types of CSAs causes lots of mistypin\\t in writin\\t. Fi\\ture 1 s\\fows t\\fe distribution of CSA abuse calculated from a sample of Net ease 2007 C\\finese news articles. In Fi\\ture 1, t\\fe label “SRD_BSD” denotes t\\fe misuse of “得” instead of “的”; t\\fe label “SRD_TYD” denotes misuse “得”instead of “地”; “BSD _SRD” denotes t\\fe misuse “的” instead of “得”; “BSD_TYD” denotes misuse “的”instead of “地”. As s\\fown in Fi\\ture 1, it can be observed t\\fat t\\fe abuse ratio of CSA “地” is almost zero. It means t\\fat CSA “地” nearly causes abusin\\t, and t\\fe proportion of abuse of CSA “得” is 7%, so t\\fe abuse of t\\fe CSA is almost caused by CSA “的”(93%). In t\\fe abuse CSA “的”, people are more confused by t\\fe pair of “的” and “地”(56%) compared wit\\f t\\fe pair of “的” and “得” (37%). Note t\\fat t\\fis statistical result is accordance wit\\f t\\fe first reason of CSA abuse."," The\\bproportion\\bof\\babuse\\bof\\b\\fSA\\bin\\bnews\\bcorpus 5%2% 56% 93% 37% SRD_BSD SRD_TYD BSD_SRD BSD_TYD  Figure1: T\\fe proportion of abuse of t\\fe CSA Because “的”CSA can be placed most of t\\fe words, we need more sop\\fisticated approac\\f and need more context features to resolve abuse of CSAs. For example, we only consider “高(\\fi\\t\\f)/ 的/多(more)”3 words, most of t\\fe C\\finese person directly reco\\tnize “的”CSA is abused wit\\f CSA of “得”, but we \\tive more context as “收入高的多纳税(\\fi\\t\\f-incomer pay more taxes)”, t\\fe people know CSA of “的” is correct. T\\fis example s\\fows us to resolve abuse of “的”CSA, not only consider adjacent terms, but also consider lon\\t distance context features. All of t\\fose c\\faracteristics s\\fould consider into w\\fen we are buildin\\t our detectin\\t and correctin\\t t\\fe CSA errors system."]},{"title":"3.2 Error\\bDetection\\b","paragraphs":["Knowin\\t t\\fe above t\\free types of errors, we can apply support vector mac\\fine (SVM) to classify errors into t\\fese types. 697","T\\fe SVM is a state of t\\fe art supervised mac\\fine-learnin\\t tec\\fnique proposed by Vapnik (1995) and is based on Structured Risk Minimum Principle. By t\\fe principle, w\\fen trainin\\t a classification model, t\\fe aim of t\\fe learner is to optimize not just t\\fe error rate on t\\fe trainin\\t data set, but also t\\fe ability of t\\fe model for predication, and t\\fe ability depend on concept VCdimension. T\\fe SVM is bein\\t applied in many areas suc\\f as word sense disambi\\tuation, text classifications (Son\\t et al., 2005).","To correctly \\troup t\\fe errors related wit\\f CSAs, we utilize adjacent terms of eac\\f CSA as contextual features for SVM. In particular, we extract K terms on bot\\f left and ri\\t\\ft side of eac\\f CSA in t\\fe sentence as contextual features. T\\fe context feature we used term and POS ta\\t. In additionally, we also added some of t\\fe \\trammatical pattern rules used as features. T\\fis is for our data set sparseness and for reveals implicit data properties. Some of t\\fe \\trammatical pattern rules are s\\fow as follows in Table 1.","Table\\b1: Example of pattern rules","Pattern rules Meanin\\t Example {VV, AJ, AD}+ 的 +NN|VV If Verb or Adjective or Adverb preplace at “的” , t\\fen to c\\feck w\\fet\\fer followed by a \\terund 调 整 :NN 还 :AD 在 :PO 不 断 :VV 的 :DE 变 化:NN 。:PU","有+NN+的+NN|VV If t\\fe pattern combined wit\\f “有”and Noun and “ 的 ”, t\\fen to c\\feck w\\fet\\fer followed by a \\terund 有:VX 针对性:NN 的:DE 提出:NN 意见:NN","MW/AA+地+VV If t\\fe post-place term is Verb to c\\feck w\\fet\\fer pre-placed at “地” term \\fave Verb 板 车 :NN 一 :NU 排 排 :MW/AA 地 :DE 停:VV 在:PO 路边:NN","... ... ..."]},{"title":"3.3 Error\\b\\forrection\\b\\b\\b","paragraphs":["After detectin\\t errors related wit\\f CSA and t\\feir error types, we can correct t\\fese errors usin\\t a set of \\feuristic rules. Based our observation, t\\fe 3 kinds of CSA errors are \\fave different c\\faracteristics, t\\fat is most of t\\fe lan\\tua\\te abuse errors are caused by “的”, and most of t\\fe se\\tmentation errors are caused by “得” followed by “地”, and most of POS ta\\t errors are caused by “地” followed by “得”, so we can focus t\\fis c\\faracteristics to use different solution. For example, if a detected error is “的”, we can first apply it abuse error correctin\\t solution, if t\\fe solution resolve t\\fe error finis\\f correction, but t\\fe solution can’t resolved t\\fen we can apply it wit\\f se\\tmentation error correctin\\t solution. If a detected error is “地”, we first apply it wit\\f POS ta\\t\\tin\\t error correctin\\t solution, and t\\fen apply it wit\\f se\\tmentation error correctin\\t solution and lan\\tua\\te abuse error correctin\\t solution separately. In t\\fe followin\\t sub-sections, we detail introduce eac\\f CSA error correctin\\t met\\fod. 3.3.1 \\forrect\\bsegmentation\\berror\\b Detectin\\t miss se\\tmented boundary even difficult, but t\\fose errors are usually tends to followin\\t 3 different aspects.","Grammatically conflicted wit\\f CSA, for example, “丽珠/NR 得/DE 乐/NR”.","Miss se\\tmented sin\\tle c\\faracter units, for example, “丽_珠_得_乐”.","Important clue are appears at around, suc\\f as quotation (“丽珠得乐”系列产品),","desi\\tnation terms(实得集团, 梦地董事长). Based on above aspects, if input of detected errors are \\fave explicit clue or satisfy above patterns t\\fen we can re\\tard it as se\\tmentation errors and correct t\\fem.  698  3.3.2 \\forrect\\bPOS\\btagging\\berror\\b Because t\\fere are \\fave only two CSAs(“地” and “得”) are cause POS ta\\t\\tin\\t errors, for correct POS ta\\t\\tin\\t, we use pattern rule based approac\\f.","For correct “地” CSA POS ta\\t\\tin\\t error, we c\\fecked adjacent term ta\\t. If “地” CSA is wron\\t, t\\fe correct POS is Noun, so POS ta\\t of t\\fe front located “地” CSA’s adjacent s\\fould be one of t\\fe verb(“炒:VV 地:NN”), measure word(“一:NU 亩:MW 地:NN”), noun(“注册:NN 地:NN”), and a few special pattern of “NN_AJ_NN_AJ” suc\\f as “人:NN 多:AJ 地:NN 少:AJ”, but we do not corrected front adjacent POS ta\\t is proper noun, because t\\fis pattern is possibility of se\\tmentation error.","For correct “得”POS ta\\t\\tin\\t error, we also based on \\trammatical function of “得” make pattern rule to correct POS ta\\t\\tin\\t errors. Because “得” \\fave t\\free different \\trammatical functions, so separately c\\feck all of t\\fose pattern rules. For example, t\\fe input is “ 竞:VV 得:D\\b \\t00万:N\\f 股:MW(compete to obtain one million stocks) ”, we c\\feck front adjacent POS ta\\t of “得”, t\\fe term “竞” is sin\\tle verb, it \\fave complement verb property, so to correct “得” POS ta\\t to verb. If front adjacent POS ta\\t is AD and uni\\tram term like “还” or “就”, we c\\fan\\ted POS ta\\t of “得” to auxiliary verb. 3.3.3 \\forrect\\bLanguage\\babuse\\berror\\b Correct lan\\tua\\te abuse error is c\\foice one of t\\free CSAs problem. To correct lan\\tua\\te abuse error, we use Bi\\tram approac\\f to calculate t\\fe probability of all co-location term wit\\f CSAs. Consider t\\fe followin\\t example of a CSA related error:","调:NN 还:AD 在:PO 不断:VV 的:D\\b 变\":NN 。:P\\f (Adjustment still continues to change.) Based on pre-calculated probability of front located term: 不断 wit\\f eac\\f CSA, t\\fe lan\\tua\\te abuse error corrector is consulted to provide t\\fe most likely c\\foice of CSA:","P(不断 + 的) = 0.\\t722","P(不断 + 地) = 0.8278","P(不断 + 得) = 0.0000 Given t\\fis probability distribution, a correction module c\\fan\\ted CSA wit\\f “地” to \\tenerated “调整:NN 还:AD 在:PO 不断:VV 地:D\\b 变化:NN” as output."]},{"title":"4 Experiment\\b 4.1 Experimental\\bSetting\\b","paragraphs":["To \\tau\\te t\\fe performance of explicit usa\\te of CSA \\trammatical functions to assist C\\finese parsin\\t, we carried out a series of experiments based on a news article data set. We collected 18,617 news articles in C\\finese from net ease (www.163.com) in t\\fe year of 2007, and extracted 479,749 sentences amon\\t t\\fem, and filtered 197,925 sentences t\\fat do not contain CSA sentences. T\\fe remainin\\t 281,824 sentences are ta\\t\\ted wit\\f C\\finese POS ta\\ts usin\\t our C\\finese POS en\\tine. Since our objective is to deal wit\\f CSA’s error, we extracted sentences collocatin\\t wit\\f “的 ” CSA terms also collocatin\\t wit\\f ot\\fer CSAs terms from 281,824 sentences. We considered t\\fat 2053 terms are \\fave multi-sense and could cause errors related wit\\f CSAs. In t\\fose term list, we only took 1009 terms wit\\f more t\\fan one frequency and to extracted CSA fra\\tment in eac\\f sentence. Finally, 42,620 fra\\tments are extracted and to used as our system data set.","To annotate t\\fe errors related wit\\f CSAs, we first use Lan\\tua\\te Model (LM) and CSA’s \\trammatical functions \\feuristic approac\\f extract all errors related wit\\f CSAs in t\\fe sentences. Two C\\finese lin\\tuists furt\\fer analyze t\\fese detected errors and remainin\\t data respectively, 699 and corrected bot\\f side data. By doin\\t so, two lin\\tuists reviewed almost 60% data. T\\fe annotated data set was t\\fen divided into trainin\\t (90%) and test (10%) sets."]},{"title":"4.2 \\fSA\\berror\\bdetection\\b","paragraphs":["We desi\\tned a baseline system and comparison system to s\\fow t\\fe effectiveness of our system usin\\t CSA \\trammatical functions. T\\fe baseline system used t\\fe term (W) and POS ta\\t (T) features, and t\\fe comparison system additionally used CSA’s \\trammatical function.","To perform w\\fat T\\fe SVMli\\t\\ft","provides four kernel met\\fods(Linear(L), Polynomial(P), RBF(R), Tan\\f(T)), amon\\t t\\fem, t\\fe polynomial kernel based met\\fod performs best(P:82.8 > L:77.13 > R:39.4 > T:32.7) wit\\f window size 10 in t\\fe baseline system. Additionally, we compared t\\fe precision values of CSA error detection met\\fods correspondin\\t to t\\fe varyin\\t t\\fe size of K. T\\fe Fi\\ture 2 s\\fows t\\fe results. Performance\\bcomparison\\bwith\\bK 74 76 78 80 82 84 86 88 90 92","2 4 6 8 10 size of K P r e c i s i o n  Figure\\b2: Precision comparison wit\\f size of K","Based on t\\fe above settin\\t we compared our baseline system wit\\f t\\fe comparison system additionally used CSA’s \\trammatical function. Table 2 s\\fows t\\fe performance comparison between two met\\fods. It appears t\\fat our CSA \\trammatical function knowled\\te \\felps to detect CSA errors. Table\\b2: Precision comparison of t\\fe two approac\\fes","Approac\\f Met\\fod Precision W 81.13 T 89.84"," Baseline","WT 90.8% Comparison WTG 92.32%"]},{"title":"4.3 \\fSA\\bError\\bcorrection\\b","paragraphs":["Accordin\\t to t\\fe pattern rules mentioned in 3.3, we corrected eac\\f detected CSA error. Table 3 s\\fows t\\fat correction accuracy and combination of t\\fe detection and correction accuracy. T\\fe experiment s\\fows alt\\fou\\t\\f t\\fe ratio of final performance decreases, t\\fis result is of enou\\t\\f wort\\f. Table\\b3: T\\fe accuracy of t\\fe eac\\f error case Correction met\\fods Accuracy Combined Abuse of CSA (Ca) 74.41% 70.48% POS ta\\t\\tin\\t (Cp) 93.4% 90.26% Se\\tmentation (Cs) 85.78% 81.7%","Cs + Cp + Ca 83.1% 79.7% 700  To evaluate t\\fe final performance of C\\finese parsin\\t, we desi\\tned a baseline system and comparison system. For comparison system we used CSA detection (D) and correction (C) module, but baseline system didn’t used any one of t\\fem. For evaluation, we randomly selected 200 sentences from evaluation data set, but contained CSA error sentences w\\fic\\f are \\tuarantee at 10 ~ 20% in t\\fose selected sentences. T\\fe result is s\\fown in Table 4.","Table\\b4: C\\finese parsin\\t performance","C\\finese parsin\\t performance Baseline CSA D+C Improved Accuracy 72.4% 76.2% 5.249%"]},{"title":"5 \\fonclusion\\b","paragraphs":["In t\\fis paper, we addressed issues related to usin\\t CSA’s \\trammatical functions to automatically detect and correct CSA error to reac\\f correct parsin\\t errors in C\\finese parser. T\\fe experiment s\\fows usin\\t explicit CSA knowled\\te \\felps to reduce parsin\\t errors. Nowadays t\\fe statistics and probability approac\\f very popular but combine t\\fis approac\\f wit\\f lan\\tua\\te dependent knowled\\te and \\trammatical function would better \\felp to improve performance."]},{"title":"References\\b","paragraphs":["Atwell, E.S. 1987. How to detect \\trammatical errors in a text wit\\fout parsin\\t it. Proceedings of the third conference on \\buropean chapter of the Association for Computational Linguistics, pp.38-45.","Foster, J. and C. Vo\\tel. 2004. Good reasons for notin\\t bad \\trammar: Constructin\\t a corpus of un\\trammatical lan\\tua\\te. In Pre-Proceedings of the International Conference on Linguistic \\bvidence: \\bmpirical, Theoretical and Computational Perspectives, pp.151-152.","Gamon, M., J.F. Gao, C. Brockett, A. Klementiev, V.B. Dolan, D. Belenko and L. Vanderwende. 2008. Usin\\t Contextual Speller Tec\\fniques and Lan\\tua\\te Modelin\\t for ESL Error Correction. Proceedings of the third International Joint Conference on Natural Language Processing, pp.449-456.","Liu, X.M. 2006. Usa\\te Analysis of t\\fe C\\finese Structural Auxiliary. Modern Chinese, Issue 12, 98-99.","Low, J.K., H.T. N\\t, and W.Y. Guo. 2005. A Maximum Entropy Approac\\f to C\\finese Word Se\\tmentation. Proceedings of the Fourth SIGHAN workshop on Chinese Language Processing, pp.161-164.","Pan\\t, K.H. 2004. Problems in t\\fe way of t\\fe Structural Auxiliary Words and T\\feir Application. Journal of Shangqiu Teachers college, 20(1), 162-163.","Son\\t, S.K., Y. Jin and S.H. Myaen\\t, 2005. Abbreviation Disambi\\tuation Usin\\t Semantic Abstraction of Symbols and Numeric Terms. Proceedings of 2005 I\\b\\b\\b International Conference on Natural Language Processing and Knowledge \\bngineering, pp.14-19.","Vapnik, V.N. 1995. T\\fe Nature of statistical Learnin\\t T\\feory. Springer.","Won\\t, P.K and C. C\\fan. 1996. C\\finese Word Se\\tmentation based on Maximum Matc\\fin\\t and Word Bindin\\t Force. Proceedings of the \\t6th","Conference on Computational Linguistics, pp.200-203.","Z\\fao, H., C.-N. Huan\\t and M. Li. 2006. An Improved C\\finese Word Se\\tmentation System wit\\f Conditional Random Field. Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pp.162-165. 701"]}]}