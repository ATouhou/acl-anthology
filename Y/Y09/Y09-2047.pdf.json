{"sections":[{"title":"Which is M\\b\\te S\\fitable f\\b\\t Chinese W\\b\\td Segmentati\\bn, the Gene\\tative M\\bdel \\b\\t the Disc\\timinative One?","paragraphs":["F"]},{"title":"∗∗∗∗ ","paragraphs":["Kun Wanga \\b \\theng\\fing Zonga","\\b and Keh-Yih Sub ","a","National Laboratory of Pattern Recognition\\b Institute of Automation\\b \\thinese Academy of Sciences\\b","Room 1010\\b No. 95\\b Zhongguancun East Road\\b Haidian District\\b Beijing 100190\\b \\thina","{kunwang\\b c\\fzong}@nlpr.ia.ac.cn","b","Behavior Design \\torporation\\b","2F\\b No. 5\\b Industry East Road IV\\b Science-Based Industrial Park\\b Hsinchu\\b Taiwan\\b RO\\t","kysu@bdc.com.tw Abst\\tact. Since the traditional word-based n-gram model\\b a generative approach\\b cannot handle those out\\bo\\t\\bvocabu\\fary (OOV) words in the testing-set\\b the character-based discriminative approach has been widely adopted recently. However\\b this discriminative model\\b though is more robust to OOV words\\b fails to deliver satisfactory performance for those in\\bvocabu\\fary (IV) words that have been observed before. Having analyzed the word-based approach\\b its capability to handle the dependency between adjacent characters within a word\\b which is believed that the human adopts for doing segmentation\\b is found to account for its excellent performance for those IV words. To incorporate the intra-word characters dependency\\b a character-based approach with a generative model is thus proposed in this paper. The experiments conducted on the second SIGHAN Bakeoffs have shown that the proposed model not only achieves a good balance between those IV words and OOV words\\b but also outperforms the above-mentioned well-known approaches under the similar conditions. Keyw\\b\\tds: \\thinese Word Segmentation\\b Generative Model\\b Discriminative Model.","\\i \\i","∗ The research work has been partially funded by the Natural Science Foundation of \\thina under grant No.60736014\\b 60723005 and 90820303\\b the National Key Technology R&D Program under grant No. 2006BAH03B02\\b the Hi-Tech Research and Development Program (863 Program) of \\thina under grant No. 2006AA010108-4\\b and also supported by the \\thina-Singapore Institute of Digital Media as well. The authors thank Behavior Design \\torporation for using their Generic-Beam-Search code according to the agreement.  \\topyright 2009 by Kun Wang\\b \\theng\\fing Zong\\b and Keh-Yih Su"]},{"title":"1 Gene\\tative M\\bdel Ve\\ts\\fs Disc\\timinative M\\bdel","paragraphs":["Unlike English and other western languages\\b there is no space delimiter between adjacent \\thinese words. Therefore\\b for most \\thinese NLP applications\\b \\thinese word segmentation (\\tWS) is the first task\\b which aims to find the corresponding word se\\fuence from the given \\thinese character se\\fuence. Among various approaches for \\tWS\\b statistical methods have been increasingly applied in the past two decades.","According to the basic unit adopted to extract features\\b statistical approaches could be classified as either a word\\bbased approach or a character\\bbased approach. Besides\\b the word segmentation problem could also be formulated as either a generative mode\\f or a discriminative mode\\f. In terms of the above classification\\b the time-honored word-based model (Zhang et al.\\b 2003; Gao et al.\\b 2003) will be called as the word\\bbased generative approach\\b while the well-known character-based tagging model (Xue\\b 2003; Ng and Low\\b 2004; Tseng et al.\\b 2005) will be named as the character\\bbased discriminative approach. Also\\b the word “model” will be loosely exchanged with the word “approach” when there is no confusion. 827 23rd Pacific Asia Conference on Language, Information and Computation, pages 827–834 ","The above two different kinds of classification are orthogonal to each other. However\\b in the literature papers that we have checked\\b almost all the word-based approaches adopt the generative mode\\fF1","F","\\b and all the character-based approaches adopt the discriminative mode\\f. Before we argue why the proposed approach would be a better combination\\b a detailed discussion for the merits and drawbacks of both the word-based generative model and the character-based discriminative model is first given in the following\\b which would help to illustrate our motivation."]},{"title":"1.1 W\\b\\td-Based Gene\\tative M\\bdel","paragraphs":["The word-based generative model is formulated as follows. (1) Where indicates a specific word se\\fuence with m words\\b and denotes the given sentence with n characters. The classical word-trigram model\\b expressed as","\\b is first formulated in the following. (2) Since and is the same for various candidates\\b only should be considered\\b and it could be further simplified with the second order Markov \\thain assumption shown as below. (3)","In e\\fuation (3)\\b dependency between characters within a word is implicitly taken care by regarding them as a joint event (treated as a single unit). This model works well when there are no out\\bo\\t\\bvocabu\\fary (OOV) words. However\\b this condition cannot be met in real applications. For example\\b named entities and numerical expressions are two kinds of OOV words which are fre\\fuently encountered. Since the associated candidates of those multi-character OOV words can not be generated during the searching process without OOV detection\\b it is impossible to identify them in the word-based approach. Most OOV words thus will be segmented into their corresponding se\\fuences of uni-character-words. High reca\\f\\f o\\t IV words (RIV) and low reca\\f\\f o\\t OOV words (ROOV) are then obtained (see Table 2). In other words\\b the word-based models are vulnerable to those OOV words. Meanwhile\\b the overall precision rate would be also low\\b as those OOV words are forced to be segmented into more words with smaller size."]},{"title":"1.2 Cha\\tacte\\t-Based Disc\\timinative M\\bdel","paragraphs":["The character-based discriminative model (Xue\\b 2003) treats segmentation as a tagging problem\\b which assigns a corresponding tag to each \\thinese character and is formulated as follows. (4) Where kt indicates the corresponding position of character kc in its associated word\\b and is a member of {Sing\\fe, Begin, Midd\\fe, End} (abbreviated as S, B, M and E in the following) in our work. For example\\b the word “北京市 (Beijing \\tity)” will be assigned with the corresponding tags as: “北/B (North) 京/M (\\tapital) 市/E (\\tity)”.","\\tompared with the word-based generative model\\b this approach is tolerable with OOV words. Since each multi-character OOV word will be converted into its corresponding se\\fuence of character-tag-pairs (and the vocabulary size of those possible character-tag-pairs is limited)\\b it","\\i \\i","1 According to Wikipedia (H","http://en.wikipedia.org/wiki/Generative_modelH",")\\b “Generative models contrast with discriminative models\\b in that a generative model is a full probability model of all variables\\b whereas a discriminative model provides a model only of the target variable(s) conditional on the observed variables” 828 is possible to correctly identify those OOV words. Therefore\\b this approach is robust to OOV words\\b and possesses a high ROOV. However\\b lower RIV is usually accompanied\\b as the dependency between adjacent characters within a word is no longer directly modeled (to be further explained in Section 2.1). Therefore\\b compared with the character-based discriminative approach\\b even the word-based unigram models possess a much higher RIV (see Table 2)."]},{"title":"2 Let the Cha\\tacte\\t-Based App\\t\\bach Ad\\bpt the Gene\\tative M\\bdel?","paragraphs":["From the analysis above\\b it is clear that we must regard characters as basic-units to get high ROOV. The remaining problem is how to further raise RIV within the character-based framework. To explore the possible direction\\b we will first inspect what kinds of character-related clues that the human usually adopts to do word segmentation\\b and then integrate this clue into the character-based framework."]},{"title":"2.1 Adhesi\\bn and Dependency Between Adjacent Cha\\tacte\\ts","paragraphs":["Humans are known to use the adhesion between two adjacent characters (also know as character-bigram) as an important clue to do word segmentation. High adhesion usually implies that we will not put a word-break between these two characters\\b while low adhesion fre\\fuently indicates that we will have a word-break between them. With this observation\\b Mutua\\f In\\tormationF2","F","(MI)\\b a statistical measure closely related to the degree of adhesion\\b has been adopted in Sproat and Shin (1990) for judging whether a character-bigram is a bi-character word. MI and other measures are used to perform word segmentation in Sun et. al (1998) (in which 91.75% precision rate is reported).","To give a sense about how \\tharacter-Bigram MI within words distributes differently from that between words\\b various character-bigrams are collected from all corpora of SIGHAN Bakeoff 2005 (Emerson\\b 2005)\\b and the associated MI is evaluated for each of them. Figure 1 gives the distributions of MI for the class of character-bigrams within words (shown by black bars) and the class of character-bigrams between words (shown by white bars). As indicated in the figure\\b the MI value for the character-bigram within words tends to be higher\\b which shows that it is a useful clue to judge whether the character-bigram should be segmented or not. \\i \\i 2 \\b where only counts the event that x precedes y (i.e.\\b excluding the event that","y precedes x). MI Distribution 0.000 0.050 0.100 0.150 0.200 0.250 0.300","-10","-8","-6","-4","-2","0 2 4","6","8","10","12","14","16 MI R e l a t i v e  R a t i o Between Words Within Words  Fig\\f\\te 1: The distributions of MI. logP(Ci|Ci-1) Distribution 0.000 0.050 0.100 0.150 0.200 0.250 0.300","-13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 logP(Ci|Ci-1) R e l a t i v e  R a t i o Between Words Within Words  Fig\\f\\te 2: The distributions of","Moreover\\b to study how the dependency between characters within a word\\b which is implicitly handled in E\\fuation (3)\\b makes the word-based approach possess significantly higher RIV\\b is also calculated for various character-bigrams collected above. Figure 2 gives the distributions of for the class of character-bigrams within words (shown by black bars) and the class of character-bigrams between words (shown by white bars). As indicated in the figure\\b for the class of character-bigram within words also tends to have higher value\\b which explains why those IV words are more likely to be selected and 829 ","high RIV is thus resulted in. In fact\\b these two measures are considerably correlated to each","other (especially for those character-bigrams within words)\\b as shown in Table 1.","Table 1: The correlation coefficients between MI and of","character-bigrams for within-words class and between-words class under various corpora. Classes AS CITYU MSR PKU Ove\\tall Between W\\b\\tds 0.468 0.497 0.498 0.503 0.492 Within W\\b\\tds 0.678 0.709 0.712 0.718 0.699"]},{"title":"2.2 P\\t\\bp\\bsed Cha\\tacte\\t-Based Gene\\tative M\\bdel","paragraphs":["As explained in Section 1.1\\b the word-based approach is vulnerable to those OOV words. To overcome the OOV problem\\b the character-based approach must be adopted. However\\b the generative model should be also applied to handle the dependency within the character-bigram for each class (within-words and between-words)\\b which has been shown to be useful in the last section. To take the advantage from both approaches mentioned above\\b is first replaced with its corresponding se\\fuence of (denoted as )\\b where tag is the same as that adopted in the above character-based discriminative model. With this new representation\\b","could be re-derived based on the character as follows. (5)","Similar with E\\fuation (2)\\b only 1([ \\b ] )n","P c t should be handled and could be further simplified to: (6)","As shown in the last section\\b within words inline to have a higher value than that between words. Therefore\\b for a bi-character-word (similarly for other multi-character-words)\\b when is an IV word\\b for is fre\\fuently higher than that for which correspond to those between-words character-bigrams. In other words\\b those IV words are more likely to be selected\\b and high RIV is thus expected.","Unlike the word-based model specified above\\b this new approach regards the character as a unit. It is possible to correctly identify those multi-character OOV words\\b as their corresponding candidates could now be generated during the searching process. Besides\\b the capability to handle the dependency between adjacent characters under different classes (within-words and between-words)\\b which has been shown to be important for getting high RIV in the word-based approach\\b is still inherited in this model with the adopted generative form. Furthermore\\b as the basic unit in this proposed model is character\\b the vocabulary size of this model is much smaller than that of the word-based approach. Thus the data sparseness problem will not be severe.","Moreover\\b compared with the character-based discriminative approach\\b the proposed model still keeps the capability to handle OOV words\\b because it also regards the character as a unit. Also\\b since the generative form is adopted\\b the dependency between adjacent characters is now directly (and separately) modeled for each class (within-words and between-words)\\b which will give sharper preference when the history of assignment is given. In contrast\\b the adhesion between adjacent characters is not explicitly modeled in the character-based discriminative approach\\b and is thus not used to assign tags."]},{"title":"3 Expe\\timents and Res\\flts","paragraphs":["We carried out our experiments on the data provided by SIGHAN Bakeoff 2005 (Emerson\\b","2005). To make a comparison with the baseline and previous work\\b only the closed testsF3","F","are \\i \\i 3 According to Sighan Bakeoff 2005 regulation\\b the closed test could only use the training data directly provided.","Any other data or information is forbidden\\b including the knowledge of characters set\\b punctuation and so on. 830 conducted. The metrics Precision (P)\\b Reca\\f\\f (R)\\b F\\bmeasure (F)\\b Reca\\f\\f o\\t OOV (ROOV) and Reca\\f\\f o\\t IV (RIV) are used to evaluate the segmentation results. The balanced F\\bmeasure is F=2PR/(P+R)."]},{"title":"3.1 W\\b\\td-Based Gene\\tative M\\bdel and Cha\\tacte\\t-Based Disc\\timinative M\\bdel","paragraphs":["We first extract a word list from the training-set as the vocabulary for the word-based generative approaches\\b and use SRI Language Modeling ToolkitF4","F","(SRILM) (Stolcke\\b 2002) to train various word n-gram models with modified Kneser-Ney smoothing (\\then and Goodman\\b 1998). Afterwards\\b a beam search decoder is applied to find out the best word se\\fuence.","The segmentation results of word-based generative model are shown in Table 2. As expected\\b it shows that all those word-based n-gram models have high RIV and very low ROOV (even its unigram model outperforms the character-based discriminative approach in RIV). After further analyzing the testing-set errors generated in the trigram model\\b we find that among total 16\\b781 error-patterns\\b 11\\b546 of them (69%) are to segment an OOV into a se\\fuence of IV words\\b which clearly illustrates its drawback in handling OOV words and accounts for its low ROOV.","For character-based discriminative approaches\\b various machine learning methods have been successfully applied. For example\\b Xue (2003) and Ng et. al (2004) use Maximum Entropy Model (ME) \\b Peng et. al (2004) and Tseng et. al (2005) use \\tonditional Random Fields (\\tRF) (Lafferty et al.\\b 2001). Among them\\b \\tRF has been reported to give better performance (Zhang et al.\\b 2006). Therefore\\b the package \\tRF++F5","F","is used to conduct the experiments for the character-based discriminative model\\b and the feature templates used are given by Ng and Low (2004)\\b which have been widely adopted and reported in many papers\\b but excluding the ones forbidden by the closed test regulation. Those feature templates are listed as below:","","Table 2 shows that the character-based discriminative model outperforms the word-trigram model on F-measure and ROOV\\b but the latter gets higher RIV. The low RIV for the character-based discriminative model clearly shows the disadvantage without utilizing the dependency characteristic between adjacent characters within multi-character words. Among those 10\\b493 error-patterns resulted from the testing-sets\\b it is observed that 6\\b396 of them (61%) are to incorrectly segment an IV word-se\\fuence\\b which clearly illustrates its weakness in handling IV words and accounts for its low RIV."]},{"title":"3.2 Cha\\tacte\\t-Based Gene\\tative M\\bdel","paragraphs":["The proposed character-based generative model is also trained by using SRILM Toolkit with the same setting utilized by the word-based model. Table 3 shows the results of the proposed character-based generative model for various character n-gram-sizes ranging from n=2 to n=5. It illustrates that the character-trigram model significantly outperforms the character-bigram model over all four corpora\\b but almost no improvement could be observed if we keep increasing the n-gram size (only 4-gram improves a little on MSR corpus\\b as it has the largest average-word-length). This strongly suggests that our training data are inade\\fuate to support more complex models other than trigram because of the data sparseness problem.","From the results\\b it can be seen that the proposed character-trigram generative model significantly exceeds the word-trigram generative model\\b and slightly outperforms the character-based discriminative model. \\tompared with the word-trigram approach\\b the proposed character-trigram model has dramatically raised the overall ROOV from 0.047 to 0.541\\b with the cost of slightly degrading the overall RIV from 0.987 to 0.977\\b which clearly shows that the handicap of the word-based model in handling OOV has been fixed.","In addition\\b unlike the character-based discriminative approach\\b the proposed trigram model is able to increase the overall RIV from 0.963 to 0.977\\b while it pays the cost for degrading the overall ROOV from 0.703 to 0.541. Also\\b the overall precision rate of the proposed trigram \\i \\i 4 http://www.speech.sri.com/projects/srilm/ 5 http://crfpp.sourceforge.net/ 831  model (0.950) is lower than that of the discriminative model (0.954). This implies that the proposed model tends to segment those OOV words into more words than the discriminative model does. However\\b the higher recall indicates that the proposed model segment more right words. Table 2: Segmentation results of the word-based model (Word-unigram/bigram/trigram)\\b the character-","based discriminative model (Discriminative) and various proposed generative n-gram models. AS R P F ROOV RIV CITYU R P F ROOV RIV Word-unigram 0.933 0.878 0.905 0.014 0.975 Word-unigram 0.924 0.851 0.886 0.162 0.984 Word-bigram 0.942 0.877 0.908 0.014 0.984 Word-bigram 0.928 0.851 0.888 0.162 0.990 Word-trigram 0.941 0.877 0.908 0.014 0.983 Word-trigram 0.929 0.852 0.889 0.162 0.990 Discriminative 0.956 0.946 0.951 0.704 0.967 Discriminative 0.940 0.945 0.943 0.701 0.960 New (Bigram) 0.954 0.934 0.944 0.509 0.975 New (Bigram) 0.949 0.932 0.941 0.603 0.976 New (Trigram) 0.958 0.938 0.948 0.518 0.978 New (Trigram) 0.951 0.937 0.944 0.609 0.978 New (4-gram) 0.958 0.938 0.948 0.518 0.978 New (4-gram) 0.951 0.938 0.944 0.610 0.978 New (5-gram) 0.957 0.938 0.948 0.518 0.977 New (5-gram) 0.951 0.938 0.944 0.610 0.978 MSR R P F ROOV RIV PKU R P F ROOV RIV Word-unigram 0.965 0.925 0.945 0.025 0.990 Word-unigram 0.939 0.909 0.924 0.016 0.972 Word-bigram 0.969 0.926 0.947 0.025 0.995 Word-bigram 0.949 0.913 0.931 0.016 0.982 Word-trigram 0.969 0.926 0.947 0.025 0.995 Word-trigram 0.949 0.913 0.930 0.016 0.982 Discriminative 0.963 0.967 0.965 0.723 0.969 Discriminative 0.943 0.954 0.948 0.689 0.952 New (Bigram) 0.965 0.955 0.960 0.522 0.977 New (Bigram) 0.949 0.946 0.948 0.494 0.965 New (Trigram) 0.974 0.967 0.970 0.561 0.985 New (Trigram) 0.952 0.951 0.952 0.503 0.968 New (4-gram) 0.974 0.967 0.971 0.568 0.985 New (4-gram) 0.952 0.952 0.952 0.511 0.967 New (5-gram) 0.974 0.967 0.971 0.568 0.985 New (5-gram) 0.952 0.952 0.952 0.510 0.968 Ove\\tall R P F ROOV RIV Word-unigram 0.943 0.897 0.919 0.047 0.980 Word-bigram 0.950 0.898 0.923 0.047 0.987 Word-trigram 0.950 0.898 0.923 0.047 0.987 Discriminative 0.953 0.954 0.954 0.703 0.963 New (Bigram) 0.955 0.943 0.949 0.527 0.973 New (Trigram) 0.960 0.950 0.955 0.541 0.977 New (4-gram) 0.960 0.950 0.955 0.541 0.977 New (5-gram) 0.960 0.950 0.955 0.545 0.977"]},{"title":"3.3 Statistics \\bf Remaining E\\t\\t\\b\\ts","paragraphs":["Having inspected those remaining testing-set tagging-errors (associated with characters) resulted from the character-trigram generative model and the character-based discriminative model\\b we divide them into two classes: (1) Wrong\\fy Broken: Two adjacent characters should be joined while the model breaks them; (2) Wrong\\fy Jointed: Two adjacent characters should be broken but the model joins them. Table 3 shows that both models share about 50% of their tagging-errors (e.g.\\b the last row\\b labeled as Overa\\f\\f\\b shows that 7\\b068 errors among total 13\\b093 ones from the generative model are shared). To illustrate that the dependency between adjacent characters does affect segmentation performance\\b those tagging-errors are classified into two classes according to their MI values. Since MI of those unseen character-bigrams could not be reliably estimated\\b it could only affect the performance of those seen character-bigrams. Low MI and High MI are classified by the MI value that two probability distribution curves in Figure 1 cross each other\\b which is about 1.5.","In the last row (Overall) under Table 3\\b it could be observed that the proposed generative model generates less percentage of wrongly broken errors than the character-based model does when those seen-bigrams are classified as “High MI” (0.165 versus 0.171\\b shown in bold\\b in the column “Wrongly Broken”); and it also generates less percentage of wrongly joined errors when those seen-bigrams are classified as “Low MI” (0.107 versus 0.149\\b shown in bold\\b in the 832","column “Wrongly Joined”). The conclusion that the proposed model mimics the human","behavior more closely thus could be drawn.","Table 3: Statistics for tagging-errors under the discriminative","model and the generative model of character-based approaches.","Seen Big\\tam e\\t\\t\\b\\ts Unseen Big\\tam e\\t\\t\\b\\ts W\\t\\bngly B\\t\\bken W\\t\\bngly J\\bined C\\b\\tp\\fs M\\bdel","T\\btal","E\\t\\t\\b\\ts Sha\\ted E\\t\\t\\b\\ts W\\t\\bngly","B\\t\\bken W\\t\\bngly","J\\binted L\\bw MI High MI L\\bw MI High MI Discriminative 4803 0.177 0.147 0.200 0.253 0.129 0.093 AS Generative 4860 2650","0.296 0.072 0.212 0.260 0.087 0.073 Discriminative 1985 0.150 0.364 0.141 0.158 0.093 0.094 CITYU Generative 1767 772","0.389 0.166 0.171 0.109 0.079 0.086 Discriminative 2923 0.127 0.282 0.173 0.182 0.196 0.112 MSR Generative 2605 1359","0.298 0.154 0.242 0.109 0.119 0.078 Discriminative 4207 0.099 0.285 0.135 0.126 0.167 0.188 PKU Generative 3861 2287","0.252 0.170 0.150 0.109 0.139 0.181 Discriminative 13918 0.139 0.248 0.167 0.171 0.149 0.126","Ove\\tall Generative 13093 7068 0.296 0.130 0.194 0.165 0.107 0.108"]},{"title":"4 Related W\\b\\tks","paragraphs":["Since the character-based discriminative approach was first proposed by Xue (2003)\\b it has been widely adopted and further developed by various researchers. For example\\b Asahara et al. (2005) use the character-based approach to first identify the OOV candidates and then integrate them into the system. Their system achieves the best result in the AS corpus in Sighan Bakeoff 2005 contest. Tseng et al. (2005) add the information of word-prefixes and word-suffixes to overcome the drawbacks of character-based approaches\\b and they get the best results in the remaining three corpora in that contest. Afterwards\\b Zhang et al. (2006) use a sub-word tagging approach to utilize the sub-word information. All of them adopt the character-based discriminative approaches. The only state-of-the-art word-based model proposed recently is Zhang and \\tlark (2007)\\b which uses Perceptron\\b a discriminative method. The comparison between those models mentioned above is given in Table 4. It shows that the proposed model achieves a good balance between those IV words and OOV words\\b and also competitive results. Table 4: Segmentation results of different Models AS R P F ROOV RIV CITYU R P F ROOV RIV Asahara 0.952 0.951 0.952 0.696 0.963 Tseng 0.941 0.946 0.943 0.698 0.961 Zhang (\\tRF) 0.956 0.947 0.951 0.649 0.969 Zhang (\\tRF) 0.952 0.949 0.951 0.741 0.969 Our model 0.958 0.938 0.948 0.518 0.978 Our model 0.951 0.938 0.944 0.610 0.978 Zhang & \\tlark N/A N/A 0.946 N/A N/A Zhang & \\tlark N/A N/A 0.951 N/A N/A MSR R P F ROOV RIV PKU R P F ROOV RIV Tseng 0.962 0.966 0.964 0.717 0.968 Tseng 0.953 0.946 0.950 0.636 0.972 Zhang (\\tRF) 0.972 0.969 0.971 0.712 0.976 Zhang (\\tRF) 0.947 0.955 0.951 0.748 0.959 Our model 0.974 0.967 0.971 0.568 0.985 Our model 0.952 0.952 0.952 0.511 0.967 Zhang & \\tlark N/A N/A 0.972 N/A N/A Zhang & \\tlark N/A N/A 0.945 N/A N/A"]},{"title":"5 C\\bncl\\fsi\\bn","paragraphs":["Since the traditional word-trigram generative model cannot handle those OOV words\\b it has very poor performance when OOV words are encountered. In addition\\b the popular character-based discriminative approach does not utilize the adhesion and dependency between adjacent characters. Therefore\\b it gives unsatisfactory performance for those IV words. To combine the strengths of these two camps\\b the character-based generative model is thus proposed in this 833  paper to let the character-based approach adopt the generative form. The experiments have shown that this new approach has achieved good balance between those IV words and OOV words. Furthermore\\b the statistics of remaining errors have shown that the proposed model mimics the human behavior more closely than the classic character-based discriminative model.","Moreover\\b the learning process of this character n-gram generative approach is found to be ten times faster than that of the \\tRF discriminative model. That gives additional advantage to the proposed approach when huge training data is at hand."]},{"title":"Refe\\tence","paragraphs":["Asahara\\b Masayuki\\b Kenta Fukuoka\\b Ai Azuma\\b \\thooi-Ling Goh\\b Yotaro Watanabe\\b Yuji Matsumoto and Takashi Tsuzuki\\b 2005. \\tombination of machine learning methods for optimum chinese word segmentation. In the Fourth SIGHAN Workshop on Chinese Language Processing\\b pages 134–137\\b Jeju\\b Korea.","\\then\\b Stanley F. and Joshua Goodman\\b 1998. An empirical study of smoothing techni\\fues for language modeling. Technical Report TR-10-98\\b Harvard University Center \\tor Research in Computing Techno\\fogy.","Emerson\\b Thomas\\b 2005. The Second International \\thinese Word Segmentation Bakeoff. In the Fourth SIGHAN Workshop on Chinese Language Processing\\b pages 123-133\\b Jeju\\b Korea.","Gao\\b Jianfeng\\b Mu Li and \\thang-Ning Huang\\b 2003. Improved Source-\\thannel Models for \\thinese Word Segmentation. In Proc. o\\t ACL\\b pages 272-279.","Lafferty\\b John\\b Andrew Mc\\tallum and Fernando Pereira\\b 2001. \\tonditional Random Fields: Probabilistic Models for Segmenting and Labeling Se\\fuence Data. In Proc. o\\t the 18th Internationa\\f Con\\terence on Machine Learning\\b pages 282-289.","Ng\\b Hwee Tou and Jin Kiat Low\\b 2004. \\thinese part-of-speech tagging: one-at-a-time or all-atonce? word-based or character-based. In Proc. o\\t EMNLP\\b pages 277-284.","Peng\\b Fuchun\\b Fangfang Feng and Andrew Mc\\tallum\\b 2004. \\thinese segmentation and new word detection using conditional random fields. In Proc. o\\t COLING\\b pages 562–568.","Sproat\\b Richard and \\thilin Shih\\b 1990. A statistical method for finding word boundaries in \\thinese text. Computer Processing o\\t Chinese and Orienta\\f Languages\\b 4(4).pages 336-351.","Stolcke\\b Andreas\\b 2002. SRILM-an extensible language modeling toolkit. In Proc. o\\t the Internationa\\f Con\\terence on Spoken Language Processing\\b pages 311-318.","Sun\\b Maosong\\b Dayang Shen and Benjamin K Tsou\\b 1998. \\thinese word segmentation without using lexicon and hand-crafted training data. In Proc. o\\t COLING/ACL\\b pages 1265-1271.","Tseng\\b Huihsin\\b Pichuan \\thang\\b Galen Andrew\\b Daniel Jurafsky and \\thristopher Manning\\b 2005. A \\tonditional Random Field Word Segmenter for Sighan Bakeoff 2005. In the Fourth SIGHAN Workshop on Chinese Language Processing\\b pages 168-171.","Xue\\b Nianwen\\b 2003. \\thinese Word Segmentation as \\tharacter Tagging. Computationa\\f Linguistics and Chinese Language Processing\\b 8(1). pages 29-48.","Zhang\\b Huaping\\b Hongkui Yu\\b Deyi Xiong and Qun Liu\\b 2003. HHMM-based \\thinese lexical analyzer I\\tT\\tLAS. In the Second SIGHAN Workshop on Chinese Language Processing\\b pages 184–187.","Zhang\\b Rui\\fiang\\b Genichiro Kikui and Eiichiro Sumita\\b 2006. Subword-based Tagging for \\tonfidence-dependent \\thinese Word Segmentation. In Proc. o\\t the COLING/ACL\\b pages 961-968\\b Sydney\\b Australia.","Zhang\\b Yue and Stephen \\tlark\\b 2007. \\thinese Segmentation with a Word-Based Perceptron Algorithm. In Proc. o\\t ACL\\b pages 840-847\\b Prague\\b \\tzech Republic. 834"]}]}