{"sections":[{"title":"Sentiment \\b\\tassi\\fication \\bonsidering Negation and \\bontrast Transition","paragraphs":["∗ ∗ ∗ ∗ "," Shoushan\\b\\ti\\band\\b\\fhu-Ren\\bHuang","\\b","Department\\bof\\b\\fhinese\\band\\bBilingual\\bStudies\\b The\\bHong\\bKong\\bPolytechnic\\bUniversity\\b {shoushan.li,\\bchurenhuang}@gmail.com\\b\\b Abstract.\\bNegation\\band\\bcontrast\\btransition\\bare\\btwo\\bkinds\\bof\\blinguistic\\bphenomena\\bwhich\\bare\\b popularly\\bused\\bto\\breverse\\bthe\\bsentiment\\bpolarity\\bof\\bsome\\bwords\\band\\bsentences.\\bIn\\bthis\\bpaper,\\b we\\b propose\\b an\\b approach\\b to\\b incorporate\\b their\\b classification\\b information\\b into\\b our\\b sentiment\\b classification\\bsystem:\\bFirst,\\bwe\\bclassify\\bsentences\\binto\\bsentiment\\breversed\\band\\bnon-reversed\\b parts.\\b Then,\\b represent\\b them\\b as\\b two\\b different\\b bags-of-words.\\b Third,\\b present\\b three\\b general\\b strategies\\b to\\b do\\b classification\\b with\\b two-bag-of-words\\b modeling.\\b We\\b collect\\b a\\b large-scale\\b product\\b reviews\\b involving\\b five\\b domains\\b and\\b conduct\\b our\\b experiments\\b on\\b them.\\b The\\b experimental\\b results\\b show\\b that\\b incorporating\\b both\\b negation\\b and\\b contrast\\b transition\\b information\\b is\\b effective\\b and\\b performs\\b robustly\\b better\\b than\\b traditional\\b machine\\b learning\\b approach\\b(based\\bon\\bone-bag-of-words\\bmodeling)\\bacross\\bfive\\bdifferent\\bdomains.\\b Keywords: Sentiment\\bclassification,\\bopinion\\bmining,\\blinear\\b\\flassifier.\\b \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b","\\b \\fopyright\\b2009\\bby\\bShoushan\\b\\ti\\band\\b\\fhu-Ren\\bHuang\\b"]},{"title":"1 Introduction","paragraphs":["Sentiment\\bclassification\\bis\\ba\\btask\\bto\\bclassify\\btext\\baccording\\bto\\bsentimental\\bpolarities\\bof\\bopinions\\b they\\b contain\\b (e.g.,\\b favorable\\b or\\b unfavorable).\\b This\\b task\\b has\\b received\\b considerable\\b interests\\b in\\b computational\\blinguistic\\bcommunity\\bdue\\bto\\bits\\bwide\\bapplications.\\b\\b","In\\b the\\b latest\\b studies\\b of\\b this\\b task,\\b machine\\b learning\\b techniques\\b become\\b the\\b state-of-the-art\\b approach\\b and\\b have\\b achieved\\b much\\b better\\b results\\b than\\b some\\b rule-based\\b approaches\\b (Kennedy\\b and\\b Inkpen,\\b 2006;\\b Pang\\bet al.,\\b 2002)\\b .\\b In\\b machine\\b learning\\b approach,\\b a\\b document\\b (text)\\b is\\b usually\\bmodeled\\bas\\ba\\bbag-of-words,\\ba\\bset\\bof\\bwords\\bwithout\\bany\\bword\\border\\bor\\bsyntactic\\brelation\\b information.\\bTherefore,\\bthe\\bwhole\\bsentimental\\borientation\\bis\\bhighly\\binfluenced\\bby\\bthe\\bsentiment\\b polarity\\bof\\beach\\bword.\\bNotice\\bthat\\balthough\\beach\\bword\\btakes\\ba\\bfixed\\bsentiment\\bpolarity\\bitself,\\bits\\b polarity\\b contributed\\b to\\b the\\b whole\\b sentence\\b or\\b document\\b might\\b be\\b completely\\b the\\b opposite.\\b Negation\\b and\\b contrast\\b transition\\b are\\b exactly\\b the\\b two\\b kinds\\b of\\b linguistic\\b phenomena\\b which\\b are\\b able\\b to\\b reverse\\b the\\b sentiment\\b polarity.\\b For\\b example,\\b see\\b a\\b sentence\\b containing\\b negation\\b \"th\\b\\t mo\\f\\be \\b\\t not good\"\\b and\\b another\\b sentence\\b containing\\b contrast\\b transition\\b \"th\\b\\t mou\\te \\b\\t good look\\bng, but \\bt work\\t terr\\bbly\".\\bThe\\bsentiment\\bpolarity\\bof\\bthe\\bword\\bgood\\bin\\bthese\\btwo\\bsentences\\b is\\bpositive\\bbut\\bthe\\bwhole\\bsentences\\bare\\bnegative.\\bTherefore,\\bwe\\bcan\\bsee\\bthat\\bthe\\bwhole\\bsentiment\\b is\\b not\\b necessarily\\b the\\b sum\\b of\\b the\\b parts\\b (Turney,\\b 2002).\\b This\\b phenomenon\\b is\\b one\\b main\\b reason\\b why\\bmachine\\blearning\\boften\\bfails\\bto\\bclassify\\bsome\\btesting\\bsamples\\b(Dredze\\bet al.,\\b2008).\\b\\b","Fortunately,\\ba\\blanguage\\busually\\bhas\\bsome\\bspecial\\bwords\\bwhich\\bindicate\\bthe\\bpossible\\bpolarity\\b shift\\bof\\ba\\bword\\bor\\beven\\ba\\bsentence.\\bThese\\bwords\\bare\\bcalled\\bcontextual\\bvalence\\bshifters\\b(\\fVSs)\\b which\\b can\\b cause\\b the\\b valence\\b of\\b a\\b lexical\\b item\\b to\\b shift\\b from\\b one\\b pole\\b to\\b the\\b other\\b or,\\b less\\b forcefully,\\b even\\b to\\b modify\\b the\\b valence\\b towards\\b a\\b more\\b neutral\\b position\\b (Polanyi\\b and\\b Zaenen,\\b 2006).\\b Generally\\b speaking,\\b \\fVSs\\b are\\b classified\\b into\\b two\\b categories:\\b sentence-based\\b and\\b 297 23rd Pacific Asia Conference on Language, Information and Computation, pages 297–306 \\b discourse-based\\b(Polanyi\\band\\bZaenen,\\b2006).\\bSentence-based\\b\\fVSs\\bare\\bresponsible\\bfor\\bshifting\\b valence\\bof\\bsome\\bwords\\bin\\ba\\bsentence.\\bThe\\bmost\\bobvious\\bshifters\\bare\\bnegatives,\\bsuch\\bas\\bnot,\\bnone,\\b ne\\fer,\\bnoth\\bng,\\band\\bhardly.\\bThese\\bshifts\\busually\\breverse\\bthe\\bsentiment\\bpolarity\\bof\\bsome\\bwords.\\b Other\\bsentence-based\\bshifters\\bcan\\bbe\\bintensifiers\\b(e.g.,\\brather,\\b\\fery),\\bmodal\\boperators\\b(e.g.,\\b\\bf),\\b etc.\\bDiscourse-based\\b\\fVSs\\boften\\bindicate\\bthe\\bvalence\\bshifting\\bin\\bthe\\bcontext.\\bSome\\bconnectives,\\b such\\bas\\bhowe\\fer,\\bbut,\\band\\bnotw\\bth\\ttand\\bng,\\bbelong\\bto\\bthis\\btype.\\b","In\\b this\\b paper,\\b we\\b mainly\\b focus\\b on\\b sentiment\\b shifting\\b including\\b negation\\b and\\b contrast\\b transition\\b because\\b this\\b kind\\b of\\b shifting\\b often\\b fully\\b reverses\\b the\\b sentiment\\b polarity\\b and\\b thus\\b mostly\\breflects\\bthe\\bweakness\\bof\\bthose\\bmachine\\blearning\\bapproaches\\bbased\\bon\\bone-bag-of-words\\b modeling.\\b Other\\b types\\b of\\b shifting,\\b for\\b instance,\\b intensification\\b with\\b intensifiers\\b (e.g.,\\brather,\\b \\fery)\\bis\\bcapable\\bof\\bchanging\\bthe\\bintension\\bof\\bsome\\bwords\\bbut\\bwould\\bnot\\breverse\\btheir\\bpolarities.\\b\\b","Note\\b that\\b contrast\\b transition\\b is\\b one\\b special\\b type\\b of\\b transition\\b and\\b is\\b used\\b to\\b express\\b contradiction\\b or\\b contrast\\b when\\b connecting\\b one\\b paragraph,\\b sentence,\\b clause\\b or\\b word\\b with\\b the\\b other.\\bIt\\bis\\bdistinguished\\bfrom\\bother\\btypes\\bof\\btransitions\\b by\\b different\\b connectives.\\b For\\b contrast\\b transitions,\\bthe\\bconnectives\\bare\\bsome\\b\\fVSs\\blike\\bhowe\\fer,\\bbut,\\band\\bnotw\\bth\\ttand\\bng\\bwhile\\bothers\\b use\\bsome\\bdifferent\\bconnectives,\\be.g.,\\bconclusion\\btransition\\btakes\\bthe\\bconnectives\\blike\\btherefore,\\b \\bn a word,\\b\\bn \\tummary,\\band\\b\\bn br\\bef.\\b","To\\bincorporate\\b sentiment\\b reversing\\b information\\b into\\b a\\b machine\\b learning\\b approach,\\b we\\b first\\b segment\\bthe\\bwhole\\bdocument\\binto\\bsub-sentences.\\bWe\\bthen\\bpartition\\bthem\\binto\\btwo\\bgroups:\\bone\\b includes\\b those\\b called\\b sentiment-reversed\\b sentences\\b and\\b the\\b other\\b includes\\b those\\b called\\b sentiment-non-reversed\\b sentences.\\b As\\b a\\b result,\\b each\\b document\\b is\\b represented\\b as\\b two-bags-of-words\\brather\\bthan\\btraditional\\bone-bag-of-words.\\bFinally,\\bwe\\bpropose\\bthe\\bclassification\\balgorithm\\b to\\bdo\\bthe\\bclassification\\bon\\bthe\\btext\\bwith\\btwo-bags-of-words\\bmodeling.\\b","The\\bremainder\\bof\\bthis\\bpaper\\bis\\borganized\\bas\\bfollows.\\bSection\\b2\\bintroduces\\bthe\\brelated\\bwork\\b on\\b \\fVS\\b applications\\b in\\b sentiment\\b classification.\\b Section\\b 3\\b presents\\b our\\b approach\\b in\\b detail.\\b Experimental\\b results\\b are\\b presented\\b and\\b analyzed\\b in\\b Section\\b 4.\\b Finally,\\b Section\\b 5\\b draws\\b our\\b conclusions\\band\\boutlines\\bthe\\bfuture\\bwork.\\b"]},{"title":"2 Re\\tated Work","paragraphs":["During\\b recent\\b several\\b years,\\b various\\b of\\b issues\\b have\\b been\\b studied\\b for\\b sentiment\\b classification,\\b such\\b as\\b feature\\b extraction\\b (Riloff\\bet al.,\\b 2006),\\b domain\\b adaptation\\b (Blitzer\\bet al.,\\b 2007)\\b and\\b multi-domain\\b learning\\b (\\ti\\b and\\b Zong,\\b 2008).\\b For\\b a\\b detailed\\b survey\\b of\\b this\\b research\\b field,\\b see\\b Pang\\band\\b\\tee\\b(2008).\\bHowever,\\bmost\\bstudies\\bdirectly\\bborrow\\bmachine\\blearning\\bapproach\\bfrom\\b traditional\\b topic-based\\b text\\b classification\\b and\\b very\\b few\\b work\\b are\\b focus\\b on\\b incorporating\\b linguistic\\bknowledge\\bthat\\bsentiment\\btext\\bparticularly\\bcontains,\\be.g.,\\bvalence\\bshifting\\bphenomena\\b and\\bcomparative\\bsentences\\b(Jindal\\band\\b\\tiu,\\b2006).\\b","Pang\\bet al.\\b (2002)\\b first\\b employ\\b machine\\b learning\\b approach\\b to\\b sentiment\\b classification\\b and\\b find\\b that\\b machine\\b learning\\b methods\\b definitely\\b outperform\\b human-produced\\b baselines.\\b In\\b their\\b approach,\\b they\\b consider\\b negation\\b by\\b adding\\b the\\b tag\\b NOT\\b to\\b every\\b word\\b between\\b a\\b negation\\b word\\b(not,\\b\\b\\tn’t,\\bd\\bdn’t,\\betc.)\\band\\bthe\\bfirst\\bpunctuation\\bmark\\bfollowing\\bthe\\b negation\\b word.\\b But\\b their\\b results\\b show\\b that\\b adding\\b negation\\b has\\b a\\b very\\b negligible\\b and\\b on\\b average\\b slightly\\b harmful\\b effect\\bon\\bthe\\bperformance.\\b\\b","Kennedy\\b and\\b Inkpen\\b (2006)\\b check\\b three\\b types\\b of\\b \\fVSs:\\b negatives,\\b intensifiers,\\b and\\b diminishers\\b and\\b add\\b their\\b valence\\b shifting\\b bigrams\\b as\\b additional\\b features.\\b Their\\b results\\b show\\b that\\bconsidering\\b\\fVSs\\bgreatly\\bimprove\\bthe\\bperformances\\bof\\bterm-counting\\bapproach.\\bBut\\bas\\bfar\\b as\\bmachine\\blearning\\bapproach\\bis\\bconcerned,\\bthe\\bimprovement\\bis\\bvery\\bslight\\b(less\\bthan\\b1%).\\b\\b","Na\\bet al.\\b (2004)\\b attempt\\b to\\b model\\b negation\\b more\\b accurately\\b and\\b achieve\\b a\\b satisfactory\\b improvement.\\b However,\\b they\\b need\\b to\\b do\\b part-of-speech\\b to\\b get\\b negation\\b phrases\\b and\\b their\\b baseline\\bperformance\\bitself\\bis\\bvery\\blow\\b(less\\bthan\\b80%).\\b","Different\\bfrom\\ball\\bthe\\babove\\bwork,\\bour\\bapproach\\bis\\beasy\\bto\\bimplement\\band\\bneed\\bno\\badditional\\b features\\b(e.g.,\\bbi-gram,\\bpart-of-speech\\btag).\\bFurthermore,\\bour\\bapproach\\bis\\bcapable\\bof\\bconsidering\\b 298 both\\b negation\\b and\\b contrast\\b transition.\\b In\\b our\\b view,\\b only\\b considering\\b negation\\b is\\b not\\b enough\\b since\\bthere\\bare\\bsome\\bnegation\\bsentences\\bappear\\bin\\ba\\bcontrast\\btransition\\bstructure.\\bFor\\bexample,\\b th\\b\\t mou\\te \\b\\t not good look\\bng, but \\bt work\\t perfect and I l\\bke \\bt.\\bApparently,\\bonly\\bconsidering\\b negation\\bis\\bstill\\bdifficult\\bto\\bgive\\ban\\bcorrect\\bsentiment\\bclassification\\bin\\bthis\\bcase.\\b"]},{"title":"3 Our Approach 3.1 \\b\\tassi\\fication A\\tgorithm","paragraphs":["In\\b a\\b standard\\b machine\\b learning\\b classification\\b problem,\\bwe\\b seek\\b a\\b predictor\\bf (also\\b called\\b a\\b classifier)\\b that\\b maps\\b an\\b input\\b vector\\bx\\b to\\b the\\b corresponding\\b class\\b label\\by.\\b The\\b predictor\\b is\\b trained\\bon\\ba\\bfinite\\bset\\bof\\blabeled\\bexamples\\b(X,\\bY)\\bwhich\\bare\\bdrawn\\bfrom\\ban\\bunknown\\bdistribution\\b D.\\bThe\\blearning\\bobjective\\bis\\bto\\bminimize\\bthe\\bexpected\\berror,\\bi.e.,\\b",",arg min \\b ( ( ), ) f X Yf L f X Y ∈="]},{"title":"∑Η","paragraphs":["\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b(1)\\b","where\\bL\\bis\\ba\\bprescribed\\b loss\\b function\\b and\\bH\\b is\\b a\\b set\\b of\\b functions\\b called\\b the\\b hypothesis\\b space,\\b","which\\bconsists\\bof\\bfunctions\\bfrom\\bx\\bto\\by.\\b As\\ba\\blinear\\bclassifier,\\bthe\\bpredictor\\btakes\\bthe\\bform\\b( ) T","\\b \\bf X w X= .\\bThen\\ba\\bregularized\\bform\\bof\\b","formula\\b(1)\\bis\\boften\\bused\\bas\\bbelow,\\bwhich\\balways\\bhas\\ba\\bunique\\band\\bnumerically\\bstable\\bsolution\\b","2","2 ,̂arg min \\b ( , ) 2 T","w X Yw L w X Y wλ = +"]},{"title":"∑","paragraphs":["\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b(2)\\b where\\b 2 2w = T","w w \\band\\bλ is\\ba\\bnon-negative\\bregularization\\bparameter.\\bIf\\b0λ = ,\\bthe\\bproblem\\bis\\bun-","regularized.\\b\\b \\b","\\b Figure 1:\\bStandard\\bonline\\bSGD\\balgorithm\\b \\b","Solving\\b (2)\\b with\\b stochastic\\b gradient\\b descent\\b (SGD),\\b we\\b get\\b the\\b standard\\b SGD\\b online\\b","updating\\bstrategy\\bas\\bfollowing\\b(Zhang,\\b2004)\\b","1","1 1 1 1̂̂̂(̂ ( , ) )T","t t t t t t t tw w S w L w X Y Xη λ\\b","\\b \\b \\b\\t= \\b + \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b(3)\\b","where\\b1( , ) ( , )L p y L p y p∂ \\t = ∂ \\band\\b( , )t tX Y is\\b the\\b instance\\b we\\b are\\b observing\\b at\\b the\\bt-th\\b step.\\b The\\b matrix\\bS\\b can\\b be\\b regarded\\b as\\b a\\b pre-conditioner.\\b For\\b simplicity,\\b we\\b assume\\b it\\b to\\b be\\b a\\b constant\\b matrix.\\b 0tη > is\\b a\\b appropriately\\b chosen\\b learning\\b rate\\b parameter.\\b The\\b whole\\b algorithm\\b is\\b described\\bin\\bFigure 1\\b(Zhang,\\b2004).\\b \\b \\b A\\tgorithm (standard SGD) Initialize\\b0ŵ \\b for\\bt=1,2,\\b...\\b \\b\\b\\b\\bDraw\\b(,t tX Y )\\brandomly\\bfrom\\bD.\\b \\b\\b\\b\\bUpdate\\b1t̂w \\b \\bas\\b","\\b\\b\\b\\b 1","1 1 1 1̂̂̂(̂ ( , ) )T","t t t t t t t tw w S w L w X Y Xη λ\\b","\\b \\b \\b\\t= \\b + \\b end\\b 299 \\b"]},{"title":"3.2 Text Mode\\ting","paragraphs":["In\\btraditional\\b text\\b classification\\b tasks,\\b a\\b text\\bT\\b (e.g.,\\b document,\\b sentence)\\b are\\b modeled\\b as\\b one\\b bag-of-words\\b and\\b the\\b input\\b vector\\b of\\b the\\b text\\b is\\b constructed\\b from\\b weights\\b of\\b the\\b words\\b (also\\b called\\b terms)\\b1( ,..., )Nt t .\\b In\\b this\\b paper,\\b we\\b focus\\b on\\b document-based\\b sentiment\\b classification.\\b Specifically,\\b the\\b terms\\b are\\b possibly\\b words,\\b word\\b n-grams,\\b or\\b even\\b phrases\\b extracted\\b from\\b the\\b training\\bdata,\\bwith\\bN\\bbeing\\bthe\\bnumber\\bof\\bterms.\\bThe\\bweights\\bare\\bstatistic\\binformation\\bof\\bthese\\b terms,\\be.g.,\\btf,\\btf \\bdf\\f .\\bThen\\bthe\\btext\\bT\\bis\\brepresented\\bas\\ba\\bvector\\b( )X T ,\\bi.e.,\\b","1 2( ) ( ), \\b ( ), \\b...\\b, \\b ( )NX T \\tta t \\tta t \\tta t=< > \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b(4)\\b","The\\b output\\b label\\b y\\b has\\b a\\b value\\b of\\b 1\\b or\\b -1\\b representing\\b a\\b positive\\b or\\b negative\\b sentiment\\b polarity.\\b","As\\ba\\bspecial\\bcase\\bof\\btext\\bclassification,\\bsentiment\\bclassification\\bapplies\\bbag-of-words\\bmodel\\b directly\\b for\\b a\\b long\\b time.\\b Although\\b machine\\b learning\\b with\\b this\\b text\\b modeling\\b approach\\b has\\b shown\\bto\\bperform\\bmuch\\bbetter\\bthan\\bsome\\brule-based\\bapproaches,\\be.g.,\\bterm-counting\\bapproach,\\b the\\b achieved\\b performance\\b is\\b much\\b worse\\b than\\b traditional\\b topic-based\\b text\\b classification.\\b \\fompared\\b to\\b topic-based\\b classification,\\b one\\b big\\b challenge\\b in\\b sentiment\\b classification\\b is\\b that\\b sentiment\\bpolarity\\bof\\bone\\bword\\bis\\bnot\\balways\\bconsistent\\bwith\\bthe\\bwhole\\borientation\\bof\\bthe\\btext.\\b \\fonsider\\bthe\\bfollowing\\btwo\\bsentences:\\b a1. Th\\b\\t \\b\\t not a good mo\\f\\be and I hate \\bt. \\b a2.\\bTh\\b\\t \\b\\t \\tuch a good mo\\f\\be and I do not hate \\bt at all.\\b","Because\\b they\\b are\\b represented\\b as\\b almost\\b the\\b same\\b bag-of-words,\\b their\\b classification\\b results\\b would\\bbe\\bthe\\bsame\\bwhen\\bapplying\\bmachine\\blearning\\bwith\\bone-bag-of-words\\bmodeling.\\bBut\\btheir\\b sentiment\\b polarities\\b are\\b obviously\\b different\\b from\\b each\\b other.\\b Therefore,\\b traditional\\b bag-of-words\\bmodeling\\bis\\bnot\\bappropriate\\bfor\\bsentiment\\bclassification\\bto\\bsome\\bextent.\\b","Instead\\bof\\bconsidering\\ba\\btext\\bas\\ba\\bbag-of-words,\\bwe\\bpropose\\ba\\bnew\\btext\\bmodeling\\bapproach\\b which\\bconsiders\\ba\\btext\\bas\\btwo\\bbags-of-words.\\bSpecifically,\\ba\\btext\\bT,\\beither\\bfor\\btraining\\bor\\btesting,\\b is\\b partitioned\\b into\\b two\\b sub-texts:\\b sentiment-reversed\\b part\\breT and\\b sentiment-non-reversed\\b part\\b nonT .\\b Sentiment-reversed\\b part\\b ideally\\b contains\\b those\\b sentences\\b which\\b holds\\b words\\b with\\b the\\b opposite\\bsentiment\\bpolarity\\bcompared\\bto\\bthe\\bwhole\\bdocument’s.\\b","Formally,\\b a\\b text\\bT\\b consists\\b of\\b multiple\\b sentences,\\b i.e.,\\b1 2( , ,..., )mT \\t \\t \\t= .\\b Suppose\\b each\\b sentence\\b takes\\b a\\b sentiment-reversed\\b tagging\\bV\\b which\\b represents\\b whether\\b it\\b is\\b a\\b sentimentreversed\\bsentence\\b(( ) 1V \\t= )\\bor\\bnot\\b(( ) 1V \\t= \\b).\\bOriginally,\\bevery\\bsentence\\bis\\bassigned\\bthe\\bsame\\b tagging\\bvalue\\bof\\b-1,\\bi.e.,\\b( ) 1o \\bV \\t = \\b, 1, 2,...,\\b m= .\\b"]},{"title":"3.3 Sentence Segmentation","paragraphs":["We\\bassume\\bthe\\bsentences\\bas\\bthe\\bbasic\\btext\\bunit\\band\\beach\\bone\\bwould\\bbe\\bassigned\\ba\\btag.\\bActually,\\b the\\bideal\\bbasic\\btext\\bunit\\bshould\\bbe\\b something\\b like\\b clauses\\b rather\\b than\\b sentences\\b (we\\b call\\b them\\b sub-sentences).\\bFor\\bexample,\\b\\b b1. Th\\b\\t \\b\\t not a good mo\\f\\be and I hate \\bt. b2. I l\\bke \\bt becau\\te I d\\bdn’t want to tran\\tfer \\f\\bdeo.","Although\\b these\\b two\\b sentences\\b contain\\b negation,\\b it\\b is\\b unsuitable\\b to\\b put\\b the\\b whole\\b sentence\\b into\\b the\\b sentiment-reversed\\b part.\\b A\\b better\\b way\\b is\\b to\\b first\\b segment\\b the\\b sentences\\b into\\b subsentences\\band\\bassign\\beach\\bone\\bthe\\bsentiment-reversed\\btagging.\\b","We\\b implement\\b a\\b simple\\b approach\\b to\\b segment\\b a\\b document\\b into\\b sub-sentences.\\b First,\\b we\\b do\\b segmentation\\b merely\\b with\\b the\\b punctuations,\\b such\\b as\\b period,\\b comma,\\b and\\b interrogation\\b mark.\\b Then,\\bwe\\buse\\bsome\\bmanually-collected\\bkey\\bwords,\\bsuch\\bas\\band,\\bbecau\\te\\b and\\b\\t\\bnce\\b for\\b further\\b segmentation.\\bThese\\bkey\\bwords\\bare\\bused\\bto\\bintroduce\\bvarious\\bcomplex\\bsentences\\bwith\\bclauses.\\b","\\b 300"]},{"title":"3.4 Sentiment-reversed Sentence Detection","paragraphs":["A\\blanguage\\busually\\bhas\\bsome\\bspecial\\bwords\\bcalled\\b\\fWSs\\bto\\bindicate\\bpossible\\bsentiment\\bshifting\\b of\\ba\\bword\\bor\\ba\\bsentence.\\bAs\\bmentioned\\bin\\bthe\\bintroduction,\\btwo\\bkinds\\bof\\b\\fWSs\\bare\\bcommonly\\b used\\bto\\bindicate\\bvalence\\bswitching:\\bnegatives\\band\\bcontrast\\btransition\\bconnectives.\\bWe\\bwould\\buse\\b these\\b\\fWSs\\bto\\btag\\bsentence\\bto\\bbe\\ba\\bsentiment-reversed\\bsentence\\bor\\bnot.\\b","If\\bthe\\bsentence\\b\\b\\t\\bcontains\\bk\\bnegatives,\\bwe\\bupdate\\bthe\\btagging\\bvalue\\bas\\bfollowing:\\b","( ) ( ) ( 1)k","Neg \\b o \\bV \\t V \\t= × \\b \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b(5)\\b","As\\bfor\\btransition\\bconnectives,\\bwe\\bfirst\\bneed\\bto\\brecognize\\bwhich\\brelated\\bsentences\\bare\\bpossible\\b to\\bbe\\bsentiment-reversed.\\bDifferent\\bfrom\\bnegatives,\\beach\\btransition\\bconnective\\bhas\\bits\\bown\\brule\\b to\\b pick\\b sentiment-reversed\\b sentences\\b around\\b it.\\b Here,\\b we\\b only\\b focus\\b on\\b two\\b transition\\b connectives:\\bbut\\b and\\bhowe\\fer\\b because\\b they\\b appear\\b most\\b frequently\\b and\\b more\\b likely\\b to\\b really\\b reverse\\bthe\\bsentiment\\bpolarity.\\bIf\\bthe\\bconnective\\bis\\bbut,\\bthe\\bsentence\\bbefore\\bit\\bmight\\bbe\\bsentiment\\b reversing.\\b If\\b the\\b connective\\b is\\bhowe\\fer,\\b there\\b might\\b be\\b not\\b only\\b one\\b sentiment-reversed\\b sentence\\b before\\b it.\\b We\\b only\\b pick\\b the\\b nearest\\b one\\b as\\b the\\b sentiment-reversed\\b sentence\\b to\\b avoid\\b introducing\\b too\\b many\\b noises.\\b Overall,\\b if\\b the\\b sentence\\b\\b\\t\\bappears\\b before\\bbut\\b or\\bhowe\\fer,\\b we\\b update\\bits\\btagging\\bvalue\\bas\\bfollowing:\\b","\\b ( ) ( ) ( 1)Tran \\b Neg \\bV \\t V \\t= × \\b \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b(6)\\b","Then,\\bwe\\bget\\bthe\\bsentiment-reversed\\bpart\\breT and\\bsentiment-non-reversed\\bpart\\bnonT \\bas\\bfollows.\\b 1 2{ ( ) 1, ( , ,..., )}re Tran mT V \\t \\t \\t \\t \\t= = ∈ \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b(7)\\b and,\\b","1 2{ ( ) 1, ( , ,..., )}non Tran mT V \\t \\t \\t \\t \\t= = \\b ∈ \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b(8)\\b","It\\bis\\b worth\\b pointing\\b out\\b that\\b the\\b sentiment-reversed\\b sentences\\b obtained\\b by\\b our\\b approach\\b sometimes\\b are\\b not\\b really\\b sentiment\\b reversed.\\b This\\b is\\b due\\b to\\b some\\b mistakes\\b in\\b sentence\\b segmentation\\b and\\b reversed-sentiment\\b detection.\\b Meanwhile,\\b some\\b real\\b sentiment-reversed\\b sentences\\bare\\bnot\\bable\\bto\\bbe\\brecognized.\\b\\fonsider\\bthe\\bfollowing\\bsentence:\\b\\b c1. It could ha\\fe been a great product. I d\\b\\tl\\bke \\bt, howe\\fer.","The\\b sub-sentence\\b (I d\\b\\tl\\bke \\bt)\\b before\\bhowe\\fer\\b is\\b actually\\b not\\b sentiment-reversed\\b but\\b the\\b previous\\bsentence\\b(It could ha\\fe been a great product)\\bis.\\bIn\\bfact,\\brecognizing\\bthose\\bsentimentreversed\\b sentences\\b can\\b hardly\\b perform\\b perfectly\\b and\\b it\\b might\\b be\\b as\\b difficult\\b as\\b sentiment\\b classification\\bitself.\\bNevertheless,\\bour\\bmain\\bobjective\\bhere\\bis\\bto\\bbuild\\ban\\bapproach\\bwhich\\bis\\bable\\b to\\bincorporate\\bthe\\bsentiment\\breversing\\binformation.\\bAs\\ba\\bpreliminary\\bstep,\\bwe\\btry\\bto\\brecognize\\b most\\bsentiment-reversed\\bsentences\\band\\bdecrease\\btheir\\binfluence\\bto\\bthe\\bwhole\\bsentiment.\\b"]},{"title":"3.5 Sentiment \\b\\tassi\\fication","paragraphs":["In\\bthis\\b section,\\b we\\b propose\\b three\\b general\\b strategies\\b for\\b classifying\\b the\\b text\\b with\\b two-bags-of-words\\b modeling:\\b (1)\\b remove\\b the\\b sentiment-reversed\\b part;\\b (2)\\b tune\\b the\\b parameters\\b of\\b the\\b sentiment-reversed\\b part\\b according\\b to\\b those\\b learned\\b from\\b the\\b sentiment-non-reversed\\b part;\\b (3)\\b simultaneity\\blearn\\bboth\\bsentiment-reversed\\band\\bsentiment-non-reversed\\bparts.\\b\\b","The\\bfirst\\bnaive\\bstrategy,\\bcalled\\bremo\\fe \\ttrategy,\\bis\\bto\\bdirectly\\bremove\\bthe\\bsentiment-reversed\\b part\\bconsidering\\bthat\\bthey\\bmight\\bbadly\\binfluence\\bthe\\bwhole\\bsentiment.\\bAccordingly,\\bthe\\btext\\bis\\b represented\\bas\\ba\\bbag-of-words\\bwhich\\bonly\\bcontains\\bthe\\bwords\\bin\\ball\\bsentiment-non-reversed\\btext,\\b i.e.,\\bnonT .\\bThen,\\bthe\\bwords\\bin\\bnonT \\bare\\bused\\bto\\bgenerate\\binput\\bvectors\\bNX for\\beach\\bdocument.\\bThe\\b learning\\bobjective\\bis\\bto\\bminimize\\bthe\\bfollowing\\bexpected\\berror\\b","2","2",",̂arg min \\b ( , ) 2n","T n","n n N n","w X Yw L w X Y wλ","= +"]},{"title":"∑","paragraphs":["\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b(9)\\b","In\\bthe\\btesting\\bphase,\\bthe\\blabel\\bY \\t\\bof\\bone\\bsample\\bNX \\t\\bis\\bestimated\\bas\\b (̂ )T n NY Sgn w X\\t \\t= \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b(10)\\b 301 \\b Where\\b ( )Sgn x is\\bdefined\\bas\\b 1\\b \\b 0 ( ) 0 \\b 0 -1 \\b 0 \\bf x Sgn x \\bf x \\bf x","> ","= =  < \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b(11)\\b","The\\b second\\b strategy,\\b called\\b\\th\\bft \\ttrategy,\\b takes\\b the\\b same\\b learning\\b process\\b as\\b the\\b first\\b strategy\\b in\\b the\\b training\\b phase\\b but\\b perform\\b different\\b estimation\\b in\\b the\\b testing\\b phase.\\b Since\\b the\\b sentences\\b in\\b the\\b sentiment-reversed\\b part\\b are\\b possibly\\b expressing\\b the\\b reversed\\b polarities,\\b we\\b would\\blike\\bto\\bshift\\bthe\\bparameters\\bn̂w when\\bthey\\bare\\bapplied\\bto\\bthe\\bsentiment-reversed\\btext.\\bThus\\b the\\blabel\\bY \\t\\bof\\bone\\bsample\\b(NX \\t, N reX \\b\\t )\\bis\\bestimated\\bas\\b","̂(̂ ( 1) )T T","n N n N reY Sgn w X w X \\b\\t \\t \\t= + \\b \\f \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b(12)\\b where\\bN reX \\b\\t represents\\bthe\\binput\\bvector\\bof\\bthe\\bsentiment-reversed\\btext.\\bHere,\\bNX \\t\\band\\bN reX \\b\\t are\\b generated\\bfrom\\bthe\\bsame\\bterm\\bset\\bas\\bthe\\bfirst\\bstrategy,\\bi.e.,\\bthe\\bwords\\bin\\bnonT .\\b","The\\b third\\b strategy,\\b called\\bjo\\bnt \\ttrategy,\\b simultaneity\\b learning\\b both\\b sentiment-reversed\\b and\\b sentiment-non-reversed\\b parts.\\b In\\b the\\b training\\b phase,\\b the\\b learning\\b objective\\b is\\b to\\b minimize\\b the\\b following\\bexpected\\berror\\b","2 2","2 2",", ,̂,̂ arg min \\b ( , )","2 2n r","T T n r","n r n N r R re n r","w w X Yw w L w X w X Y w wλ λ","\\b= + + +"]},{"title":"∑","paragraphs":["\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b(13)\\b where\\bR reX \\b represents\\bthe\\binput\\bvector\\bof\\bthe\\bsentiment-reversed\\btext.\\bHere,\\bNX \\band\\bR reX \\b are\\b generated\\bfrom\\bdifferent\\bterm\\bsets:\\bthe\\bwords\\bin\\bnonT \\band\\bin\\breT respectively.\\b\\b In\\bthe\\btesting\\bphase,\\bthe\\blabel\\bY\\t\\bof\\bone\\bsample\\b(NX \\t, R reX \\b\\t )\\bis\\bestimated\\bas\\b","̂(̂ )T T","n N r R reY Sgn w X w X \\b\\t \\t \\t= + \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b(14)\\b","Although\\ball\\bstrategies\\bare\\bexpressed\\bin\\bterms\\bof\\blinear\\bclassifiers,\\bthe\\bcorresponding\\bideas\\b for\\b the\\b first\\b and\\b third\\b strategies\\b are\\b general\\b for\\b any\\b other\\b classification\\b algorithms.\\b Overall\\b speaking,\\b only\\b the\\b third\\b one\\b really\\b utilizes\\b both\\b the\\b reversed-sentiment\\b and\\b non-reversed\\b sentiment\\b information\\b for\\b learning.\\b Also,\\b it\\b shares\\b the\\b similar\\b computational\\b complexity\\b as\\b traditional\\bmachine\\blearning\\bapproaches\\bbased\\bon\\bone-bag-of-words\\bmodeling.\\b"]},{"title":"4 Experimenta\\t Studies 4.1 Experimenta\\t Setup Data Set","paragraphs":[":\\b\\bThere\\bare\\bsome\\bfamous\\bpublic\\bdata\\bsets\\bavailable\\bfor\\bsentiment\\bclassification\\bstudies.\\b Among\\b them,\\b \\fornell\\b movie-review\\b dataset1","\\b(Pang\\b and\\b \\tee,\\b 2004)\\b and\\b product\\b reviews2","\\b (Blitzer\\bet al.,\\b 2007)\\b are\\b most\\b popularly\\b used.\\b Both\\b of\\b them\\b are\\b 2-category\\b (positive\\b and\\b negative)\\btasks\\band\\beach\\bconsists\\bof\\b2,000\\b reviews\\b in\\b a\\b domain.\\b The\\b results\\b in\\b some\\b previous\\b work\\bare\\bsometimes\\bnot\\bconsistent\\bdue\\bto\\bthe\\bapplication\\bof\\bdifferent\\bdomains\\bof\\breviews\\bwhen\\b negation\\b is\\b considered\\b (Pang\\bet al.,\\b 2002\\b and\\b Na\\bet al.,\\b 2004).\\b Thus\\b we\\b follow\\b the\\b way\\b of\\b Blitzer\\bet al.\\b (2007)\\b to\\b collect\\b more\\b data\\b involving\\b data\\b in\\b our\\b experiments.\\b Specifically,\\b we\\b totally\\bcollect\\b5\\bdomains\\bof\\breviews\\bfrom\\bAmazon.cn,\\bnamely\\bBook,\\b\\famera,\\bHD\\b(Hard\\bDisk),\\b Health\\b and\\b Kitchen.\\b Each\\b domain\\b consists\\b of\\b 2,400\\b reviews\\b and\\b each\\b category\\b (negative\\b or\\b positive)\\bcontains\\b1,200\\breviews.\\b","Experiment Imp\\tementation:\\bWe\\bperform\\b5-fold\\bcross\\bvalidation\\bin\\ball\\bexperiments.\\bThat\\b is\\bto\\bsay,\\bthe\\bdataset\\bin\\beach\\bdomain\\bis\\brandomly\\band\\bevenly\\bsplit\\binto\\b5\\bfolds.\\bThen\\bwe\\buse\\beach\\b \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b 1 \\bhttp://www.cs.cornell.edu/People/pabo/movie-review-data/\\b\\b 2 \\bhttp://www.seas.upenn.edu/~mdredze/datasets/sentiment/\\b 302 4\\b folds\\b for\\b training\\b and\\b the\\b remaining\\b 1\\b fold\\b for\\b testing.\\b We\\b use\\b accuracy\\b to\\b measure\\b the\\b classification\\bperformances.\\b","Features:\\b The\\b features\\b are\\b single\\b words\\b with\\b a\\b BOO\\t\\b weight\\b (0\\b or\\b 1),\\b representing\\b the\\b presence\\bor\\babsence\\bof\\ba\\bfeature.\\b","\\b\\tassi\\fication A\\tgorithm:\\b We\\b use\\b SGD\\b linear\\b predictors\\b with\\b Huber\\b function\\b as\\b the\\b loss\\b function\\b(Zhang,\\b2004).\\b\\fompared\\bto\\bsupport\\bvector\\bmachine\\b(SVM),\\bSGD\\blinear\\bclassifier\\bnot\\b only\\b performs\\b online\\b learning\\b but\\b also\\b gives\\b comparable\\b or\\b even\\b better\\b results.\\b \\b We\\b compare\\b the\\btwo\\bclassification\\balgorithms\\bwith\\bthe\\b\\fornell\\bmovie-review\\bdata\\bset\\b(Pang\\band\\b\\tee,\\b2004).\\b The\\b5-fold\\bcross\\bvalidation\\baverage\\bresults\\bare\\b0.843\\bby\\bSVM\\band\\b0.859\\bby\\bSGD,\\bfrom\\bwhich\\b we\\b can\\b see\\b that\\b SGD\\b outperforms\\b SVM\\b (implemented\\b with\\b \\tIBSVM3 \\bwith\\b linear\\b kernel).\\b Actually,\\bsimilar\\bconclusion\\bcan\\bbe\\bfound\\bin\\bDredze et al. (2008).\\b"]},{"title":"4.2 Distribution o\\f Negation and \\bontrast Transition Sentences","paragraphs":["Before\\b classification,\\b each\\b document\\b is\\b necessarily\\b partitioned\\b into\\b two\\b sub-texts:\\b sentimentreversed\\bpart\\band\\bsentiment-non-reversed\\bpart.\\bTo\\bachieve\\bthat,\\bwe\\buse\\bsome\\b\\fVSs\\bto\\bclassify\\b those\\b segmented\\b sub-sentences\\b into\\b two\\b categories:\\b sentiment-reversed\\b and\\b sentiment-nonreversed.\\b Specifically,\\b negatives\\b are\\b used\\b to\\b recognize\\b the\\b negation\\b sentences\\b and\\b the\\b connectives\\bof\\b‘but’\\band\\b‘howe\\fer’\\bare\\bused\\bto\\brecognize\\bcontrast\\btransition\\bsentences.\\bFirst\\bof\\b all,\\b let\\b us\\b see\\b the\\b distribution\\b of\\b these\\b negation\\b sentences\\b and\\b contrast\\b transition\\b sentences\\b in\\b our\\breview\\bcorpus.\\b \\b\\t\\f   \\b   \\b","Figure 2:\\bThe\\bproportion\\bof\\bnegation\\b(left)\\band\\btransition\\b(right)\\bsentences\\bin\\bnegative\\band\\bpositive\\b reviews\\b \\b","Figure\\b2\\b(left)\\bshows\\bthe\\bproportions\\bof\\bnegation\\bsentences\\bto\\ball\\bsentences\\bin\\bnegative\\band\\b positive\\breviews\\brespectively.\\bThe\\bproportion\\bis\\bcomputed\\bin\\beach\\bdomain.\\bFrom\\bFigure\\b2,\\bwe\\b can\\b see\\b that\\b negation\\b sentences\\b occur\\b frequently\\b in\\b reviews\\b and\\b are\\b more\\b likely\\b expressed\\b in\\b negative\\breviews.\\bThe\\bproportion\\bof\\bnegation\\bsentences\\bin\\bnegative\\breviews\\bis\\babout\\b8%,\\bwhich\\b is\\babout\\btwice\\bas\\bthe\\bone\\bin\\bpositive\\breviews.\\bThis\\bresult\\bagrees\\bwith\\bour\\bgeneral\\bknowledge\\bthat\\b people\\bare\\bmore\\blikely\\bto\\buse\\bnegation\\bsentences\\bwhen\\bexpressing\\btheir\\bnegative\\bopinions.\\b","Figure\\b 2\\b (right)\\b shows\\b the\\b proportions\\b of\\b contrast\\b transition\\b sentences\\b to\\b all\\b sentences\\b in\\b negative\\b and\\b positive\\b reviews\\b respectively.\\b From\\b this\\b figure,\\b we\\b can\\b also\\b see\\b that\\b contrast\\b sentences\\bare\\bmore\\blikely\\bexpressed\\bin\\bnegative\\breviews\\bthan\\bin\\bpositive\\breviews.\\b\\fompared\\bto\\b negation\\bsentences,\\bcontrast\\btransition\\bsentences\\bare\\bmuch\\bfewer.\\b","\\b \\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\-\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b 3 \\bhttp://www.csie.ntu.edu.tw/~cjlin/libsvm/\\b 303 \\b"]},{"title":"4.3 \\b\\tassi\\fication Resu\\tts with Di\\f\\ferent Strategies Tab\\te 1:","paragraphs":["\\bThe\\bclassification\\bresults\\bof\\bdifferent\\bstrategies\\bwhen\\bonly\\bconsidering\\bnegation\\b Negation\\bDomain\\b Baseline\\b Remove\\b Switch\\b Joint\\b Book\\b 0.849\\b 0.845\\b 0.834\\b0.860 \\famera\\b 0.920\\b 0.912\\b 0.907\\b 0.924 HD\\b 0.934\\b 0.929\\b 0.917\\b0.946 Health\\b 0.841\\b 0.830\\b 0.819\\b 0.854 Kitchen\\b 0.860\\b 0.861\\b 0.858\\b 0.872 \\b Tab\\te 2: The\\bclassification\\bresults\\bof\\bdifferent\\bstrategies\\bwhen\\bonly\\bconsidering\\bcontrast\\btransition\\b Transition\\bDomain\\b Baseline\\b Remove\\b Switch\\b Joint\\b Book\\b 0.849\\b 0.850 0.843\\b 0.848\\b \\famera\\b 0.920\\b 0.924\\b 0.917\\b 0.930 HD\\b 0.934\\b 0.934\\b 0.930\\b0.939 Health\\b 0.841\\b 0.848\\b 0.845\\b 0.854 Kitchen\\b 0.860\\b 0.865 0.855\\b 0.864\\b  Table\\b1\\bshows\\bthe\\bclassification\\bresults\\bof\\bdifferent\\bstrategies\\bwhen\\bonly\\bnegation\\bis\\bconsidered\\b for\\bsentiment-reversed\\bsentence\\bdetection.\\bBaseline\\bshows\\bthe\\bresults\\bof\\busing\\ball\\bunigrams\\bwith\\b one-bag-of-words\\bmodeling.\\b\\tet\\bus\\bcompare\\bthe\\bresults\\bbetween\\bthe\\bbaseline\\band\\beach\\bstrategy.\\b","First,\\b comparing\\b baseline\\b and\\bremo\\fe\\b strategy,\\b we\\b find\\b that\\b simply\\b removing\\b all\\b negation\\b sentences\\b is\\b not\\b helpful.\\b Sometimes,\\b the\\b performances\\b even\\b decrease\\b more\\b than\\b one\\b percent\\b (see\\bthe\\bdomain\\bof\\bhealth).\\b\\b","Second,\\b comparing\\b baseline\\b and\\b\\tw\\btch strategy,\\b we\\b find\\b that \\tw\\btch strategy\\b is\\b worse\\b and\\b always\\b harmful\\b for\\b sentiment\\b classification.\\b This\\b is\\b different\\b from\\b our\\b first\\b thought\\b of\\b this\\b strategy.\\bBut\\bafter\\bclose\\bthinking\\bof\\bit,\\bwe\\bwould\\bnotice\\bthat\\bassigning\\ball\\bthe\\bwords\\ba\\bnegative\\b parameter,\\b i.e.,\\b (̂ 1) * Nw\\b in\\b a\\b sentiment-reversed\\b sentence\\b is\\b not\\b reasonable.\\b In\\bfact,\\b it\\b is\\b only\\b necessary\\b to\\b assign\\b a\\b positive\\b parameter\\b to\\b those\\b words\\b which\\b express\\b sentiment.\\b Moreover,\\b some\\bwords\\bare\\bcommonly\\bused\\bin\\bboth\\bnegation\\band\\bnon-negation\\bsentences\\bfor\\bexpressing\\bthe\\b same\\bsentiment\\bpolarity.\\bFor\\bexample,\\bsee\\bthe\\bword\\bwa\\tte\\bin\\bthe\\bfollowing\\btwo\\bsentences.\\b d1.\\bIt \\b\\t a wa\\tte of your money. d2.\\bDo not wa\\tte your money.","Third,\\b comparing\\b to\\b baseline,\\b we\\b find\\b that\\bjo\\bnt strategy\\b is\\b successful\\b and\\b consistently\\b improves\\b the\\b performance.\\b But\\b the\\b improved\\b performances\\b in\\b some\\b domains\\b are\\b insignificant\\b (less\\bthan\\b0.5%\\bin\\bcamera).\\bTherefore,\\bit\\bis\\bnot\\bstrange\\bthat\\bthe\\bconclusions\\bin\\bPang\\bet\\bal.\\b(2002)\\b and\\bNa\\bet\\bal.\\b(2004)\\bis\\ba\\blittle\\bdifferent\\bfrom\\beach\\bother.\\bWhether\\binducing\\bnegation\\bis\\beffective\\b or\\bnot\\bis\\binfluenced\\bby\\bthe\\bapplication\\bdomains.\\b\\b 304","Table\\b2\\bshows\\bthe\\bclassification\\bresults\\bof\\bdifferent\\bstrategies\\bwhen\\bonly\\bcontrast\\btransition\\b is\\bconsidered\\bfor\\bsentiment-reversed\\bsentence\\bdetection.\\b\\tet\\bus\\bcompare\\bthe\\bresults\\bbetween\\bthe\\b baseline\\band\\beach\\bstrategy\\brespectively.\\b","First,\\b quite\\b different\\b from\\b the\\b case\\b of\\b negation,\\b simply\\b removing\\b the\\b contrast\\b transition\\b sentences\\bcan\\balways\\bimprove\\bclassification\\bperformances.\\bWe\\bthink\\bthis\\bis\\bmainly\\bbecause\\bthe\\b amount\\b of\\b transition\\b sentences\\b is\\b much\\b less\\b than\\b negation\\b sentences.\\b Removing\\b them\\b is\\b beneficial\\b for\\b deleting\\b classification\\b noise\\b without\\b losing\\b too\\b much\\b useful\\b classification\\b information.\\b","Second,\\bswitch strategy\\b generally\\b fails\\b to\\b improve\\b the\\b performance.\\b It\\b can\\b only\\b make\\b very\\b small\\bimprovement\\bin\\bthe\\bdomain\\bof\\bhealth\\b","Third,\\bjo\\bnt strategy\\b is\\b still\\b effective\\b in\\b dealing\\b with\\b contrast\\b transition.\\b However,\\b some\\b results\\bare\\bno\\bbetter\\bthan\\bremo\\fe\\bstrategy.\\b","Overall\\b speaking,\\b contrast\\b transition\\b is\\b also\\b helpful\\b for\\b classification.\\b But\\b the\\b improved\\b performances\\bare\\ba\\blittle\\blower\\bthan\\bthe\\bones\\bby\\busing\\bnegation.\\bThis\\bis\\bmainly\\bbecause\\bnegation\\b appears\\b more\\b often\\b than\\b contrast\\b transition,\\b which\\b makes\\b the\\b sentences’\\b sentiment\\b reversed\\b more\\bfrequently.\\b","Tab\\te 3:\\bThe\\bclassification\\bresults\\bof\\bdifferent\\bstrategies\\bwhen\\bconsidering\\bboth\\bnegation\\band\\b contrast\\btransition\\b Negation\\b+\\bTransition\\bDomain\\b Baseline\\b Remove\\b Switch\\b Joint\\b Book\\b 0.849\\b 0.847\\b 0.821\\b0.863 \\famera\\b 0.920\\b 0.919\\b 0.900\\b0.930 HD\\b 0.934\\b 0.923\\b 0.913\\b0.946 Health\\b 0.841\\b 0.848\\b 0.812\\b0.864 Kitchen\\b 0.860\\b 0.861\\b 0.852\\b0.873 \\b","Table\\b 3\\b shows\\b the\\b classification\\b results\\b of\\b different\\b strategies\\b when\\b both\\b negation\\b and\\b contrast\\b transition\\b are\\b considered\\b for\\b sentiment-reversed\\b sentence\\b detection.\\b Apparently,\\bjo\\bnt strategy\\b is\\b more\\b powerful\\b than\\b the\\b other\\b two\\b strategies\\b and\\b consistently\\b achieves\\b much\\b better\\b classification\\b results\\b than\\b the\\b baseline\\b (The\\b improved\\b accuracy\\b is\\b no\\b less\\b than\\b 1%\\b in\\b all\\b domains).\\b","\\fomparing\\bthe\\bresults\\bin\\bTable\\b3\\bto\\bthe\\bresults\\bin\\bTable\\b1\\bor\\bTable\\b2,\\bwe\\bcan\\bconclude\\bthat\\b considering\\bboth\\bnegation\\band\\bcontrast\\btransition\\bis\\bgenerally\\ba\\bbetter\\bchoice\\bthan\\bconsidering\\b only\\bone\\bof\\bthem.\\b"]},{"title":"5 \\bonc\\tusion","paragraphs":["In\\bthis\\b paper,\\b we\\b propose\\b an\\b approach\\b for\\b incorporating\\b sentiment\\b reversing\\b information\\b into\\b machine\\blearning\\bbased\\bsentiment\\bclassification\\bsystem.\\bSpecifically,\\bwe\\bconsider\\btwo\\bkinds\\bof\\b linguistic\\bphenomena:\\bnegation\\band\\bcontrast\\btransition,\\bwhich\\bare\\bpopularly\\bused\\bto\\breverse\\bthe\\b sentiment\\b polarity.\\b Experimental\\b results\\b on\\b a\\b newly\\b collected\\b corpus\\b show\\b that\\b simply\\b removing\\b the\\b contrast\\b transition\\b sentences\\b is\\b helpful\\b but\\b it\\b is\\b not\\b effective\\b for\\b negation.\\b\\b Furthermore,\\b we\\b see\\b that\\b our\\b approach\\b with\\bjo\\bnt strategy\\b is\\b able\\b to\\b robustly\\b improve\\b the\\b performances\\bacross\\ball\\bfive\\bdomains.\\b","In\\bour\\bapproach,\\bwe\\bonly\\buse\\bnegation\\band\\bcontrast\\btransition\\bkeywords\\bto\\bdetect\\bsentiment\\b reversed\\b sentences.\\b In\\b addition,\\b there\\b certainly\\b exist\\b some\\b other\\b structures\\b which\\b can\\b reverse\\b the\\bsentiment\\b polarity\\b of\\b a\\b word\\b or\\b sentence.\\b In\\b our\\b future\\b work,\\b we\\b hope\\b to\\b find\\b some\\b more\\b 305 \\b effective\\b detection\\b approaches\\b and\\b consider\\b more\\b structures\\b to\\b recognize\\b sentiment-reversed\\b sentences.\\b"]},{"title":"Re\\ferences","paragraphs":["Blitzer,\\bJ.,\\bM.\\bDredze\\band\\bF.\\bPereira.\\b2007.\\bBiographies,\\bBollywood,\\bBoom-boxes\\band\\bBlenders:\\b Domain\\b Adaptation\\b for\\b Sentiment\\b \\flassification.\\b In\\bProceed\\bng\\t of Annual Meet\\bng on A\\t\\toc\\bat\\bon for Computat\\bonal L\\bngu\\b\\tt\\bc\\t (ACL-07).\\b","Dredze,\\b M.,\\b K.\\b \\frammer\\b and\\b F.\\b Pereira.\\b 2008.\\b \\fonfidence-weighted\\b \\tinear\\b \\flassification.\\b In\\b Proceed\\bng\\t of Internat\\bonal Conference on Mach\\bne Learn\\bng (ICML-08).\\b","Jindal,\\b N.\\b and\\b B.\\b \\tiu.\\b 2006.\\b Identifying\\b \\fomparative\\b Sentences\\b in\\b Text\\b Documents.\\b In\\b Proceed\\bng\\t of the 29th Annual Internat\\bonal ACM SIGIR Conference on Re\\tearch & De\\felopment on Informat\\bon Retr\\be\\fal (SIGIR-06).\\b","Kennedy,\\bA.\\band\\bD.\\bInkpen.\\b2006.\\bSentiment\\b\\flassification\\bof\\bMovie\\bReviews\\busing\\b\\fontextual\\b Valence\\bShifters\\b.\\bComputat\\bonal Intell\\bgence,\\bVol.\\b22,\\bNo.\\b2,\\bpp.\\b110-125.\\b","\\ti,\\b S.\\b and\\b \\f.\\b Zong.\\b 2008.\\b Multi-domain\\b Sentiment\\b \\flassification.\\b In\\bProceed\\bng\\t of Annual Meet\\bng of the A\\t\\toc\\bat\\bon for Computat\\bonal L\\bngu\\b\\tt\\bc\\t: Human Language Technology (ACL-08: HLT).\\b","Na,\\b J.,\\b H.\\b Sui,\\b \\f.\\b Khoo,\\b S.\\b \\fhan\\b and\\b Y.\\b Zhou.\\b 2004.\\b Effectiveness\\b of\\b Simple\\b \\tinguistic\\b Processing\\bin\\bAutomatic\\bSentiment\\b\\flassification\\bof\\bProduct\\bReviews.\\bIn\\bConference of the Internat\\bonal Soc\\bety for Knowledge Organ\\bzat\\bon (ISKO),\\bpages\\b49–54.\\b","Pang,\\b B.,\\b \\t.\\b \\tee\\b and\\b S.\\b Vaithyanathan.\\b 2002.\\b Thumbs\\b up?\\b Sentiment\\b \\flassification\\b using\\b Machine\\b \\tearning\\b Techniques.\\b In\\bProceed\\bng\\t of Conference on Emp\\br\\bcal Method\\t \\bn Natural Language Proce\\t\\t\\bng (EMNLP-02).\\b","Pang,\\b B.\\b and\\b \\t.\\b \\tee.\\b 2004.\\b A\\b Sentimental\\b Education:\\b Sentiment\\b Analysis\\b using\\b Subjectivity\\b Summarization\\bbased\\bon\\bMinimum\\b\\futs.\\bIn\\bProceed\\bng\\t of Annual Meet\\bng on A\\t\\toc\\bat\\bon for Computat\\bonal L\\bngu\\b\\tt\\bc\\t (ACL-04).\\b","Pang,\\bB.\\band\\b\\t.\\b\\tee.\\b2008.\\bOpinion\\bMining\\band\\bSentiment\\bAnalysis.\\bFoundat\\bon and Trend\\t \\bn Informat\\bon Retr\\be\\fal,\\b2(1-2):1–135.\\b","Polanyi,\\b \\t.\\b and\\b A.\\b Zaenen.\\b 2006.\\b \\fontextual\\b Valence\\b Shifters.\\b \\b In\\bComput\\bng att\\btude and affect \\bn text: Theory and appl\\bcat\\bon.\\bSpringer\\bVerlag.\\b","Riloff,\\b E.,\\b S.\\b Patwardhan\\b and\\b J.\\b Wiebe.\\b 2006.\\b Feature\\b Subsumption\\b for\\b Opinion\\b Analysis.\\b In\\b Proceed\\bng\\t of Conference on Emp\\br\\bcal Method\\t \\bn Natural Language Proce\\t\\t\\bng (EMNLP-06).\\b","Turney,\\b P.\\b 2002.\\b Thumbs\\b Up\\b or\\b Thumbs\\b Down?\\b Sentiment\\b Orientation\\b Applied\\b to\\b Unsupervised\\b\\flassification\\bof\\bReviews.\\bIn\\bProceed\\bng\\t of Annual Meet\\bng on A\\t\\toc\\bat\\bon for Computat\\bonal L\\bngu\\b\\tt\\bc\\t (ACL-02).\\b","Zhang,\\b T.\\b 2004.\\b \\b Solving\\b \\targe\\b Scale\\b \\tinear\\b Prediction\\b Problems\\b using\\b Stochastic\\b Gradient\\b Descent\\b Algorithms.\\b In\\bProceed\\bng\\t of Internat\\bonal Conference on Mach\\bne Learn\\bng (ICML-04).\\b \\b 306"]}]}