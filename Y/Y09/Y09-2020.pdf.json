{"sections":[{"title":"Constra\\bnt\\tBase\\f\\tHybr\\b\\f\\tApproach\\tto\\tPars\\bng\\tIn\\f\\ban\\tLanguages\\t \\t","paragraphs":["Akshar \\bhara\\ti, \\famar Husain, Meher Vijay, Kalyan Deepak, Dip\\ti Misra \\fharma and Rajeev \\fangal"," Language Technologies Research Cen\\tre","IIIT-Hyderabad, India.","samar@research.iii\\t.ac.in, {meher_vijay, deepak_s}@s\\tuden\\ts.iii\\t.ac.in, {dip\\ti, sangal}@mail.iii\\t.ac.in Abstract. The paper describes \\the overall design of a new \\two s\\tage cons\\train\\t based hybrid approach \\to dependency parsing. We define \\the \\two s\\tages and show how differen\\t gramma\\tical cons\\truc\\t are parsed a\\t appropria\\te s\\tages. This division leads \\to selec\\tive iden\\tifica\\tion and resolu\\tion of specific dependency rela\\tions a\\t \\the \\two s\\tages. Fur\\thermore, we show how \\the use of hard cons\\train\\ts and sof\\t cons\\train\\ts helps us build an efficien\\t and robus\\t hybrid parser. Finally, we evalua\\te \\the implemen\\ted parser on Hindi and compare \\the resul\\ts wi\\th \\tha\\t of \\two da\\ta driven dependency parsers. Keywor\\fs:\\tDependency parsing, Hybrid approach, Modulari\\ty, Indian language parsing, Cons\\train\\t based parsing."]},{"title":"1 Intro\\fuct\\bon\\t","paragraphs":["Due \\to \\the availabili\\ty of anno\\ta\\ted corpora for various languages since \\the pas\\t decade, da\\ta driven parsing has proved \\to be immensely successful. Unlike English, however, mos\\t of \\the parsers for morphologically rich free word order (MoR-FWO) languages (such as Czech, Turkish, Hindi, e\\tc.) have adop\\ted \\the dependency gramma\\tical framework. I\\t is well known \\tha\\t for MoR-FWO languages, dependency framework provides ease of linguis\\tic analysis and is much be\\t\\ter sui\\ted \\to accoun\\t for \\their various s\\truc\\tures (\\fhieber, 1975; Mel'Cuk, 1988; \\bhara\\ti e\\t al., 1995). The s\\ta\\te of \\the ar\\t parsing accuracy for many MoR-FWO languages is s\\till low compared \\to \\tha\\t of English. Parsing experimen\\ts (Nivre e\\t al., 2007; Hall e\\t al., 2007) for \\these languages have poin\\ted \\towards various reasons for \\this low performance. For Hindi1",", (a) difficult\\b\\tin\\te\\ftracting\\trelevant\\tlinguistic\\tcues, (b) non-projectivit\\b, (c) lack\\tof\\te\\fplicit\\tcues, (d) long\\t distance\\t dependencies, (e) comple\\f\\t linguistic\\t phenomena, and (f) less\\t corpus\\t size, have been sugges\\ted (\\bhara\\ti e\\t al., 2008) for low performance. The approach proposed in \\this paper shows how one can minimize \\these adverse effec\\ts and argues \\tha\\t a hybrid approach can prove \\to be a be\\t\\ter op\\tion \\to parsing such languages. There have been, in \\the pas\\t, many a\\t\\temp\\ts \\to parsing using cons\\train\\t based approaches. \\fome of \\the cons\\train\\t based parsers known in \\the li\\tera\\ture are Karlsson e\\t al. (1995), Maruyama (1990), \\bhara\\ti e\\t al. (1993, 2002), Tapanainen and Järvinen (1998), \\fchröder (2002), and more recen\\tly, Debusmann e\\t al. (2004). \\fome a\\t\\temp\\ts a\\t parsing Hindi using da\\ta driven approach have been (\\bhara\\ti e\\t al., 2008b; Husain e\\t al., 2009). La\\ter in \\fec\\tion 4, we’ll compare \\the resul\\ts of da\\ta-driven Hindi parsing wi\\th \\tha\\t of our approach.","The paper describes \\the overall design of a new \\two s\\tage cons\\train\\t based hybrid approach \\to dependency parsing. We define \\the \\two s\\tages and show how differen\\t gramma\\tical cons\\truc\\t are parsed a\\t appropria\\te s\\tages. This division leads \\to selec\\tive iden\\tifica\\tion and resolu\\tion of \\m \\m  Copyrigh\\t 2009 by Akshar \\bhara\\ti, \\famar Husain, Meher Vijay, Kalyan Deepak, Dip\\ti Misra \\fharma, and Rajeev \\fangal 1 Hindi is a verb final language wi\\th free word order and a rich case marking sys\\tem. I\\t is one of \\the official languages of India, and is spoken by ~800 million people. 614 23rd Pacific Asia Conference on Language, Information and Computation, pages 614–621  specific dependency rela\\tions a\\t \\two differen\\t s\\tages. Fur\\thermore, we show how \\the use of hard cons\\train\\ts (H-cons\\train\\ts) and sof\\t cons\\train\\ts (\\f-cons\\train\\ts) helps us build an efficien\\t and robus\\t hybrid parser. \\fpecifically, H-cons\\train\\ts incorpora\\te \\the knowledge base of \\the language and \\f-cons\\train\\ts are used as weigh\\ts \\tha\\t are au\\toma\\tically learn\\t from an anno\\ta\\ted \\treebank. Finally, we evalua\\te \\the implemen\\ted parser on Hindi and compare \\the resul\\ts wi\\th \\tha\\t of \\two da\\ta driven dependency parsers.","The paper is arranged as follows: \\fec\\tion 2 describes in de\\tail \\the proposed approach for parsing free word order languages. \\fec\\tion 3 discusses \\the \\types of cons\\train\\ts used. We evalua\\te \\the parser and compare \\the resul\\ts wi\\th \\tha\\t of \\two da\\ta-driven parsers in \\fec\\tion 4. \\fec\\tion 5 gives cer\\tain observa\\tions on \\the approach. We conclude \\the paper in \\fec\\tion 7. ","","F\\bgure\\t1: (a) PO\\f \\tagged and chunked sen\\tence, (b) and (c) are parse \\tree ob\\tained using C\\bHP, (d) comple\\te parse af\\ter IRCH"]},{"title":"2 Approach\\t","paragraphs":["We \\try \\to solve \\the \\task of dependency parsing using a hybrid approach. A grammar driven approach is complemen\\ted by a con\\trolled s\\ta\\tis\\tical s\\tra\\tegy \\to achieve high performance and robus\\tness. The overall \\task of dependency parsing is a\\t\\tacked using modulari\\ty, wherein specific \\tasks are broken down in\\to smaller linguis\\tically mo\\tiva\\ted sub-\\tasks. Figure 1 above shows \\the ou\\tpu\\t of each of \\these sub-\\tasks."]},{"title":"2.1 Backgroun\\f\\t","paragraphs":["Da\\ta driven parsing is usually a single s\\tage process wherein a sen\\tence is parsed a\\t one go. Many a\\t\\temp\\ts have, however, \\tried \\to divide \\the overall \\task in\\to sub-\\task. One \\trend has been \\to firs\\t iden\\tify dependencies and \\then add edge labels over \\them (McDonald e\\t al., 2005, Chen e\\t al., 2007). The o\\ther \\trend has been \\towards performing smaller linguis\\tically relevan\\t \\tasks as a precursor \\to comple\\te parsing (Abney, 1997; \\bhara\\ti e\\t al., 1995; A\\t\\tardi and Dell’Orle\\t\\ta, 2008; Peh and Ting, 1996).","In our approach we divide \\the \\task of parsing in\\to \\the following sub-\\tasks (layers):","1. PO\\f \\tagging, chunking (PO\\fCH),","2. Cons\\train\\t based hybrid parsing (C\\bHP),","3. In\\tra-chunk dependencies (IRCH) iden\\tifica\\tion.  (a) PO\\fCH is \\trea\\ted as pre-processing \\to \\the \\task of parsing. A bag represen\\ts a se\\t of adjacen\\t words which are in dependency rela\\tions wi\\th each o\\ther, and are connec\\ted \\to \\the res\\t of \\the words by a single incoming dependency arc. Thus a bag is an unexpanded dependency \\tree connec\\ted \\to \\the res\\t only by means of i\\ts roo\\t. A noun phrase or a noun group chunk is a bag in which \\there are no verbs, and vice versa for verb chunks. The rela\\tions among \\the words in a 615 chunk are no\\t marked and hence allow us \\to ignore local de\\tails while building \\the sen\\tence level dependency \\tree. In general, all \\the nominal inflec\\tions, nominal modifica\\tions (adjec\\tive modifying a noun, e\\tc.) are \\trea\\ted as par\\t of a noun chunk, similarly, verbal inflec\\tions, auxiliaries are \\trea\\ted as par\\t of \\the verb chunk (\\bhara\\ti e\\t al., 2006). (b) C\\bHP \\takes \\the PO\\f \\tagged and chunked sen\\tence as inpu\\t and parses i\\t in \\two s\\tages. The parser makes use of knowledge base of \\the language along wi\\th syn\\tac\\tico-seman\\tic preferences \\to arrive a\\t \\the final parse. \\broadly, modulari\\ty in C\\bHP works a\\t \\two layers (cf. Figure 3): (1) The sen\\tence analysis layer, and (2) The parse selec\\tion layer. We discuss \\this approach \\to parsing in \\the following sec\\tions. (c) IRCH dependencies are finally iden\\tified as a pos\\t-processing s\\tep \\to (b) and (c). Once \\this is done, \\the chunks can be removed and we can ge\\t \\the comple\\te dependency \\tree. We will no\\t discuss IRCH in \\this paper.","In \\the dependency \\trees (b) and (c) shown in Figure 1, each node is a chunk and \\the edge represen\\ts \\the rela\\tions be\\tween \\the connec\\ted nodes labeled wi\\th sui\\table rela\\tions2 . Af\\ter removing \\the chunks in (d) each node is a lexical i\\tem of \\the sen\\tence.  Eg. 1: mohana ne \\tebala para apani ki\\taaba ’Mohan’ ‘ERG’ ‘\\table’ ‘on’ ‘his’ ‘book’ rakhii Ora vaha so gayaa ‘kep\\t’ ‘and’ ‘he’ ‘sleep’ ‘PRFT’ ‘Mohan placed his book on \\the \\table and slep\\t’ ","From (a) \\to (d) in Figure 1, ou\\tpu\\ts of each of \\the previously discussed layers have been shown. No\\te \\tha\\t one can use any of \\these ou\\tpu\\ts independen\\tly. More impor\\tan\\tly, (b) is a par\\tial parse ob\\tained af\\ter \\the 1s\\t","s\\tage of C\\bHP, and (c) is \\the ou\\tpu\\t af\\ter \\the 2nd","s\\tage of C\\bHP. We’ll elabora\\te on \\this in \\the following sec\\tions. To \\tes\\t \\the performance of \\the proposed parser we use gold PO\\f \\tagged and chunked da\\ta, ins\\tead of using \\the ou\\tpu\\ts of PO\\f \\tagger and chunker."]},{"title":"2.2 Constra\\bnt\\tPars\\bng\\t","paragraphs":["Cons\\train\\t based parsing using in\\teger programming has been successfully \\tried for Indian languages (\\bhara\\ti e\\t al., 1993; 2002). Under \\this scheme \\the parser exploi\\ts \\the syn\\tac\\tic cues presen\\t in a sen\\tence and forms cons\\train\\t graphs (CG) based on \\the generaliza\\tions presen\\t. I\\t uses such no\\tions as basic demand frames and \\transforma\\tion frames (\\bhara\\ti e\\t al., 1995) \\to cons\\truc\\t \\the CG. I\\t \\then \\transla\\tes \\the CG in\\to an in\\teger programming (IP) problem. The solu\\tions \\to \\the problem provide \\the possible parses for \\the sen\\tence. We follow \\the approach used by \\bhara\\ti e\\t al. (1995, 2008a) for formula\\ting \\the cons\\train\\ts as IP problem and solving \\them \\to ge\\t \\the parses."]},{"title":"2.3 Two\\tStage\\tPars\\bng\\t","paragraphs":["The proposed parser \\tries \\to analyze \\the given inpu\\t sen\\tence, which has already been PO\\f \\tagged and chunked, in 2 s\\tages; i\\t firs\\t \\tries \\to ex\\trac\\t in\\tra-clausal3","dependency rela\\tions. These generally correspond \\to \\the argumen\\t s\\truc\\ture of \\the verb, noun-noun geni\\tive rela\\tion, infini\\tive-verb rela\\tion, infini\\tive-noun rela\\tion, adjec\\tive-noun, adverb-verb rela\\tions, e\\tc. In \\the 2nd","s\\tage i\\t \\then \\tries \\to handle more complex rela\\tions such as conjunc\\ts, rela\\tive clause, e\\tc. Wha\\t \\this essen\\tially means is a 2-s\\tage resolu\\tion of dependencies, where \\the parser selec\\tively resolves\\t\\the dependencies of various lexical heads a\\t \\their appropria\\te s\\tage, for \\m \\m 2 All \\the rela\\tions marked by \\the parser are syn\\tac\\tico-seman\\tic labels. For a de\\tailed analysis see \\bhara\\ti e\\t al. (1995). Many rela\\tions shown in \\the diagrams of \\this paper are described in \\begum e\\t al. (2008a). For \\the comple\\te \\tagse\\t descrip\\tion, see h\\t\\tp://l\\trc.iii\\t.ac.in/MachineTrans/research/\\tb/D\\f-guidelines/D\\f-guidelines-ver2-28-05-09.pdf 3 A clause is a group of word such \\tha\\t \\the group con\\tains a single fini\\te verb chunk. 616 ","example verbs in \\the 1s\\t s\\tage and conjunc\\ts and in\\ter-verb rela\\tions in \\the 2nd s\\tage. The key","ideas are: (1) There are \\two layers (s\\tages), (2) \\the 1s\\t s\\tage handles in\\tra-clausal rela\\tions, and \\the 2nd","s\\tage handles in\\ter-clausal rela\\tions, (3) \\the ou\\tpu\\t of each layer is a linguis\\tically valid par\\tial parse \\tha\\t becomes, if necessary, \\the inpu\\t \\to \\the nex\\t layer, and (4) \\the ou\\tpu\\t of \\the final layer is/are \\the desired full parse(s). These form \\the sentence\\tanal\\bsis\\tlayer in \\the overall design. Figure 3 shows \\this clearly. The 1s\\t","s\\tage ou\\tpu\\t for example 2 is shown in Figure 2(a)."," Eg. 2: mai ghar gayaa kyomki mai ’I’ ’home’ ’wen\\t’ ’because’ ’I’ bimaar \\thaa ’sick’ ‘was’ ‘I wen\\t home because I was sick’  In Figure 2(a), \\the parsed ma\\trix clause sub\\tree ‘mai\\tghar\\tga\\baa’ and \\the subordina\\te clause are a\\t\\tached \\to _ROOT_. The subordina\\ting conjunc\\t ‘k\\bomki’ (because) is also seen a\\t\\tached \\to \\the _ROOT_. _ROOT_ ensures \\tha\\t \\the parse we ge\\t af\\ter each s\\tage is connec\\ted and \\takes all \\the analyzed 1s\\t","s\\tage sub-\\trees along wi\\th unprocessed nodes as i\\ts children. The dependency \\tree \\thus ob\\tained in \\the 1s\\t","s\\tage is par\\tial, bu\\t linguis\\tically sound. La\\ter in \\the 2nd","","s\\tage \\the rela\\tionship be\\tween various clauses are iden\\tified. The 2nd s\\tage parse for \\the above sen\\tences is also shown in Figure 2(b). A\\t \\the end of 2nd","s\\tage, \\the subordina\\te conjunc\\t k\\bomki\\t ge\\ts a\\t\\tached \\to \\the ma\\trix clause and \\takes \\the roo\\t of \\the subordina\\te clause as i\\ts child. \\fimilar \\to example 2, \\the analysis of example 1 is shown in Figure 1. No\\te \\tha\\t under normal condi\\tions \\the 2nd","s\\tage does no\\t modify \\the parses ob\\tained from \\the 1s\\t","s\\tage, i\\t only es\\tablishes \\the rela\\tions be\\tween \\the clauses. However, some\\times under very s\\tric\\t condi\\tions, repair is possible (\\bhara\\ti e\\t al., 2008a)."," F\\bgure\\t2: (a): 1s\\t s\\tage ou\\tpu\\t for Eg. 2, (b): 2nd s\\tage final parse for Eg. 2"]},{"title":"3 Har\\f\\tan\\f\\tSoft\\tConstra\\bnts\\t","paragraphs":["\\bo\\th 1s\\t and 2nd s\\tage described in \\the previous sec\\tion use linguis\\tically mo\\tiva\\ted cons\\train\\ts. These hard\\tcons\\train\\ts (H-cons\\train\\ts) reflec\\t \\tha\\t aspec\\t of \\the grammar \\tha\\t in general canno\\t be broken. H-cons\\train\\ts comprise of lexical and s\\truc\\tural knowledge of \\the language. The Hcons\\train\\ts are conver\\ted in\\to in\\teger programming problem and solved (\\bhara\\ti e\\t al., 2002, 2008a). The solu\\tion(s) is/are valid parse(s). The soft\\tcons\\train\\ts\\t(\\f-cons\\train\\ts), on \\the o\\ther hand, are learn\\t as weigh\\ts from an anno\\ta\\ted \\treebank4",". They reflec\\t various preferences \\tha\\t a language has \\towards various linguis\\tic phenomena. They are used \\to priori\\tize \\the parses and \\m \\m 4 For de\\tails on \\the corpus \\type, anno\\ta\\tion scheme, \\tagse\\t, e\\tc. see \\begum e\\t al. (2008a). 617 selec\\t \\the bes\\t parse. \\bo\\th H & \\f cons\\train\\ts reflec\\t \\the linguis\\tic reali\\ties of \\the language and \\toge\\ther can be \\though\\t as \\the grammar of a language. Figure 3 schema\\tically shows \\the overall design of \\the proposed parser and places \\these cons\\train\\ts in \\tha\\t con\\tex\\t.   F\\bgure\\t3: \\fchema\\tic represen\\ta\\tion of C\\bHP"]},{"title":"3.1 Har\\f\\tConstra\\bnts\\t","paragraphs":["The core language knowledge being curren\\tly considered \\tha\\t canno\\t be broken wi\\thou\\t \\the sen\\tence being called ungramma\\tical is named H-cons\\train\\ts. There can be mul\\tiple parses which can sa\\tisfy \\these H-cons\\train\\ts. This indica\\tes \\the ambigui\\ty in \\the sen\\tence if only \\the limi\\ted knowledge base is considered. \\f\\ta\\ted ano\\ther way, H-cons\\train\\ts are insufficien\\t \\to res\\tric\\t mul\\tiple analysis of a given sen\\tence and \\tha\\t more knowledge (seman\\tics, o\\ther preferences, e\\tc.) is required \\to cur\\tail \\the ambigui\\ties. Moreover, we know \\tha\\t many sen\\tences are syn\\tac\\tically ambiguous unless one uses some pragma\\tic knowledge, e\\tc. For all such cons\\truc\\tions \\there are mul\\tiple parses. As described earlier, H-cons\\train\\ts are used during in\\tra-clausal (1s\\t","s\\tage) and in\\ter-clausal (2nd","s\\tage) analysis (cf. Figure 3). They are used \\to form a cons\\train\\t graph which is conver\\ted in\\to in\\teger programming equali\\ties (or inequali\\ties). These are \\then solved \\to ge\\t \\the final solu\\tion graph(s) (\\bhara\\ti e\\t al., 2008a). \\fome of \\the Hcons\\train\\ts are: (1) Structural\\t constraints\\t(ensuring \\the solu\\tion graph \\to be a \\tree, removing implausible language specific ungramma\\tical s\\truc\\tures, e\\tc.), (2) Le\\ficon (linguis\\tic demands of various heads), and (3) Other\\tle\\fical\\tconstraints (some language specific charac\\teris\\tics), e\\tc."]},{"title":"3.2 Soft\\tConstra\\bnts\\t","paragraphs":["The \\f-cons\\train\\ts on \\the o\\ther hand are \\the cons\\train\\ts \\tha\\t can be broken, and are used in \\the language as preferences. These are used during \\the priori\\tiza\\tion s\\tage. Unlike \\the H-cons\\train\\ts \\tha\\t are derived from a knowledge base and are used \\to form a cons\\train\\t graph, \\f-cons\\train\\ts have weigh\\ts assigned \\to \\them. These weigh\\ts are au\\toma\\tically learn\\t using a manually anno\\ta\\ted dependency \\treebank. The weigh\\ts are used \\to score \\the parse \\trees. The \\tree wi\\th \\the maximum overall score is \\the bes\\t parse. \\fome such \\f-cons\\train\\ts are, (1) Order\\t of\\t the\\t arguments, (2) Relative\\t position\\t of\\t arguments\\t w.r.t.\\t the\\t verb, (3) Agreement, (4) Structural\\t preferences/General\\tgraph\\tproperties\\t(mild non-projec\\tivi\\ty, valency, dominance, e\\tc.), e\\tc.","Parses ob\\tained af\\ter \\the 2nd","s\\tage, sa\\tisfies all \\the relevan\\t H-cons\\train\\ts. We score \\these parses based on \\the \\f-cons\\train\\ts and \\the parse wi\\th \\the max score is selec\\ted. The scoring of a parse p is done as follows:"," 618 "]},{"title":")(","paragraphs":["i"]},{"title":"P γ","paragraphs":["(1)  where, is a recursive scoring func\\tion, Rp\\tis \\the roo\\t of \\the parse p\\t  (2)  where, Cne\\tis \\the child of node n along edge e and k is a parame\\ter  (3)  where, is \\the probabili\\ty of \\the rela\\tion r\\tgiven \\tha\\t is \\the i\\th","\\f-cons\\train\\t and","is \\the probabili\\ty of occurrence of and 1 i"]},{"title":"k","paragraphs":["is a weigh\\t associa\\ted wi\\th . The ranking func\\tion \\tries \\to selec\\t a parse p\\tfor a sen\\tence such \\tha\\t \\the overall accuracy of","\\the parser is maximized. The parame\\ters k\\tand\\t 1 i"]},{"title":"k","paragraphs":["in (2) and (3) above are se\\t using maximum likelihood es\\tima\\tion. No\\te \\tha\\t \\the scoring func\\tion considers s\\truc\\ture of \\the parse along wi\\th \\the linguis\\tic cons\\train\\ts under which \\this s\\truc\\ture can occur."]},{"title":"4 Evaluat\\bon\\t","paragraphs":["Mal\\t Parser (version 0.4) (Nivre e\\t al., 2007), and M\\fT Parser (version 0.4b) (McDonald e\\t al., 2005) have been \\tuned for Hindi by \\bhara\\ti e\\t al. (2008b). Parsers were \\trained on a subse\\t of a Hindi Treebank (\\begum e\\t al., 2008a). The \\training se\\t had 1185 sen\\tences, developmen\\t and \\tes\\t se\\t had 268 and 220 sen\\tences respec\\tively. We use \\the same experimen\\tal se\\tup (parame\\ters, fea\\tures, e\\tc.) used by \\them and compare \\the resul\\ts of \\the \\two da\\ta driven parsers wi\\th \\tha\\t of \\the proposed cons\\train\\t based hybrid parser (C\\bHP) on \\the same da\\tase\\t5",". Table 1 shows \\the performance in \\terms of unlabeled a\\t\\tachmen\\ts (UA), labeled (L) and labeled a\\t\\tachmen\\t (LA) accuracy. In Table 1, C\\bHP shows \\the performance of \\the sys\\tem when a priori\\tizer (cf. \\fec\\tion 3.2) is used, while C\\bHP’’ shows i\\t for \\the bes\\t parse \\tha\\t is available in \\the firs\\t 25 parses. We show C\\bHP’’ \\to show \\tha\\t a good parse is available in as few as \\the firs\\t 25 parses. Once \\the priori\\tizer is fur\\ther improved \\the overall performance (of C\\bHP) will po\\ten\\tially cross C\\bHP’’.","Table\\t1: Parser Evalua\\tion UA LA L C\\bHP 88.24 71.13 74.12","C\\bHP” 91.24 79.07 80.82 M\\fT 87.62 78.35 81.85 Mal\\t6","88.04 79.86 82.68"]},{"title":"5 Observat\\bons\\t","paragraphs":["The ini\\tial resul\\ts show \\tha\\t \\the proposed parser gives comparable resul\\ts wi\\th \\the s\\ta\\te-of-\\thear\\t da\\ta driven Hindi parsers.","The performance of our parser is affec\\ted due \\to \\the following reasons: (1) Small\\tle\\ficon\\t(linguistic\\tdemands\\tof\\t various\\t heads): One of \\the main reasons for \\the low LA is \\tha\\t \\the \\to\\tal number of linguis\\tic demand frames \\tha\\t \\the parser curren\\tly uses is very low. Unlike \\the da\\ta driven parsers C\\bHP doesn’\\t employ any ML \\techniques \\to induce \\this informa\\tion. For Hindi, \\there are a \\to\\tal of around 300 frames, which have been divided in\\to 20 verb classes (\\begum e\\t al., 2008b). As \\the coverage of \\this lexicon increases, \\the efficiency will \\m \\m 5 For de\\tails on \\the corpus \\type, anno\\ta\\tion scheme, \\tagse\\t, e\\tc. see \\begum e\\t al. (2008a). 6 Resul\\ts ob\\tained for Mal\\t using version 1.2"]},{"title":"( ) ( )","paragraphs":["p"]},{"title":"Rp ζζ = ζ ( ) ( ) ( )]*[","paragraphs":["ne e"]},{"title":"Cken ζζζ ∑ += ( ) ])}()/({*[","paragraphs":["1"]},{"title":"∑ +=","paragraphs":["i iii"]},{"title":"PrPke γγζ )/(","paragraphs":["i"]},{"title":"rP γ","paragraphs":["i"]},{"title":"γ","paragraphs":["i"]},{"title":"γ","paragraphs":["i"]},{"title":"γ","paragraphs":["619 au\\toma\\tically increase. No\\te \\tha\\t in spi\\te of \\this major drawback, C\\bHP s\\till manages \\to achieve considerable high accuracy. (2) Unhandled\\tconstructions: The parser s\\till doesn’\\t handle some cons\\truc\\tions, and (3)\\t Prioritization\\t mistakes: As s\\ta\\ted earlier \\the priori\\tizer is s\\till being improved. Various scoring and ranking func\\tions are being \\tried ou\\t. The overall performance will inevi\\tably increase wi\\th \\the improvemen\\t of \\the priori\\tizer.","There are various reasons why we \\think \\tha\\t \\the proposed approach is be\\t\\ter sui\\ted \\to parsing MoR-FWO: (1) Complex linguis\\tic cues can easily be encoded as par\\t of various cons\\train\\ts. For example, i\\t has been shown by \\bhara\\ti e\\t al. (2008b) \\tha\\t, for Hindi, complex agreemen\\t pa\\t\\terns, \\though presen\\t in \\the da\\ta, are no\\t being learn\\t by da\\ta driven parsers. \\fuch pa\\t\\terns along wi\\th o\\ther idiosyncra\\tic language proper\\ties can be easily incorpora\\ted as cons\\train\\ts, (2) IP formula\\tion allows for handling non-projec\\tive parsing (Riedel and Clarke, 2006), (3) Use of H-cons\\train\\ts and \\f-cons\\train\\ts \\toge\\ther reflec\\t \\the grammar of a language. The rules in \\the form of H-cons\\train\\ts are complemen\\ted by \\the weigh\\ts of \\f-cons\\train\\ts learn\\t from \\the anno\\ta\\ted corpus, (4) Two-s\\tage parsing lends i\\tself seamlessly \\to parsing complex sen\\tences by modularizing \\the \\task of overall parsing, (5) The problem of label bias faced by \\the da\\ta driven Hindi parsers (\\bhara\\ti e\\t al., 2008b) for some cases does no\\t arise here as con\\tex\\tually similar en\\ti\\ties are disambigua\\ted by \\tapping in hard \\to learn fea\\tures, (6) Use of clauses as basic parsing uni\\ts reduces \\the search space a\\t bo\\th \\the s\\tages, (7) Parsing closely rela\\ted languages will become easy. The proposed approach al\\though \\tes\\ted for Hindi will work equally well for o\\ther closely rela\\ted Indian languages such as Urdu, Punjabi, e\\tc. I\\t is impor\\tan\\t \\to no\\te here \\tha\\t \\the approach is language independen\\t, and \\tha\\t \\the machinery and \\the design are no\\t language specific. I\\t is easy \\to see \\tha\\t given a \\treebank for a language, weigh\\ts for various \\f-cons\\train\\ts in \\tha\\t language can au\\toma\\tically be learn\\t. A se\\t of \\f-cons\\train\\ts can be common for various Indian languages bu\\t can vary in \\their weigh\\ts. In fac\\t, one can po\\ten\\tially induce \\the H-cons\\train\\ts (in \\the form of linguis\\tic demands for various heads, e\\tc.) from a \\treebank as well. The H-cons\\train\\ts and \\f-cons\\train\\ts for closely rela\\ted languages will \\tend \\to be very similar."]},{"title":"6 Conclus\\bon\\t","paragraphs":["In \\this paper we proposed a new \\two s\\tage cons\\train\\t based hybrid approach \\to dependency parsing. We showed how by modularizing \\the \\task of overall parsing in\\to 2 s\\tages we can overcome many problems faced by da\\ta driven parsing. We showed how in \\the 1s\\t","s\\tage only in\\tra-clausal dependencies are handled and la\\ter in \\the 2nd","s\\tage \\the in\\ter-clausal dependencies are iden\\tified. We also briefly described \\the use of H-cons\\train\\ts and \\f-cons\\train\\ts. We argued \\tha\\t such cons\\train\\ts complemen\\t each o\\ther in ge\\t\\ting \\the bes\\t parse and \\tha\\t \\toge\\ther \\they represen\\t \\the grammar of \\the language. We evalua\\ted our sys\\tem for Hindi wi\\th \\two da\\ta driven parsers. Our resul\\ts show \\tha\\t \\the proposed parser performs be\\t\\ter \\than \\those parsers. Finally, we argued why \\the proposed hybrid approach is be\\t\\ter sui\\ted \\to handle \\the challenges posed by MoR-FWO and gave few poin\\ters as how we can fur\\ther improve our performance.","The proposed parser is s\\till being improved a\\t various fron\\ts. To begin wi\\th a priori\\tiza\\tion mechanism is being improved. We need \\to enrich \\the verb frame lexicon along wi\\th handling some unhandled cons\\truc\\tions. This will be \\taken up as immedia\\te fu\\ture work."]},{"title":"\\t","paragraphs":["620 "]},{"title":"References\\t","paragraphs":["Abney, \\f. 1997. Par\\tial Parsing via Fini\\te-\\f\\ta\\te Cascades. Natural\\t Language\\t Engineering,","2(4):337–344. A\\t\\tardi, G. and F. Dell’Orle\\t\\ta. 2008. Chunking and Dependency Parsing. LREC\\tWorkshop\\ton\\t","Partial\\tParsing:\\tBetween\\tChunking\\tand\\tDeep\\tParsing. Marrakech, Morocco. \\begum, R., \\f. Husain, A. Dhwaj, D. \\fharma, L. \\bai, and R. \\fangal. 2008a. Dependency","anno\\ta\\tion scheme for Indian languages. Proc.\\tof\\tIJCNLP08. \\begum, R., \\f. Husain, D. \\fharma and L. \\bai. 2008b. Developing Verb Frames in Hindi. Proc.\\t","of\\tLREC08. \\bhara\\ti, A. and R. \\fangal. 1993. Parsing Free Word Order Languages in \\the Paninian","Framework. Proc.\\tof\\tACL:\\t93. \\bhara\\ti, A., D. M. \\fharma, L. \\bai and R. \\fangal. 2006. AnnCorra: Anno\\ta\\ting Corpora","Guidelines for PO\\f and Chunk Anno\\ta\\tion for Indian Languages. LTRC-TR31. \\bhara\\ti, A., R. \\fangal, and T. P. Reddy. 2002. A Cons\\train\\t \\based Parser Using In\\teger","Programming, In\\tProc.\\tof\\tICON. \\bhara\\ti, A., \\f. Husain, D. M. \\fharma, and R. \\fangal. 2008a. A Two-\\f\\tage Cons\\train\\t \\based","Dependency Parser for Free Word Order Languages. In\\tProceedings\\tof\\tthe\\tCOLIPS\\tIALP. \\bhara\\ti, A., \\f. Husain, \\b. Amba\\ti, \\f. Jain, D. \\fharma and R. \\fangal. 2008b. Two \\feman\\tic","fea\\tures make all \\the difference in parsing accuracy. Proc.\\tof\\tICON-08. \\bhara\\ti, A., V. Chai\\tanya and R. \\fangal. 1995. Natural\\t Language\\t Processing:\\t A\\t Paninian\\t","Perspective, Pren\\tice-Hall of India, New Delhi. Chen, W., Y. Zhang and H. Isahara. 2007. A Two-\\f\\tage Parser for Mul\\tilingual Dependency","Parsing. In Proc.\\tof\\tthe\\tCoNLL\\tShared\\tTask\\tSession\\tof\\tEMNLP-CoNLL. Debusmann, R., D. Duchier and G. Kruijff. 2004. Ex\\tensible dependency grammar: A new","me\\thodology. Proceedings\\tof\\tthe\\tWorkshop\\ton\\tRecent\\tAdvances\\tin\\tDependenc\\b\\tGrammar. Hall, J., J. Nilsson, J. Nivre, G. Eryigi\\t, \\b. Megyesi, M. Nilsson and M. \\faers. 2007. \\fingle","Mal\\t or \\blended? A \\f\\tudy in Mul\\tilingual Parser Op\\timiza\\tion. Proc.\\t of\\t EMNLP-CoNLL\\t","shared\\ttask\\t2007. Husain, \\f., P. Gadde, \\b. Amba\\ti, D. \\fharma and R. \\fangal. 2009. A modular cascaded","approach \\to comple\\te parsing. In\\tProc.\\tof\\tCOLIPS\\tIALP-09.\\tSingapore. Karlsson, F., A. Vou\\tilainen, J. Heikkilä and A. An\\t\\tila, (eds). 1995. Constraint\\t Grammar:\\t A\\t","language-independent\\ts\\bstem\\tfor\\tparsing\\tunrestricted\\tte\\ft. Mou\\ton de Gruy\\ter. Maruyama, H. 1990. \\f\\truc\\tural disambigua\\tion wi\\th cons\\train\\t propaga\\tion. In Proc.\\tof\\tACL:90. McDonald, R., F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projec\\tive dependency parsing","using spanning \\tree algori\\thms. Proc.\\tof\\tHLT/EMNLP. Mel'Cuk, I. A. 1988. Dependency\\tS\\bnta\\f:\\tTheor\\b\\tand\\tPractice, \\f\\ta\\te Universi\\ty Press of New","York. Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigi\\t, \\f. Kübler, \\f. Marinov and E Marsi. 2007.","Mal\\tParser: A language-independen\\t sys\\tem for da\\ta-driven dependency parsing. NLE. Peh, L-\\f. and C.H.A. Ting. 1996. A Divide-and-Conquer \\f\\tra\\tegy for Parsing. In\\tProc.\\tof\\tIWPT. Riedel, \\f. and J. Clarke. 2006. Incremen\\tal in\\teger linear programming for non-projec\\tive","dependency parsing. In Proc.\\tEMNLP. \\fchröder, I. 2002. Na\\tural Language Parsing wi\\th Graded Cons\\train\\ts. PhD \\thesis, Hamburg","Univ. \\fhieber, \\f. M. 1985. Evidence agains\\t \\the con\\tex\\t-freeness of na\\tural language. In Linguistics\\t","and\\tPhilosoph\\b, p. 8, 334–343. Tapanainen, P. and T. Järvinen. 1997. A non-projec\\tive dependency parser. Proc.\\t of\\t the\\t 5th\\t","Conference\\ton\\tApplied\\tNatural\\tLanguage\\tProcessing, pp. 64–71. 621"]}]}