{"sections":[{"title":"A Hybri\\b \\to\\be\\f for Sense Guessing of Chinese Unknown Wor\\bs","paragraphs":["∗∗∗∗   Likun Qiua,b","?b ?tai Z?faob ?b and C?fangjian Hub ","a","Department of C?finese Language and Literature?b Peking University?b C?fina","b","NEC Laboratories?b C?fina?b {qiulikun?b z?faokai?b ?fuc?fangjian}@researc?f.nec.com.cn Abstract. T?fis paper proposes a ?fybrid model to address t?fe task of sense guessing for C?finese unknown words. T?free types of similarity?b i.e.?b positional?b syntactic and semantic similarity?b are analyzed; and t?free models are developed accordingly. T?fen t?fe t?free models are combined to form a ?fybrid one (HPPS Model). To verify t?fe effectiveness and consistency of HPPS?b experiments were conducted on ten test sets w?fic?f were collected from two popular C?finese t?fesauruses Cili\\b and \\tow\\fet. In addition?b extra experiments were made on a test set of 2000 words w?fic?f were collected from newspaper. T?fe experiments s?fow t?fat HPPS Model consistently produces 4%~6% F-score improvement over t?fe best results reported in previous researc?fes. Keywor\\bs: Semantic category?b Unknown word?b LC Principle?b Semantic classification."]},{"title":"1 Intro\\buction","paragraphs":["In mat?fematics?b semantics?b and p?filosop?fy of language?b t?fe principle of compositionality ?fas profound influence (Partee?b 2004). It usually takes t?fe following form: T?fe meaning of a complex expression is t?fe function of t?fe meanings of its immediate sub-expressions and t?feir mode of combination.","Since most C?finese words are composed of meaningful c?faracters wit?fout ric?f inflections?b t?fe lexical structure of C?finese words mig?ft be considered as ?faving similar property to t?fe syntactic structure of p?frases and sentences. T?ferefore?b it is reasonable to transform t?fe principle of compositionality to t?fe following form: t?fe sense of a C?finese word is t?fe function of t?fe syntactic and semantic properties of its immediate constituents and t?feir mode of combination. Specifically?b t?fe principle mig?ft be transformed furt?fer to t?fe following form: The words formed by similar co\\bstitue\\bts i\\b the same mode fall i\\bto the same sema\\btic category. T?fis is referred to as the pri\\bciple of lexical compositio\\bality (LC Principle).","T?fe task of sense guessing is to assign a semantic category to an unknown word. T?fe assigned semantic category is c?fosen from a predefined set of semantic categories (Figure 1). For example?b a sense-guessing algorit?fm c?fooses huma\\b?b w?fic?f is one of t?fe 1758 semantic categories defined by t?fesauruses \\tow\\fet (Dong and Dong?b 2006)?b as semantic category of t?fe unknown word 基民 ji1mi\\b2 ‘stock fu\\bd i\\bvestor’.","T?fis paper investigates sense guessing of C?finese unknown words based on t?fe LC Principle. T?fe word similar in t?fe LC Principle ?fas ric?f meaning. T?free types of similarity can be defined ?fere.","If two constituents (of two wor\\bs) take the same position in the wor\\bs, they have positiona\\f simi\\farity.","If two constituents share the same POS tag, they have syntactic simi\\farity.","If two constituents share the same semantic category, they have semantic simi\\farity.","T?free models are developed based on t?fe t?free types of similarity accordingly. Firstly?b a c?faracter-sense association model is developed based on t?fe positional similarity. Given a \\f \\f \\f  Copyrig?ft 2009 by Likun Qiu?b ?tai Z?fao?b and C?fangjian Hu 464 23rd Pacific Asia Conference on Language, Information and Computation, pages 464–473","group of words t?fat start or end wit?f t?fe same c?faracter?b t?fe association between t?fe c?faracter","and word sense is computed. __________________________________________________________________ Input: (1) a set of existing wor\\bs {w1, w2, ..., wn}; (2) a set of semantic categories {C1, C2, ..., Cm}; (3) the re\\fation between the wor\\bs an\\b the categories: {<wi, Cj> | 1≤≤≤≤i≤≤≤≤n, 1≤≤≤≤j≤≤≤≤m }. For an unknown wor\\b w, Output: Ck, where 1≤≤≤≤k≤≤≤≤m. __________________________________________________________________","Figure 1: T?fe task of sense guessing","","Secondly?b a POS-sense association model is developed by a sequence-labeling met?fod. All training words and testing words are segmented into constituents and tagged wit?f POS. T?fen we solve t?fe sense-guessing problem wit?f algorit?fms suc?f as CRFs and ME.","T?firdly?b a sense-sense association model is developed based on t?fe semantic similarity between t?fe constituents of testing words and training words.","Finally?b t?fe t?free models are combined to form HPPS (a Hybrid model based on Position?b POS and Sense).","T?fe remainder of t?fis paper is organized as follows. Section 2 introduces previous work on sense guessing of unknown words. Met?fod of (Lu?b 2007) is described in Section 3 and is taken as baseline of t?fis paper. Section 4 describes t?fe HPPS Model. Section 5 gives t?fe experiment results of t?fe HPPS Model toget?fer wit?f an error analysis. Section 6 presents conclusions."]},{"title":"2 Re\\fate\\b Work","paragraphs":["Met?fods involved in t?fe sense-guessing process of unknown words mig?ft be classified into two types: structure-based met?fods and context-based met?fods. Most researc?fes focusing on C?finese unknown words utilized structure-based met?fods. A ?fybrid model is proposed in (Lu?b 2007). T?fe accuracy of t?fe ?fybrid model is 61.6% on Cili\\b. T?fis is t?fe best result in previous researc?fes.","A similarity-based model is proposed in (C?fen and C?fen?b 2000). T?fe similarity of t?fe modifiers of two words t?fat s?fare t?fe same ?fead is computed to represent t?fe similarity of t?fe two words. T?fe F-score is 81%. However?b t?fe test set contains only 200 unknown nouns?b w?fic?f is too small to make a reliable evaluation.","By using a morp?fological analyzer?b t?fe morp?fosyntactic relations?fip between t?fe morp?femes of a word is detected in (Tseng?b 2003). Before a most similar word of t?fe test word is retrieved?b t?fe words wit?f a different morp?fosyntactic relations?fip are filtered. However?b t?fe unknown words are only classified into t?fe 12 major categories of Cili\\b (Mei et al.?b 1984)?b w?fic?f is coarse-grained.","T?fe met?fod in (C?fen?b 2004) retrieves a word wit?f t?fe greatest association wit?f t?fe test word. T?fe accuracy is 61.6% on disyllabic V-V compounds. However?b t?fe test words are included in t?fe training data. T?fis result is worse t?fan t?fe result of (Lu?b 2007).","Meanw?file?b we only found two researc?fes t?fat used context-based met?fods to processing C?finese unknown words. T?fe experiments in (Lu?b 2007; C?fen and Lin?b 2000) ac?fieved 37% and 34.4% in terms of F-score respectively and s?fow t?fat t?fe use of contextual information does not lead to performance en?fancement. For Englis?f?b context-based met?fods are used more popularly suc?f as (Ciaramita and Jo?fnson?b 2003; Curran?b 2006; Pekar and Staab?b 2003). However?b t?feir results are similar to t?fose analogous studies for C?finese unknown words. For instance?b Pekar and Staab (2003) tried to classify nouns into 137 classes and only ac?fieved a precision of 35.1%.","T?fe idea of t?fe LC Principle ?fas been touc?fed more or less by previous researc?fes?b suc?f as (C?fen?b 2004; Lu?b 2007). However?b it ?fas not been clearly stated and systematically studied. 465"]},{"title":"3 Base\\fine \\to\\be\\f","paragraphs":["T?fe met?fod in (Lu?b 2007) is taken as t?fe baseline. It contains two separated models: a c?faracter-sense association model and a rule-based model."]},{"title":"3.1 Character-Sense Association \\to\\be\\f (CS \\to\\be\\f)","paragraphs":["T?fe first model is t?fe c?faracter-sense association model?b w?fic?f is used by bot?f (C?fen?b 2004; Lu?b 2007). It is referred to as t?fe CS Model in t?fis paper. To make t?fe comparison reliable?b we follow t?fe designs of c?faracter-sense association model in (Lu?b 2007).","T?fis model uses 2"]},{"title":"x","paragraphs":["to capture t?fe relations?fip between t?fe semantic category of an unknown word and t?fat of its component c?faracters. In (1)?b )?b( jtcAsso denotes t?fe association between a c?faracter c and a semantic category j"]},{"title":"t","paragraphs":["?b and )( Xf denotes t?fe frequency of X. )?b(max )?b( )?b(2 kk j jx tc tc tcAsso α α =  (1) w?fere )()( )]?b([ )?b( 2 j j j tfcf tcf tc +=α"]},{"title":"∑","paragraphs":["= = || 1 )?b()?b( w i jiij tcAssotwAsso λ  (2)  Once t?fe c?faracter-sense associations are calculated?b t?fe association between a word w and","a category j"]},{"title":"t","paragraphs":["?b i.e.?b"]},{"title":")?b(","paragraphs":["j"]},{"title":"twAsso","paragraphs":["?b is calculated in (2) as t?fe sum of t?fe weig?fted associations between eac?f of t?fe word’s c?faracters and t?fe semantic category?b w?fere i"]},{"title":"c","paragraphs":["denotes t?fe i’t?f c?faracter of w?b |w| denotes t?fe lengt?f of w?b and i"]},{"title":"λ","paragraphs":["denotes t?fe weig?fts. All t?fe"]},{"title":"λ","paragraphs":["s add up to 1. T?fe position-sensitive associations between a category and a c?faracter are computed in t?fe initial?b middle?b and final positions of a word respectively."]},{"title":"3.2 Ru\\fe-base\\b \\to\\be\\f","paragraphs":["T?fere are two types of rules: Rules of type-1 and Rules of type-2. Rules of type-1 deal wit?f coordinate multi-syllabic word. It presupposes t?fat a coordinate multi-syllabic word and bot?f of its components s?fare t?fe same category. In Rules of type-1?b t?fe unknown word w is divided into two parts A and B. Let fA and fB denote t?fe number of times A and B occur in initial and final positions of word in C(w) respectively. Here?b C(w) refers to t?fe semantic category of word w. If C(A)=C(B) and bot?f fA and fB surpass t?fe predetermined t?fres?folds?b assign C(A) for AB.","Rules of type-2 guess t?fe semantic category of a tri-syllabic or four-syllable word by finding a similar tri-syllabic or four-syllable word. A word w1 is said similar to anot?fer word w2?b if t?feir remaining parts ?fave t?fe same semantic category after t?fe same c?faracters at t?fe same position are removed. By Rules of type-2?b for an unknown word w?b its similar words are collected from t?fe t?fesaurus. T?fe semantic categories of similar words are output as t?fe categories of w. If t?fere is no similar word?b no result is output.","Formally?b for a tri-syllabic word ABC?b if t?fere is a word XYC suc?f t?fat C(AB)=C(XY)?b t?fen C(ABC)=C(XYC); if t?fere is a word XBC suc?f t?fat C(A)=C(X)?b t?fen C(ABC)=C(XBC). For instance?b for a test word 推销商 tui1xiao1sha\\bg1 ‘salesma\\b’?b collect its similar word 销售 商 xiao1shou4sha\\bg1 ‘salesma\\b’ from t?fe t?fesaurus?b i.e.?b C(推销)=C(销售). T?fen C(销售商) is assigned to 推销商 as its semantic category."]},{"title":"3.3 Combination","paragraphs":["T?fe former two models are combined toget?fer (see Figure 2). 466 ____________________________________________________________ For an unknown wor\\b w, the ru\\fe-base\\b mo\\be\\f is app\\fie\\b. Denote the output as {C1, C2, ..., Cn}. If n=1, then C1 is output. If n>1, rank a\\f\\f Ci, where i=1, ..., n, accor\\bing to their association with w (app\\fy CS \\to\\be\\f to","achieve the association). Then the top-ranke\\b one is output. If n=0, the character-sense association mo\\be\\f is app\\fie\\b. Denote its output as {C1, C2, ..., Cm}.","If m=1, then C1 is output.","If m>1, rank a\\f\\f Ci accor\\bing to their association, an\\b output the top-ranke\\b one.","If m=0, nothing is output. ____________________________________________________________","Figure 2: T?fe baseline met?fod"]},{"title":"4 Propose\\b \\tetho\\b: HPPS \\to\\be\\f","paragraphs":["T?free models are developed based on t?fe t?free types of similarity of t?fe LC Principle. T?fe first model is in?ferited from t?fe CS model wit?fout modification. T?fe second model uses a sequence-labeling met?fod to guess sense based on constituents of words and POS tags of t?fe constituents. T?fe t?fird model automatically generates mapping rules from t?fe semantic category of constituents to t?fe semantic category of t?fe w?fole word. T?fen t?fe t?free models are combined to form t?fe HPPS model."]},{"title":"4.1 Sequence-Labe\\fing \\to\\be\\f (SL \\to\\be\\f)","paragraphs":["T?fe second model considers t?fe sense-guessing task as a sequence-labeling problem. T?fis model builds mapping from t?fe constituents of a word and t?feir POS tags?b to t?fe semantic category of t?fe word. T?fis is referred to as t?fe SL Model. Since many studies ?fave s?fown t?fat CRFs (Conditional Random Fields) are t?fe best model for sequence-labeling problem (Lafferty et al.?b 2001; Vail et al.?b 2007)?b CRFs are adopted as met?fod in t?fis model.","For any unknown word w?b it is not necessary to infer its semantic category from all words in t?fe t?fesaurus?b because most words in t?fe t?fesaurus ?fave no relation wit?f w. Generally?b only t?fose words s?faring t?fe same c?faracter wit?f w may possibly s?fare t?fe same semantic category wit?f w. T?ferefore?b only t?fis kind of words is selected to form t?fe training set of w. In detail?b if w is a noun?b t?fe words s?faring t?fe same final c?faracter wit?f w are c?fosen. If w is a verb or adjective?b t?fe words s?faring eit?fer t?fe initial c?faracter or t?fe final c?faracter are c?fosen.","Two types of features are employed: t?fe constituent c?faracters of a word and t?fe POS tags of t?fose constituents. In bot?f training and testing process?b t?fe internal constituent structure of t?fe words is analyzed and POS-tags are attac?fed to constituents. For example?b 文化部门 we\\b2hua4-bu4me\\b2 ‘bra\\bch of culture’ ?fas t?fe following c?faracters: 文?b 化?b 部?b 门?b and is segmented and POS-tagged as “文/N 化/V 部/N 门/N”?b in w?fic?f “文/N” means t?fat “文” is a noun and “化/V” means t?fat “化” is a verb.","Particularly?b twelve n-gram templates are selected as features for CRFs: C−1?b C0?b C1?b C−1C0?b C0C1?b C−1C1?b P−1?b P0?b P1?b P−1P0?b P0P1?b P−1P1?b w?fere C stands for a c?faracter?b P for t?fe POS of a c?faracter?b and t?fe subscripts -1?b 0 and 1 for t?fe previous?b current and next position respectively.","In t?fe training process?b firstly?b eac?f training word is segmented and POS tagged by a standard tool. T?fat is?b for word w?b t?fe following form is ac?fieved: <A1?b P1>?b <A2?b P2>?b ...<An-1?b Pn-1>?b <An?b Pn> w?fere eac?f Ai is a constituent after segmentation?b and Pi is its POS tag. Secondly?b eac?f constituent is attac?fed wit?f a category label. Given C(w)=C1?b t?fe following form is ac?fieved <A1?b P1/C1_I>?b <A2?b P2/C1_M>?b ... <An-1?b Pn-1/C1_M>?b <An?b Pn/C1_F> w?fere C1_I?b C1_M?b C1_F denotes t?fe I\\bitial?b Middle?b and Fi\\bal part of C1 respectively. For instance?b for w=文化部门?b t?fe following form is ac?fieved: <文?b N/Di09_I>?b <化?b V/Di09_M>?b <部?b N/Di09_M>?b <门?b N/Di09_F> in w?fic?f Di09 is t?fe semantic category of w in Cili\\b. T?firdly?b t?fe feature templates are used to extract features. Fourt?fly?b CRFs are applied on t?fe training sets to obtain a model. 467","In t?fe testing process?b t?fe unknown word is segmented and POS-tagged by t?fe same tool first. T?fen features are extracted. Finally t?fe sequence is input to t?fe obtained model to acquire a semantic category. For instance?b given an unknown word 花费 hua1fei4 ‘expe\\bd’?b it would be analyzed as 花/V 费/V for feature extraction. T?fen t?fe model gives an output: <花?b V/He13_I> <费?b V/He13_F> . T?fat is?b t?fe semantic category He13 is assigned to t?fe word 花费 (in Cili\\b He13 referring to expending or storing)."]},{"title":"4.2 Sense-Sense Association \\to\\be\\f (SS \\to\\be\\f)","paragraphs":["T?fe t?fird model simulates t?free ways of word forming in C?finese based on t?fe semantic similarity. T?fis is referred to as t?fe SS Model. In detail?b t?fe first way is t?fe same as Rules of type-1 of Lu (2007) and is called coordinate analogy. T?fe ot?fer two are called double parallel analogy and paired parallel analogy respectively. Compared wit?f t?fe Rules of type-2 of Lu (2007)?b t?fe two newly proposed analogies ?fave t?free advantages. T?fe first?b a pattern is given instead of rules. T?fat is?b rules will be automatically generated from a t?fesaurus based on t?fe patterns. T?fe second?b t?fe pattern is in probabilistic form?b w?fic?f extends t?fe coverage. T?fe t?fird?b restriction on word lengt?f in t?fe Rules of type-2 is removed?b w?fic?f covers more cases. Doub\\fe Para\\f\\fe\\f Ana\\fogy In linguistics?b a group of words is said parallel if t?fey s?fare t?fe same c?faracter(s) at t?fe same position?b i.e.?b {D1A?b D2A?b ...?b DnA}?b w?fere eac?f DiA is a word?b and D and A are constituents containing one or more c?faracters. In many cases?b parallel words also s?fare t?fe same semantic category?b i.e.?b C(D1A)=C(D2A) =... =C(DnA). T?fat gives a ?fint for sense guessing: it is probably correct to guess an unknown word as C(D1A) if it takes a similar structure Dn+1A.","However?b t?fere are also many violations?b especially w?fen A is polysemous. To filter t?fose violations?b an extra limitation may be set on t?fe semantic categories of t?fe different part of t?fe parallel words. Particularly?b t?fe semantic categories of t?fe different part are required to be t?fe same?b i.e.?b C(D1)=C(D2)=...=C(Dn). T?fis limitation ?felps filter many violations. Since t?fe semantic categories of bot?f part of and t?fe w?fole words are required to be t?fe same?b it is called double.","If a group of words in t?fe t?fesaurus are found to be double parallel?b t?fen it is confident to guess a similar-structure unknown word Dn+1A as C(D1A). In real cases?b one or more negative examples may occur. Here?b a negative example refers to a word E1A satisfying C(E1)=C(D1)=...=C(Dn) but C(E1A)\\bC(DiA)?b w?fere 1≤i≤n. Less negative examples?b more possible a guess is correct. T?ferefore a t?fres?fold T is introduced. In addition?b to ensure t?fe correctness of guessing?b a limitation is added to t?fe number of parallel words?b i.e.?b {D1A?b D2A?b ...?b DnA}. n must be not less t?fan a t?fres?fold N.","Denote t?fe t?fesaurus as S. Given two t?fres?folds N and T. Double Parallel Analogy gives a pattern as follows. For a constituent A t?fat contains one or more c?faracters?b collect parallel word set PS={DiA| DiA\\tS}?b w?fere Di contains one or more c?faracters. On PS?b if","\\fCMDDA 1 ≥= |})(C|{| and TCMDCMDA 12 >== ))(C|)(CP( ?b w?fere CM1 and CM2 are two semantic categories?b t?fen a rule is generated: For an unknown word w=BA?b C(BA)=CM2 if C(B)=CM1.","For example?b for A=人 re\\b2 ‘perso\\b’?b collect parallel word set PS={DiA} from Cili\\b. PS contains more t?fan 300 words. Among t?fem?b four words (Table 1) satisfies C(Di)=Ed03?b w?fere 1≤i≤4. Given N=3 and T=0.5. Since \\fDDA >== 4|}Ed03)(C|{| and","TDDA >===","43",")Ed03)(C|Ak03)(CP( ?b a rule is generated: C(D 人)=Ak03 if C(D)=Ed03. T?fen?b for an unknown word 圣人 she\\bg4re\\b2 ‘sage’?b since C(圣)=Ed03?b t?fis rule assigns Ak03 to 圣人.","T?fe symmetrical form of t?fe analogy also applies. T?fat is?b if t?fe word set takes AD form?b t?fen t?fe rule takes AB form. If eac?f word is restricted to 3 or 4 c?faracters?b N=1 and T=0?b t?fen t?fis analogy regresses to Rules of type-2 of Lu (2007). T?fat is?b t?fe double parallel analogy covers Rules of type-2. 468 Tab\\fe 1: Words of parallel set {DiA} satisfying C(Di)=Ed03 DiA Word C(DiA) D1A 坏人 huai4re\\b2 ‘bad perso\\b’ Ak03 D2A 歹人 dai3re\\b2 ‘ga\\bgster’ Ak03 D3A 好人 hao3re\\b2 ‘good perso\\b’ Ak03 D4A 美人 mei3re\\b2 ‘beautiful perso\\b’ Ac03  Tab\\fe 2: Parallel sets of c?faracter pair A=峰 and B=头 DiA/B Word C(DiA/B) D1A 上峰 sha\\bg4fe\\bg1 ‘leader’ Aj08 D1B 上头 sha\\bg4tou5 ‘leader’ Aj08 D2A 山峰 sha\\b1fe\\bg1 ‘peak’ Be04 D2B 山头 sha\\b1tou2 ‘peak’ Be04 D3A 尖峰 jia\\b1fe\\bg1 ‘high-poi\\bt’ Dd13 D3B 尖头 jia\\b1tou2 ‘sharp-e\\bd’ Bc01 D4A 洪峰 ho\\bg2fe\\bg1 ‘flood’ Bg01 D5B 木头 mu4tou5 ‘wood’ Bm03"," Paire\\b Para\\f\\fe\\f Ana\\fogy Many pairs of c?faracters ?fave t?fe ability to form words wit?f t?fe same semantic category?b if t?fe pair of words ?fas t?fe same semantic category itself. T?fat is?b a pair of c?faracters A and B ?fas t?fe ability to form words DA and DB wit?f C(DA)=C(DB)?b if C(A)=C(B) ?folds. Denote t?fe t?fesaurus as S. Given a t?fres?fold T?b a probabilistic pattern is given as follows.","For a pair of c?faracters A and B wit?f C(A)=C(B)?b combine t?feir own parallel word sets as","} |{} |{ SBDBDSADADPS iiii \\t\\f\\t= . If TPSDBPSDADBDA >\\t\\t= )) ?b(|)(C)(CP( ?b t?fen a rule is generated: For an unknown word w=EA?b C(EA)=C(EB) if EB\\tS.","For example?b A=峰 fe\\bg1 ‘peak’?b B=头 tou2 ‘top’. In Cili\\b?b C(A)=C(B)=Bc01. From PS (Table 2)?b t?free words-pairs are found: {{D1A?b D1B}?b {D2A?b D2B}?b {D3A?b D3B}}. Given T=0.5. Since TPSDBPSDADBDA >=\\t\\t=","32",")) ?b(|)(C)(CP( ?b a rule is generated: C(D峰)=C(D 头) if t?fe word D 头 \\t S. T?fen?b for an unknown word 眉峰 mei2fe\\bg1 ‘eyebrow’?b t?fe above rule is applicable because DB= 眉 头 mei2tou2 ‘eyebrow’ exists in t?fe t?fesaurus?b wit?f semantic category Bk12. T?fen Bk12 is assigned to 眉峰. T?fe symmetrical form of t?fe analogy also applies."]},{"title":"4.3 HPPS \\to\\be\\f","paragraphs":["HPPS is a ?fybrid met?fod of SS?b CS?b and SL models. About t?fe t?free models?b t?fe SS Model is most credible. T?fat is?b if it gives a guess?b t?fe guess is always correct. But it cannot give guess in many cases?b because of its strict constrains. T?fe CS and SL Model ?fave similar credibility and coverage. However?b CS Model is more credible in Case-1?b w?file SL Model more credible on Case-2.","For a Case-1 word w=AB?b in t?fe training set?b t?fere exist at least two words w1=A* and w2=*B?b satisfying C(w1)=C(w2). Here?b * means any c?faracter. For example?b for w=包 间 bao1jia\\b1 ‘compartme\\bt’?b in Cili\\b?b t?fere exist two words 包厢 bao1xia\\bg1 ‘balco\\by’ and 房间 fa\\bg2jia\\b1 ‘room’?b satisfying C(包厢)=C(房间). Ot?fer words are Case-2 words. For example?b in Cili\\b?b for w=半径 ba\\b4ji\\bg4 ‘radius’?b w2=直径 zhi2ji\\bg4 ‘diameter’ can be found?b but t?fere is no w1=半* satisfying C(w1)= C(w2). 469","According to t?fe above observations?b HPPS Model is designed as s?fown in Figure 3. SS","Model is running first. For words w?fic?f SS Model gives no guess?b give t?fem to CS or SL","Model. CS and SL Model ?fave t?feir own advantages: CS Model is more credible w?fen bot?f","initial and final positive examples are found w?file SL works better w?fen only one positive","example is found. T?ferefore?b t?fey are used to process Case-1 and Case-2 words respectively ________________________________________________________________ For an unknown wor\\b w, App\\fy the SS \\to\\be\\f. Denote the output as {C1, C2, ..., Cn}. If n=1, then C1 is output. If n>1, rank a\\f\\f Ci, where 1≤≤≤≤i≤≤≤≤n, accor\\bing to their association with w (app\\fy CS \\to\\be\\f to achieve","the association). Then the top-ranke\\b one is output. If n=0:","App\\fy CS in Case-1;","App\\fy SL in Case-2. ________________________________________________________________","Figure 3: T?fe HPPS Model"]},{"title":"5 Experiments 5.1 Data Preparation","paragraphs":["T?free types of test sets were constructed. T?fe first two are based on popular C?finese t?fesauruses Cili\\b and \\tow\\fet. Cili\\b contains over 70?b000 words?b w?fic?f are classified into 1428 semantic categories. \\tow\\fet contains over 90?b000 words and 1758 semantic categories. To compare wit?f previous work fairly?b t?fe test sets were constructed following t?fe procedure of Lu (2007): (1) select t?fe January/1998 part of t?fe Co\\btemporary Chi\\bese Corpus from Peking University (Yu et al. 2002). T?fat corpus contains all t?fe articles publis?fed in People’s Daily?b a major newspaper in C?fina; (2) remove words t?fat are not in Cili\\b; (3) remove words t?fat are not 2-4 c?faracters lengt?f; (4) remove words t?fat are not noun?b verb?b or adjective. T?fen 35151 words were left. (5) construct ten test sets?b eac?f of w?fic?f contains 3?b000 words. Basically t?fe words were randomly selected from t?fe 35151 words?b but wit?f a frequency control: in eac?f test set?b 1/3 of words occurring 1-3 times?b 3-6 times?b and 7 or more times in t?fe corpus respectively. T?fe ten sets are referred to as IV (in-vocabulary) sets?b because all t?fe words are currently included in Cili\\b. T?fe ten IV sets of \\tow\\fet were constructed in t?fe same way.","T?fe t?fird type of test set was constructed by simulating t?fe real unknown words identification process: words occurring in February-June/1998 period of t?fe Co\\btemporary Chi\\bese Corpus?b but not in t?fe January/1998 period of t?fe corpus?b Cili\\b and \\tow\\fet?b were collected. It seems t?fat t?fose words are unknown words in January 1998. T?fen t?fese words were filter by lengt?f?b POS and frequency like above. From t?fe left words?b 2000 were randomly c?fosen?b w?fic?f forms t?fe test set. It is referred to as OOV (out-of-vocabulary) set. Compared wit?f t?fose IV sets?b t?fe OOV set is more close to t?fe real case of unknown words. Only 2000 words were c?fosen because of t?fe ?fig?f cost of ?fuman tagging. Two annotators performed t?fe tagging task. Eac?f word was asked to assign a semantic category in Cili\\b and \\tow\\fet respectively. T?fere was about 15% disagreement initially between t?fe annotators. T?fen t?fey discussed t?fe disagreement and solved it. Only one category was remained for one word (in fact?b one category in Cili\\b and one category in \\tow\\fet)."]},{"title":"5.2 Base\\fine: Resu\\fts of \\tetho\\b of Lu (2007) on Cilin an\\b H\\bw\\tet","paragraphs":["T?fe met?fod of Lu (2007) includes t?fe CS Model and a rule-based model. For t?fe CS Model?b a training process is needed. W?fen t?fe training set is constructed?b a remove-o\\be policy is used. T?fat is?b for a test word w?b all ot?fer words in t?fe t?fesaurus are included in t?fe training set except w (i.e.?b remove one word w from t?fe t?fesaurus). T?fat policy is a little confusing for polysemous words. 470","A polysemous word ?fas more t?fan one token in t?fe t?fesaurus. For example?b t?fe word 老爷 爷 lao3ye2ye2 ‘gra\\bdpa’ appears twice in Cili\\b, corresponding to two semantic categories oldma\\b and gra\\bdpa. T?fe remove-one policy may ?fave two meanings?b w?fen a polysemous word w is taken as t?fe test word:","Remove all tokens of w. For example?b if w=老爷爷?b t?fen t?fe two tokens in old-ma\\b and gra\\bdpa are removed from t?fe training set. We call it all-toke\\b-removi\\bg policy.","Remove one token of w. For example?b if 老爷爷 of category old-ma\\b is selected as test word?b t?fe token of category old-ma\\b is removed?b but t?fe token of category gra\\bdpa is still remained in t?fe training set. We call it o\\be-toke\\b-removi\\bg policy.","It is not clear w?fic?f policy was adopted in Lu (2007). T?ferefore we implemented and tested bot?f policies?b wit?f parameters t?fe same as Lu (2007). Table 3 summarizes t?fe test results."," Tab\\fe 3: F-score of CS Model on IV set of Cili\\b","Development Test","All-token-removing 0.561 0.545","One-token-removing 0.591 0.578 Lu (2007) 0.586 0.582","","From Table 3?b we can see t?fat t?fe one-token-removing policy ac?fieved muc?f more similar performance to Lu (2007) t?fan all-token-removing policy. T?ferefore?b we guess t?fat one-token-removing policy was adopted in Lu (2007). However?b all-token-removing is more reasonable t?fan one-token-removing?b because w?fen people say t?fat one word is an unknown word?b t?fey mean t?fat t?fe word did not occur before?b and t?fis is t?fe first time it appears. T?ferefore?b an unknown word surely ?fas no token included in t?fe w?fole t?fesaurus. According to t?fe above analysis?b t?fe all-token-removing policy is adopted in t?fe following experiments. Among t?fe ten test tests (of Cili\\b or \\tow\\fet)?b one set is used for development?b w?file t?fe ot?fer nine sets are tested t?fen based on t?fe parameters ac?fieved in development process (test process)."," Tab\\fe 4: Results of Rule-based Model of Lu (2007) and SS Model on IV set of Cili\\b and \\tow\\bet Development Test Model T?fesaurus","P R F P R F Cili\\b 0.819 0.154 0.259 0.778 0.152 0.254 Rule-based","Model of Lu \\tow\\fet 0.751 0.175 0.284 0.726 0.171 0.277 Cili\\b 0.814 0.249 0.381 0.787 0.253 0.383 SS Model","\\tow\\fet 0.769 0.311 0.442 0.763 0.311 0.442 ","Table 4 summarizes t?fe results of rule-based model of Lu (2007) in terms of precision?b recall and F-measure on IV sets of Cili\\b. T?fe model ac?fieves an overall 77.8% precision and 15.2% recall. Combined t?fe CS Model and t?fe rule-based model toget?fer?b Lu (2007)’s ?fybrid model ac?fieves 56.5% F-score (see Table 5)."]},{"title":"5.3 Resu\\fts of Propose\\b \\tetho\\bs on Cilin an\\b H\\bw\\tet","paragraphs":["Table 4 also s?fows t?fat on Cili\\b?b t?fe SS Model improves 0.9% in precision and 10.1% improvement in recall over rule-based model of Lu; on \\tow\\bet t?fe SS Model improves 3.7% in precision and 14% in recall over rules-based model of Lu. T?fe improvement verifies t?fat t?fe probabilistic-pattern based met?fod can cover more words t?fan manually crafted rules.","After development process?b t?fe following parameters were ac?fieved: t?fe two T t?fres?folds of SS Model are bot?f 0.65; t?fe t?fres?fold N in double parallel analogy is 3 for disyllabic words and 1 for ot?fer words; t?fres?folds for fA and fB in coordinate analogy are 1 and 1 for nouns?b and 0 and 3 for ot?fer words. In SL Model?b ICTCLAS 3.0 (Z?fang?b 2002) was used as word 471 segmentation and POS tagging tool?b w?file “CRF++?b Yet Anot?fer CRF” toolkit (?tudo?b 2005) was used as t?fe implementation of CRFs."," Tab\\fe 5: Results of proposed met?fods on IV sets and OOV set of Cili\\b and \\tow\\fet","Development Test Data Type","T?fesaur us Model","P R F P R F baseline 0.581 0.580 0.580 0.566 0.565 0.565 Cili\\b HPPS 0.619 0.618 0.619 0.610 0.609 0.610 baseline 0.525 0.525 0.525 0.510 0.510 0.510 IV \\tow\\fet HPPS 0.576 0.575 0.575 0.564 0.564 0.564 baseline / / / 0.569 0.569 0.569 Cili\\b HPPS / / / 0.630 0.630 0.630 baseline / / / 0.557 0.557 0.557 OOV \\tow\\fet HPPS / / / 0.604 0.604 0.604 ","Table 5 s?fows t?fat t?fe HPPS Model improves 4.5% on IV sets of Cili\\b and 5.4% on \\tow\\fet in F-score over t?fe baseline model. It also summarizes t?fe results on OOV set of Cili\\b and \\tow\\fet. T?fe HPPS Model ac?fieves improvements of 6.1% F-score on OOV set of Cili\\b and 4.7% F-score on OOV set of \\tow\\fet over t?fe baseline. Compared wit?f IV sets?b t?fe improvement on OOV set is a little bigger on Cili\\b?b but a little smaller on \\tow\\fet. Generally speaking?b t?fe performance improvement over t?fe baseline is consistent on t?fe t?free types of test set. T?fe average improvement is 5.2%. 5.4"]},{"title":"Error Ana\\fysis","paragraphs":["T?fe result of HPPS on one IV set of Cili\\b is selected to do error analysis. T?fere are mainly four types of error.","T?fe first type of error is caused by t?fe ambiguity of constituents. For instance?b t?fe words ended wit?f c?faracter 头 tou2 ‘head’ are among several semantic categories. It is difficult to identify t?fat 丫头 ya1tou5 ‘girl’ is girl w?file 白头 bai2tou2 ‘white-head’ is head. T?fe second type of error is caused by t?fe defect of t?fe t?fesaurus. For instance?b HPPS assigned amou\\bt to 库 存 量 ku4cu\\b2lia\\bg4 ‘the qua\\btity of goods i\\b stock’. However?b it is assigned artifact in \\tow\\fet?b t?fe same category as 库存 ku4cu\\b2 ‘i\\bve\\btory’. T?fe t?fird type of error is caused by some c?faracters t?fat ?fave no ability of forming new words. For instance?b t?fe c?faracter 拓 ?fas two meanings. One is ta4 ‘rub’?b and t?fe ot?fer is tuo4 ‘develop’. However?b t?fe meaning ta4 ‘rub’ comes from Arc?faic C?finese and rarely used in modern C?finese. T?ferefore unknown words like 拓展 tuo4zha\\b3 ‘develop’ must ?fave t?fe ‘develop’ meaning. T?fe fourt?f type of error is caused by metap?fors?b idioms?b domain specific terms?b transliterations?b abbreviations and so on. For instance?b 二恶英 e4er4yi\\bg1 ‘dioxi\\b’ is a domain specific term and 夸克 kua4ke4 ‘quark’ is a transliteration.","T?fe ratio of t?fe four types of error is 45%?b 25%?b 5% and 25% respectively."]},{"title":"6 Conc\\fusions","paragraphs":["T?fis paper contributes to t?fe researc?f of sense guessing for C?finese unknown words. Specifically?b we (1) propose a met?fod for generating rules for sense guessing (Sense-Sense Association Model)?b (2) consider t?fe sense guessing task as a sequence-labeling process and tackle it wit?f CRFs (Sequence Labeling Model)?b and (3) combine t?fe two models wit?f C?faracter-Sense Association Model toget?fer as HPPS Model. Experiments conducted bot?f on IV set and OOV set s?fow t?fe effectiveness of t?fe HPPS Model. 472"]},{"title":"References","paragraphs":["C?fen?b C.-J. 2004. C?faracter-sense association and compounding template similarity: Automatic semantic classification of C?finese compounds. In Proceedi\\bgs of the 3rd SIG\\tA\\f Workshop o\\b Chi\\bese La\\bguage Processi\\bg?b pages 33-40.","C?fen?b H.-H. and C.-C. Lin. 2000. Sense-tagging C?finese Corpus. In Proceedi\\bgs of the 2\\bd Chi\\bese La\\bguage Processi\\bg Workshop?b pages 7-14.","C?fen?b ?t.-J. and C.-J. C?fen. 2000. Automatic semantic classification for C?finese unknown compound nouns. In Proceedi\\bgs of the 18th I\\bter\\batio\\bal Co\\bfere\\bce o\\b Computatio\\bal Li\\bguistics?b pages 173-179.","Ciaramita?b M. and M. Jo?fnson. 2003. Supersense Tagging of Unknown Nouns in WordNet. In Proceedi\\bgs of the 2003 Co\\bfere\\bce o\\b Empirical Methods o\\b \\fatural La\\bguage Processi\\bg.","Curran?b J. R. 2005. Supersense Tagging of Unknown Nouns using Semantic Similarity. In Proceedi\\bgs of the 43rd A\\b\\bual Meeti\\bg of the Associatio\\b for Computatio\\bal Li\\bguistics?b pages 26-33.","Dong?b Z. D. and Q. Dong. 2006. \\tow\\fet a\\bd the Computatio\\b of Mea\\bi\\bg. World Scientific Publis?fing Co.?b Inc. River Edge?b NJ?b USA.","?tudo?b T. 2005. CRF++: Yet Anot?fer CRF toolkit. ?fttp://c?fasen.org/~taku/software/CRF++.","Lafferty?b J.?b A. McCallum and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedi\\bgs of I\\bter\\batio\\bal Co\\bfere\\bce o\\b Machi\\be Lear\\bi\\bg?b pages 282-289.","Lu?b X. F. 2007. Hybrid Models for Semantic Classification of C?finese Unknown Words. In Proceedi\\bgs of \\forth America\\b Chapter of the Associatio\\b for Computatio\\bal Li\\bguistics - \\tuma\\b La\\bguage Tech\\bologies 2007 co\\bfere\\bce?b pages 188–195.","Mei?b J. J.?b Y. M. Z?fu?b Y. Q. Gao and H. X. Yin. (eds.) 1984. To\\bgyici Cili\\b. Commercial Press?b Hong ?tong.","Partee?b B. H. 2004. Compositio\\bality i\\b Formal Sema\\btics: Selected Papers by Barbara \\t. Partee. Oxford: Blackwell Publis?fing?b pages 153-181.","Pekar?b V. and S. Staab. 2003. Word classification based on combined measures of distributional and semantic similarity. In Proceedi\\bgs of 10th Co\\bfere\\bce of the Europea\\b Chapter of the Associatio\\b for Computatio\\bal Li\\bguistics.","Tseng?b H.-H. 2003. Semantic classification of C?finese unknown words. In Proceedi\\bgs of ACL-2003 Stude\\bt Research Workshop?b pages 72-79.","Vail?b D. L.?b M. M. Veloso and J. D. Lafferty. 2007. Conditional Random Fields for Activity Recognition. In Proceedi\\bgs of 2007 I\\bter\\batio\\bal Joi\\bt Co\\bfere\\bce o\\b Auto\\bomous Age\\bts a\\bd Multi-age\\bt Systems.","Yarowsky?b D. 1992. Word-Sense Disambiguation Using Statistical Models of Roget’s Categories Trained on Large Corpora. In Proceedi\\bgs of the 15th I\\bter\\batio\\bal Co\\bfere\\bce o\\b Computatio\\bal Li\\bguistics?b pages 454-460.","Yu?b S. W.?b H. M. Duan?b X. F. Z?fu and B. Swen. 2002. T?fe basic processing of Contemporary C?finese Corpus at Peking University. Jour\\bal of Chi\\bese I\\bformatio\\b Processi\\bg 16(5):49– 64.","Z?fang?b ?t. ICTCLAS1.0. ?fttp://www.nlp.org.cn/ project/project.p?fp?proj_id=6. 473"]}]}