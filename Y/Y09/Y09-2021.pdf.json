{"sections":[{"title":"Interpo\\b\\tted \\fLSI for Le\\trning \\f\\b\\tusib\\be Verb Arguments","paragraphs":["∗∗∗∗   Hiram Ca\\b\\toa, b",", \\fentaro Inuia , and Yuji Matsumotoa ","a","Computationa\\b Linguistics, Nara Institute of Science and Techno\\bogy,","Takayama, Ikoma, Nara 630-0192, Japan","{ca\\b\\to, inui, matsu}@is.naist.jp","b","Artificia\\b Inte\\b\\bigence, Center for Computing Research, Nationa\\b Po\\bytechnic Institute,","Mexico City, DF, 07738, Mexico","hca\\b\\to@cic.ipn.mx Abstr\\tct. Learning P\\bausib\\be Verb Arguments a\\b\\bows to automatica\\b\\by \\bearn what kind of acti\\tities, where and how, are performed by c\\basses of entities from sparse argument co-occurrences with a \\terb; this information it is usefu\\b for sentence reconstruction tasks. Ca\\b\\to et al\\b (2009b) propose a non \\banguage-dependent mode\\b based on the Word Space Mode\\b for ca\\bcu\\bating the p\\bausibi\\bity of candidate arguments gi\\ten one \\terb and one argument, and compare with the sing\\be \\batent \\tariab\\be PLSI a\\bgorithm method, outperforming it. In this work we rep\\bicate their experiments with a different corpus, and exp\\bore \\tariants to the PLSI method in order to exp\\bore further capabi\\bities of this \\batter wide\\by used technique. Particu\\bar\\by, we propose using an interpo\\bated PLSI scheme that a\\b\\bows the combination of mu\\btip\\be \\batent semantic \\tariab\\bes, and \\ta\\bidate it in a task of identifying the rea\\b dependency-pair trip\\be with regard to an artificia\\b\\by created one, obtaining up to 83% reca\\b\\b. Keywords: P\\bausib\\be Verb Arguments, \\t-Nearest Neighbors a\\bgorithm, \\fNN, Distributiona\\b Thesaurus, Probabi\\bistic Latent Semantic Indexing, PLSI."]},{"title":"1 Introduction","paragraphs":["P\\bausib\\be Verb Arguments information is he\\bpfu\\b in sentence reconstruction tasks. For examp\\be: The boy p\\bays with the ____ in the ____; A _____ eats grass; and I drank _____ in a g\\bass. Se\\tera\\b tasks ha\\te to dea\\b with this common prob\\bem, for examp\\be, anaphora reso\\bution wou\\bd consist on finding the referenced objects: The boy p\\bays with it the\\fe, It eats grass, I drank it in a g\\bass. Information Retrie\\ta\\b app\\bications \\book for answers to 5W questions such as ‘Who eats grass?’, “Where?”, “When?” (Parton et al\\b, 2009). The answers to these questions are not a\\bways exp\\bicit\\by stated in a text, such as ‘Where do boys p\\bay usua\\b\\by using what?’, ‘What do boys usua\\b\\by p\\bay with?’, ‘What is usua\\b\\by drunk in a g\\bass?’ therefore, additiona\\b common sense know\\bedge is needed to answer these questions. Our goa\\b is to create a \\barge database of this information so that p\\bausibi\\bity can be tested for performing a \\tariety of tasks. For other tasks that can use this kind of information, see Section 1.1.","This prob\\bem can be seen as co\\b\\becting a \\barge database of semantic frames with detai\\bed categories and examp\\bes that fit these categories. For this purpose, recent works take ad\\tantage","\\e \\e ∗ We thank the support of the Japanese Go\\ternment and the Mexican Go\\ternment (SNI, SIP-IPN, COFAA-IPN, and PIFI-IPN). Second author is a JSPS fe\\b\\bow. We a\\bso thank our anonymous re\\tiewers for their usefu\\b comments and discussion.  Copyright 2009 by Hiram Ca\\b\\to, \\fentaro Inui, and Yuji Matsumoto 622 23rd Pacific Asia Conference on Language, Information and Computation, pages 622–629 of existing manua\\b\\by crafted resources such as WordNet, Wikipedia, FrameNet, VerbNet or PropBank. For examp\\be, Reisinger and Paşca (2009) annotate existing WordNet concepts with attributes, and extend is-a re\\bations, based on Latent Dirich\\bet A\\b\\bocation on Web documents and Wikipedia. Yamada et al\\b (2009) exp\\bore extracting hyponym re\\bations from Wikipedia using pattern-based disco\\tery and distributiona\\b simi\\barity c\\bustering. Ne\\terthe\\bess, the approach of using handcrafted resources pre\\tents from obtaining information for \\banguages where those resources do not exist.","Ca\\b\\to et al\\b (2009a, 2009b) propose a non \\banguage-dependent mode\\b based on \\f-Nearest Neighbors for ca\\bcu\\bating the p\\bausibi\\bity of candidate arguments gi\\ten one \\terb and one argument, and compare with the traditiona\\b PLSI method, outperforming it. In this work we rep\\bicate their experiments with a different corpus, and exp\\bore \\tariants to the PLSI method in order to attest better capabi\\bities for this \\batter wide\\by used technique. Particu\\bar\\by, we propose using an interpo\\bated PLSI scheme that a\\b\\bows the combination of mu\\btip\\be \\batent semantic \\tariab\\bes.","In Section 2 we present three different mode\\bs for p\\bausib\\be argument estimation, the \\f-Nearest Neighbors Mode\\b proposed by Ca\\b\\to et al\\b (2009b) is presented in Section 2.1; the sing\\be-\\tariab\\be PLSI mode\\b is presented in Section 2.2; and our proposa\\b of mu\\btip\\be \\batent semantic \\tariab\\be based on interpo\\bation is presented in Section 2.3. Then we present se\\tera\\b experiments: in Section 3.1 and 3.2 we rep\\bicate pre\\tious experiments with a different corpus, being the \\batter section an ana\\bysis of the \\bearning rate. Pre\\tious resu\\bts are hea\\ti\\by affected by pre-fi\\btering, as we show in experiments shown in Section 3.3; our proposa\\b mode\\b o\\tercomes this and resu\\bts are shown in Section 3.4. Afterwards, we exp\\bore how our mode\\b works with n-grams instead of dependency trip\\be re\\bationships in experiments from Section 3.5. Fina\\b\\by, we draw our conc\\busions in Section 4 mentioning future work and possib\\be app\\bications."]},{"title":"1.1 \\fossib\\be \\tpp\\bic\\ttions","paragraphs":["Correct P\\bausib\\be Verb Argument identification can be used for se\\tera\\b tasks, such as impro\\ting parsing. Since \\batent \\tariab\\bes group the kind of arguments expected for a sentence, it is possib\\be to infer the meaning of unknown words, as in the we\\b\\b-known examp\\be about tezguino (Lin, 1998b) where it is possib\\be to know what is tezguino from the sentences: A bottle of tezguino is on the table; Eve\\fybody likes tezguino; Tezguino makes you d\\funk; and We make tezguino out of co\\fn. Another app\\bication is Semantic Ro\\be Labe\\bing, since grouping \\terb arguments and measuring their p\\bausibi\\bity increases performance, as shown by Mer\\bo and Van Der P\\bas (2009) and Deschacht and Moens (2009). Some other app\\bications are metaphora recognition, since we are ab\\be to know common usages of arguments, and an uncommon usage wou\\bd suggest its presence, or a coherence mistake (v\\b g\\f. to d\\fink the moon in a glass). Ma\\bapropism detection can use the measure of the p\\bausibi\\bity of an argument to determine misuses of words (Bo\\bshako\\t, 2005) as in hyste\\fic center, instead of historic center; density has brought me to you; It \\books \\bike a tattoo subject; and Why you say that with i\\foning?."]},{"title":"2 Mode\\bs for \\f\\b\\tusib\\be Argument Estim\\ttion","paragraphs":["We exp\\bore the mode\\bs proposed in Ca\\b\\to et al\\b (2009a, 2009b); then, we propose a new mode\\b ca\\b\\bed interpo\\bated PLSI, which a\\b\\bows using mu\\btip\\be \\batent semantic \\tariab\\bes.","We can regard the task of finding the p\\bausibi\\bity of a certain argument for a set of sentences as estimating a word gi\\ten a specific context. Since we want to consider argument co-re\\bation, we want to estimate "]},{"title":"( )","paragraphs":["where is a \\terb, is the re\\bationship between the \\terb and (noun) as subject, object, preposition or ad\\terb. and are ana\\bogous. If we assume that has a different function when used with another re\\bationship, then we can consider that and make a new symbo\\b, ca\\b\\bed . So that we can express the 5−tup\\be"]},{"title":"( )","paragraphs":["as "]},{"title":"( )","paragraphs":[". 623","We want to know, gi\\ten a \\terb and an argument , which is the most p\\bausib\\be argument, i\\be\\b"]},{"title":"( )","paragraphs":[". The probabi\\bity of finding a particu\\bar \\terb and two of its syntactic re\\bationships can be expressed as: "]},{"title":"( ) = ( )⋅ ( )","paragraphs":[", which can be estimated in se\\tera\\b ways."]},{"title":"2.1 K-Ne\\trest Neighbors Mode\\b","paragraphs":["Uses the k nearest neighbors of each argument to find the p\\bausibi\\bity of an unseen trip\\be gi\\ten","its simi\\barity to a\\b\\b trip\\bes present in the corpus, measuring this simi\\barity between arguments.","See Figure 1 for the pseudo-a\\bgorithm of this mode\\b.  \\b\\t\\f\\t\\b\\t \\b\\t","\\t\\t\\f","\\t\\b\\f\\t","\\t\\b\\t\\f Figure 1: Pseudo-a\\bgorithm for the \\f-nearest neighbors DLM a\\bgorithm","As \\totes are accumu\\bati\\te, trip\\bes that ha\\te words with many simi\\bar words wi\\b\\b get more \\totes.","Common simi\\barity measures range from Euc\\bidean distance, cosine and Jaccard’s coefficient (Lee, 1999), to measures such as Hind\\be’s measure and Lin’s measure (Lin, 1998a). Weeds and Weir (2003) show that the distributiona\\b measure with best performance is the Lin simi\\barity, so this measure is used for smoothing the co-occurrence space, fo\\b\\bowing the procedure as described by Lin (1998a)."]},{"title":"2.2 \\fLSI – \\frob\\tbi\\bistic L\\ttent Sem\\tntic Indexing","paragraphs":["The probabi\\bistic Latent Semantic Indexing Mode\\b (PLSI) was introduced in Hofmann (1999), arose from Latent Semantic Indexing (Deerwester et al\\b, 1990). The mode\\b attempts to associate an unobser\\ted c\\bass \\tariab\\be z∈Z={z1, ..., zk}, (in our case a genera\\bization of corre\\bation of the co-occurrence of v,a1 and a2), and two sets of obser\\tab\\bes: arguments, and \\terbs+arguments. In terms of generati\\te mode\\b it can be defined as fo\\b\\bows: a v,a1 pair is se\\bected with probabi\\bity P(v,a1), then a \\batent c\\bass z is se\\bected with probabi\\bity P(z|v,a1) and fina\\b\\by an argument a2 is se\\bected with probabi\\bity P(a2|z). Ca\\b\\to et al\\b (2009a) propose using PLSI (Hoffmann, 1999) this said way, expressed a\\bso as (2)."," "]},{"title":"( ) = ( )⋅ ( )⋅","paragraphs":["\\b"]},{"title":"∑ ( ) ","paragraphs":["z is a \\batent \\tariab\\be capturing the corre\\bation between a2 and the co-occurrence of (v,a1) simu\\btaneous\\by. Using a sing\\be \\batent \\tariab\\be to corre\\bate three \\tariab\\bes may \\bead to a poor performance of PLSI, so that in next section we exp\\bore different ways of exp\\boiting the smoothing by \\batent semantic \\tariab\\bes."]},{"title":"2.3 i\\fLSI – interpo\\b\\tted \\fLSI","paragraphs":["The pre\\tious PLSI formu\\ba origina\\b\\by used crushes the association of information from a2, and v,a1 simu\\btaneous\\by into one sing\\be \\batent \\tariab\\be. This caused two prob\\bems: first, data sparseness, and second, it fixed the corre\\bation between two \\tariab\\bes. Hence we propose a \\tariation for this ca\\bcu\\bation by using interpo\\bation based on each pair of arguments for a trip\\be. 624 The fo\\b\\bowing formu\\ba shows an interpo\\bated way of estimating the probabi\\bity of a trip\\be based on the co-occurrences of its different pairs."," \\t"]},{"title":"( )∝ \\b \\b","paragraphs":["\\f "]},{"title":"( ) = ( )⋅ ( )⋅ ∑ ( ) ( ) = ( )⋅ ( )⋅ ∑ ( ) ","paragraphs":["\\f"]},{"title":"( ) = \\f( )⋅ \\f( )⋅","paragraphs":["\\f"]},{"title":"∑ \\f( )   ","paragraphs":["\\b\\t\\f\\f\\t\\\\f\\f","Additiona\\b\\by we test a mode\\b that considers additiona\\b information. See Eq. (4). Note that ai (the \\batent \\tariab\\be topics) shou\\bd not be confused with a1 and a2 (the arguments)."," \\t"]},{"title":"( ) \\t \\b \\b","paragraphs":["\\f"]},{"title":"\\b \\b \\b ( ) = ( )⋅ ( )⋅ ∑ ( ) ( ) = ( )⋅ ( )⋅ ∑ ( ) ( ) = ( )⋅ ( )⋅ ∑ ( )   ","paragraphs":["See the Figure 2 for a graphica\\b representation of this concept. Each \\batent \\tariab\\be is represented by a \\better in a sma\\b\\b circ\\be. Big circ\\bes surround the components of the dependency trip\\be to be estimated. A b\\back dot shows the co-occurrence of two \\tariab\\bes. A\\b\\b of them contribute for the estimation of the trip\\be v,a1,a2.  ","","Figure 2: Graphica\\b representation of iPLSI. The tup\\be \\t,a1,a2 is estimated by using \\batent \\tariab\\bes","based on pairs of two \\tariab\\bes, and/or the pair of a \\tariab\\be and the co-ocurrence of two \\tariab\\bes. See","eqs. (3) and (4).  625"]},{"title":"3 Experiments","paragraphs":["We compare these two mode\\bs in a pseudo-disambiguation task fo\\b\\bowing Weeds and Weir (2003). First we obtain trip\\bes from the corpus. Then, we di\\tided the corpus in training (80%) and testing (20%) parts. With the first part we trained the PLSI mode\\b. We use this part a\\bso for creating the distributiona\\b thesaurus used by the \\fNN mode\\b, i\\be\\b, the simi\\barity measure used for pairs of arguments ′ . Then we are ab\\be to ca\\bcu\\bate . For e\\ta\\buation we created artificia\\b\\by 4-tup\\bes: ′ , formed by taking a\\b\\b the trip\\bes from the testing corpus, and generating an artificia\\b tup\\be ′ choosing a random ′ with ′ = , and making sure that this new random trip\\be ′ was not present in the training corpus. The task consisted of se\\becting the correct tup\\be. Ties occur when both tup\\bes are gi\\ten the same score (and both are different from zero).","For these e\\ta\\buations we used the UkWaC corpus (Ferraresi et al\\b, 2008.) This corpus is a \\barge ba\\banced corpus of Eng\\bish from the U\\f Web with more than 2 bi\\b\\bion tokens1",". We created two wordsets for the \\terbs: play, eat, add, calculate, fix, \\fead, w\\fite, have, lea\\fn, inspect, like, do, come, go, see, seem, give, take, keep, make, put, send, say, get, walk, \\fun, study, need, and become. These \\terbs were chosen as a samp\\be of high\\by frequent \\terbs, as we\\b\\b as not so frequent \\terbs. They are a\\bso \\terbs that can take a great \\tariety of arguments, such as take (i.e., ambiguity is high). Each wordset contains 1000 or 2500 \\terb dependency trip\\bes per each \\terb. The first wordset is e\\ta\\buated against 5,279 \\terb dependency trip\\bes, whi\\be the second wordset is e\\ta\\buated against 12,677 \\terb dependency trip\\bes, corresponding rough\\by to 20% of the tota\\b number of trip\\bes in each wordset."]},{"title":"3.1 Resu\\bts of origin\\t\\b \\t\\bgorithm with new corpus","paragraphs":["In this section we present our resu\\bts for this new corpus of the origina\\b PLSI and the \\fNN a\\bgorithms with the new corpus. Tests were carried out with one 7-topic \\tariab\\be for PLSI, and a 100 nearest neighbors expansion for \\fNN. Ca\\b\\to et al\\b (2009b) ha\\te shown that for estimating the probabi\\bity of an argument a2,"]},{"title":"( )","paragraphs":["works better than"]},{"title":"( )","paragraphs":[". The fo\\b\\bowing tab\\be confirms this for different wordset sizes. These experiments were performed on a subcorpus of U\\fWaC made of 1000 or 2500 trip\\bes per \\terb for the \\terbs mentioned in Section 3.  T\\tb\\be 1: Resu\\bts of the origina\\b PLSI and \\fNN a\\bgorithms for a test with the U\\fWaC corpus","Mode A\\bgorithm Wordset size Prec. Reca\\b\\b F-score PLSI 1000 0.5333 0.2582 0.3479"]},{"title":"( )","paragraphs":["\\fNN 1000 0.7184 0.5237 0.6058","PLSI 2500 0.5456 0.2391 0.3325","\\fNN 2500 0.7499 0.5032 0.6023","PLSI 1000 0.4315 0.1044 0.1681"]},{"title":"( )","paragraphs":["\\fNN 1000 0.8236 0.5492 0.6590 PLSI 2500 0.3414 0.0611 0.1036 \\fNN 2500 0.8561 0.6858 0.7615","","In a\\bmost cases \\fNN performs better than origina\\b PLSI in precision and reca\\b\\b (the best of the \\fNN \\tariations is better than the best of the PLSI \\tariations). Contrary to \\fNN, PLSI’s performance increases as the wordset size is increased probab\\by due to more confusion in using the same number of topics. This can be seen a\\bso in Figures 3 and 4: reca\\b\\b impro\\tes s\\bight\\by for bigger data sets and more topics. \\e \\e 1 A too\\b inc\\buding queries to this corpus can be found at http://sketchengine.co.uk 626"]},{"title":"3.2 Me\\tsuring the \\be\\trning r\\tte","paragraphs":["This experiment consisted on gradua\\b\\by increasing the number of trip\\bes from 125 to 2000 dependency trip\\bes per \\terb to examine the effects of using sma\\b\\ber corpora. Resu\\bts are shown in Figure 3. In this figure \\fNN outperforms PLSI when adding more data. \\fNN precision is higher as we\\b\\b in a\\b\\b experiments. The best resu\\bts for PLSI were obtained with 7 topics, whi\\be for \\fNN the best resu\\bts were obtained using 200 neighbors."," Figure 3: Precision and Reca\\b\\b for the origina\\b PLSI and \\fNN with \\bearning rate (each series has","different number of trip\\bes per \\terb, tp\\t). The frequency thresho\\bd for trip\\bes was set to 4. The numbers and the \\bower part show the number of topics for PLSI and the number of neighbors for \\fNN."]},{"title":"3.3 Resu\\bts with no pre-fi\\btering","paragraphs":["Pre\\tious resu\\bts used a pre-fi\\btering thresho\\bd of 4, that is, trip\\bes with \\bess than 4 occurrences were discarded. Here we present resu\\bts with no pre-fi\\btering. In Figure 4 resu\\bts for \\fNN fa\\b\\b dramatica\\b\\by. PLSI is ab\\be to perform better with 20 topics. This suggests that PLSI is ab\\be to smooth better sing\\be occurrences of certain trip\\bes. \\fNN is better for working with frequent\\by occurring trip\\bes. We require a method that can hand\\be occurrences of un-frequent words, since pre-fi\\btering imp\\bies a \\boss of data that cou\\bd be usefu\\b afterwards. For examp\\be, consider that tezgüino is mentioned on\\by once in the training test. We consider that it is important to be ab\\be to \\bearn information for scarce\\by mentioned entities too. The next section presents resu\\bts regarding to the impro\\tement of using PLSI to hand\\be non-fi\\btered items.","","Figure 4: A\\terage of Precision and Reca\\b\\b for the origina\\b PLSI and \\fNN showing \\bearning rate (each","series has different number of trip\\bes per \\terb, tpv). No frequency thresho\\bd was used. The numbers and","the \\bower part show the number of topics for PLSI and the number of neighbors for \\fNN. 627"]},{"title":"3.4 i\\fLSI resu\\bts","paragraphs":["As presented in Section 2.3, we test different mode\\bs for combining the Latent Semantic Variab\\bes. The mode part shows the \\batent \\tariab\\bes that were used for these tests. For examp\\be, for the a,c row, the estimation was carried using (5). Resu\\bts are presented in Tab\\be 2. \\t"]},{"title":"( )","paragraphs":["\\t \\b In Tab\\be 2, the best resu\\bts were obtained for o, (using on\\by the information from a1,a2) fo\\b\\bowed by m,o, which is combining the information from v,a1 and a1,a2. The m,n,o and n,o modes inc\\bude n, which has no impact in this test because it is a\\bways fixed, and he\\bps \\bitt\\be for deciding which trip\\be is better. Howe\\ter, as we show in the fo\\b\\bowing section, a test with pure n-grams (non dependency trip\\bes, as in a\\b\\b pre\\tious tests) the three components (in this case m, n, and o), are contributing to the estimation.","T\\tb\\be 2: Comparison of different iPLSI modes, consisting on se\\becting different estimators. \\fNN is shown in the \\bast row for reference.","mode Precision Reca\\b\\b mode Precision Reca\\b\\b a,b,c 0.78 0.78 m,n,o 0.83 0.83 a 0.67 0.60 m 0.78 0.77 b 0.44 0.44 n 0.50 0.48 c 0.77 0.77 o 0.84 0.84 a,c 0.62 0.62 m,n 0.77 0.77 a,b 0.78 0.78 m,o 0.83 0.83 b,c 0.76 0.76 n,o 0.84 0.84 \\fNN 0.74 0.51 a,b,c,m,n,o 0.80 0.80"]},{"title":"3.5 N-gr\\tms test","paragraphs":["We conducted this test to attest that the three components are contributing to the interpo\\bation, as we\\b\\b as a\\toiding the bias the parser might induce. The n-grams test was conducted by se\\becting trigrams of bigrams from the U\\fWaC corpus in a simi\\bar manner than the pre\\tious experiments, howe\\ter in this case we did not use dependency re\\bationships, but s\\biding windows of hexagrams distributed in trigrams in order to mimic the way function words (v\\bg\\f. prepositions or determiners) affect trip\\bets in the dependency mode\\b. The n-grams were extracted for n-grams re\\bated to the same \\terbs described in Section 3. The task consisted, as with the dependency trip\\bes task, to choose one amongst two options of Pair 1. The correct case is the a\\bways first pair, a\\bthough the system does not know about this. We used 80% of the trigrams as a base for prediction (training), and 20% for testing. Tests were conducted for 500 trip\\bes per \\terb to 5000 trip\\bes per \\terb, in the best performance mode\\bs of the pre\\tious experiment (m,n and m,n,o).","T\\tb\\be 3: Resu\\bts of iPLSI for hexagrams grouped as trigrams of bigrams. It shows that it is possib\\be to se\\bect the correct trigram amongst two in 75% of the cases. Size, Mode Prec. Reca\\b\\b Size, Mode Prec. Reca\\b\\b 500 m,n 0.75 0.70 2000 m,n,o 0.77 0.77 500 m,n,o 0.78 0.74 3000 m,n 0.70 0.70 1000 m,n 0.70 0.70 3000 m,n,o 0.75 0.75 1000 m,n,o 0.76 0.76 5000 m,n 0.72 0.72 2000 m,n 0.73 0.72 5000 m,n,o 0.76 0.76","From T\\tb\\be 3 we can see that m,n,o is a\\bways ha\\ting the best performance."]},{"title":"4 Conc\\busions","paragraphs":["We ha\\te confirmed pre\\tious resu\\bts that show that the \\fNN a\\bgorithm outperforms sing\\be- \\tariab\\be PLSI, and we study the \\bearning rate of both a\\bgorithms, showing that \\fNN increases reca\\b\\b when more data is added, without trading much reca\\b\\b; howe\\ter, \\fNN requires strong\\by a pre-fi\\btering phrase which e\\tentua\\b\\by \\beads to an important \\boss of scarce\\by occurring words. 628 These words are important to our purposes, because fi\\btering them out wou\\bd pre\\tent us to genera\\bizing rare words for measuring their p\\bausibi\\bity. The iPLSI (interpo\\bated PLSI) a\\bgorithm proposed here dea\\bs with that issue, yie\\bding better resu\\bts than sing\\be-\\tariab\\be PLSI. We ha\\te found that it is possib\\be to se\\bect the most feasib\\be hexagram out of two with a 75% of reca\\b\\b for raw n-grams grouped as trigrams of bigrams, and up to 83% reca\\b\\b for dependency trigrams. The conducted tests pro\\te that it is possib\\be to se\\bect the correct candidate for a trip\\be, which can be regarded as part of a sentence. This a\\b\\bows ca\\bcu\\bating the most p\\bausib\\be argument in a sentence, using a broader context gi\\ten by a \\terb and other argument. iPLSI has outperformed the pre\\tious \\fNN mode\\b, but sti\\b\\b there is room for impro\\tement. As a future work, we shou\\bd exp\\bore with true three-\\tariab\\be PLSI instead of a two-way interpo\\bation, as we\\b\\b as other \\tariants of iPLSI such as two-staged iPLSI, which wou\\bd consist on re\\bating two \\batent semantic \\tariab\\bes with a \\batent \\tariab\\be in a second stage. Fina\\b\\by, since the test we conducted creates random a\\bternati\\tes, our system might se\\bect more probab\\be candidates than the actua\\b one, such as gi\\ten “cow eats hay in yard”, se\\becting the random\\by created “cow eats grass in yard” wou\\bd count as negati\\te resu\\bts. A\\bthough the effect of this is expected to be \\bow, it shou\\bd be considered on further ana\\byses."]},{"title":"References","paragraphs":["Bo\\bshako\\t, I. A. 2005. An Experiment in Detection and Correction of Ma\\bapropisms through the Web, LNCS 3406, pp. 803-815.","Ca\\b\\to, H., \\f. Inui, Y. Matsumoto. 2009a. Learning Co-Re\\bations of P\\bausib\\be Verb Arguments with a WSM and a Distributiona\\b Thesaurus. P\\focs\\b of the 14th Ibe\\foame\\fican Cong\\fess on Patte\\fn Recognition, CIARP 2009, Springer, Ver\\bag. To appear.","Ca\\b\\to, H., \\f. Inui, Y. Matsumoto. 2009b. Dependency Language Mode\\bing using \\fNN and PLSI. P\\focs\\b of the 8th Mexican Inte\\fnational Confe\\fence on A\\ftificial Intelligence, MICAI 2009, Springer, Ver\\bag, to appear.","Deerwester, S., S. T. Dumais, G. W. Furnas, Thomas \\f. L, and Richard Harshman. 1990. Indexing by \\batent semantic ana\\bysis. Jou\\fnal of the Ame\\fican Society fo\\f Info\\fmation Science, pp. 391–407.","Deschacht, \\f. and M. Moens. 2009. Semi-super\\tised Semantic Ro\\be Labe\\bing using the Latent Words Language Mode\\b. P\\focs\\b 2009 Conf\\b on Empi\\fical Methods in Natu\\fal Language P\\focessing, pp. 21– 29.","Ferraresi, A., E. Zanchetta, M. Baroni and S. Bernardini. 2008. Introducing and e\\ta\\buating ukWaC, a \\tery \\barge web-deri\\ted corpus of Eng\\bish. P\\focs\\b of the WAC4 Wo\\fkshop at LREC. Marrakech, pp. 45–54.","Hoffmann, T. 1999. Probabi\\bistic Latent Semantic Ana\\bysis, Unce\\ftainity in A\\ftificial Intelligence, UAI.","Lee, L., 1999. Measures of Distributiona\\b Simi\\barity, P\\focs\\b 37th ACL.","Lin, D. 1998a. Automatic Retrie\\ta\\b and C\\bustering of Simi\\bar Words. P\\focs\\b 36th","Annual Meeting of the ACL and 17th","Inte\\fnational Confe\\fence on Computational Linguistics.","Lin, D. 1998b. Dependency-based E\\ta\\buation of MINIPAR, P\\foc\\b Wo\\fkshop on the Evaluation of Pa\\fsing Systems.","Mer\\bo, P. and L. Van Der P\\bas. 2009. Abstraction and Genera\\bisation in Semantic Ro\\be Labe\\bs: PropBank, VerbNet or both? P\\focs\\b 47th","Annual Meeting of the ACL and the 4th","IJCNLP of the AFNLP, pp. 288– 296.","Parton, \\f., \\f. R. Mc\\feown, B. C., M. T. Diab, R. Grishman, D. Hakkani-Tür, M. Harper, H. Ji, W. Y. Ma, A. Meyers, S. Sto\\bbach, A. Sun, G. Tur, W. Xu and S. Yaman. 2009. Who, What, When, Where, Why? Comparing Mu\\btip\\be Approaches to the Cross-Lingua\\b 5W Task. 2009. P\\focs\\b 47th","Annual Meeting of the ACL and the 4th","IJCNLP of the AFNLP, pp. 423–431.","Reisinger, J and M. Paşca. 2009. Latent Variab\\be Mode\\bs of Concept-Attribute Attachment. P\\focs\\b 47th"," Annual Meeting of the ACL and the 4th","IJCNLP of the AFNLP, pp. 620–628.","Weeds, J. and D. Weir. 2003. A Genera\\b Framework for Distributiona\\b Simi\\barity, P\\focs\\b conf on EMNLP, Vo\\b. 10:81-88.","Yamada I., \\f. Torisawa, J. \\fazama, \\f. \\furoda, M. Murata, S. de Saeger, F. Bond and A. Sumida. 2009. Hypernym Disco\\tery Based on Distributiona\\b Simi\\barity and Hierarchica\\b Structures. P\\focs\\b 2009 Conf\\b on Empi\\fical Methods in Natu\\fal Language P\\focessing, pp. 929–937. 629"]}]}