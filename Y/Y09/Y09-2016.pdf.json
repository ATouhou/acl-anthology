{"sections":[{"title":"A Frame\\b\\trk f\\tr \\fffectively Integrating Hard and S\\tft Syntactic Rules int\\t Phrase Based Translati\\tn∗∗∗∗ ","paragraphs":["Jiajun \\b\\tang and \\f\\tengqing \\bong  National Laboratory of Pattern Recognition","Institute of Automation, \\f\\tinese Academy of Sciences, Beijing 100190, \\f\\tina","{jjz\\tang, cqzong}@nlpr.ia.ac.cn Abstract. In adding syntactic knowledge into p\\trase-based translation, using \\tard or soft syntactic rules to reorder t\\te source-language aiming to closely approximate t\\te target-language word order \\tas been successful in improving translation quality. However, it suffers from propagating t\\te pre-reordering errors to t\\te later translation step (decoding). In t\\tis paper, we propose a novel framework to integrate \\tard and soft syntactic rules into p\\trase-based translation more effectively. For a source sentence to be translated, \\tard or soft syntactic rules are first acquired from t\\te source parse tree prior to translation, and t\\ten instead of reordering t\\te source sentence directly, t\\te rules are used as a strong feature integrated into our elaborately designed model to \\telp p\\trase reordering in t\\te decoding stage. T\\te experiments on NIST \\f\\tinese-to-Englis\\t translation s\\tow t\\tat our approac\\t, w\\tet\\ter incorporating \\tard or soft rules, significantly outperforms t\\te previous met\\tods. Key\\b\\trds: \\tard syntactic rules, soft syntactic rules, effective integration, p\\trase-based translation"]},{"title":"1 Intr\\tducti\\tn","paragraphs":["Adding syntax into p\\trase-based translation \\tas become a \\tot researc\\t topic. Many works, suc\\t as (\\follins et al., 2005; Wang et al., 2007; \\f\\terry 2008; Marton and Resnik, 2008; and Badr, 2009), \\tave investigated \\tow to use t\\te linguistic information in p\\trase-based SMT and empirically proved t\\tat syntactic knowledge is very \\telpful to improve translation performance especially in p\\trase reordering. For example, in \\f\\tinese-to-Englis\\t translation, t\\te \\f\\tinese p\\trase PP-VP is translated into Englis\\t VP-PP in most cases. T\\tus, if a special rule is designed to deal wit\\t t\\te case of t\\tis kind, t\\te translation result will be better.","T\\te popular way of integrating t\\te linguistic information into p\\trase reordering is to reorder t\\te source sentences wit\\t syntactic reordering rules so as to make t\\te input muc\\t closer to t\\te target language in word order. (\\follins et al., 2005; Wang et al, 2007 and Badr et al., 2009) used hard syntactic rules (namely manually created) obtained from source parse trees to directly reorder t\\te input sentences. (Li et al., 2007) employed s\\tft syntactic rules (namely probabilistic) to get an n-best reordered sentence list for decoding. T\\te former met\\tod depends muc\\t on t\\te aut\\tor’s professional knowledge in linguistics and t\\te performance in parsing tec\\tnology. T\\te latter approac\\t is more robust to t\\te errors in parsing stage but increases t\\te burden of decoding as it \\tas to translate an n-best sentences, and furt\\termore, it mig\\tt still produce pre-reordering errors prior to translation because t\\te n-best list includes only part of but not all of t\\te reordering \\typot\\teses. It s\\tould be noted t\\tat bot\\t t\\te two met\\tods are implemented directly in parse trees, and it is pointed out in previous work (Habas\\t, 2007) t\\tat \\e \\e ∗ We would like to t\\tank Yu \\b\\tou for \\ter suggestions to revise t\\te earlier draft and t\\tank anonymous reviewers","for t\\teir \\telpful comments. T\\te researc\\t work \\tas been partially funded by t\\te Natural Science Foundation of","\\f\\tina under grant No.60736014, 60723005 and 90820303, t\\te National Key Tec\\tnology R&D Program under","grant No. 2006BAH03B02, t\\te Hi-Tec\\t Researc\\t and Development Program (863 Program) of \\f\\tina under","grant No. 2006AA010108-4, and also supported by t\\te \\f\\tina-Singapore Institute of Digital Media as well.  \\fopyrig\\tt 2009 by Jiajun \\b\\tang and \\f\\tengqing \\bong 579 23rd Pacific Asia Conference on Language, Information and Computation, pages 579–588 syntactic reordering does not improve translation if t\\te parse quality is not good enoug\\t. T\\terefore, it becomes a c\\tallenge t\\tat \\tow to use t\\te \\tard and soft syntactic rules properly and adequately even t\\toug\\t t\\te parse quality is not very good (taking \\f\\tinese parsers as an example).","It is natural t\\tat many researc\\ters apply syntactic rules rat\\ter t\\tan distortion or lexical features to improve p\\trase reordering because t\\te syntactic knowledge is more reliable. However, due to t\\te parsing errors and t\\te discrepancy between translation units and syntactic rules, reordering t\\te source sentences prior to translation could cause many errors w\\tic\\t mig\\tt not be made up for in later translation steps. For example, t\\te \\f\\tinese parser would mistakenly parse t\\te \\f\\tinese noun p\\trase “NP以 isr77tli7巴 p7l7tstini7n7和平  p7t7c7t” into a prepositional p\\trase “PP以with7巴 p7l7tstini7n7和平 p7t7c7t”, and t\\te pre-reordering \\tard rules1","will wrongly move t\\tis fake prepositional p\\trase after its rig\\tt sibling verb p\\trase if any, so t\\te translation would be wrong.","Our motivation is based on t\\te above analysis. Instead of using t\\tese syntactic rules to reorder t\\te source sentences arbitrarily, we use t\\tem as a strong feature integrated into our finely designed model to guide p\\trase reordering in decoding stage and meanw\\tile create an extra feature to reward t\\te syntactic reordering during decoding. T\\tus, we not only utilize t\\te good syntactic rules adequately, but also make up for t\\te bad syntactic rules wit\\t ot\\ter important features suc\\t as p\\trase translation probability and target language model. Moreover, it does not increase t\\te time complexity of decoding.","In t\\te model construction, we still employ t\\te log-linear model to combine translation model, target language model and reordering model. T\\te difference lies in two aspects: on t\\te one \\tand, we divide t\\te reordering model into syntactic reordering model and non-syntactic one in order to easily integrate syntactic rules. On t\\te ot\\ter \\tand, we add an extra feature to reward syntactic reordering so as to emp\\tasize t\\te importance of syntactic rules. For a source sentence to be translated, our framework of translation can be illustrated in Figure 1(b). ","parser tree of test source sentence Rule Acquisition Module \\tard or soft rules Pre-reordering Module","1 or n-best reordered test sentences P\\trase-based Decoder target translation","parser tree of test source sentence Rule Acquisiti\\tn M\\tdule \\tard or soft rules","and syntactic flags of eac\\t source p\\trase test source sentence Phrase-based Dec\\tder (syntactic and n\\tn-syntactic re\\trdering distinguished) target translation (a) (b)","","Figure 1: (a) s\\tows t\\te translation flowc\\tart of previous pre-reordering met\\tods. (b) illustrates our","translation framework of incorporating \\tard or soft rules into t\\te decoding stage. We will detail","respectively t\\te two key parts w\\tic\\t are in b\\tldface in Section 3 and Section 4. ","To verify t\\te competitiveness of our approac\\t, we \\tave developed two systems: one uses t\\tis approac\\t to integrate \\tard syntactic rules, and t\\te ot\\ter employs t\\te approac\\t to incorporate soft syntactic rules. T\\te two systems will be compared wit\\t t\\tose using t\\te previous met\\tods.","We introduce t\\te related work in Section 2. Section 3 describes t\\te acquisition and representation of syntactic rules. Section 4 details t\\te integration algorit\\tm of syntactic rules \\e \\e 1 T\\te \\tard rules will be detailed in Section 3.1 580 into t\\te decoding module. In Section 5, we discuss t\\te experiments and analysis. Section 6 concludes t\\te paper."]},{"title":"2 Related W\\trk","paragraphs":["In recent years, it \\tas been widely studied on \\tow to incorporate t\\te syntactic information of source language to improve p\\trase reordering.","\\follins et al. (2005) described six types of transforming rules to reorder t\\te German clauses in German-to-Englis\\t translation. Wang et al. (2007) analyzed t\\te systematic difference between \\f\\tinese and Englis\\t and proposed specific pre-reordering rules for t\\tree categories of \\f\\tinese p\\trase: verb p\\trases, noun p\\trases, and localizer p\\trases. Badr (2009) addressed two syntactic constructs (Subject-Verb structure and noun p\\trase structure) and exploited welldefined pre-reordering rules for Englis\\t-to-Arabic translation. However, all t\\te rules in t\\te above t\\tree met\\tods are \\tard ones (manually built) and sometimes cause many pre-reordering errors. In order to improve t\\te robustness, Li et al. (2007) used t\\te weig\\tted reordered n-best source sentences as input for t\\te decoder. T\\tey utilized t\\te soft rules based on source parse trees in \\f\\tinese-to-Englis\\t translation to determine w\\tet\\ter t\\te c\\tildren of a node s\\tould be reordered or not, and finally to obtain a reordered n-best list. However, all t\\tese met\\tods are separated from decoder and reorder t\\te source sentences arbitrarily prior to translation. Once a pre-reordering error \\tappens, it is very difficult to make up for t\\te mistake in later translation steps. In our approac\\t, we just retain t\\te syntactic rules rat\\ter t\\tan use t\\tem to reorder t\\te source sentences directly. During decoding, t\\te syntactic rules will serve as a strong feature to guide and en\\tance t\\te p\\trase reordering.","\\b\\tang et al., (2007) only allowed reordering between syntactic p\\trases and enforced t\\te non-syntactic p\\trases translated in order. Xiong et al. (2008) proposed a linguistically annotated BTG for SMT. T\\te met\\tod used some \\teuristic rules to linguistically annotate every source p\\trase wit\\t t\\te source-side parse tree in decoding and built a linguistical reordering model. T\\te two approac\\tes bot\\t acquired and applied t\\te syntactic rules in t\\te decoding stage but meanw\\tile increased t\\te decoding time to a large extent. Our work differs from t\\teirs in t\\tree ways. First, w\\ten translating a test sentence, we obtain t\\te corresponding syntactic rules prior to translation rat\\ter t\\tan in decoding stage and t\\tus alleviate t\\te decoding complexity. Second, we distinguis\\t syntactic reordering from non-syntactic reordering because we believe t\\tey play different roles in translation. We t\\tink t\\tis idea is not considered in previous works. T\\tird, we add an extra feature to reward t\\te syntactic reordering."]},{"title":"3 Acquisiti\\tn and Representati\\tn \\tf Syntactic Rules","paragraphs":["In t\\tis paper, we use \\f\\tinese-to-Englis\\t translation as a case study. However, our approac\\t is also suited for ot\\ter language translations only if syntactic rules of t\\te test sentence are provided. W\\tet\\ter incorporate \\tard syntactic rules or soft syntactic rules, obtaining t\\tese rules is our first task."]},{"title":"3.1 Hard Rule Acquisiti\\tn","paragraphs":["T\\te \\tard syntactic rules w\\tic\\t are \\tandcrafts and do not need to be trained s\\tould reflect t\\te true structural difference between t\\te two languages \\f\\tinese and Englis\\t. (Wang et al., 2007) described t\\tree kinds of \\tard rules for \\f\\tinese-to-Englis\\t w\\tic\\t we t\\tink are reasonable. Here, we revisit and conclude t\\tese specific rules. "]},{"title":"Verb Phrases","paragraphs":["If t\\tere is a node in \\f\\tinese parse tree labeled as VP2 , we \\tave t\\tree rules to reorder its c\\tildren. (1) VP(PP◇VP)VP(◇VP PP) 3 and VP(L\\fP◇VP)VP(◇VP L\\fP)","indicate t\\tat PP or L\\fP in a parent VP needs to be repositioned after t\\te sibling VP. (2) \\e \\e 2 All t\\te p\\trase labels we use are borrowed from Penn \\f\\tinese Treebank p\\trase tags 3 T\\te notation ◇ is a place\\tolder w\\tic\\t indicates ot\\ter syntactic nodes between PP and VP 581  VP(NP(NT) ◇VP)VP(◇VP NP(NT)) means a preverbal NP s\\tould be moved after t\\te sibling VP if t\\tere is at least one NT in t\\te NP subtree. (3) VP(QP◇VP)VP(◇VP QP) states QP in a parent VP will be repositioned after t\\te sibling VP. "]},{"title":"N\\tun Phrases","paragraphs":["W\\ten we find a NP node in \\f\\tinese parse tree, four rules are considered. (1) NP(DNP(PP|L\\fP)◇NP)NP(◇NP DNP(PP|L\\fP)) indicates t\\tat DNP is repositioned after t\\te last sibling NP if a parent NP \\tas a c\\tild DNP w\\tic\\t in turn \\tas a c\\tild PP or L\\fP. (2) NP(DNP(!PN) ◇NP)NP(◇NP DNP(!PN)) denotes t\\tat if a parent NP \\tas a c\\tild DNP w\\tic\\t in turn \\tas a c\\tild NP t\\tat is not a PN, t\\ten t\\te DNP s\\tould be moved after t\\te last sibling NP. (3) NP(\\fP◇NP)NP(◇NP \\fP) means t\\te c\\tild \\fP will be repositioned after its sibling NP. (4) \\fP(IP DE\\f)\\fP(DE\\f IP) says t\\tat if \\fP in rule (3) is formed by “IP+DE\\f”, we \\tave to exc\\tange t\\tese two nodes. "]},{"title":"L\\tcalizers","paragraphs":["We \\tave one rule for t\\te node L\\fP: L\\fP(◇L\\f)L\\fP(L\\f◇) denoting t\\te c\\tild L\\f node will be moved before its left sibling under a parent L\\fP node.","Given t\\te parse tree of a test source sentence, we can extract all t\\te \\tard rules belonging to t\\te above ones. Taking a \\f\\tinese sub-sentence and its parse tree below as an example, t\\tere exists a \\tard rule VP(PP◇VP)VP(◇VP PP) in w\\tic\\t PP is 7到27新37的47办公57大楼6 and VP is 是一个挑战. Note t\\tat if we apply pre-reordering met\\tod to reorder t\\te input sentence and move PP after VP, we may get a wrong translation “relocation will be a c\\tallenge to a new office building” because t\\te syntactic tree is parsed wit\\t error. Chinese"]},{"title":":","paragraphs":["迁移 到 新 的 办公 大楼 将 是 一个 挑战 Chinese pinyin: qiānyí dào xīn de bàngōng dàlóu jiāng s\\tì yīgè tiǎoz\\tàn \\fnglish: relocation to a new office building will be a c\\tallenge","","Figure 2: T\\te simplified \\f\\tinese parse tree of t\\te example sentence and t\\te leaves are \\f\\tinese words wit\\t t\\teir index and corresponding Englis\\t translation."]},{"title":"3.2 S\\tft Rule Acquisiti\\tn","paragraphs":["About t\\te soft rules, we use a similar way wit\\t (Li et al., 2007) to extract t\\tem and predict t\\teir probabilities. Li et al., (2007) only concern t\\te nodes wit\\t two or t\\tree c\\tildren and predict a probability for eac\\t permutation of t\\te c\\tildren. We turn to anot\\ter strategy. For t\\te nodes wit\\t two c\\tildren, we just design a rule to determine w\\tet\\ter t\\tey s\\tould be reordered. For t\\te nodes wit\\t more t\\tan two c\\tildren, we first searc\\t t\\te central node (VP or NP), if it exists, we design a rule to decide w\\tet\\ter any preceding modifier node s\\tould be repositioned after t\\te central node. T\\te second rule is based on t\\te p\\tenomenon t\\tat t\\te modifiers before VP or NP in \\f\\tinese usually appear after VP or NP in Englis\\t. T\\te two rules can be formalized as following: P: N","◇N","N","◇N","77777str7ight","◇N","N","77777inv7trt7td7 \\e \\e (1)","W\\tere ◇ is NULL if t\\te parent node P \\tas two c\\tildren (left node N","7and rig\\tt node N","), or is","ot\\ter nodes between modifier node N","and central node N","if P \\tas more t\\tan two c\\tildren.","For t\\te two kinds of soft rules, we adopt a maximum entropy (ME) model to predict t\\teir","probabilities. W\\ten extracting training examples, we use t\\te \\f\\tinese parse tree and t\\te word 582","alignment between \\f\\tinese and Englis\\t as input. If t\\te Englis\\t sides aligned to t\\te two \\f\\tinese","nodes we \\tandled are not crossing, a training example can be extracted. T\\te ric\\t features we","employ for ME training and predicting include leftmost/rig\\ttmost word of N","and N",", t\\te part-","of-speec\\t of t\\tese words, t\\te word immediately before/after t\\te leftmost/rig\\ttmost word of N","/","N","7 plus t\\te combining p\\trase tags of N",", N","and t\\teir parent. Taking t\\te rule used in section","3.1 as an example, namely N","PP7到27新37的47办公57大楼6 and N","VP是一个挑战. T\\te specific features about t\\tis rule are listed in Table 1.","Given t\\te parse tree of a test source sentence, we first extract all t\\te soft rules and t\\ten predict t\\teir probabilities wit\\t t\\te trained ME model. For t\\te pre-reordering met\\tod, t\\tese soft rules are employed to produce an n-best reordered source sentences as t\\te input of t\\te decoder. In our approac\\t, we apply t\\tese rules to guide p\\trase reordering in t\\te decoding stage.  Table 1: T\\te specific features for a rule, “l/r” denotes leftmost/rig\\ttmost, “w” means word, “p” indicates","part-of-speec\\t, and “b/a” means before/after. lw of N"," rw of N"," lp of N"," rp of N"," lw of N"," rw of N"," lp of N"," rp of N"," bw of N"," aw of N"," tag of rule","到 大楼 P NN 是 挑战 VV NN 迁移 NULL PP-VP-VP"]},{"title":"3.3 Rule Representati\\tn","paragraphs":["Let us first \\tave a review of t\\te forms of t\\te \\tard and soft syntactic rules. T\\te \\tard syntactic rules \\tave t\\te form like VP(PP◇VP)VP(◇VP PP),"]},{"title":"\\fP(IP DE\\f)\\fP(DE\\f IP) and L\\fP(","paragraphs":["◇L\\f)L\\fP(L\\f◇). It s\\tould be noted t\\tat ◇ in t\\te last rule cannot be NULL and we","regard it as a special node. T\\terefore, all t\\te \\tard rules are binary relations between two nodes.","It is t\\te same relation in t\\te soft syntactic rules w\\tic\\t use t\\te forms","N","◇N 7N","◇N","7, Psand N","◇N","◇N","N",", Pi w\\tere Ps and Pi denotes","probabilities of straig\\tt and inverted respectively. It is obvious and easy to c\\tange t\\te \\tard rule","into an equivalent probabilistic format. For example, VP(PP◇VP)VP(◇VP PP) is","equivalent to PP◇VP◇VP7PP, 1.0. T\\tus, we can see t\\tat t\\te \\tard rule is a special case of","soft rule, and t\\te only difference lies in t\\tat t\\te \\tard rule only \\tas t\\te inverted format.","For t\\te sake of convenience, \\tereafter, we only consider t\\te generalized rule formats","N","◇N 7N","◇N",", Ps and N","◇N","◇N","N",", Pi. Since Ps Pi 1.0, we can use","only one format to denote t\\tese two ones. It is N",", N",", Pi w\\tic\\t means t\\te left node N","will","be repositioned after t\\te rig\\tt node N","wit\\t t\\te probability Pi. Pi 1.0 if it is a \\tard rule,","ot\\terwise Pi is predicted by ME model. As t\\te unit of p\\trase-based translation is a source","p\\trase but not a parse tree node, we \\tave to make a conversion from tree nodes to source","p\\trases in order to incorporate t\\te syntactic rules. Since eac\\t tree node can be projected to be a","span on t\\te source sentence, we can just use spans to denote t\\te tree nodes. Finally, any","syntactic rule can be represented as a triple7sp7nN",", sp7nN",", Pi."]},{"title":"4 Integrating Syntactic Rules","paragraphs":["We integrate t\\te syntactic rules into a p\\trase-based SMT to \\telp t\\te decoder performs more linguistically. In t\\tis paper, we c\\toose t\\te decoder wit\\t Bracket Transduction Grammar (BTG) style model (Wu, 1997; Xiong et al., 2006) as our baseline."]},{"title":"4.1 BTG-based M\\tdel","paragraphs":["T\\te BTG-based translation can be viewed as a monolingual parsing process, in w\\tic\\t only lexical rules , and two binary merging rules 7",",","and 7",", are allowed. During decoding, t\\te source sentence is first divided into p\\trase sequence, t\\ten t\\te lexical rule , translates t\\te source p\\trase into target p\\trase and forms a block . T\\te 583 ","straig\\tt rule7 7",", and t\\te inverted rule 7",", merge t\\te two neig\\tboring","blocks into a bigger one until t\\te w\\tole source sentence is covered. It is natural to adopt a","\\fKY-style algorit\\tm for t\\tis decoding process. T\\te straig\\tt rule requires t\\te order of two","blocks in source and target language consistent, w\\tile t\\te inverted rule swaps t\\te target parts of","t\\te two blocks. Score of t\\te lexical rule is computed as follows: |","· |","· |","","77777777777777777777777777777777777777777777777777777777· |","·","· ||","·","\\e (2) W\\tere t\\te first two factors are bidirectional p\\trase translation probabilities, | and | denote bidirectional word translation probabilities, and || denote p\\trase number penalty and t\\te target lengt\\t penalty respectively and is t\\te probability of target language model. T\\te are t\\teir corresponding feature weig\\tts. We compute t\\te score of merging rules as: \\e 7","·","\\e \\e (3) In w\\tic\\t is t\\te reordering score and is its weig\\tt. Similar to (Xiong et al., 2006), t\\te reordering score is calculated by t\\te ME model wit\\t only lexical boundary words (leftmost and rig\\ttmost) of p\\trases as features."]},{"title":"4.2 M\\tdel Adaptati\\tn f\\tr Syntactic Rules","paragraphs":["We first give t\\te definition of syntactic p\\trase and non-syntactic p\\trase in t\\tis section. T\\te p\\trase t\\tat exactly covers a sub-tree of source parse tree is defined as a syntactic phrase. T\\te p\\trase covering continuous c\\tild nodes of a tree node is also considered as a syntactic p\\trase. Ot\\ter p\\trases are regarded as n\\tn-syntactic phrases. According to t\\te definition, t\\te syntactic rules are all about reordering between syntactic p\\trases. Our basic idea of integrating t\\te syntactic rule is to use its probability as t\\te p\\trase reordering probability if t\\te merging p\\trases matc\\t t\\te syntactic rule. T\\terefore, t\\te syntactic rules only influence syntactic p\\trase reordering. T\\te features t\\te baseline reordering model use are just lexical boundary words, w\\tile our syntactic rules embedded muc\\t more linguistical features. T\\tus, we believe t\\te syntactic p\\trase reordering plays a more important role t\\tan non-syntactic one and t\\tey s\\tould be distinguis\\ted from eac\\t ot\\ter. T\\te new score of merging rules will be computed as follows:","7","· · ·","·","(4) W\\tere and are syntactic and non-syntactic reordering score respectively.7 and are indicator functions w\\tic\\t indicate t\\tat is used w\\ten is merging two syntactic p\\trases, ot\\terwise is employed.","To emp\\tasize t\\te importance of syntactic p\\trase reordering, we furt\\ter create anot\\ter feature to en\\tance syntactic reordering (because weig\\tts tuning cannot promise t\\te weig\\tt of syntactic reordering model bigger and more importance t\\tan t\\tat of non-syntactic reordering model). T\\te final score of merging rules are calculated as follows:","\\e 7","· · ·","· ·","\\e \\e (5) In w\\tic\\t is a binary feature in order to reward syntactic reordering and it equals to 1 if Ω is active. All t\\te ten feature weig\\tts ~ in our new model are tuned wit\\t MERT (Oc\\t, 2003)."]},{"title":"4.3 Alg\\trithm \\tf Integrating Syntactic Rules","paragraphs":["After knowing t\\te translation model and t\\te decoding algorit\\tm we \\tave used, t\\te most","important t\\ting we care about is \\tow to integrate t\\te syntactic rules during decoding.","T\\te ultimate format of syntactic rule we adopt is designed as sp7nN",", sp7nN",", Pi,","and t\\te merging rules used in decoding always \\tandle two continuous p\\trases, so if sp7nN","","and sp7nN","are successive, t\\ten Pi will be used to replace t\\te syntactic reordering score","w\\tic\\t is predicted wit\\t lexical boundary words in baseline. However, sp7nN","and sp7nN"," 584 will not be consecutive if t\\tere is a non-empty ◇ between t\\te two nodes. A simple strategy is developed to solve t\\tis non-continuous problem. Transf\\trmati\\tn strategy: We take a soft syntactic rule in Figure 1 as an example to illustrate t\\tis detailed strategy. T\\te original rule format is sp7nN",", sp7nN",", Pi in w\\tic\\t N","PP7到27新37的47办公57大楼6","and N","VP是一个挑战, and so t\\te real rule is","2,6, 8,10, Pi and t\\tese two spans are not continuous. Fortunately, it is natural to see t\\te","fact t\\tat if we reorder t\\te rule","2, 6, 8, 10, Pi, t\\te span 2, 10 will be 7, 10 followed by 2, 6 and t\\te result is t\\te same wit\\t t\\te inverted case for spans 2, 6 and 7, 10. T\\terefore, t\\te rule 2, 6, 8, 10, Pi is equivalent to 2, 6, 7, 10, Pi in w\\tic\\t t\\te spans are consecutive. T\\tus, for a discontinuous syntactic rule i, k, h, j, Pi w\\tere i k and h k 1 , we can simply convert it into an equivalent format i, k, k 1, j, Pi. Integrating syntactic rules: During decoding, w\\ten t\\te \\fKY algorit\\tm translate t\\te source spani, j, and at t\\te same time t\\tere is a syntactic rule i, k, h, j, Pi matc\\tes t\\te span, t\\ten we first convert t\\te rule into a continuous one i, k, k 1, j, Pi, and finally Pi is utilized as a more reliable score to replace t\\te syntactic reordering score 7predicted wit\\t only lexical boundary words as features in baseline."]},{"title":"5 \\fxperiments and Analysis 5.1 Baselines Used","paragraphs":["T\\te first baseline is t\\te BTG-based translation system w\\tic\\t uses a lexicalized reordering model trained wit\\t Maximum Entropy and it is re-implemented according to (Xiong et al., 2006). We denote t\\tis baseline as M\\fBTG. We modified t\\te baseline decoder (MBDec\\tder) to incorporate t\\te \\tard syntactic rules or soft syntactic rules as described as Section 4.2 and 4.3.","To s\\tow t\\te competitiveness of our approac\\t, we \\tave to compare our usage of \\tard syntactic rules wit\\t t\\te previous usage in (Wang, et al., 2007), and compare our met\\tod of using soft syntactic rules wit\\t t\\te previous met\\tod in (Li et al., 2007). T\\te classical implementation of t\\te previous usage of syntactic rules is to reorder t\\te source sentences of training, development and test data, t\\ten train t\\te translation model wit\\t reordered source training data, tune t\\te weig\\tts of features wit\\t reordered source development data, and at last use a p\\trase-based system (BTG-based system in t\\tis paper) to get t\\te target translation of t\\te reordered test data. T\\te system using \\tard rules is named M\\fBTG+HRP w\\tic\\t means M\\fBTG system wit\\t Hard Rules Pre-Reordering. Likewise, t\\te system using soft rules is called M\\fBTG+SRP indicating M\\fBTG system wit\\t Soft Rules Pre-reordering (only 1-best reordered source sentence used for source-side of training data and 10-best for test data)."]},{"title":"5.2 C\\trp\\tra and \\fxperimental Settings","paragraphs":["We carried out t\\te experiments on \\f\\tinese-to-Englis\\t translation using NIST05 test set. T\\te development set including 571 \\f\\tinese sentences is c\\tosen from t\\te test set of NIST06 and NIST08. T\\te training set consists of 297K parallel sentences w\\tic\\t are filtered from LD\\f. Word-level alignments were obtained using GI\\bA++ (Oc\\t and Ney, 2000). T\\te target fourgram language model was built wit\\t t\\te Englis\\t part of training data using t\\te SRI Language Modeling Toolkit (Stolcke, 2002). In order to acquire syntactic rules, we parse t\\te \\f\\tinese sentence using t\\te Stanford parser (Klein and Manning, 2003) wit\\t its default \\f\\tinese grammar. We built t\\te maximum entropy model wit\\t a MaxEnt Toolkit developed by (\\b\\tang, 2004). All t\\te models were optimized and tested using t\\te case-sensitive BLEU-4 wit\\t “s\\tortest” reference lengt\\t. Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koe\\tn, 2004)."]},{"title":"5.3 \\fxperimental Results","paragraphs":["Before giving t\\te experimental results, some notations of our new systems are first introduced \\tere. T\\te system INcorporating t\\te Hard Rules into t\\te M\\tdified Baseline Dec\\tder is named 585  IN-HR-MBDec\\tder. Likewise, IN-SR-MBDec\\tder is used to denote t\\te system incorporating t\\te soft rules into modified baseline decoder. In Table 2, we present our results. Like (Wang et al., 2007) and (\\b\\tang et al., 2007), we find t\\tat reordering t\\te source sentences w\\tet\\ter wit\\t \\tard rules or wit\\t soft rules can bot\\t obtain a significant improvement over t\\te baseline MEBTG by 0.58 and 0.60 BLEU respectively. As t\\tese two approac\\tes may cause many pre-reordering errors, t\\te gain is not very promising. However, after using our new approac\\t, t\\te system integrating t\\te \\tard rules into t\\te modified decoder IN-HR-MBDecoder ac\\tieves a larger improvement of up to 1.02 BLEU over MEBTG, and also significantly outperforms t\\te system pre-reordering wit\\t t\\te \\tard rules. Furt\\termore, t\\te system incorporating wit\\t t\\te soft rules IN-SR-MBDecoder performs even better. It outperforms MEBTG and MEBTG+SRP bot\\t significantly by 1.35 and 0.75. T\\te significant improvements of IN-HR-MBDecoder and IN-SR-MBDecoder indicate t\\tat our approac\\t of using syntactic rules as a strong feature to \\telp p\\trase reordering in t\\te decoding stage is more effective t\\tan t\\te previous approac\\t of using t\\tem for pre-reordering.  Table 2: Translation results on development set and test set. * or **: significantly better t\\tan baseline MEBTG (p 0.05 or p 0.01 respectively). +: significantly better t\\tan MEBTG+HRP (p 0.05). ##:","significantly better t\\tan MEBTG+SRP (p 0.01). System Dev Test MEBTG 0.2567 0.3296 MEBTG+HRP 0.2635 0.3354* MEBTG+SRP 0.2652 0.3356* IN-HR-MBDecoder 0.2671 0.3398**+ IN-SR-MBDecoder 0.2713 0.3431**##  Table 3: T\\te effect of new features. “SynNon” means syntactic and non-syntactic reordering model; “SR” denotes soft rules integrated. * or **: significantly better t\\tan baseline MEBTG ( 0.05 or","0.01 respectively). @@: significantly better t\\tan “SynNon” ( 0.01. Features BL\\fU-4 SynNon 0.3347* SynNon+SR 0.3416**@@ SynNon+SR+Reward 0.3431**@@"]},{"title":"5.4 Analysis","paragraphs":["In t\\tis section, we \\tave a detailed analysis about t\\te translation results. Why M\\fBTG+HRP and M\\fBTG+SRP perf\\trm similar? It is interesting t\\tat pre-reordering wit\\t t\\te \\tard rules \\tas a similar performance wit\\t pre-reordering using t\\te soft rules. We find t\\tat because of many \\f\\tinese parsing errors, t\\te accuracy of t\\te \\tard rules is not \\tig\\t, only 62.1% reported in (Wang et al., 2007). So, it causes many pre-reordering errors. Alt\\toug\\t t\\te system pre-reordering wit\\t soft rules does not produce as many errors as MEBTG+HRP does, it may miss some correct reordering instances. T\\tus, t\\te two systems would \\tave similar translation quality. Two translation instances are illustrated in Figure 3 and Figure 4 to s\\tow t\\te situations w\\tic\\t \\tard rules and soft rules may run into. Why IN-SR-MBDec\\tder \\tutperf\\trms IN-HR-MBDec\\tder? \\fompared wit\\t t\\te systems using syntactic rules for pre-reordering, w\\ty our usage of syntactic rules for \\tard and soft rules could yield a bigger gap (0.33 vs. 0.02)? We know t\\tat t\\te two systems are almost t\\te same except t\\tat t\\tey incorporate different syntactic rules, soft rules versus \\tard ones. Instead of using t\\tem to directly reorder t\\te source sentence, we use t\\tem to \\telp p\\trase reordering in t\\te decoding stage wit\\t t\\te same algorit\\tm. T\\terefore, we believe t\\te difference mig\\tt lie in t\\te number of rules t\\tey \\tave employed. We find t\\tat only average 4.18 586 \\tard rules are acquired from eac\\t test sentence, w\\tile 17.08 soft rules in average are obtained. During decoding, t\\te more syntactic information, t\\te better p\\trase reordering. The effect \\tf ne\\b features? As described in Section 4.2 and 4.3, our system \\tas t\\tree new features: (1) syntactic p\\trase reordering model and non-syntactic one are employed to replace t\\te baseline reordering model; (2) a binary rewarding feature is used to en\\tance t\\te syntactic reordering and (3) syntactic rules are incorporated into p\\trase reordering in decoding step. T\\tus, it is interesting to investigate t\\te effect of eac\\t new feature. IN-SR-MBDecoder is employed to conduct t\\tis experiment. Table 3 gives t\\te results. We can see t\\tat only distinguis\\ting syntactic p\\trase reordering from non-syntactic one could obtain a significant improvement over t\\te baseline MEBTG. It \\tas verified our conjecture t\\tat syntactic reordering and non-syntactic reordering play different roles and s\\tould not be considered t\\te same. On t\\tis basis, we integrate t\\te soft rules and t\\te result is promising wit\\t 0.69 BLEU improvement. It indicates t\\tat t\\te syntactic rules can \\telp p\\trase reordering in decoding to a large extent. Finally, we add an extra rewarding feature to encourage syntactic p\\trase reordering. T\\te result s\\tows t\\tat t\\tis feature can also improve t\\te translation quality. However, our contribution is t\\te combination of t\\te t\\tree features as a framework to integrate syntactic rules and t\\te results \\tave s\\town t\\te effectiveness. Syntactic rules better than lexical \\tnes? T\\te key idea in our paper is employing syntactic rules to replace lexical ones if matc\\t. We may argue t\\tat w\\tet\\ter t\\te syntactic rules are indeed more reliable t\\tan lexical ones. T\\te experimental results \\tave proved t\\tat"]},{"title":"empirically","paragraphs":[". And according to our analysis, we find t\\tat t\\te syntactic rules are obviously better t\\tan lexical ones if t\\te parse tree is correct. For example, t\\te probability in soft rule , , in Figure 4 is 0.9796 recommending strong reordering (correct case) w\\tile t\\te lexical one predicted wit\\t boundary words of p\\trases is 0.6687. And we also find t\\tat w\\ten t\\te tree is parsed wit\\t error, most syntactic rules in low quality still can be made up for during decoding wit\\t translation model and language model. For example, t\\te probability of soft rule , , in Figure 3 is 0.6826 w\\tic\\t is slig\\ttly bigger t\\tan 0.6094 of lexical one and so t\\te syntactic rule \\tas a slig\\ttly bigger trend to wrong reordering; and \\towever t\\tis incorrect rule is made up for in our approac\\t and a similar translation to t\\tat of MEBTG (using lexical rules) is obtained as Figure 3 s\\tows. Based on t\\te above analysis, we can conclude t\\tat t\\te syntactic rules are better t\\tan lexical ones on t\\te w\\tole.","","Figure 3: An example t\\tat t\\te \\tard rule is wrong because t\\te NP and PP are parsed wit\\t error, and t\\te","reordering system M\\fBTG+HRP leads to a wrong translation w\\tic\\t is even worse t\\tan t\\te baseline M\\fBTG, but our approac\\t gets a correct translation.","","","Figure 4: An example t\\tat t\\te soft rules miss t\\te reordering instance t\\tat t\\te CP s\\tould moved after its","sibling NP, and t\\te reordering system M\\fBTG+SRP causes a wrong translation just as t\\te baseline","M\\fBTG does; \\towever our approac\\t obtains t\\te rig\\tt one. Src: (首家7获准7在7中国7经营7人民币7业务7的7比利时7银行7 Ref: t\\te first Belgian bank aut\\torized to operate renminbi business in \\f\\tina M\\fBTG: t\\te first aut\\torized to operate rmb business Belgian bank in \\f\\tina M\\fBTG+SR: t\\te first aut\\torized to operate rmb Belgian bank in \\f\\tina SR-IN-MDec\\tder: t\\te first Belgian bank aut\\torized to operate rmb business in \\f\\tina Src: (7迁移7到7新7的7办公7大楼7将7是7一个7挑战7 Ref: relocation to a new office building will be a c\\tallenge M\\fBTG: relocation to new office building will be a c\\tallenge M\\fBTG+HR: migration will be a c\\tallenge to t\\te new office building HR-IN-MDec\\tder: relocation to a new office building will be a c\\tallenge 587 "]},{"title":"6 C\\tnclusi\\tn","paragraphs":["In t\\tis paper, we \\tave presented a framework for effectively incorporating syntactic rules into t\\te p\\trase-based SMT. For a test sentence to be translated, we first acquire t\\te syntactic reordering rules from t\\te \\f\\tinese parse trees. Instead of using t\\tem to reorder t\\te source sentences, we incorporate t\\tese rules to guide p\\trase reordering in t\\te decoding stage. To en\\tance t\\te syntactic p\\trase reordering, we distinguis\\ted t\\te syntactic p\\trase reordering from non-syntactic one and created an extra binary feature to reward t\\te syntactic reordering. T\\te experiments s\\tow t\\tat our approac\\t of using syntactic rules significantly outperforms t\\te previous approac\\t w\\tet\\ter for \\tard rules or for soft rules. Furt\\termore, we \\tave found t\\tat just distinguis\\ting syntactic reordering from non-syntactic one could improve t\\te translation quality muc\\t and meanw\\tile facilitate t\\te integration of syntactic rules."]},{"title":"References","paragraphs":["Badr, I. 2009. Syntactic P\\trase Reordering for Englis\\t-to-Arabic Statistical Mac\\tine","Translation. In Procee\\b\\tngs o\\f EACL 2009. \\f\\terry, \\f. 2008. \\fo\\tesive P\\trase-based Decoding for Statistical Mac\\tine Translation. In Procee\\b\\tngs o\\f ACL-HLT 2008.","\\follins, M., P. Koe\\tn, and I. Kucerova. 2005. \\flause Restructuring for Statistical Mac\\tine Translation. In Procee\\b\\tngs o\\f ACL 2005.","Habas\\t, N. 2007. Syntactic Preprocessing for Statistical Mac\\tine Translation. In Procee\\b\\tngs o\\f Mach\\tne Translat\\ton Subm\\tt 2007. Klein D. and \\f.D. Manning. 2003. Accurate Unlexicalized Parsing. In Procee\\b\\tngs o\\f ACL. Koe\\tn, P. 2004. Statistical Significance Tests for Mac\\tine Translation Evaluation. In","Procee\\b\\tngs o\\f EMNLP 2004. Li, \\f., D.D. \\b\\tang, M. Li, M. \\b\\tou, M.H. Li and Y. Guan. 2007. A probabilistic Approac\\t to Syntax-based Reordering for Statistical Mac\\tine Translation. In Procee\\b\\tngs o\\f ACL 2007.","Marton Y. and P. Resnik. 2008. Soft Syntactic \\fonstrains for Hierarc\\tical P\\trase-Based Translation. In Procee\\b\\tngs o\\f ACL-HLT 2008. Oc\\t, F.J. and H. Ney. 2000. Improved Statistical Alignment Models. In Procee\\b\\tngs o\\f ACL.","Oc\\t, F.J. 2003. Minimum Error Rate Training in Statistical Mac\\tine Translation. In Procee\\b\\tngs o\\f ACL 2003.","Stolcke, A. 2002. SRILM- An Extensible Language Modeling Toolkit. In Procee\\b\\tngs o\\f the Internat\\tonal Con\\ference on Spoken Language Un\\berstan\\b\\tng, 2002. Wang, \\f., M. \\follins and P. Koe\\tn. 2007. \\f\\tinese Syntactic Reordering for Statistical Mac\\tine","Translation. In Procee\\b\\tngs o\\f EMNLP-CoNLL 2007. Wu, D.K. 1997. Stoc\\tastic Inversion Transduction Grammars and Bilingual Parsing of Parallel \\forpora. Computat\\tonal L\\tngu\\tst\\tcs, 23(3):377-403.","Xiong, D.Y., Q. Liu and S.X. Lin. 2006. Maximum Entropy based P\\trase Reordering Model for Statistical Mac\\tine Translation. In Procee\\b\\tngs o\\f ACL-COLING 2006.","Xiong, D.Y., M. \\b\\tang, A. Aw and H.\\b. Li. 2008. Linguistically Annotated BTG for Statistical Mac\\tine Translation. In Procee\\b\\tngs o\\f COLING 2008. \\b\\tang, D.D., M. Li, \\f.H. Li and M. \\b\\tou. 2007. P\\trase Reordering Model Integrating Syntactic","Knowledge for SMT. In Procee\\b\\tngs o\\f EMNLP-CoNLL 2007. \\b\\tang, L. 2004. Maximum Entropy Modeling Toolkit for Pyt\\ton and \\f++. http://homepages.\\tn\\f.e\\b.ac.uk/s0450736/maxent_toolk\\tt. 588"]}]}