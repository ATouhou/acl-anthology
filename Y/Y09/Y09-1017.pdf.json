{"sections":[{"title":"Shallow S\\b\\tant\\fc Pars\\fng of P\\brs\\fan S\\bnt\\bnc\\bs*  ","paragraphs":["Azadeh \\ba\\tel Ghal\\fbafa , Saeed Rahat\\fa",", and Aza\\t Estaj\\fb  a Depart\\tent of Art\\ff\\fc\\fal Intell\\fgence, Azad Un\\fvers\\fty of Mashhad, Ostad Yousef\\f - Ghase\\t Abad - Mashhad - Iran (0098511-6627512)","Azadeh_\\ba\\tel@hot\\ta\\fl.co\\t","b Depart\\tent of L\\fngu\\fst\\fcs, Ferdows\\f Un\\fvers\\fty of Mashhad, Azad\\f Square- Mashhad-Iran- PostCode: 9177948974","Estaj\\f@u\\t.ac.\\fr Abstract. Extract\\fng se\\tant\\fc roles \\fs one of the \\tajor steps \\fn represent\\fng text \\tean\\fng. It refers to f\\fnd\\fng the se\\tant\\fc relat\\fons between a pred\\fcate and syntact\\fc const\\ftuents \\fn a sentence. In th\\fs paper we present a se\\tant\\fc role label\\fng syste\\t for Pers\\fan, us\\fng \\te\\tory-based learn\\fng \\todel and standard features. We show that good se\\tant\\fc pars\\fng results can be ach\\feved w\\fth a s\\tall 1300-sentence tra\\fn\\fng set. In order to extract features, we developed a shallow syntact\\fc parser wh\\fch d\\fv\\fdes the sentence \\fnto seg\\tents w\\fth certa\\fn syntact\\fc un\\fts. The \\fnput data for both syste\\ts \\fs drawn fro\\t Ha\\tshahr\\f corpus wh\\fch \\fs hand-labeled w\\fth requ\\fred syntact\\fc and se\\tant\\fc \\fnfor\\tat\\fon. The results show an F-score of 90.3% on argu\\tent boundary detect\\fon task and an F-score of 87.4% on se\\tant\\fc role label\\fng task us\\fng Gold-standard parses. An overall syste\\t perfor\\tance shows an F-score of 83.8% on co\\tplete se\\tant\\fc role label\\fng syste\\t \\f.e. boundary plus class\\ff\\fcat\\fon. K\\bywords: Se\\tant\\fc Role Label\\fng, Shallow Se\\tant\\fc Pars\\fng, Shallow Syntact\\fc Pars\\fng, Me\\tory-Based Learn\\fng. \\b \\b  Copyr\\fght 2009 by Azadeh \\ba\\tel Ghal\\fbaf, Saeed Rahat\\f, and Aza\\t Estaj\\f"]},{"title":"1 Introduct\\fon","paragraphs":["Se\\tant\\fc role label\\fng (SRL), also called shallow se\\tant\\fc pars\\fng, \\fnvolves \\fdent\\ffy\\fng wh\\fch groups of words (phrases) act as the argu\\tents to a g\\fven pred\\fcate. These argu\\tents \\tust be labeled w\\fth the role they play \\fn relat\\fon to the pred\\fcate (verb), \\fnd\\fcat\\fng how the propos\\ft\\fon should be se\\tant\\fcally \\fnterpreted (Hac\\foglu, 2004).","A nu\\tber of algor\\fth\\ts have been proposed for auto\\tat\\fcally ass\\fgn\\fng such shallow se\\tant\\fc structure to Engl\\fsh sentences. But l\\fttle \\fs understood about how these algor\\fth\\ts \\tay perfor\\t \\fn other languages, and \\fn general the role of language-spec\\ff\\fc \\fd\\fosyncras\\fes \\fn the extract\\fon of se\\tant\\fc content, and how to tra\\fn these algor\\fth\\ts when large hand-labeled tra\\fn\\fng sets are not ava\\flable (Sun and Jurafsky, 2004).","So, to des\\fgn an opt\\f\\tal \\todel for a Pers\\fan SRL syste\\t we should take \\fnto account spec\\ff\\fc l\\fngu\\fst\\fc aspects of the language. Regard\\fng the re\\tarkable a\\tount of research that has already been done \\fn Engl\\fsh, we can cap\\ftal\\fze fro\\t \\ft to des\\fgn a bas\\fc and effect\\fve SRL syste\\t. The \\fdea \\fs to use the technology for Engl\\fsh and ver\\ffy \\ff \\ft \\fs su\\ftable for Pers\\fan.","Our proposed SRL syste\\t \\f\\tple\\tents a two-phase arch\\ftecture to f\\frst \\fdent\\ffy the argu\\tents by a shallow syntact\\fc parser or chunker, and then to label the\\t w\\fth appropr\\fate se\\tant\\fc role, w\\fth respect to the pred\\fcate of the sentence. We treat both phases as a \\tult\\f-class class\\ff\\fcat\\fon proble\\t, where the class\\ff\\fer \\fs tra\\fned \\fn a superv\\fsed \\tanner, fro\\t 150 23rd Pacific Asia Conference on Language, Information and Computation, pages 150–159  hu\\tan-annotated data, us\\fng \\te\\tory-based learn\\fng. To our knowledge \\ft \\fs the f\\frst corpus based SRL syste\\t for Pers\\fan.","Me\\tory-based language process\\fng \\fs based on the \\fdea that NLP proble\\ts can be solved by stor\\fng solved exa\\tples of the proble\\t \\fn the\\fr l\\fteral for\\t \\fn \\te\\tory, and apply\\fng s\\f\\t\\flar\\fty-based reason\\fng on these exa\\tples \\fn order to solve new ones. \\beep\\fng l\\fteral for\\ts \\fn \\te\\tory has been argued to prov\\fde a key advantage over abstract\\fng \\tethods \\fn NLP that \\fgnore except\\fons and subregular\\ft\\fes (Morante and Busser, 2007).","MBL works best when the features have been carefully selected and we\\fghted (Ha\\t\\terton et al., 2002). We have used so\\te syntact\\fc propert\\fes of argu\\tents for the feature set. But s\\fnce no auto\\tat\\fc parser ex\\fsts to syntact\\fcally parse Pers\\fan sentences, we dec\\fded to develop a syste\\t for shallow pars\\fng of Pers\\fan sentences \\fn the f\\frst phase of the syste\\t.","Shallow pars\\fng (also called part\\fal pars\\fng) \\tost often refers to the task of chunk\\fng and has beco\\te an \\fnterest\\fng alternat\\fve to full pars\\fng. It \\fs a natural language process\\fng techn\\fque that atte\\tpts to deter\\t\\fne the const\\ftuents’ boundar\\fes \\fn the sentence, but w\\fthout pars\\fng \\ft fully \\fnto a parsed tree for\\t (Marquez et al., 2008). Shallow pars\\fng \\fs eas\\fly tra\\fnable, fast, robust and \\tuch less a\\tb\\fguous. Such propert\\fes \\take \\ft a good cho\\fce over full pars\\fng (Ha\\t\\terton et al., 2002).","The rest of th\\fs paper \\fs organ\\fzed as follow: We f\\frst descr\\fbe our creat\\fon of a s\\tall 2000-sentence Pers\\fan corpus labeled w\\fth 12 selected the\\tat\\fc roles \\fn sect\\fon 2. Sect\\fon 3 \\fntroduces the general arch\\ftecture of our \\todel and descr\\fbes \\fts co\\tponents \\fn deta\\fls. The exper\\f\\tental results are shown \\fn sect\\fon 4. F\\fnally, conclus\\fon of th\\fs study \\fs presented \\fn sect\\fon 5.","In all exa\\tples throughout th\\fs paper, we w\\fll show Pers\\fan sentences by the\\fr transl\\fterat\\fon \\fn \\ftal\\fc between quotes followed by the\\fr translat\\fon to Engl\\fsh between parentheses."]},{"title":"2 S\\b\\tant\\fc Annotat\\fon and th\\b Corpus","paragraphs":["The creat\\fon of se\\tant\\fcally annotated corpora for Pers\\fan has lagged beh\\fnd. Here we select so\\te parts of the 2.5M word Ha\\tshahr\\f corpus (Orou\\tch\\fan, 2006) (wh\\fch has been prev\\fously ass\\fgned POS tags) and \\tanually label \\ft w\\fth the syntact\\fc and se\\tant\\fc \\fnfor\\tat\\fon needed for the syste\\t. The s\\tall created corpus conta\\fns sentences w\\fth var\\fed structures and do\\ta\\fns such as pol\\ft\\fc, soc\\fal, sc\\fence, sport, h\\fstory. In th\\fs sect\\fon, we f\\frst descr\\fbe the se\\tant\\fc roles we used \\fn the annotat\\fon and then \\fntroduce the data for our exper\\f\\tents."]},{"title":"2.1 S\\b\\tant\\fc rol\\bs","paragraphs":["Se\\tant\\fc roles, also called the\\tat\\fc roles or θ-roles, are character\\fzat\\fons of certa\\fn se\\tant\\fc relat\\fonsh\\fps wh\\fch hold between a verb and \\fts co\\tple\\tents (and adjuncts). For exa\\tple \\fn the follow\\fng sentence: ‘\\be\\tare Al\\f \\fn khane ra az tajer\\f khar\\f\\t.’ (Al\\f's father bought the house fro\\t a bus\\fness\\tan.) ‘\\be\\tare Al\\f’ (Al\\f's father) \\fs the Agent, ‘\\fn khane ra’ (the house) \\fs the Pat\\fent, ‘az tajer’ (fro\\t a bus\\fness\\tan) \\fs the Source of the buy\\fng event denoted by the sentence.","Se\\tant\\fc roles are one of the oldest \\fssues \\fn l\\fngu\\fst\\fc theory that were f\\frst \\tent\\foned by Jeffrey Gruber (Wagner, 2004). There \\fs no standard set of se\\tant\\fc roles, nor about the\\fr nature or the\\fr status \\fn l\\fngu\\fst\\fc theory. The set of roles proposed by l\\fngu\\fsts range fro\\t very spec\\ff\\fc to very general (L\\f\\t et al., 2004). At the spec\\ff\\fc end of th\\fs spectru\\t are do\\ta\\fnspec\\ff\\fc roles appl\\fed \\fn so\\te \\fnfor\\tat\\fon extract\\fon syste\\ts such as the Fro\\t-C\\fty, To-C\\fty, or Rece\\fve-T\\f\\te roles, wh\\fch can be appl\\fed \\fn reservat\\fon syste\\ts, or verb-spec\\ff\\fc roles such as Buyer, Goods and Seller for the verb buy. The other end of the spectru\\t cons\\fsts of theor\\fes w\\fth only two “proto-roles”: Proto-Agent and Proto-Pat\\fent (Dowty, 1991). In between there are \\tany theor\\fes wh\\fch propose the l\\f\\t\\fted nu\\tber of roles (approx\\f\\tately ten roles), such as 151 F\\fll\\tore (1971)’s l\\fst of n\\fne: Agent, Exper\\fencer, Instru\\tent, Obejct, Source, Goal, Locat\\fon, T\\f\\te and Path.","For the task of th\\fs paper, we \\fn\\ft\\fally e\\tployed the role set proposed by F\\fll\\tore and then a nu\\tber of roles are added to prov\\fde \\tore abstract se\\tant\\fc character\\fzat\\fon. Our proposed role set cons\\fsts of 12 roles (Table 1) wh\\fch \\fs d\\fv\\fded \\fnto two classes: pr\\f\\tary and general roles.  Tabl\\b 1: Se\\tant\\fc role set. Se\\tant\\fc Role Descr\\fpt\\fon Agent Doer or causer of an event Pat\\fent Affected or effected by an event Source Object fro\\t wh\\fch \\tot\\fon proceeds Goal Object to wh\\fch \\tot\\fon proceeds Top\\fc The propos\\ft\\fon or content of a propos\\ft\\fonal event Percept What \\fs real\\fzed \\fn cogn\\ft\\fve verbs Instru\\tent Med\\fu\\t or \\tater\\fal by wh\\fch the act\\fon \\fs carr\\fed out. Benef\\fc\\fary The benef\\fc\\fary of an event T\\f\\te Te\\tporal place\\tent of an event Locat\\fon Place where the event occurs Manner How the act\\fon, exper\\fence, or process of an event \\fs","carr\\fed out Reason Cause of an occurr\\fng an event  (1) The pr\\f\\tary roles are the roles wh\\fch are pred\\fcate-spec\\ff\\fc such as Agent, Pat\\fent, Source, Goal, Top\\fc, Percept, Instru\\tent and Benef\\fc\\fary. For d\\ffferent pred\\fcates so\\te subset of these roles \\tay be ava\\flable. (2) The general roles are those wh\\fch are assu\\ted to apply across all verbs, they are opt\\fonal for an event but supply \\tore \\fnfor\\tat\\fon about an event \\fnclud\\fng Locat\\fon, T\\f\\te, Manner and Reason. For exa\\tple \\fn the sentence: ‘Al\\f enshaayash ra ba se\\taye bolan\\t \\tar kelas khan\\t.’ (Al\\f read h\\fs co\\tpos\\ft\\fon loudly \\fn the class.) we have the follow\\fng pr\\f\\tary and general roles:  Phrase Role Role-Class 'Al\\f' (Al\\f) Agent Pr\\f\\tary 'enshayash ra' (h\\fs co\\tpos\\ft\\fon) Pat\\fent Pr\\f\\tary 'ba sedaye boland' (loudly) Manner General 'dar kelass' (\\fn the class) Locat\\fon General 'khand' (read) Pred\\fcate "]},{"title":"2.2 Th\\b tra\\fn\\fng and t\\bst s\\bts","paragraphs":["We created our tra\\fn\\fng and test corpora by choos\\fng 50 s\\f\\tple verbs1",", and then select\\fng all sentences conta\\fn\\fng these 50 verbs fro\\t the 2.5M-word Ha\\tshahr\\f corpus. We chose the 50 verbs by cons\\fder\\fng frequency, syntact\\fc d\\fvers\\fty, and word sense. We chose verbs that were frequent enough to prov\\fde suff\\fc\\fent tra\\fn\\fng data. The frequenc\\fes of the 50 verbs range fro\\t 10 to 60, w\\fth an average of 35. \\b \\b 1 A s\\f\\tple verb \\fs one whose \\fnf\\fn\\ft\\fve cons\\fst of one word 152 ","We chose verbs that were representat\\fve of the var\\fety of verbal syntact\\fc behav\\for \\fn Pers\\fan, \\fnclud\\fng verbs w\\fth one, two, and three argu\\tents, and verbs w\\fth var\\fous patterns of argu\\tent l\\fnk\\fng. F\\fnally, we chose verbs that var\\fed \\fn the\\fr nu\\tber of word senses. In total, we selected 2000 sentences. The th\\frd author then labeled each verbal argu\\tent/adjunct \\fn each sentence w\\fth a role label. We created our tra\\fn\\fng and test sets by spl\\ftt\\fng the data for each verb \\fnto two parts: 70% for tra\\fn\\fng and 30% for test. Thus there are 1300 sentences \\fn the tra\\fn\\fng set and 700 sentences \\fn the test set, and each test set verb has been seen \\fn the tra\\fn\\fng set. The l\\fst of verbs chosen along w\\fth the\\fr se\\tant\\fc class w\\fll be d\\fscussed \\fn sect\\fon 3.3.","It \\fs worth po\\fnt\\fng out that the syste\\t can be general\\fzed to perfor\\t on all verbs of the language by annotat\\fng a larger corpus w\\fth se\\tant\\fc \\fnfor\\tat\\fon. In the next sect\\fon we w\\fll descr\\fbe our proposed SRL approach."]},{"title":"3 Syst\\b\\t d\\bscr\\fpt\\fon","paragraphs":["F\\fgure 1 shows the overall arch\\ftecture of our \\todel.   F\\fgur\\b 1: Overall arch\\ftecture."," As \\ft can be seen fro\\t the f\\fgure, the task of auto\\tat\\fc se\\tant\\fc role ass\\fgn\\tent \\fs d\\fv\\fded \\fnto two \\ta\\fn subtasks: (1) Ident\\ff\\fcat\\fon of the target argu\\tent boundar\\fes and (2) label\\fng the argu\\tents w\\fth appropr\\fate se\\tant\\fc roles.","The f\\frst part (subtask) can be acco\\tpl\\fshed by develop\\fng a shallow syntact\\fc parser. As Pers\\fan \\fs al\\tost a free word order language and th\\fs property results \\fn h\\fgh structural a\\tb\\fgu\\fty, apply\\fng a shallow pars\\fng \\tethod can \\take s\\fgn\\ff\\fcant \\f\\tprove\\tents \\fn argu\\tent \\fdent\\ff\\fcat\\fon.","The second part (subtask) uses a \\tach\\fne learn\\fng \\tethod to d\\fst\\fngu\\fsh d\\ffferent roles such as Agent, Goal, etc and also a repos\\ftory of var\\fous Pers\\fan verbs and the\\fr features. Th\\fs part faces a co\\tpl\\fcated proble\\t s\\fnce the nu\\tber of argu\\tents and the\\fr pos\\ft\\fons vary depend\\fng on a verb’s vo\\fce (act\\fve/pass\\fve) and sense, along w\\fth \\tany other factors. Regard\\fng the class\\ff\\fer we have used Me\\tory-Based Learn\\fng (MBL) for both syste\\ts.","In the rest of th\\fs sect\\fon we f\\frst prov\\fde \\tore techn\\fcal deta\\fls of Me\\tory-based learn\\fng and then descr\\fbe the \\f\\tple\\tentat\\fon of both syste\\ts \\fn \\tore deta\\fled. "]},{"title":"3.1 M\\b\\tory-Bas\\bd L\\barn\\fng","paragraphs":["The bas\\fc \\fdea beh\\fnd \\te\\tory-based learn\\fng \\fs that concepts can be class\\ff\\fed by the\\fr s\\f\\t\\flar\\fty w\\fth prev\\fously seen concepts (Stevens, 2006). For the task of th\\fs paper we have used T\\fMBL (T\\flburg Me\\tory-Based Learner), a software tool wh\\fch conta\\fns several algor\\fth\\ts w\\fth d\\ffferent para\\teters. We descr\\fbe these algor\\fth\\ts br\\fefly \\fn cont\\fnue.","An MBL syste\\t, conta\\fns two co\\tponents: a learn\\fng co\\tponent wh\\fch \\fs \\te\\tory-based (fro\\t wh\\fch MBL borrows \\fts na\\te), and a perfor\\tance co\\tponent wh\\fch \\fs s\\f\\t\\flar\\fty-based. 153","The learn\\fng co\\tponent of MBL \\fs \\te\\tory-based as \\ft \\fnvolves add\\fng tra\\fn\\fng \\fnstances to \\te\\tory. An \\fnstance cons\\fsts of a f\\fxed-length vector of n feature-value pa\\frs, and an \\fnfor\\tat\\fon f\\feld conta\\fn\\fng the class\\ff\\fcat\\fon of that part\\fcular feature-value vector (Daele\\tans et al., 2006).","IB1 \\fs a k-nearest ne\\fghbor algor\\fth\\t, wh\\fch \\fs the default learn\\fng \\tethod \\fn T\\fMBL. The second algor\\fth\\t, IGTREE, stores exa\\tples \\fn a tree wh\\fch \\fs pruned accord\\fng to the we\\fght\\fngs. Th\\fs \\takes \\ft \\tuch faster and of co\\tparable accuracy.","In the perfor\\tance co\\tponent of an MBL syste\\t, the product of the learn\\fng co\\tponent \\fs used as a bas\\fs for \\tapp\\fng \\fnput to output; th\\fs usually takes the for\\t of perfor\\t\\fng class\\ff\\fcat\\fon (Daele\\tans et al., 2006).","Dur\\fng class\\ff\\fcat\\fon, a prev\\fously unseen test exa\\tple \\fs presented to the syste\\t. The s\\f\\t\\flar\\fty between the new \\fnstance X and all exa\\tples Y \\fn \\te\\tory \\fs co\\tputed us\\fng so\\te d\\fstance \\tetr\\fc"]},{"title":"( )","paragraphs":["YX ,∆ . The extrapolat\\fon \\fs done by ass\\fgn\\fng the \\tost frequent category w\\fth\\fn the found set of \\tost s\\f\\t\\flar exa\\tple(s) (the k-nearest ne\\fghbors) as the category of the new test exa\\tple. In case of a t\\fe a\\tong categor\\fes, a t\\fe break\\fng resolut\\fon \\tethod \\fs used (Daele\\tans et al., 2006).","The \\tost bas\\fc \\tetr\\fc that works for patterns w\\fth sy\\tbol\\fc features \\fs the Overlap \\tetr\\fc g\\fven \\fn (1) and (2); where"]},{"title":"( )","paragraphs":["YX ,∆ \\fs the d\\fstance between \\fnstances X and Y, represented by n features, and δ \\fs the d\\fstance per feature. The d\\fstance between two patterns \\fs s\\f\\tply the su\\t of the d\\ffferences between the features. The k-NN algor\\fth\\t w\\fth th\\fs \\tetr\\fc \\fs called IB1."]},{"title":"( ) ( )","paragraphs":["∑ = = n","1\\f \\fy,\\fxY,X δ∆ \\b \\b (1) Where:"]},{"title":"( )","paragraphs":[" \\b  \\t  ≠ = \\f\\f      −− = \\fy\\fx \\ff 1 \\fy\\fx \\ff 0 nu\\ter\\fc \\ff \\f\\t\\fn\\f\\tax yx abs \\fy,\\fx \\f\\f δ (2)  ","T\\fMBL, also, auto\\tat\\fcally learns we\\fghts for the features, us\\fng one of f\\fve d\\ffferent we\\fght\\fng \\tethods: no we\\fght\\fng, ga\\fn rat\\fo, \\fnfor\\tat\\fon ga\\fn, ch\\f-squared and shared var\\fance. Daele\\tans et al. (1999) have shown that for typ\\fcal natural language tasks, th\\fs approach has the advantage that \\ft also extrapolates fro\\t except\\fonal and low-frequency \\fnstances. "]},{"title":"3.2 Phas\\b 1: Shallow Syntact\\fc Pars\\fng","paragraphs":["The \\ta\\fn goal of a shallow parser \\fs to d\\fv\\fde a sentence \\fnto seg\\tents wh\\fch correspond to certa\\fn syntact\\fc un\\fts (\\tostly e\\fther noun, verb, or prepos\\ft\\fon phrase). These seg\\tents represent se\\tant\\fc argu\\tents of a g\\fven pred\\fcate (often shown by the verb). There are d\\ffferent tagg\\fng \\tethods for deter\\t\\fn\\fng const\\ftuents' boundar\\fes \\fn the sentence. The bracket style and IOB tag set are the two co\\t\\ton tagg\\fng styles. In th\\fs paper the alternat\\fve style for represent\\fng chunks \\fs IOB for\\t to deter\\t\\fne the beg\\fn\\fng and cont\\fnuat\\fon of chunks \\fn a sentence.","IOB was f\\frst used by Ratnaparkh\\f (1997). In th\\fs approach, each word \\fs assoc\\fated w\\fth one of three tags: I (for a word \\fns\\fde a chunk), O (for outs\\fde of a chunk), and B (for between the end of one and the start of a chunk). The B and I tags are suff\\fxed w\\fth the chunk type. For 154 ","\\fnstance, \\ff we try to chunk a sentence \\fnto NP, VP, and PP chunks, we \\t\\fght have the","follow\\fng tags: • B-X: the word beg\\fns a chunk of type X (NP, VP, PP, and so forth) • I-X: the word belong to a chunk of type X but does not beg\\fn \\ft • O: the word does not belong any chunk There are d\\ffferent chunk representat\\fon, on the bas\\fs of IOB, fro\\t wh\\fch we can \\tent\\fon","IOB1, IOB2, IOE1, IOE2, where ‘E’ shows the last word \\fn phrase. The exa\\tple below","\\fllustrates three d\\ffferent chunk types (NP, VP and PP) for the sentence ‘Al\\f ketab ra beh","\\toostash \\taa\\t.’ (Al\\f gave the book to h\\fs fr\\fend.) shown \\fn IOB structure:","Al\\f Al\\f B-NP","ketab book B-NP","ra -- I-NP","beh to B-PP","doostash h\\fs fr\\fend I-PP","daad gave B-VP",". . O We have \\tanually tagged 1500 sentences of Ha\\tshahr\\f corpus w\\fth IOB tag set to serve as","tra\\fn\\fng data and bench\\tark corpus for the exper\\f\\tents. The follow\\fng features (Table 2), used for shallow pars\\fng, are selected accord\\fng to the","e\\tp\\fr\\fcal observat\\fon and so\\te se\\tant\\fc \\tean\\fngs.","Tabl\\b 2: Shallow Parser Feature Set.","F\\batur\\b typ\\b \\fnd\\bx F\\batur\\b typ\\b na\\t\\b 1 Pre-1 word POS tag 2 Pre-2 word POS tag 3 Current POS tag 4 Post -1 word POS tag 5 Post -2 word POS tag ","Current POS tag \\fs the part of speech tag for the current word. Pre- 1 \\fs the POS tag of the f\\frst word before the labeled word \\fn the sentence. If the Pre-1 word does not ex\\ft null tag w\\fll be ass\\fgned. Pre- 2 \\fs the POS tag of the second word before the labeled word \\fn the sentence. Post - 1 \\fs the POS tag of the f\\frst word after the label\\fng word \\fn the sentence. And post - 2 \\fs the POS tag of the second word after the label\\fng word \\fn the sentence.","We have developed a progra\\t \\fn VB.NET to extract these features auto\\tat\\fcally and feed to MBL class\\ff\\fer."]},{"title":"3.3 Phas\\b 2: S\\b\\tant\\fc Rol\\b Lab\\bl\\fng","paragraphs":["After \\fdent\\ffy\\fng the argu\\tents, \\ft \\fs t\\f\\te to tag the\\t w\\fth se\\tant\\fc roles. The SRL syste\\t \\takes use of the \\fnfor\\tat\\fon prov\\fded by syntact\\fc parser. In th\\fs way we replaced features der\\fved fro\\t the h\\ferarch\\fcal structure w\\fth ones der\\fved fro\\t a flat chunked representat\\fon.","The feature set plays an \\f\\tportant role \\fn MBL perfor\\tance, and choos\\fng features \\fs certa\\fnly not a tr\\fv\\fal task. Features were \\ta\\fnly selected fro\\t the rev\\few of prev\\fous l\\fterature. We \\fnvest\\fgated each of these features \\fn Pers\\fan, so\\te acted qu\\fte s\\f\\t\\flarly to Engl\\fsh, wh\\fle others showed \\fnterest\\fng d\\ffferences. S\\fx features showed \\fnterest\\fng patterns that are d\\fscussed below:","• Curr\\bnt argu\\t\\bnt phras\\b typ\\b: the syntact\\fc type of const\\ftuents (NP,PP,VP,ADV,SP).","• Pr\\bv\\fous argu\\t\\bnt phras\\b typ\\b: S\\fnce we do not have any h\\ferarch\\fcal syntact\\fc parser","for Pers\\fan sentences, we tr\\fed to explo\\ft the syntact\\fc structure of the sentence by","\\tov\\fng a sl\\fd\\fng w\\fndow of s\\fze three, over the sentence's const\\ftuents and \\take use of","the collocat\\fon pattern of phrase type \\fn the sentence. 155 • N\\bxt argu\\t\\bnt phras\\b typ\\b • Pos\\ft\\fon: The pos\\ft\\fon feature \\fnd\\fcates that a const\\ftuent \\fs before or after the target verb. Nor\\tal sentences \\fn Pers\\fan are structured SOV, subject-prepos\\ft\\fon-object-verb. However, Pers\\fan can have relat\\fvely free word order, often called scra\\tbl\\fng. Th\\fs \\fs because the parts of speech are generally una\\tb\\fguous, and prepos\\ft\\fons and the accusat\\fve \\tarker help d\\fsa\\tb\\fguate the case of a g\\fven noun phrase. In our corpus, 75% of the roles are before the verb wh\\fle 25% are after the verb. As \\fn Engl\\fsh the pos\\ft\\fon \\fs a useful cue for role \\fdent\\fty. For exa\\tple all the agents are before the verb and 30% of pat\\fents are after the verb, \\tostly appear\\fng \\fn the for\\t of co\\tple\\tent clause. • Vo\\fc\\b: The act\\fve and pass\\fve verb for\\ts \\fn Pers\\fan share the sa\\te pred\\fcate argu\\tent structure; but the gra\\t\\tat\\fcal funct\\fons \\tay be \\tapped to d\\ffferent sets of se\\tant\\fc roles. Our ent\\fre 1324-sentence corpus cons\\fsts of 1152 (87%) act\\fve sentences and 172 (13%) pass\\fve. • V\\brb Class: These classes are based on the se\\tant\\fc roles each verb can take. More deta\\fled descr\\fpt\\fons are g\\fven below.","Although several class\\ff\\fcat\\fons are now ava\\flable for Engl\\fsh verbs, there \\fs no such","class\\ff\\fcat\\fon for Pers\\fan verbs. In th\\fs work, we prov\\fded a class\\ff\\fcat\\fon for Pers\\fan verbs","cons\\fst of 18 classes wh\\fch groups on the bas\\fs of both syntact\\fc and se\\tant\\fc alternat\\fons. For","th\\fs purpose, we f\\frst grouped a nu\\tber of Pers\\fan verbs (50 verbs at the f\\frst stage) accord\\fng","to the nu\\tber of syntact\\fc argu\\tents and then class\\ff\\fed the\\t \\fnto s\\taller groups wh\\fch have","s\\f\\t\\flar set of se\\tant\\fc roles. Hav\\fng a \\te\\tbersh\\fp \\fn a part\\fcular class says so\\teth\\fng about","the pred\\fcate-argu\\tent structure of a verb and when a verb \\fs absent \\fn the tra\\fn\\fng data, the","class \\fnfor\\tat\\fon \\tay tell the syste\\t how to label the se\\tant\\fc roles of the verbs belong\\fng to","a part\\fcular class.","For exa\\tple verb ‘Gor\\fkhtan’ (Escape) belongs to verb class 9 wh\\fch \\fs descr\\fbed as","follows: Verb class 9: [+Agent, source,goal, \\fnst]","Se\\tant\\fc roles for the sentence ‘se zen\\tan\\f ba hel\\fco\\bter rooze shanbe az zen\\tan gor\\fkhtan\\t’","(three pr\\fsoners have escaped fro\\t pr\\fson w\\fth a hel\\fcopter on Saturday) are as follows:","Agent Se zen\\tan\\f (three pr\\fsoners)","Instru\\tent Ba hel\\fco\\bter (w\\fth hel\\fcopter)","T\\f\\te Rooze shanbe (on saturday)","source Az zen\\tan (fro\\t pr\\fson)","Pred\\fcate Gor\\fkhtand (escape)  The exa\\tple de\\tonstrates the fact that argu\\tents don't necessar\\fly appear \\fn the order that they are wr\\ftten \\fn the role set. The co\\tplete l\\fst of verbs and the\\fr se\\tant\\fc classes are g\\fven \\fn Table 3."]},{"title":"4 Exp\\br\\f\\t\\bntal R\\bsults","paragraphs":["The exper\\f\\tents are carr\\fed out w\\fth the T\\fMBL software ava\\flable at http://\\flk.uvt.nl/. Regard\\fng the learn\\fng algor\\fth\\t, we use the IB1 class\\ff\\fer, para\\teter\\fzed by us\\fng overlap as the s\\f\\t\\flar\\fty \\tetr\\fc, \\fnfor\\tat\\fon ga\\fn for feature we\\fght\\fng, us\\fng 1 k-nearest ne\\fghbors, and we\\fght\\fng the class vote of ne\\fghbors as a funct\\fon of the\\fr \\fnverse l\\fnear d\\fstance.","We used three \\teasures for the evaluat\\fon of our syste\\t: prec\\fs\\fon, recall and a co\\tb\\fned \\teasure: F-Score. Prec\\fs\\fon \\fs def\\fned as the proport\\fon of pred\\fcted argu\\tents that \\fs pred\\fcted correctly, recall as the proport\\fon of correctly pred\\fcted argu\\tents. The F-Score \\fs the har\\ton\\fc \\tean of prec\\fs\\fon and recall. To \\teasure the perfor\\tance of the auto\\tat\\fc syste\\ts, the auto\\tat\\fcally ass\\fgned labels were co\\tpared to the labels ass\\fgned by a hu\\tan annotator.  156  Tabl\\b 3: Verb classes w\\fth the\\fr se\\tant\\fc roles.","Class Se\\tant\\fc Role Propert\\fes Verbs 1 [+Agent,+(top\\fc or pat\\fent), +goal] To th\\fnk ,to teach, to wr\\fte 2 [+Agent, locat\\fon] To stand , to sleep , to s\\ft 3 [+Agent,+pat\\fent] To k\\fss, to choose, to k\\fll, to test 4 [+Agent,+pat\\fent, \\fnst] To wear, to bu\\fld, to break, to cut 5 [+Agent,+pat\\fent,goal, benf ] To weave, to send 6 [+Agent,+pat\\fent, source] To buy , to steal , to snatch 7 [+Agent,+pat\\fent, goal] To sell , to lose , to \\fnclude , to","threw, to press 8 [+Agent,+pat\\fent, goal, source] To splash, to pour, to pay 9 [+Agent, source,goal, \\fnst] To fly, to escape 10 [+Agent,goal, \\fnst] To run, to laugh, to look, to f\\fght 11 [+Agant,+(pat\\fent or top\\fc)] To accept, to see, to understand 12 [+Agent,+(pat\\fent or top\\fc),","source,benf] To hear, to read, to ask 13 [+Agent,+(top\\fc or source)] To fr\\fghten 14 [+Agent,+top\\fc, goal] To co\\t\\tand, to try, to say 15 [+(Agent or pat\\fent), +goal, \\fnst] To st\\fck 16 [+Pat\\fent, \\fnst] To burn 17 [+Agent,+pat\\fent,+percept] To recogn\\fze 18 [+Agent,+(pat\\fent or top\\fc), +percept] To know ","The results obta\\fned fro\\t the shallow syntact\\fc parser w\\fth 1000 sentences tra\\fn\\fng data and 500 sentences for test are shown \\fn Table 4: Tabl\\b 4: Results for Syntact\\fc Parser Subtask 90.3 Total Accuracy 90.3 F_Score (M\\fcro-avg) 88.5 F_Score (Macro-avg) ","Table 5 shows the perfor\\tance of se\\tant\\fc role label\\fng, tra\\fned on 1300 sentences and tested w\\fth 700 sentences, for each se\\tant\\fc role. We have assu\\ted that the syste\\t \\fnput \\fs correct (\\fgnor\\fng the errors caused by the syntact\\fc parser). Tabl\\b 5: Per-class perfor\\tance of the SRL Recall Prec\\fs\\fon Se\\tant\\fc Role 95.3 89.6 Agent 1 1 Pred\\fcate 99.5 98.1 Top\\fc 1 1 # 81.7 62.8 Goal 37.4 38.9 Manner 20.3 50.7 T\\f\\te 63.8 58.3 Reason 52.6 56.2 Locat\\fon 87.7 87.4 Pat\\fent 94.3 70.3 Instru\\tent 72.8 61.6 Source 52.3 56.7 Benef\\fc\\fary 82.4 75.2 Percept 157 ","Low scores are generally related to low frequency of the SR \\fn the tra\\fn\\fng corpus, and h\\fgh scores are related to h\\fgh frequency or to overt \\tark\\fng of the SR.","Table 6 shows the overall results for the SRL phase, w\\fth Gold Standard (hand-corrected) \\fnput, wh\\fch \\fs the average of values fro\\t Table 5. Co\\t\\ton \\tethods for averag\\fng F-scores are \\t\\fcro-averag\\fng and \\tacro-averag\\fng. In \\t\\fcro-averag\\fng, each class’ F-score \\fs we\\fghted proport\\fonally to the frequency of the class \\fn the test set. A \\tacro-average adds all the F-scores and d\\fv\\fdes th\\fs su\\t by the nu\\tber of classes \\fn the tra\\fn\\fng set  Tabl\\b 6: Results for Se\\tant\\fc role set us\\fng hand-corrected \\fnput. 87.4 Total Accuracy 87.4 F_Score (M\\fcro-avg) 70.6 F_Score (Macro-avg)  Table 7 presents the overall SRL syste\\t. In pract\\fcal use, auto\\tat\\fc parses w\\fll not be as accurate. The f\\fnal syste\\t perfor\\tance w\\fll depend on the results obta\\fned fro\\t both phases and \\fs obv\\fously less than what was reported on Table 6.  Tabl\\b 7: F\\fnal Results for SRL Syste\\t. 83.8 Total Accuracy 83.8 F_Score (M\\fcro-avg) 60.9 F_Score (Macro-avg) ","It \\fs d\\fff\\fcult to co\\tpare our syste\\t w\\fth ex\\fst\\fng syste\\ts, s\\fnce our syste\\t \\fs the f\\frst one to be appl\\fed to Pers\\fan texts. Moreover, our data for\\tat and data s\\fze are d\\ffferent fro\\t earl\\fer research. However, to put our results so\\tewhat \\fn perspect\\fve, we looked at the perfor\\tance of state-of-the-art SRL syste\\ts for Engl\\fsh.","The CoNLL shared tasks prov\\fde an excellent source of \\fnfor\\tat\\fon on Engl\\fsh PropBank SRL syste\\ts that use features extracted fro\\t b\\fnary phrase structure trees. The best perfor\\t\\fng syste\\t that part\\fc\\fpated \\fn CoNLL 2005 (Pradhan et al., 2005) reached an F-score of around 93%. By cons\\fder\\fng the s\\fgn\\ff\\fcant d\\ffference between our corpus s\\fze and the one they used, along w\\fth the syntact\\fc parsers ex\\fsts for Engl\\fsh, th\\fs d\\ffference \\fn perfor\\tance can be expla\\fned.","A syste\\t that d\\fd not part\\fc\\fpate \\fn the CoNLL task, but st\\fll prov\\fdes \\fnterest\\fng \\tater\\fal for co\\tpar\\fson s\\fnce \\ft \\fs the only SRL syste\\t developed for Pers\\fan, \\fs Mousav\\f and Sha\\tsfard’s (2007) syste\\t. They used a rule-based approach to se\\tant\\fcally label Pers\\fan sentences and ach\\feved 76.8% prec\\fs\\fon and 75.1% recall. S\\fnce Pers\\fan \\fs a free word order language \\ft's not pract\\fcal to extract a l\\f\\t\\fted l\\fst of rules wh\\fch covers all d\\ffferent sentence structures plus except\\fons and \\frregular\\ft\\fes. But our syste\\t learns fro\\t \\fns\\fde the text, and so deals better w\\fth unseen data, and as the data s\\fze \\fncreases the syste\\t can \\fdent\\ffy \\tore cases w\\fth h\\fgher accuracy."]},{"title":"5 Conclus\\fon","paragraphs":["In th\\fs paper we addressed the quest\\fon of ass\\fgn\\fng se\\tant\\fc roles to sentences \\fn Pers\\fan (Fars\\f). We have developed a two phase se\\tant\\fc role label\\fng syste\\t for Pers\\fan us\\fng \\te\\tory-based learn\\fng \\todel. S\\fnce no se\\tant\\fc annotated corpus \\fs ava\\flable for Pers\\fan we created a s\\tall 2000 sentence corpus and hand-labeled \\ft for se\\tant\\fc roles. The syste\\t y\\felds results that are very pro\\t\\fs\\fng, 90.3% for chunk\\fng phase and 83.8% for the overall SRL syste\\t. 158 ","We can draw a nu\\tber of conclus\\fons fro\\t our \\fnvest\\fgat\\fon of se\\tant\\fc pars\\fng \\fn Pers\\fan. F\\frst, reasonably good perfor\\tance can be ach\\feved w\\fth a very s\\tall (1300 sentences) tra\\fn\\fng set. Second, the features that we extracted for Engl\\fsh se\\tant\\fc pars\\fng worked well when appl\\fed to Pers\\fan. And that shallow pars\\fng can be a good replace\\tent for full pars\\fng.","We also need to conduct \\tore exper\\f\\tent w\\fth the features to f\\fgure out wh\\fch features are \\tost useful for Pers\\fan. It would also be \\fnterest\\fng to see how the class\\ff\\fer would perfor\\t on larger collect\\fons and new genres of data. The follow-up of the Pers\\fan SRL project w\\fll prov\\fde new se\\tant\\fcally annotated data to fac\\fl\\ftate research \\fn th\\fs area, and also \\f\\tprove Pers\\fan pars\\fng."]},{"title":"R\\bf\\br\\bnc\\bs","paragraphs":["Daele\\tans, W., J. Zavrel, and \\b. Sloot. 2006. T\\fMBL: T\\flburg Memory-Base\\t Learner. T\\flburg Un\\fvers\\fty and CNTS Research Group, Un\\fvers\\fty of Antwerp.","Dowty, D. 1991. The\\tat\\fc Proto-roles and Argu\\tent Select\\fon. Language, 67, pp.547–619.","F\\fll\\tore, C. 1997. The Case for Case. Acade\\t\\fc Press, New York.","Hac\\foglu, \\b. 2004. Se\\tant\\fc role label\\fng us\\fng dependency trees. Procee\\t\\fngs of the 20th Internat\\fonal Conference on Com\\butat\\fonal L\\fngu\\fst\\fcs.","Ha\\t\\terton, J., M. Osborne, S. Ar\\tstrong, and W. Daele\\tans. 2002. Introduct\\fon to Spec\\fal Issue on Mach\\fne Learn\\fng Approaches to Shallow Pars\\fng. Journal of Mach\\fne Learn\\fng Research, 551-558.","L\\f\\t, J., Y. Hwang, S. Park, and H. R\\f\\t. 2004. Se\\tant\\fc role label\\fng us\\fng \\tax\\f\\tu\\t entropy \\todel. In Procee\\t\\fngs of CoNLL-2004.","Marquez, L., X. Carreras, \\b.C. L\\ftkowsk\\f, and S. Stevenson. 2008. Se\\tant\\fc Role Label\\fng: An Introduct\\fon to the Spec\\fal Issue. Com\\butat\\fonal L\\fngu\\fst\\fcs, 34(2), 145-159.","Morante, R. and B. Busser. 2007. Se\\tant\\fc Role Labell\\fng for Catalan and Span\\fsh us\\fng T\\fMBL. Procee\\t\\fngs of the 4th Internat\\fonal Worksho\\b on Semant\\fc Evaluat\\fons (SemEval-2007), pages 183–186.","Mousav\\f, M.S. and M. Sha\\tsfard. 2007. The\\tat\\fc Role Extract\\fon Us\\fng Shallow Pars\\fng. Internat\\fonal Journal of Com\\butat\\fonal Intell\\fgence, Volu\\te 4, 126-132.","Orou\\tch\\fan, F. 2006. Creat\\fng a Feas\\fble Cor\\bus for Pers\\fan POS Tagg\\fng. UOWD Techn\\fcal Reports Ser\\fes, Un\\fvers\\fty of Tehran.","Pradhan, S., \\b. Hac\\foglu, V. \\barugler, W. Ward, J.H. Mart\\fn, and D. Jurafsky. 2005. Support Vector Learn\\fng for Se\\tant\\fc Argu\\tent Class\\ff\\fcat\\fon. S\\br\\fnger Sc\\fence, 11–39.","Ratnaparkh\\f. 1997. A l\\fnear observed t\\f\\te stat\\fst\\fcal parser based on \\tax\\f\\tu\\t entropy \\todels. In EMNLP-97, The Secon\\t Conference on Em\\b\\fr\\fcal Metho\\ts \\fn Natural Language Process\\fng.","Stevens, G. 2006. Automat\\fc semant\\fc role label\\fng \\fn a Dutch cor\\bus. Master thes\\fs, Un\\fvers\\fty of Utrecht, Faculty of arts.","Sun, H. and D. Jurafsky. 2004. Shallow Se\\tant\\fc Pars\\fng of Ch\\fnese. In Procee\\t\\fngs of NAACL 2004, Boston, USA.","Wagner, A. 2004. Learn\\fng themat\\fc role relat\\fons for lex\\fcal semant\\fc nets. PHD thes\\fs, Tub\\fngen Un\\fvers\\fty.   159"]}]}