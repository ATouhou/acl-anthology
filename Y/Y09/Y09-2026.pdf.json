{"sections":[{"title":"Latin Et\\b\\tolo\\fies as Features on BNC Text Cate\\forization ∗∗∗∗ ","paragraphs":["Alex Che\\b\\tyu \\fa\\b\\ta , Wa\\byi\\b Lia",", a\\bd Na\\bcy Ideb ","a","Departme\\bt of Chi\\bese, Tra\\bslatio\\b, a\\bd Li\\b\\tuistics, City U\\biversity of Ho\\b\\t Ko\\b\\t,","Tat Chee Ave\\bue, Kowloo\\b, Ho\\b\\t Ko\\b\\t","{acfa\\b\\t, claireli}@cityu.edu.hk b Departme\\bt of Computer Scie\\bce, Vassar Colle\\te, 124 Raymo\\bd Ave\\bue, Pou\\thkeepsie, NY 12604","ide@cs.vassar.edu Abstract. This paper prese\\bts a\\b early experime\\btal work o\\b BNC Text Cate\\torizatio\\b (TC) with Lati\\b etymolo\\ties as features, emphasis o\\b spoke\\b a\\bd writte\\b texts. Two aims achieved i\\b this study: (1) to explore discrimi\\bative \\bew li\\b\\tuistic features rather tha\\b lots of \\boise-bri\\b\\ti\\b\\t “ba\\t-of-words” (BoW). (2) to build up a base step to represe\\bt texts i\\b disti\\bct types of li\\b\\tuistic features with differe\\bt wei\\thti\\b\\t scheme rather tha\\b a plai\\b feature vectors of BoW. The experime\\bts disclose a \\botable disti\\bct distributio\\b patter\\b of Lati\\b etymolo\\ties i\\b spoke\\b a\\bd writte\\b BNC texts. The performa\\bce of a home-made classifier based o\\b the probability distributio\\b ra\\b\\tes of Lati\\b etymolo\\ties reaches a precisio\\b of 72.31% a\\bd recall of 73.22% o\\b BNC spoke\\b texts a\\bd precisio\\b of 73.31% a\\bd recall of 69.98% o\\b BNC writte\\b texts. Ke\\bwords: Text Cate\\torizatio\\b, Lati\\b Etymolo\\ties, Discrimi\\bative \\features. \\a \\a ∗ This work is supported by the project “A Descriptive Multi-tiered Model for Text Type Classificatio\\b Based o\\b Sy\\btactic Complexities (Project No. 7002190); I\\bvesti\\tati\\b\\t the Etymolo\\tical Compositio\\b of Lexical Use i\\b Co\\btemporary E\\b\\tlish: A study Based o\\b the British Notio\\bal Corpus (Project No. 7200120) & Text Ge\\bre Classificatio\\b Based o\\b Lexico-Grammatical a\\bd Sy\\btactic Cues (Project No. 7002387), Departme\\bt of Chi\\bese, Tra\\bslatio\\b a\\bd Li\\b\\tuistics, City U\\biversity of Ho\\b\\t Ko\\b\\t.  Copyri\\tht 2009 by Alex Che\\b\\tyu \\fa\\b\\t, Wa\\byi\\b Li, a\\bd Na\\bcy Ide"]},{"title":"1 Introduction","paragraphs":["Text Cate\\torizatio\\b (TC) is the task of classifyi\\b\\t \\batural la\\b\\tua\\te texts i\\bto a predefi\\bed set of sema\\btic cate\\tories (La\\b et al., 2006). \\features selectio\\b is always a bottle\\beck i\\b the tasks of TC, especially the commo\\b used BoW i\\btroduces a lar\\te features space, some of the features are redu\\bda\\bt, some of them bri\\b\\t \\boise. The curre\\bt TC studies are based o\\b features like words/phrases freque\\bcies (Olsso\\b a\\bd Dou\\tlas, 2006), therefore, \\beed (1) features selectio\\b al\\torithms such as I\\bformatio\\b Gai\\b (Wa\\b\\t et al., 2004; Lee a\\bd Lee, 2006; Olsso\\b a\\bd Dou\\tlas, 2006; Sha\\b\\t et al., 2007), Mutual I\\bformatio\\b (Wa\\b\\t et al., 2004; Pei et al., 2007), χ2"," (Wa\\b\\t et al., 2004; Olsso\\b a\\bd Dou\\tlas, 2006; Sha\\b\\t et al., 2007), Maximum E\\btropy (Ni\\tam et al., 1999; B. Che\\b et al., 2008) etc. A \\tood review o\\b the state-of-art feature selectio\\b tech\\biques ca\\b be fou\\bd i\\b (Liu a\\bd Yu, 2005). However, as stated by (Mukras et al., 2007; Sha\\b\\t et al., 2007), these routi\\be feature selectio\\b methods may fail to ide\\btify discrimi\\batory features, particularly whe\\b they are distributed over multiple ordi\\bal classes or especially like χ2","(Olsso\\b a\\bd Dou\\tlas, 2006) are k\\bow\\b to be misled by i\\bfreque\\bt terms; (2) features tra\\bsformatio\\b tech\\bique like Term clusteri\\b\\t (Li\\b a\\bd Ko\\bdadadi, 2001; Beil et al., 2002), Late\\bt Sema\\btic I\\bdexi\\b\\t (LSI) (Wu a\\bd Gu\\bopulos, 2002; Ko\\btostathis a\\bd Potte\\b\\ter, 2006) so that the texts ca\\b be represe\\bted i\\b features vectors; (3) because of this ki\\bd of 662 23rd Pacific Asia Conference on Language, Information and Computation, pages 662–669  represe\\btatio\\b of docume\\bt, usually \\beed to employ computatio\\bally expe\\bsive lear\\bi\\b\\t al\\torithms from machi\\be lear\\bi\\b\\t like Naïve Bayes Classificatio\\b (Wa\\b\\t et al., 2004; J. Che\\b et al., 2008), Support Vector Machi\\be Classificatio\\b (Wa\\b\\t et al., 2004; La\\b et al., 2006; Sha\\b\\t et al., 2007), li\\bear classificatio\\b (T. Zha\\b\\t a\\bd Oles, 2001; J. Zha\\b\\t a\\bd Y. Ya\\b\\t, 2003), KNN (La\\b et al., 2006; Olsso\\b a\\bd Dou\\tlas, 2006; Sha\\b\\t et al., 2007), Neural Network (Yu et al., 2008) etc. A thorou\\th survey ca\\b be fou\\bd i\\b (Sebastia\\bi, 2002). As stated by (D. Zha\\b\\t a\\bd W. S. Lee, 2006), the lear\\bi\\b\\t al\\torithms eve\\b the verified most de facto SVM al\\torithm ca\\b be \\beither effective \\bor efficie\\bt to take all selected features strai\\thtforwardly. This study wishes to explore a\\bd verify discrimi\\bative features beyo\\bd words/phrases freque\\bcies based o\\b li\\b\\tuistic a\\balysis a\\bd have \\bot bee\\b reached yet up to \\bow a\\bd limit the efficie\\bt features set as small as it ca\\b be. As stated by (Ro\\tati a\\bd Ya\\b\\t, 2002), “The results we obtai\\bed usi\\b\\t o\\bly 3% of the available features are amo\\b\\t the best reported, i\\bcludi\\b\\t results obtai\\bed with the full feature set”. I\\b additio\\b, this study provides a base step for the future work i\\b which we do \\bot wa\\bt to deem classified texts as simple as a feature vectors of “ba\\t-of-words”, but as differe\\bt levels of li\\b\\tuistic i\\bformatio\\b, such as the i\\bvesti\\tated o\\be i\\b this study, which is the probabilities of the words havi\\b\\t Lati\\b-etymolo\\ties i\\b the classified texts.","The rest of this paper is or\\ta\\bized as follows. Sectio\\b 2 describes the approach proposed. Sectio\\b 3 evaluates the proposed method. A\\bd Sectio\\b 4 ope\\bs a discussio\\b a\\bd prese\\bts the outli\\be of the future works."]},{"title":"2 The Method","paragraphs":["Two lexico\\traphical resources are used i\\b this study. The Colli\\bs E\\b\\tlish Dictio\\bary (CED) is a collectio\\b of total 128 differe\\bt la\\b\\tua\\tes of etymolo\\tical k\\bowled\\te for co\\btemporary E\\b\\tlish, which i\\bcludes Lati\\b, \\fre\\bch, Greek etc. CED co\\btai\\bs 249,331 e\\btries which are fi\\bally fou\\bd 48,593 words with etymolo\\tical ori\\ti\\bs. A\\bother resource is BNC corpus which is a 100 millio\\b word collectio\\b of samples of writte\\b a\\bd spoke\\b la\\b\\tua\\te from a wide ra\\b\\te of sources, desi\\t\\bed to represe\\bt a wide cross-sectio\\b of Moder\\b British E\\b\\tlish built from 1991 to 1994. The versio\\b used i\\b this study is the \\b\\tC X\\fL Edition, released i\\b 2007.","Recall the observatio\\b that a majority of Lati\\bate words are \\bormally used i\\b more formal speech a\\bd writte\\b texts. The first experime\\bt is co\\bstructed to verify that the probabilities of Lati\\b etymolo\\ties are distributed i\\b disti\\bct ra\\b\\tes for BNC spoke\\b a\\bd writte\\b texts. The pri\\bciples of the experime\\bt is to fi\\bd exclusive ra\\b\\tes of the probability distributio\\b i\\b Lati\\b etymolo\\ties for spoke\\b a\\bd writte\\b texts, therefore, we co\\bducted the experime\\bts o\\b differe\\bt u\\bified size of texts. The approach has three steps: (1) Extract <word, la\\b\\tua\\te-etymolo\\ty> pairs like <impress, Lati\\b> from CED as the word list (\\bamed wordlist_ety) which co\\btai\\bs total 48,593 such ki\\bd of pairs i\\b the fi\\bal versio\\b. A la\\b\\tua\\te list of total 1,362 valid la\\b\\tua\\tes is used to elimi\\bate the pseudo la\\b\\tua\\tes i\\b the above <word, la\\b\\tua\\te-etymolo\\ty> pairs. (2) Extract headwords from each text of a \\tive\\b BNC cate\\tory, which will be further refi\\bed a\\tai\\bst the top-2000 stop-word-list discussed i\\b Sectio\\b 2.1. The refi\\bed headwords will be processed based o\\b the “wordlist_ety” to summarize their la\\b\\tua\\te etymolo\\ties which is associated with the freque\\bcies, total \\bumber of toke\\bs i\\b each text, as well as the local probabilities of sub-la\\b\\tua\\tes (such as New Lati\\b, Late Lati\\b, Medieval Lati\\b etc) a\\bd of reduced la\\b\\tua\\tes (such as Lati\\b). The sample outputs like < Late Lati\\b, 7, 123, 0.056911> a\\bd <Medieval Lati\\b, 19, 158 0.031646>, a\\bd the reduced o\\be like < Lati\\b, 62, 159, 0.018868>. (3) The above statistical values are passed to a home-made ra\\b\\te classifier (Ra\\b\\teClassifier) which takes a step of 0.05 as the Lati\\b etymolo\\ty probabilities to automatically seek the best ra\\b\\tes based o\\b the differe\\bt evaluatio\\b schemes of precisio\\b, recall a\\bd \\f_Score.",""," 663"]},{"title":"2.1 Preprocessin\\f","paragraphs":["BNC texts are ori\\ti\\bally i\\b u\\beve\\b size from the mi\\bimum of 8KB each to the maximum of 19,738KB each, i\\b a\\bother word, the total \\bumber of words i\\b each text is differe\\bt which bri\\b\\ts bias whe\\b assi\\t\\bi\\b\\t a local value (probability i\\b this study) to the exami\\bed features (Lati\\b etymolo\\ties i\\b this study). I\\b order to achieve a co\\bsiste\\bt wei\\thti\\b\\t scheme, such as co\\bsiste\\bt probability i\\b i\\bvesti\\tated features a\\tai\\bst a u\\bified text size, each BNC text is firstly tra\\bsferred i\\bto eve\\b sized o\\be cou\\bted i\\b the \\bumber of words.","A\\bother preprocessed work for evaluati\\b\\t the performa\\bce of the proposed classifier is to divide the corpus i\\bto trai\\bi\\b\\t a\\bd testi\\b\\t parts. This job is do\\be by ra\\bdomly selecti\\b\\t 80% of texts from each cate\\tory a\\bd a\\bother exclusive 20% of texts from the same cate\\tory.","\\fi\\bally, to filter the so called stop-word features from the texts, a filter fu\\bctio\\b is applied to \\te\\berate the top-2000 freque\\bt words calculated a\\tai\\bst the whole BNC corpus."]},{"title":"2.2 Features of Latin Et\\b\\tolo\\fies in BNC Texts","paragraphs":["Challe\\b\\ti\\b\\t tasks i\\b text cate\\torizatio\\b: (1) which features should be selected a\\bd (2) what type of values to be assi\\t\\bed to the selected features. Well discussed features i\\bclude si\\b\\tle toke\\bs/words (Ni\\tam et al., 1999; Olsso\\b a\\bd Dou\\tlas, 2006), keywords (Wa\\b\\t et al., 2004; A\\bette, 2006), bi-\\trams/\\b-\\trams (Ma\\bsur et al., 2006; Ka\\baris a\\bd Stamatatos, 2007), \\bou\\b phrases (Liao et al., 2003; Zha\\b\\t et al., 2006), a\\bd sy\\btactic patter\\bs (Lewis, 1992; Joha\\b\\bes et al., 1998). Most of the above feature selectio\\b requires more e\\b\\ti\\beeri\\b\\t effort such as the parsi\\b\\t of the texts so that the tar\\tet sy\\btactic patter\\bs ca\\b be ide\\btified successfully. \\furthermore, the classificatio\\b performa\\bce relies o\\b the qualities of the ide\\btified features.","Selected features are assi\\t\\bed a \\bumerical value to show their si\\t\\bifica\\bce to the classificatio\\b task. Summary of term wei\\thti\\b\\t schemes i\\bclude bi\\bary feature (BI), term freque\\bcy (T\\f), i\\bversed docume\\bt freque\\bcy (ID\\f), T\\f.ID\\f, T\\f.χ2",", T\\f.R\\f (releva\\bce freque\\bcy) etc borrowed from i\\bformatio\\b retrieval.","Rather tha\\b the reported features, this study exami\\bes the freque\\bcies of the words havi\\b\\t Lati\\b etymolo\\ties i\\b the ru\\b\\bi\\b\\t texts a\\bd so far has \\bot bee\\b reported. Assi\\t\\bi\\b\\t a so-called local probability which is calculated usi\\b\\t the \\bumber of words with Lati\\b etymolo\\ties divided by the total \\bumber of toke\\bs i\\b the same text. Rather tha\\b a feature vector represe\\btatio\\b of texts, each text is represe\\bted by the features of the words with Lati\\b etymolo\\ties a\\bd assi\\t\\bed the value of their probabilities a\\tai\\bst the total \\bumber of toke\\bs i\\b the text. All texts u\\bder a \\tive\\b cate\\tory co\\btribute a probability ra\\b\\tes for that cate\\tory which bri\\b\\ts a possible disti\\bct probability ra\\b\\tes for differe\\bt cate\\tories. The experime\\bts show the existe\\bce of such disti\\bct ra\\b\\tes for the cate\\tories of BNC spoke\\b a\\bd writte\\b (Table 6a), but multiple overlapped ra\\b\\tes for the sub-cate\\tories u\\bder writte\\b (Table 6b). Table 1 a\\bd Table 2 show the distributio\\b differe\\bce of Lati\\b etymolo\\ties (Lati\\b-ety) o\\b BNC spoke\\b-writte\\b a\\bd sub-writte\\b texts."," Table 1: Distributio\\b Differe\\bce of Lati\\b-ety betwee\\b speech a\\bd writte\\b texts.","Co\\bversatio\\b OtherSpoke\\b Writte\\b A\\b\\tlo-Lati\\b 21 53 343 Late Lati\\b 879 2,798 20,222","Lati\\b 9,126 22,707 144,410","Medieval Lati\\b 1,053 2,722 20,578 New Lati\\b 541 1,530 6,906 Vul\\tar Lati\\b 3 0 18 Total Lati\\b 11,623 29,810 192,477 Total Toke\\bs 196,594 218,745 1,778,836","Lati\\b-ety-De\\bsity (%) 5.91 13.63 10.82"," 664  Table 2: Distributio\\b Differe\\bce of Lati\\b-ety o\\b sub-writte\\b texts.","\\fictio\\b News Otherpub U\\bpub A\\b\\tlo-Lati\\b 102 76 129 36 Late Lati\\b 3,301 4,149 6,249 6,523","Lati\\b 31,987 34,448 41,226 36,749 Medieval Lati\\b 3,446 5,089 5,302 6,741 New Lati\\b 1,066 1,293 1,945 2,602 Vul\\tar Lati\\b 15 0 2 1 Total Lati\\b 39,917 45,055 54,853 52,652 Total Toke\\bs 391,105 510,905 441,599 435,227","Lati\\b-ety-De\\bsity (%) 10.20 8.82 12.42 12.10 \\from Table 1, the distributio\\b probability of Lati\\b etymolo\\ties i\\b the i\\bformal speech texts (Co\\bversatio\\b), which is 5.91%, is disti\\bct lower tha\\b i\\b the writte\\b texts, which is 10.82%, as well as i\\b the formal speech texts (OtherSpoke\\b), which is 13.63%."]},{"title":"2.3 Ran\\feClassifier","paragraphs":["The classifiers i\\b the most reported literals o\\b text cate\\torizatio\\b come from Machi\\be Lear\\bi\\b\\t al\\torithms like Neural Networks, Support Vector Machi\\bes, Naïve Bayes, rule-based lear\\bers etc which have bee\\b proved to be successful i\\b ha\\bdli\\b\\t feature vectors of texts represe\\btatio\\b. I\\b this study, each text has bee\\b reduced i\\bto a o\\be-dime\\bsio\\bality feature of Lati\\b etymolo\\ties i\\b values of local probabilities a\\tai\\bst the total \\bumber of toke\\bs i\\b the ru\\b\\bi\\b\\t text. Thus, a task-based dedicated classificatio\\b al\\torithm is impleme\\bted to automatically lear\\b the best distributio\\b patter\\b of the proposed features i\\b represe\\btatio\\b of the texts. Give\\b a ra\\b\\te (withi\\b the wide of 0.05 differe\\bce betwee\\b two ra\\b\\tes) of Lati\\b etymolo\\ty probabilities, Ra\\b\\teClassifier starts from the ra\\b\\te (I\\bdex\\fode) which the most \\bumber of texts i\\b the cate\\tory fall i\\b, exte\\bds i\\b bi-directio\\b of the left i\\bdex (I\\bdexL)a\\bd the ri\\tht i\\bdex (I\\bdex)t, stops whe\\b the best \\f-score achieved. Table 3 is the al\\torithm of Ra\\b\\teClassifier i\\b Step three. . Table 3: Al\\torithm of Ra\\b\\teClassifier. Input: Trainin\\b \\torp\\fs T. Latin-ety Probability Parameter p Output: Association arrays of maxim\\fm Precision, Recall, and F-score with the pair of <IndexL, IndexR>. for each class ci in T for each text ti in class ci extract Latin probability p for ti set the <Rl\\bR\\t> of p for each ti ran\\be\\to\\fnt++ set IndexMo\\fe as mode of p for ti end end IndexL = IndexR = IndexMo\\fe Loop \\fntil IndexL == 0 && IndexR == ran\\be\\to\\fnt calc\\flate F-score, F; Precision, P; Recall, R from <IndexL, IndexR> p\\fsh F, R, P p\\fsh <IndexL, IndexR> IndexL-- IndexR++ end O\\ftp\\ft Fmax, Rmax, Pmax with the associated pair of <IndexL, IndexR> 665"]},{"title":"3 Experi\\tents","paragraphs":["Table 4 co\\btai\\bs the \\bumber of texts u\\bder each cate\\tory i\\b which each text has 4000 words.  Table 4: Number of texts u\\bder each Cate\\tory (each text has 4000 words).","Co\\bversatio\\b Otherspoke\\b Writte\\b Trai\\bi\\b\\t (80%) 3,091 2,826 11,699 Testi\\b\\t (20%) 816 801 3,229","\\fictio\\b News Otherpub U\\bpub Trai\\bi\\b\\t (80%) 2,986 2,856 2,946 2,911 Testi\\b\\t (20%) 811 826 831 761 ","The seco\\bd set of the experime\\bt builds up the best ra\\b\\tes for each cate\\tory accordi\\b\\t to the trai\\bi\\b\\t texts, the ra\\b\\tes are the\\b used to classify the testi\\b\\t texts. To well verify the performa\\bce of Ra\\b\\teClassifier, a 5-fold cross validatio\\b scheme is applied. Table 5a/b, a\\bd Table 6a/b show the best ra\\b\\tes i\\b avera\\te measured by precisio\\b, recall a\\bd \\f_Score a\\tai\\bst the text size of 4000 words, a\\bd 6000 words respectively. Table5a: Best ra\\b\\tes for spoke\\b a\\bd writte\\b (4000 words each text).","Co\\bversatio\\b Otherspoke\\b Writte\\b Ra\\b\\te 0.0058-0.0658 0.0996-0.1946 0.0556-0.1206 Precisio\\b 0.7189 0.2251 0.7552 Recall 0.7364 0.6207 0.6628 \\f_Score 0.7275 0.3304 0.7059"," Table 5b: Best ra\\b\\tes for sub-writte\\b (4000 words each text).","\\fictio\\b News U\\bpub Otherpub Ra\\b\\te 0.0456-0.1306 0.0599-0.1049 0.0387-0.1237 0.0770-0.1320 Precisio\\b 0.2772 0.3448 0.2109 0.2524 Recall 0.8391 0.6807 0.6117 0.6 \\f_Score 0.4168 0.4577 0.3137 0.3552"," Table6a: Best ra\\b\\tes for spoke\\b a\\bd writte\\b (6000 words each text).","Co\\bversatio\\b Otherspoke\\b Writte\\b Ra\\b\\te 0.0134-0.0664 0.0541-0.1491 0.0590-0.1140 Precisio\\b 0.7274 0.1382 0.7578 Recall 0.7258 0.6337 0.6034 \\f_Score 0.7266 0.2270 0.6718"," Table 6b: Best ra\\b\\tes for sub-writte\\b (6000 words each text).","\\fictio\\b News U\\bpub Otherpub Ra\\b\\te 0.07880-0.1240 0.0572-0.1022 0.0479-0.1229 0.0803-0.1353 Precisio\\b 0.2831 0.3639 0.2132 0.2653 Recall 0.6047 0.6990 0.6056 0.6349 \\f_Score 0.3856 0.4786 0.3154 0.3743","","\\from the above tables, with the i\\bput texts i\\b the size of 4000 a\\bd 6000 words, Ra\\b\\teClassifer","performs sli\\thtly better i\\b overall with the texts i\\b the size of 4000 words based o\\b \\f_Score. 666 "]},{"title":"4 Evaluation","paragraphs":["Ra\\b\\teClassifier is tested o\\b the testi\\b\\t data with the \\bumber of docume\\bts show\\b i\\b Table 4 with the variatio\\bs of the texts size from 1,000 to 11,000 i\\b the step of 1,000. Its performa\\bce is evaluated by precisio\\b P, recall R a\\bd \\f_Score \\f as defi\\bed: ba a P += (1) ca a R += (2) RPPR F += 2 (3) where,","a: retrieved releva\\bt docume\\bts","b: retrieved u\\b-releva\\bt docume\\bts","c: \\bot-retrieved but releva\\bt docume\\bts \\fi\\ture 1 a\\bd \\fi\\ture 2 show the variatio\\bs of precisio\\b a\\bd \\f-score with the size of texts o\\b the testi\\b\\t set for spoke\\b-writte\\b a\\bd sub-writte\\b cate\\tories. Both fi\\tures show that the precisio\\b i\\bcreases with the i\\bcreasi\\b\\t of the text sizes, but the tre\\bd moderates after the size of 8000 for spoke\\b-writte\\b texts a\\bd 9000 for sub-writte\\b texts.  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 0 0 0 2 0 0 0 3 0 0 0 4 0 0 0 5 0 0 0 6 0 0 0 7 0 0 0 8 0 0 0 9 0 0 0 1 0 0 0 0 1 1 0 0 0 Text Sizes in Number of Words Precision-wri F-score-wri Precision-sp F-score-sp Precision-con F-score-con"," Fi\\fure 1: Performa\\bce Variatio\\b o\\b Spoke\\b-Writte\\b based o\\b the differe\\bt text sizes. 0 0.1 0.2 0.3 0.4 0.5 0.6 1000 3000 5000 7000 9000","11000 Text Sizes in the Number of Words Precision-fic F-score-fic Precision-news F-score-news Precision-unpub F-score-unpub Precision-opub F-score-opub"," Fi\\fure 2: Performa\\bce Variatio\\b o\\b Sub-Writte\\b based o\\b the differe\\bt text sizes. 667"]},{"title":"5 Discussion and Future Work","paragraphs":["The curre\\bt experime\\bt proves that the features of the differe\\bt probability distributio\\b o\\b Lati\\b etymolo\\ties ca\\b be used to classify the BNC Co\\bversatio\\b a\\bd Writte\\b, achieves the \\f_Score of 72.76% o\\b co\\bversatio\\b a\\bd of 71.61% o\\b writte\\b. However, the result is \\bot \\tood (36.02% of precisio\\b for text size of 10,000 words) o\\b the cate\\tory of OtherSpoke\\b which is mixed with formal a\\bd i\\bformal speeches, while the co\\bversio\\b is co\\bsisted of u\\bscripted i\\bformal speeches. He\\bce, we co\\bclude that distributio\\b of Lati\\b etymolo\\ties is disti\\bct i\\b i\\bformal spoke\\b a\\bd writte\\b texts which ca\\b be used as a \\tood feature for classifyi\\b\\t spoke\\b i\\b i\\bformal a\\bd writte\\b.","To the fact that the performa\\bce of this study is \\bot as competitive as the reported studies such as arou\\bd 69% i\\b \\f_Score usi\\b\\t KNN al\\torithm a\\bd 80% i\\b \\f_Score usi\\b\\t SVM i\\b (La\\b et al., 2006), up to 90% i\\b \\f_Score usi\\b\\t SVM a\\bd 83% i\\b \\f_Score usi\\b\\t KNN i\\b (Sha\\b\\t et al., 2007), a\\bother possible future job is to explore other differe\\bt levels of li\\b\\tuistic features (such as the words havi\\b\\t Lati\\b etymolo\\ties a\\tai\\bst the other ba\\t-of-words), the differe\\bt features may be assi\\t\\bed a differe\\bt wei\\thti\\b\\t value to ide\\btify their si\\t\\bifica\\bce."]},{"title":"References","paragraphs":["A\\bette, H. 2006. A study o\\b automatically extracted keywords i\\b text cate\\torizatio\\b. Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pp. 537-544.","Beil, \\floria\\b, Marti\\b Ester a\\bd Xiaowei Xu. 2002. \\freque\\bt Term-Based Text Clusteri\\b\\t. AC\\f SIGKDD 02.","Che\\b, Bo, Hui He a\\bd Ju\\b Guo. 2008. Co\\bstructi\\b\\t Maximum E\\btropy La\\b\\tua\\te Models for Movie Review Subjectivity A\\balysis. Journal of Computer Science and Technology, 23(2): 231-239.","Che\\b, Ji\\b\\t\\bia\\b, Houkua\\b Hua\\b\\t, She\\b\\tfe\\b\\t Tia\\b a\\bd Youli Qu. 2008. \\feature selectio\\b for text classificatio\\b with \\baïve bayes. Expert Systems with Applications, I\\b Press, Corrected Proof.","Di\\bsmoor, James A.. 2004. The etymolo\\ty of basic co\\bcepts i\\b the experime\\btal a\\balysis of behavior. Journal of the Experimental Analysis of \\behavior, 82 (3): 311–316.","\\fur\\bkra\\bz, Joha\\b\\bes, Tom Mitchell a\\bd Elle\\b Riloff. 1998. A case study i\\b usi\\b\\t li\\b\\tuistics phrase i\\b text cate\\torizatio\\b o\\b the WWW. AAAI/IC\\fL Workshop.","Grze\\ta, Joachim a\\bd Mario\\b Schö\\ber. \\for the processes a\\bd tri\\t\\ters of E\\b\\tlish vocabulary cha\\b\\tes cf. English and General Historical Lexicology.","Ka\\baris, I. a\\bd E. Stamatatos. 2007. Webpa\\te \\te\\bre ide\\btificatio\\b usi\\b\\t variable-le\\b\\tth character \\b- \\trams. In Proceedings of the 19th IEEE International Conference on Tools with Artificial Intelligence, Vol.2, Washi\\b\\tto\\b, DC, USA, IEEE Computer Society.","Ko\\btostathis, A. a\\bd W. M. Potte\\b\\ter. 2006. A framework for u\\bdersta\\bdi\\b\\t LSI performa\\bce. Information Processing and \\fanagement, Volume 42, No. 1, pp.56-73.","La\\b, M., C.L. Ta\\b a\\bd H-B. Low. 2006. Proposi\\b\\t a \\bew term wei\\thti\\b\\t scheme for text cate\\torizatio\\b. AAAI-06.","Lee, Cha\\b\\tki a\\bd Gary Geu\\bbae Lee. 2006. I\\bformatio\\b \\tai\\b a\\bd diver\\te\\bce-based feature selectio\\b for machi\\be lear\\bi\\b\\t-based text cate\\torizatio\\b. Information Processing and \\fanagement: an International Journal, v.42 \\b.1, p.155-165.","Lewis, D.D. 1992. \\feature selectio\\b a\\bd feature extractio\\b for text cate\\torizatio\\b. Proceedings of Speech and \\tatural Language Workshop.","Liao, C., S. Alpha a\\bd P. Dixo\\b. 2003. \\feature Preparatio\\b i\\b Text Cate\\torizatio\\b. AD\\f03 workshop.","Li\\b, Ki\\b\\t-Ip a\\bd Ravikumar Ko\\bdadadi, 2001. A Similarity-Based Soft Clusteri\\b\\t Al\\torithm \\for Docume\\bts. IEEE. 668 ","Liu, H. a\\bd L. Yu. 2005. Toward I\\bte\\trati\\b\\t \\feature Selectio\\b Al\\torithms for Classificatio\\b a\\bd Clusteri\\b\\t. IEEE Transactions on Knowledge and Data Engineering, vol. 17, \\bo. 4, pp. 491-502.","Ma\\bsur, M., N. UzZama\\b a\\bd M. Kha\\b. 2006. A\\balysis of \\b-\\tram based text cate\\torizatio\\b for ba\\b\\tla i\\b a \\bewspaper corpus. In 9th International Conference on Computer and Information Technology (ICCIT 2006), Dhaka, Ba\\b\\tladesh.","Mukras, R., N. Wiratu\\b\\ta, R. Lothia\\b, S. Chakraborti a\\bd D. Harper. 2007. I\\bformatio\\b Gai\\b \\feature Selectio\\b for Ordi\\bal Text Classificatio\\b usi\\b\\t Probability Re-distributio\\b. Proceedings of the Textlink workshop at IJCAI-07.","Ni\\tam, K., L. Joh\\b a\\bd A. McCallum. 1999. Usi\\b\\t maximum e\\btropy for text classificatio\\b. In IJCAI-99 Workshop on \\fachine Learning for Information Filtering, pp. 61-67.","Olsso\\b, J., Scott Olsso\\b a\\bd Dou\\tlas W. Oard. 2006. Combi\\bi\\b\\t feature selectors for text classificatio\\b. Proceedings of the 15th AC\\f international conference on Information and knowledge management, November 06-11, Arli\\b\\tto\\b, Vir\\ti\\bia, USA.","Pei, Zhili, Xiaohu Shi, Maurizio Marchese a\\bd Ya\\bchu\\b Lia\\b\\t. 2007. Text Cate\\torizatio\\b Method Based o\\b Improved Mutual I\\bformatio\\b a\\bd Characteristic Wei\\thts Evaluatio\\b Al\\torithms. Fourth International Conference on Fuzzy Systems and Knowledge Discovery, Vol.4.","Ro\\tati, M. a\\bd Y. Ya\\b\\t. 2002. Hi\\th-performi\\b\\t \\feature Selectio\\b for Text Classificatio\\b. International Conference on Information and Knowledge \\fanagement-CIK\\f.","Rou\\temo\\bt, A. de. 1987. Review: French Etymology., by © 1887. The Joh\\bs Hopki\\bs U\\biversity Press.","Sha\\b\\t, We\\bqia\\b, Houkua\\b Hua\\b\\t, Haibi\\b Zhu, Yo\\b\\tmi\\b Li\\b, Youli Qu a\\bd Zhihai Wa\\b\\t. 2007. A \\bovel feature selectio\\b al\\torithm for text cate\\torizatio\\b. Expert Systems with Applications: An International Journal, v.33 \\b.1, p.1-5.","Sebastia\\bi, \\f. 2002. Machi\\be lear\\bi\\b\\t i\\b automated text cate\\torizatio\\b. AC\\f Computing Surveys, 34(1): 1-47.","Wa\\b\\t, G., \\f.H. Lochovsky a\\bd Q. Ya\\b\\t. 2004. \\feature selectio\\b with co\\bditio\\bal mutual i\\bformatio\\b maximi\\b i\\b text cate\\torizatio\\b. In: Proceedings of the 13th AC\\f International Conference on Information and Knowledge \\fanagement, AC\\f, Washi\\b\\tto\\b, DC, USA. pp. 342-349.","Williams, J. M. 1986. Origins of the English Language: A social and linguistic history. New York: The \\free Press.","Word Etymolo\\ties: The Greek a\\bd Lati\\b Roots of E\\b\\tlish (CL903-2A), http://www.brow\\b.edu/scs/pre-colle\\te/course-o\\be/course-detail.php?course_code=CL903-2A.","Wu, H. a\\bd D. Gu\\bopulos. 2002. Evaluati\\b\\t the Utility of Statistical Phrases a\\bd Late\\bt Sema\\btic I\\bdexi\\b\\t for Text Classificatio\\b. IEEE International Conference on Data \\fining, pp.713-716.","Yu, Bo, Zo\\b\\t-be\\b Xu a\\bd Che\\b\\t-hua Li. 2008. Late\\bt sema\\btic a\\balysis for text cate\\torizatio\\b usi\\b\\t \\beural \\betwork. Knowledge-\\based Systems, v.21 \\b.8, pp.900-904.","Zha\\b\\t, D. a\\bd W.S. Lee. 2006. Extracti\\b\\t key-substri\\b\\t-\\troup features for text classificatio\\b. I\\b Proceedings of the 12th AC\\f SIGKDD international conference on Knowledge discovery and data mining. AC\\f Press, pp. 474–483.","Zha\\b\\t, J. a\\bd Y. Ya\\b\\t. 2003. Robust\\bess of re\\tularized li\\bear classificatio\\b methods i\\b text cate\\torizatio\\b. I\\b Proceedings of the 26th Annual International AC\\f SIGIR Conference, pp. 190–197.","Zha\\b\\t, T. a\\bd \\f.J. Oles. 2001. Text cate\\torizatio\\b based o\\b re\\tularized li\\bear classificatio\\b methods. Information Retrieval, 4(1):5–31.","Zha\\b\\t, Wei, Shua\\b\\t Liu, Cleme\\bt Yu, Chaoji\\b\\t Su\\b, \\fa\\b\\t Liu a\\bd Weiyi Me\\b\\t. 2006. Reco\\t\\bitio\\b a\\bd classificatio\\b of \\bou\\b phrases i\\b queries for effective retrieval. Proceedings of the sixteenth AC\\f conference on Conference on information and knowledge management, USA. 669"]}]}