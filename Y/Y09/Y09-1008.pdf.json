{"sections":[{"title":"Dependency\\b\\tra\\f\\far\\bBased\\bEnglish\\bSubject-Verb\\b\\b Agree\\fent\\bEvaluation1 \\b \\b","paragraphs":["Dongfeng \\b\\tia",", \\fonghu\\t Hua , Xuelei Mi\\toa",", \\tnd \\f\\tn Songb","  a","Knowledge Engineering Rese\\trch \\benter Sheny\\tng Institute of Aeron\\tutic\\tl Engineering","No.37 D\\toyi South Avenue, D\\toyi Development District, Sheny\\tng \\bhin\\t 110136","c\\tidf@vip.163.com, s\\tm830829@hotm\\til.com, mi\\toxl@ge-soft.com","b","Dep\\trtment of \\bhinese, Tr\\tnsl\\ttion \\tnd Linguistics","\\bity University of Hong Kong","83 T\\tt \\bhee Ave, Kowloon, Hong Kong","y\\tnsong@student.cityu.edu.hk Abstract.\\bAs \\t key f\\tctor in English gr\\tmm\\tr checking, subject-verb \\tgreement ev\\tlu\\ttion pl\\tys \\tn import\\tnt p\\trt in \\tssessing tr\\tnsl\\tted English texts. In this p\\tper, we propose \\t hybrid method for subject-verb \\tgreement ev\\tlu\\ttion on dependency gr\\tmm\\trs with the processing of phr\\tse synt\\tctic p\\trsing \\tnd sentence simplific\\ttion for subject-verb discovery. Experiment\\tl results on p\\ttent text show th\\tt we \\tchieve \\tn F-score of 91.98% for subject-verb p\\tir recognition, \\tnd \\t precision r\\tte of 97.93% for subject-verb \\tgreement ev\\tlu\\ttion on correctly recognized p\\tirs in the previous st\\tge. Keywords:\\bSubject-verb \\tgreement, Sentence simplific\\ttion, Dependency gr\\tmm\\tr, phr\\tse synt\\tctic p\\trsing"]},{"title":"1 Introduction\\b","paragraphs":["Subject-verb \\tgreement error is the most common type of mist\\tkes m\\tde in tr\\tnsl\\tting other l\\tngu\\tges to English text, \\tnd \\tffects the qu\\tlity of the gener\\tted text consider\\tbly. By m\\tking \\t det\\tiled \\tn\\tlysis on 300,000 error-noted English p\\ttent texts, we found th\\tt the subject-verb \\tgreement errors comprise 21.7% of \\tll the tr\\tnsl\\ttion errors. It is obviously indic\\tted th\\tt subject-verb \\tgreement is one of the common problems tr\\tnsl\\ttors would encounter. Due to the complic\\tte gr\\tmm\\tr \\tnd flexible us\\tge of sentence types, especi\\tlly the complic\\tted rel\\ttionship between subjects \\tnd predic\\tte verbs, the subject-verb \\tgreement ev\\tlu\\ttion is \\t difficult mission to t\\tckle.","\\burrently, m\\tnu\\tl proofre\\tding is still the m\\tin \\tppro\\tch widely \\tpplied in detecting subject-verb \\tgreement errors m\\tde by tr\\tnsl\\ttors. However, it costs too much while in low efficiency, \\tnd m\\tnu\\tl work is not c\\tp\\tble of reuse. To solve this problem, \\t comput\\ttion\\tl \\tppro\\tch is proposed in this p\\tper to \\tutom\\ttic\\tlly recognize the subject-verb p\\tirs \\tnd ev\\tlu\\tte their \\tgreement by obt\\tining the dependency rel\\ttionship between the subjects \\tnd its predic\\tte verbs. Phr\\tse synt\\tctic p\\trsing \\tnd sentence simplific\\ttion \\tre used \\tnd proved to be effective in our routine.","The rest of the p\\tper is org\\tnized \\ts follows: \\t concise survey of rel\\tted works is presented in the next section; section 3 is the description of our method; section 4 illustr\\ttes the procedure of our experiments; \\tnd the experiment\\tl results with \\tn\\tlysis \\tre presented in section 5; section 6 is the conclusion. \\h \\h  \\bopyright 2009 by Dongfeng \\b\\ti, \\fonghu\\t Hu, Xuelei Mi\\to, \\tnd \\f\\tn Song 63 23rd Pacific Asia Conference on Language, Information and Computation, pages 63–71 "]},{"title":"2 Related\\bWorks\\b\\b","paragraphs":["For there \\tre limited rese\\trches exclusively focus on subject-verb \\tgreement, m\\tny rel\\tted works \\tre reported on de\\tling with gr\\tmm\\ttic\\tl errors, some of which includes subject-verb \\tgreement c\\tse. Atwell (1987), Bigert \\tnd Knutsson (2002), \\bhodorow \\tnd Le\\tcock (2000) proposed the n-gr\\tm error checking for finding gr\\tmm\\ttic\\tl errors. H\\tnd-cr\\tfted error production rules (or “m\\tl-rules”), with context-free gr\\tmm\\tr, \\tre designed for \\t writing tutor for de\\tf students (Mich\\tud et \\tl., 2000). Simil\\tr str\\ttegies with p\\trse trees \\tre pursued in (Bender et \\tl., 2004), \\tnd error templ\\ttes \\tre utilized in (Heidorn, 2000) for \\t word processor. An \\tppro\\tch combining \\t h\\tnd-cr\\tfted context free gr\\tmm\\tr \\tnd stoch\\tstic prob\\tbilities is proposed in (Lee \\tnd Seneff, 2006) for correcting verb form errors, but it is designed for restricted dom\\tin. A m\\tximum entropy model using lexic\\tl \\tnd p\\trt of speech(POS) fe\\ttures, is tr\\tined in (Izumi et \\tl., 2003) to recognize \\t v\\triety of errors, \\tnd \\tchieves 55% precision \\tnd 23% rec\\tll on ev\\tlu\\ttion d\\tt\\t. John Lee \\tnd Steph\\tnie Seneff (2008) proposed \\t method b\\tsed on irregul\\trities in p\\trsing tree \\tnd n-gr\\tm, to correct English verb form errors m\\tde by non-n\\ttive spe\\tkers, \\tnd \\tchieved \\t precision \\tround 83.93%. However, on subject-verb \\tgreement processing, it m\\tinly \\timed \\tt those sentences which \\tre rel\\ttively simple, \\tnd proved some wh- subject problems to be difficult for its \\tppro\\tch."]},{"title":"3 Our\\bMethod\\b 3.1 Research\\bIssues\\b","paragraphs":["During the tr\\tnsl\\ttion process, the subject-verb dis\\tgreement phenomenon is common, especi\\tlly the confusion between the b\\tse form \\tnd the third person singul\\tr form. E.g. the sentence: th\\b \\ttilit\\f mod\\bl disclos\\b a mosaic thr\\tst b\\baring sh\\bll. The subject ‘mod\\bl’ \\tnd the predic\\tte verb ‘disclos\\b’ do not \\tgree with e\\tch other. This \\tp\\trts the sentence from good qu\\tlity \\tnd should be checked in the proofre\\tding process. Sentences th\\tt reg\\trd subject-verb dis\\tgreement errors \\ts the m\\tin t\\trget \\tre considered here.","There \\tre m\\tny f\\tctors involved th\\tt c\\tn disturb the recognition \\tnd \\tgreement ev\\tlu\\ttion of subject-verb, m\\tinly on sem\\tntic level \\tnd synt\\tctic level. In det\\til \\ts follows:\\b Se\\fantic\\blevel\\bIt is concerned with in\\tppropri\\tte choices of tense, \\tspect, voice or mood.\\bE.g., the subject-verb p\\tir recognition is correct, but the verb form does not \\tgree with the context on the sem\\tntic level. Such \\ts, H\\b *at\\b som\\b br\\bad for his br\\bakfast. The predic\\tte verb ‘at\\b’ is in p\\tst tense, it \\tgrees with the subject on sentence level. But if its context fe\\ttures need it to be in future tense, the verb form will h\\tve to be modified. Here, the checking is only done on synt\\tctic level without considering the context. Syntactic\\blevel\\bAs the second type, it c\\tn be subdivided into two sub-cl\\tsses: (1) Too m\\tny modifiers in the sentence m\\ty disturb the dependency p\\trsing \\tnd phr\\tse synt\\tctic p\\trsing. E.g., Th\\b \\tnd\\br *fram\\b, th\\b t\\bnsion *spring, th\\b swing *arm and th\\b t\\bnsil\\b forc\\b constant *d\\bvic\\b ar\\b all \\bq\\tipp\\bd in th\\b prot\\bcting cov\\br. Pars\\bd as follows in Fig\\tr\\b 1:","       64 Phrase \\b\\tnta\\fti\\f parsing \\i Dependen\\f\\t parsing  Figure\\b1: Ex\\tmple for cl\\tss 1 on Synt\\tctic level."," In this sentence, ‘fram\\b-3’, ‘spring-7’, ‘arm-11’ \\tnd ‘d\\bvic\\b-17’ \\tctu\\tlly sh\\tre the s\\tme verb ‘ar\\b-18’. But \\ts \\t result of the modifiers such \\ts ‘JJ’ \\tnd ‘NN’ (S\\tntorini, 1990), the subject is only recognized \\ts ‘d\\bvic\\b-17’ from ‘ns\\tbjpass(\\bq\\tipp\\bd-20, d\\bvic\\b-17)’ (de M\\trneffe et \\tl., 2008.), with other four omitted. As reg\\trd to this, sentence simplific\\ttion is introduced to compress the sentence structure \\tnd \\tvoid the disturb\\tnce of too m\\tny modifiers \\tnd some other elements. (2) The subject-verb p\\tirs h\\tve been recognized, but the inform\\ttion th\\tt the subject \\tnd the predic\\tte verb offer is not enough to ev\\tlu\\tte if they \\tre in \\tgreement. E.g., Th\\b op\\bning of \\bxisting hook *which is hang\\bd on a straight rod is \\tnclos\\bd. The sentence cont\\tins \\t wh- subordin\\tte cl\\tuse. The phr\\tse synt\\tctic p\\trsing \\tnd dependency p\\trsing \\tre:  Phrase \\b\\tnta\\fti\\f parsing \\i Dependen\\f\\t parsing  Figure\\b2: Ex\\tmple for cl\\tss 2 on Synt\\tctic level.  In Figure 2, subject-verb p\\tir ‘(op\\bning-2 is-13)’ c\\tn be concluded from dependency p\\trsing ‘ns\\tbjpass(\\tnclos\\bd-14, op\\bning-2)’ \\tnd ‘a\\txpass(\\tnclos\\bd-14, is-13)’. In the s\\tme w\\ty, the other p\\tir ‘(which-6 is-7)’ is obt\\tined, too. However, the problem is th\\tt ‘which-6’ is not the true subject c\\tp\\tble to ev\\tlu\\tte if the subject-verb is in \\tgreement, the true one should be 65  ‘hook-5’. But no links between ‘which-6’\\tnd ‘hook-5’ is served in the p\\trsing \\tbove in Figure 2. As reg\\trds to this kind outcome \\ts ‘(which-6 is-7)’, we re-recognize the subject-verb \\tfter reverting the wh- word b\\tck to the most possible sentence element th\\tt wh- word points to."]},{"title":"3.2 Sentence\\bSi\\fplification\\b","paragraphs":["Sentence simplific\\ttion is \\tn interesting point in this p\\tper. Grefenstette (1998) \\tpplies sh\\tllow p\\trsing \\tnd simplific\\ttion rules to the problem of telegr\\tphic text reduction, with \\ts go\\tl the development of \\tn \\tudio sc\\tnner for the blind or for people using their sight for other t\\tsks like driving. Another rel\\tted \\tpplic\\ttion \\tre\\t is the shorting of text to fit the screen of mobile devices (\\borston-Oliver, 2001; Euler 2002).","We employ the sentence simplific\\ttion \\ts \\t pre-processing oper\\ttion by deleting some kinds of \\tdjective, \\tdverb, modified noun \\tnd some kind preposition\\tl phr\\tse, so th\\tt the sentence becomes more simple with the trunk elements, such \\ts the subject, the verbs \\tnd the object, left. By \\tn\\tlyzing the tr\\tining d\\tt\\t, \\t positive simplific\\ttion c\\ttegories set is picked out \\tnd shown \\ts follows:"," Table\\b1: \\b\\ttegories to simplify \\t sentence. # Origin\\tl Delete # Origin\\tl Delete 1 RB1 \\b\\b RB2 JJ RB1 \\b\\b RB2","11 DT JJ \\b\\b VBG NN* JJ \\b\\b VBG 2 RB1 JJ|RB2|MD RB1","12 !VB JJ1 NN*|JJ2 JJ1 3 DT NN1 \\b\\b NN2 NN* \\b\\b NN2","13 , JJ , JJ, 4 !IN&&!TO NN|\\bD NN (!%) NN|\\bD","14 JJ1 VBG NN*|JJ2 JJ1 VBG 5 NN VBP|VBZ|VBG VBP|VBZ NN","15 DT VBG1 \\b\\b VBG2 NN*|JJ VBG1 \\b\\b VBG2 6 NN VBP|VBZ JJ IN NN","16 DT VBD VBG NN VBD VBG 7 DT NN* \\b\\b NN VBN NN* \\b\\b NN1 VBN","17 DT VBG|VBN JJ|NN* VBG|VBN 8 DT NN1 VBG1 \\b\\b VBG2 NN2 NN1 VBG1 \\b\\b VBG2","18 only VB*(is|\\tre|\\tm|w\\ts|were) “Only” TO “there” 9 DT NN1 VBG|VBN NN2 NN1 VBG|VBN","19 NN* PP (not with VB* in) PP ( not with VB* in) 10 JJ1 \\b\\b JJ2 JJ3|NN JJ1 \\b\\b JJ2","#"," In T\\tble 1, the ‘Origin\\tl’ POS sequence c\\tn be reg\\trded \\ts triggering environment, ‘Delete’ points to the sequence th\\tt should be deleted. And the sign\\tl ‘!’ is not \\t punctu\\ttion, but \\ts \\t logic oper\\ttor. ‘NN*’ me\\tns NN or NNS. In \\tddition, the simplific\\ttion oper\\ttion of ‘JJ’, ‘NN’, ‘VB*’, ‘RB’ or their POS sequence is done b\\tsed on POS, while the oper\\ttion of ‘PP’ chunk is done b\\tsed on Phr\\tse Structure P\\trsing. The best t\\trget of sentence simplific\\ttion \\tre sentences th\\tt \\tre tot\\tlly correctly t\\tgged (POS) \\tnd p\\trsed (Phr\\tse Structure P\\trsing). For those incorrectly done, in\\tppropri\\tte simplific\\ttion outcome \\tppe\\tr. But since incorrectly done, no m\\ttter whether the simplific\\ttion oper\\ttion is correct, it will not decline the system perform\\tnce. So, we m\\tke e\\tch sentence in the corpus simplified."]},{"title":"3.3 wh-type\\bWord\\bReverting\\b","paragraphs":["The wh- words, such \\ts “which”, “who”, “wh\\tt” \\tnd “th\\tt”, usu\\tlly exist in \\t sentence \\ts the subject, \\tnd if the sentence is \\t subordin\\tte cl\\tuse, \\t more det\\tiled sentence subject should be found. In order to obt\\tin \\t much ex\\tcter subject, we do \\t reverting oper\\ttion to the wh- word.","Firstly, retrieve the most possible subject element in the sentence th\\tt wh- word m\\ty point to. Secondly, repl\\tce wh- word with the subject element \\tnd extr\\tct the subordin\\tte cl\\tuses to be independent, so th\\tt \\t complic\\tte \\tnd long sentence becomes sever\\tl rel\\ttive simple ones. Then, discover the subject-verb p\\tirs of \\tll the new gener\\tted sentences by m\\tking dependency gr\\tmm\\tr \\tn\\tlysis. Termin\\tlly, we combine the subject-verb p\\tirs b\\tck into the outcome of the origin\\tl sentence. 66 The \\tlgorithm for reverting is \\ts follows:  Input:Ph\\b\\tse\\fsynt\\tctic\\fp\\t\\bsing\\ffile;\\f Output:The\\felement\\fth\\tt\\fwh- word most\\fpossibly\\fpoints\\fto,\\fonly\\f\\b\\t is\\f \\f\\f\\f\\f\\f\\f\\f\\fconside\\bed\\fhe\\be;\\f int\\fDist\\tn(WD\\f,\\f\\b\\ti)\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f//\\fthe\\fdist\\tnce\\fbetween\\fWD\\f\\f\\tnd\\f\\b\\ti;\\f\\f \\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f// WD\\f\\fis\\fthe\\fP\\t\\bt\\fOf\\fSpeech\\fof\\fwh- word;\\f \\f\\fbegin\\f \\f\\f//\\fweight\\fof\\fe\\tch\\fb\\b\\tnch\\fw =\\f1;\\f \\f\\f//\\fV\\tlue_Dist\\tnce(node1,node2)\\f= w ×\\fthe\\fnumbe\\b\\fof\\fb\\b\\tnches\\fconnect\\f\\f\\f\\f\\f\\f \\f\\f\\f\\f\\f\\fnode1\\f\\tnd\\fnode2;\\f \\f\\fDefinition:int\\fdistance\\f=\\f0;\\f\\f \\f\\f\\f\\fif\\fnode\\f\\t\\f\\ts\\fthe\\fne\\t\\best\\f\\tnd\\fcommon\\f\\tncesto\\b\\fof\\fWD\\f\\f\\tnd\\f\\b\\ti; \\f\\f\\f\\f\\f\\f\\f\\fdistance = Value_Dist\\tnce(\\t,WD\\f)\\f+\\fV\\tlue_Dist\\tnce(\\t,\\b\\ti);\\f \\f\\f\\f\\f\\f\\f\\f\\f\\betu\\bn\\fdist\\tnce;\\f \\f\\f\\f\\felse\\f \\f\\f\\f\\f\\f\\f\\f\\f\\betu\\bn\\f+∞;\\f \\f\\f\\f\\fend\\fif\\f end\\f \\f st\\bing\\fReve\\bt()\\f \\fDefinition:\\fint\\fdis;\\f \\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\fint\\fDIS;\\f\\f//\\fthe\\fdist\\tnce\\fbetween\\fthe\\fwh-\\fwo\\bd\\f\\tnd\\fthe \\b\\t;\\f \\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\fst\\bing\\fSUBJ;\\f\\f\\f//\\fthe\\fmost\\fpossible\\f\\b\\t\\fwh-\\fwo\\bd\\fpoints\\fto;\\f \\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\fSUBJ\\f=\\fNull,DIS\\f=\\f+∞;\\f \\f\\fbegin\\f \\f\\ffo\\b\\fe\\tch\\f\\b\\ti\\fbefo\\be\\fthe\\fwh-\\fwo\\bd\\fin\\fP\\t\\bsed-T\\bee\\f// \\b\\ti\\fmust\\fbefo\\be\\fthe\\f\\f \\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f\\f//\\fwh- word in the sentence;\\f \\f\\f\\f\\f\\f dis\\f=\\fDist\\tn(WD\\f, \\b\\ti);\\f//\\fc\\tlcul\\tte\\fthe\\fdist\\tnce\\fof\\f\\b\\ti \\tnd WD\\f; \\f\\f\\f\\f\\fIf\\fdis < DIS // search the nearest \\b\\ti; \\f\\f\\f\\f\\f\\f\\f\\f\\f\\fDIS = dis;\\f SUBJ\\f=\\f\\b\\ti;\\f \\f\\f\\f\\f\\f\\felse\\f \\f\\f\\f\\f\\f\\f\\f\\f\\f\\fcontinue;\\f \\f\\f\\f\\f\\f\\fend\\fif\\f \\f\\fend\\ffo\\b\\f \\f\\f\\betu\\bn\\fSUBJ;\\f end\\f"]},{"title":"4 Experi\\fents\\b","paragraphs":["How the subject \\tnd the predic\\tte verb link up with e\\tch other in \\t sentence is r\\tther flexible, especi\\tlly for the science \\tnd technology liter\\tture sentences, such \\ts p\\ttent corpus, which \\tre too long \\tnd with too m\\tny modifiers in. This m\\tkes the subject-verb \\tgreement ev\\tlu\\ttion more difficult. In this p\\tper, we utilize the p\\ttent corpus."]},{"title":"4.1 Develop\\fent\\bData\\b It","paragraphs":["is m\\tinly used for le\\trning the sentence simplific\\ttion c\\ttegories. By \\tn\\tlyzing the t\\tgging \\tnd p\\trsing outcome of the sentences given, we choose c\\ttegories th\\tt positively function to simplifying \\t sentence to be \\t set, \\ts in T\\tble 1. Tot\\tlly, 600 m\\tnu\\tlly proofre\\td English p\\ttent sentences \\tre used to develop the c\\ttegories set."]},{"title":"\\b","paragraphs":["67 "]},{"title":"\\b 4.2 Evaluation\\bData\\b","paragraphs":["For the ev\\tlu\\ttion, we experiment on 1000 English p\\ttent sentences tr\\tnsl\\tted by non-n\\ttive spe\\tkers. In order to m\\tke \\t gener\\tl comp\\trison, the corpus is sep\\tr\\tted into four different p\\trts \\ts follows:"," Table\\b2: An\\tlysis of ev\\tlu\\ttion corpus.","corpus Short sentences Long sentences Number(sen.) 332 sen. 668 sen. Percent\\tge(%) 33.2% 66.8%  subject-verb p\\tirs \\tgreed subject-verb p\\tirs dis\\tgreed subject-verb p\\tirs \\tgreed","subject-verb p\\tirs","dis\\tgreed","Number(sen.) 172 sen. 160 sen. 328 sen. 340 sen.","Percent\\tge(%) 17.2% 16% 32.8% 34% Note:\\bLong sentence: if length of the sentence > 40 words; Short sentence: if length of the sentence < 40 words."," In order to compute the precision of the system outcome, we \\tnnot\\tte the correct subject-verb p\\tirs \\tnd their \\tgreement of the 1000 sentences m\\tnu\\tlly \\ts the reference. E.g., for the sentence in Figure 2, it is ‘op\\bning-2 is-13 1|hook-5 is-7 1|’, where ‘1|’ me\\tns the subject-verb is in \\tgreement, ‘0|’ me\\tns dis\\tgreement in contr\\tst."]},{"title":"4.3 Evaluation\\bMetric\\b","paragraphs":["According to the common three ev\\tlu\\ttion guidelines, the following st\\ttistics \\tre computed \\ts the criterion to ev\\tlu\\tte the perform\\tnce of the system: Precision The proportion of the system subject-verb p\\tirs which \\tre correct. \\b\\tlcul\\tted \\ts follows:  100% N P M= × (1) \\h Not\\b: N is the number of the correct subject-verb p\\tirs in system outcome. M is the tot\\tl number of the subject-verb p\\tirs in system outcome.\\b Recall\\bOut of \\tll the subject-verb p\\tirs in the reference, the proportion th\\tt \\tppe\\tr in the system outcome. \\b\\tlcul\\tted \\ts follows:  100% N R T= × (2)","\\h Not\\b: T is the tot\\tl number of the subject-verb p\\tirs in the reference. F-Score Which is \\t combin\\ttion of P \\tnd R, \\tnd is \\t more gener\\tl ev\\tlu\\ttion score. The formul\\t is \\ts follows: \\b 2 2( 1) 100% P R F R Pβ","β× × +","= × + × (3) \\h Not\\b: β is \\tn import\\tnt weight p\\tr\\tmeter between P \\tnd R, it is reg\\trded \\ts 1 in this p\\tper, i.e. P \\tnd R sh\\tre the s\\tme weight. 68 "]},{"title":"4.4 Experi\\fent\\bSetting\\b","paragraphs":["The experiment is implemented \\ts following steps: Step\\b1\\bPre-Processing Tokenize the p\\ttent corpus in §4.2. Step\\b 2\\b Phrase\\b Syntactic\\b Parsing\\be.g. Th\\b op\\bning of \\bxisting hook which is hang\\bd on a straight rod is \\tnclos\\bd, and th\\b \\tnd\\br fram\\b, th\\b t\\bnsion spring, th\\b swing arm and th\\b t\\bnsil\\b forc\\b constant d\\bvic\\b ar\\b all \\bq\\tipp\\bd in th\\b prot\\bcting cov\\br. \\m (1) P\\trsed with St\\tnford-p\\trser: (ROOT (S (S (NP (NP (DT Th\\b) (NN op\\bning)) (PP (IN of) (NP (VBG \\bxisting) (NN hook) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VBN hang\\bd) (PP (IN on) (NP (DT a) (JJ straight) (NN rod)))))))))) (VP (VBZ is) (VP (VBN \\tnclos\\bd)))) (, ,) (CC and) (S (NP (DT th\\b) (ADJP (JJ \\tnd\\br) (NP (NP (NN fram\\b)) (, ,) (NP (DT th\\b) (NN t\\bnsion) (NN spring)) (, ,) (NP (DT th\\b) (NN swing) (NN arm)) (CC and) (NP (DT th\\b) (JJ t\\bnsil\\b) (NN forc\\b)))) (JJ constant) (NN d\\bvic\\b)) (VP (VBP ar\\b) (RB all) (ADJP (VBN \\bq\\tipp\\bd) (PP (IN in) (NP (DT th\\b) (JJ prot\\bcting) (NN cov\\br)))))) (. .))) \\m (2) Step\\b 3\\b Sentence\\b Si\\fplification. Simplify the sentences by deleting some elements, such \\ts some kind JJ or NN or RB or PP chunk th\\tt listed in T\\tble 1. As is simplified, (2) becomes into (3): Th\\b op\\bning of \\bxisting hook which is hang\\bd on a rod is \\tnclos\\bd, and th\\b fram\\b, th\\b spring, th\\b arm and th\\b forc\\b d\\bvic\\b ar\\b \\bq\\tipp\\bd in th\\b cov\\br. \\m (3) Step\\b 4\\b Do\\b Dependency\\b Parsing to sentence (3), the subjects \\tnd their predic\\tte verbs \\tre linked up, \\tnd subject-verb p\\tirs: ‘op\\bning-2 is-12 |which-6 is-7 |fram\\b-17 ar\\b-28 |spring-20 ar\\b-28 |arm-23 ar\\b-28 |d\\bvic\\b-27 ar\\b-28 |’ \\h \\h \\h (4) c\\tn be recognized. Step\\b5\\bRevert\\bthe\\bwh- subject\\bFor the p\\tirs such \\ts ‘which-6 is-7 |’ in which wh-type subject is recognized, the sentence will be rechecked by reverting the wh- word b\\tck into the word or chunk (usu\\tlly \\ts NP chunk before the wh- word) th\\tt the wh- word most possibly points to. Once the wh- word is reverted, retrieve the subordin\\tte cl\\tuses to be independent. \\to\\bto\\bstep\\b2.","For the outcome of (4), ‘which-6’ is repl\\tced \\ts ‘hook-5’, \\tnd the origin\\tl sentence becomes: Th\\b op\\bning of \\bxisting hook is \\tnclos\\bd, and th\\b fram\\b , th\\b spring , th\\b arm and th\\b forc\\b d\\bvic\\b ar\\b \\bq\\tipp\\bd in th\\b cov\\br. \\m (5) \\tnd Existing hook is hang\\bd on a rod. \\m (6) Since both \\tre rechecked, combine the subject-verb p\\tirs of (5) \\tnd (6) to be: op\\bning-2 is-12 |hook-5 is-7 |fram\\b-17 ar\\b-28 |spring-20 ar\\b-28 |arm-23 ar\\b-28 |d\\bvic\\b-27 ar\\b-28 | \\m \\m \\m (7) Step\\b6\\bTer\\final\\boutco\\fe\\bEv\\tlu\\tte if the subject-verb p\\tirs \\tre in \\tgreement \\tccording to their POS (P\\trt Of Speech). According to (2), the POS of (7) is: NN VBZ |NN VBZ | NN VBP |NN VBP | NN VBP | NN VBP | So, the \\tgreement outcome is: op\\bning-2 is-12 1|hook-5 is-7 1|fram\\b-17 ar\\b-28 0|spring-20 ar\\b-28 0|arm-23 ar\\b-28 0|d\\bvic\\b-27 ar\\b-28 0| \\m \\m \\m(8) Not\\b: ‘0|’ stands for disagr\\b\\bm\\bnt; ‘1|’ stands for agr\\b\\bm\\bnt.","In \\tddition, four different subjects in (8) sh\\tre the s\\tme verb ‘ar\\b-28’, it is \\t plur\\tl c\\tse. So, their \\tgreement l\\tbels should be modified to ‘1|’. Then, the termin\\tl result comes to be: op\\bning-2 is-12 1|hook-5 is-7 1|fram\\b-17 ar\\b-28 1|spring-20 ar\\b-28 1|arm-23 ar\\b-28 1|d\\bvic\\b-27 ar\\b-28 1| \\m \\m (9)"]},{"title":"\\b","paragraphs":["69 "]},{"title":"\\b 5 The\\bExperi\\fental\\bResults\\band\\bAnalysis\\b","paragraphs":["T\\tble 3 comp\\tres the outcomes of different ph\\tses of the subject-verb discovery: the first one is merely b\\tsed on dependency gr\\tmm\\tr; sentence simplific\\ttion is \\tdded to be the second one; \\tnd the third one \\tdds wh-type word reverting oper\\ttion to the second. Outcome of the first is present \\ts the b\\tseline."," Table\\b3: The outcome of the subject-verb discovery.","Dep. SSIM+Dep. SSIM+ Dep.+WH-.","","Short","sentences","Long sentences","Short sentences","Long sentences","Short sentences","Long sentences Subject-verb \\tgreed(\\f/N) \\f N \\f N \\f N \\f N \\f N \\f N R(%) 96.89 96.33 91.05 91.12 96.89 96.33 93.68 90.89 96.89 97.91 93.82 91.13 P(%) 93.96 92.93 89.29 85.68 94.92 94.85 92.35 86.14 94.92 96.89 92.48 86.66 F(%) 95.41\\b 94.60\\b 90.16\\b 88.32\\b 95.90\\b 95.58\\b 93.01\\b 88.45\\b 95.90\\b 97.40\\b 93.14\\b 88.84\\b Rtot\\tl(%) 92.16 93.07 93.38 Ptot\\tl(%) 88.53 90.16 90.63 Ftot\\tl(%) 90.31\\b 91.59\\b 91.98\\b","Note: Dep. Me\\tns Dependency P\\trsing; SSIM me\\tns Sentence Simplific\\ttion; WH- is the oper\\ttion of","rechecking of wh- type subject."," The comp\\trison of the subject-verb \\tgreement ev\\tlu\\ttion on the p\\tirs th\\tt correctly recognized is \\ts follows in T\\tble 4:"," Table\\b4:\\bPrecision of \\tgreement ev\\tlu\\ttion on the subject-verb p\\tirs th\\tt correctly recognized. Dep. SSIM+ Dep. SSIM+ Dep.+WH-. Short","sentences","Long sentences","Short sentences","Long sentences","Short sentences","Long sentences Subject-verb \\tgreed(\\f/N) \\f N \\f N \\f N \\f N \\f N \\f N","P(%) 99.47 97.27 97.53 97.88 99.47 97.81 97.04 98.28 99.47 97.86 97.04 98.41","Ptot\\tl(%) 97.86\\b 97.88\\b 97.93\\b"," In T\\tble 3, the subject-verb discovery outcomes of the three methods \\tre presented, including the Precision(P), Rec\\tll(R) \\tnd F-score(F) on e\\tch subset of the corpus, \\ts well \\ts the tot\\tl F-score on the whole corpus. In T\\tble 4, it is the precision of the subject-verb \\tgreement ev\\tlu\\ttion b\\tsed on the subject-verb p\\tirs th\\tt h\\tve been recognized correctly in T\\tble 3.","By comp\\trison, the figures show th\\tt both the SSIM \\tnd WH- oper\\ttions function positively th\\tt the fin\\tl Ftot\\tl of the recognition improves 1.67%. And from the percent\\tge it improves step by step, SSIM is shown to get \\t more rem\\trk\\tble Ftot\\tl. This is bec\\tuse every sentence c\\tn be simplified while not \\tll of them cont\\tin \\t wh- subordin\\tte cl\\tuse, \\tctu\\tlly there \\tre only 269 wh- words in the corpus. Moreover, the c\\ttegories for SSIM must be selected c\\trefully, or else it m\\ty result in neg\\ttive effect. But WH- is \\tlw\\tys positive, since it only \\tims \\tt the incorrect subject-verb recognition. However, m\\tybe there could be more \\tppropri\\tte c\\ttegories for SSIM or more perfect method for WH-, on th\\tt the system will perform better. 70","As to the subject-verb p\\tirs th\\tt is discovered correctly, for the re\\tson of the precision of P\\trt Of Speech t\\tgging, the \\tgreement ev\\tlu\\ttion is impossible to be whole correct. The Precision(P) on the subsets of the corpus \\tnd the whole corpus \\tre \\ts T\\tble 4."]},{"title":"6 Conclusion\\b","paragraphs":["Subject-verb \\tgreement is \\t complic\\tted \\tnd difficult problem in M\\tchine Tr\\tnsl\\ttion Ev\\tlu\\ttion, it is involved with complic\\tted gr\\tmm\\tr, long dependency rel\\ttionship, \\tnd subordin\\tte cl\\tuse f\\tctors, \\tnd so on. Especi\\tlly for the science \\tnd technology liter\\tture sentences, such \\ts p\\ttent corpus, which \\tre too long or with too m\\tny modifiers in, it gets worse.","We h\\tve proposed \\t hybrid method for subject-verb \\tgreement ev\\tlu\\ttion on dependency gr\\tmm\\trs with the processing of phr\\tse synt\\tctic p\\trsing \\tnd sentence simplific\\ttion for subject-verb discovery. It is completely \\tutom\\ttic\\tlly done, \\tnd the results show its efficiency.","By the w\\ty, the c\\ttegories we use for sentence simplific\\ttion \\tnd wh- type subject reverting oper\\ttion m\\ty be not much \\tppropri\\tte, the better c\\ttegories \\tre m\\tde, the better the system performs."]},{"title":"References\\b","paragraphs":["Atwell, E. S. 1987. How to detect gr\\tmm\\ttic\\tl errors in \\t text without p\\trsing it. Proc\\b\\bding of th\\b 3rd","EACL. 38-45.","Bigert, J. \\tnd O. Knutsson. 2002. Robust error detection: A Hybrid Appro\\tch \\bombining unsupervised error detection \\tnd linguistic knowledge. Proc\\b\\bding of Rob\\tst M\\bthod in Anal\\fsis of Nat\\tral Lang\\tag\\b Data. 10-19.","Bender, E., D. Flickinger, S. Oepen, A. W\\tlsh, \\tnd T. B\\tldwin. 2004. Arboretum: Using \\t Precision Gr\\tmm\\tr for Gr\\tmm\\tr \\bhecking in \\bALL. Proc. In-STIL/ICALL S\\fmposi\\tm on Comp\\tt\\br Assist\\bd L\\barning.","S\\tntorini, B. 1990. P\\trt Of Speech T\\tgging Guidelines for the Penn Treeb\\tnk Project (3rd  Version, 2nd","Printing). http://b\\tlba.sds\\t.\\bd\\t/j\\ban\\btt\\b/th\\bsis/P\\bnnTags.html. \\bhodorow, M. \\tnd \\b. Le\\tcock. 2000. An Unsupervised Method for detecting Gr\\tmm\\ttic\\tl Errors. Proc\\b\\bding of NAACL’00. 140-147. de M\\trneffe, M.-\\b. \\tnd \\b.D. M\\tnning. 2008. Stanford t\\fp\\bd d\\bp\\bnd\\bnci\\bs man\\tal-[EB]. Heidorn, G. 2000. Intelligent Writing Assist\\tnce. Handbook of Nat\\tral Lang\\tag\\b Proc\\bssing. Obert D\\tle, Herm\\tnn Moisi \\tnd H\\trold Somers (ed.). M\\trcel Dekker, Inc. Izumi, E., K. Uchimoto, T. S\\tig\\t, T. Supnithi, \\tnd H. Is\\th\\tr\\t. 2003. Autom\\ttic Error Detection in the J\\tp\\tnese Le\\trner’s English Spoken D\\tt\\t. Companion Vol\\tm\\b to Proc. ACL. S\\tpporo, J\\tp\\tn. Lee, J. \\tnd S. Seneff. 2008. \\borrecting Misuse of Verb Forms. 22nd Int\\brnational Conf\\br\\bnc\\b on Comp\\ttational Ling\\tistics. Lee, J. \\tnd S. Seneff. 2006. Autom\\ttic Gr\\tmm\\tr \\borrection for Second-L\\tngu\\tge Le\\trners. Proc. Int\\brsp\\b\\bch. Pittsburgh, PA. Mich\\tud, L., K. Mc\\boy, \\tnd \\b. Pennington. 2000. An Intelligent Tutoring System for De\\tf Le\\trners of Written English. Proc. 4th Int\\brnational ACM Conf\\br\\bnc\\b on Assistiv\\b T\\bchnologi\\bs. Tesniere, L. 1959. El\\bm\\bnts d\\b S\\fntax\\b Str\\tct\\tral\\b. P\\tris: Klincksieck.   71"]}]}