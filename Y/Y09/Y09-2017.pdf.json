{"sections":[{"title":"A Bootstr\\b\\t\\tin\\f Method for Finer-Gr\\bined O\\tinion Minin\\f Usin\\f Gr\\b\\th Model","paragraphs":["∗ ∗ ∗ ∗ "," Shu Zhan\\b\\t Yin\\b\\fu Xia\\t Yao Men\\b\\t and Hao Yu"," Fu\\fitsu Research and Development Center","Don\\b Si Huan Zhon\\b Rd\\t Chaoyan\\b District\\t Bei\\fin\\b 100025\\t China {zhan\\bshu\\t y\\fxia\\t men\\byao\\t yu}@cn.fu\\fitsu.com Abstr\\bct. Pursuin\\b on the analysis of product reviews\\t a bootstrappin\\b method is proposed to find the product features and opinion words in iterative steps. Different from conventional methods\\t a \\braph model is built to link and measure the relationship between the pairs of product features and opinion words. A rule-based method is presented to \\bet the initial seeds of product features and opinion words automatically. Our experimental results on electronic product reviews are encoura\\bin\\b\\t which prove the proposed method and techniques are effective in performin\\b this task of feature level opinion minin\\b. Keywords: opinion minin\\b\\t product reviews\\t bootstrappin\\b\\t \\braph model \\X \\X  Copyri\\bht 2009 by Shu Zhan\\b\\t Yin\\b\\fu Xia\\t Yao Men\\b\\t and Hao Yu"]},{"title":"1 Introduction","paragraphs":["In recent years\\t there has been a wealth of opinion source in the web. These opinions have attracted some special \\broups of people. For example\\t the \\bovernment and politicians are interested in the response to \\bovernmental policies\\t merchants and potential customers are interested in the feedback of and comment on commercial products. Therefore\\t how to analyze and monitor the tide of prevalent attitudes on the web\\t how to extricate people from wadin\\b throu\\bh a lar\\be number of opinions to find their interest\\t have received considerable attention in research community.","A series of topic symposiums and evaluation sessions on opinion minin\\b have appeared in TREC and NTCIR. For the opinion minin\\b on document and sentence level\\t the task is to classify either positively or ne\\batively in a review. However\\t the sentiment orientation of a review is not sufficient for many applications. Opinion minin\\b be\\bins to focus on the finer- \\brained features level minin\\b. The task is to find not only the sentiment orientation but also the commented features. This information could be used to deeply analyze prevalent attitudes or \\benerate various types of opinion summaries.","In feature level opinion minin\\b\\t most researches are oriented on product reviews. The task is typically divided into three main subtasks: (i) identifyin\\b product features\\t (ii) identifyin\\b opinions re\\bardin\\b the product features\\t and (iii) determinin\\b the sentiment orientation of the opinions. In this paper\\t we focus on the first two steps to find the product features and opinion words in product reviews. Different from the previous work\\t we unify these two separate tasks into one process. A bootstrappin\\b method is proposed to iteratively find both of them. A \\braph is built by linkin\\b pairs of product features and opinion words. The short len\\bth reviews are separately analyzed by rule-based method to find the initial seeds used in the bootstrappin\\b.","The remainder of the paper is or\\banized as follows: section 2 describes the related work on feature level opinion analysis. Section 3 \\bives our approach of bootstrappin\\b. Section 4 presents the \\braph model. Section 5 describes the techniques of initial seeds selection and 589 23rd Pacific Asia Conference on Language, Information and Computation, pages 589–595  candidates extraction. Section 6 \\bives the experiments and results. Finally section 7 summarizes this paper."]},{"title":"2 Rel\\bted Work","paragraphs":["With the study of finer-\\brained opinion minin\\b\\t there are some feature level based opinion minin\\b systems. For example\\t Opinion Observer (Liu et al.\\t 2005) which focuses on customer reviews on the web and provides the visual comparison of consumer opinions. Red Opal (Scaffidi et al.\\t 2007) as a search system which enables customers to locate products rapidly based on features.","At present\\t the techniques on identifyin\\b product features are primarily based on unsupervised minin\\b. The most representative research is that of (Hu and Liu\\t 2004). They adopt association rule minin\\b for extractin\\b nouns as frequent features. Compactness prunin\\b and redundancy prunin\\b are used to filter the incorrect features. Then frequent features are used to find ad\\facent opinion words (ad\\fectives) that modify them. Infrequent features are extracted with these opinion words. Their method is effective in findin\\b the product features. However\\t there is a problem that some frequent nouns and noun phrases may not be real product features. Directly usin\\b frequent nouns as features may lead to the reduction of precision.","Popescu and Etzioni (2005) utilize relation-specific extraction patterns with web PMI assessor to assess feature candidates. They consider the product features findin\\b as a domain words extraction process. Usin\\b the web as the resource\\t product features are identified without opinion words.","For the identification of opinion words\\t the method of the nearest vicinity match is simple and effective. Further\\t Researchers (Su et al., 2008; Du and Tan\\t 2009) focus on the association between the features and opinion words by mutual reinforcement approach. Din\\b and Liu (2007) combines the lin\\buistic rules and opinion a\\b\\bre\\bation function to determine the semantic orientation.","Our work is different from theirs: the short len\\bth reviews are first separately analyzed for their simple structures. A \\braph model is built to capture the relationship between product features and opinion words. With an iterative reinforcement\\t product features and opinion words are found in one sta\\be."]},{"title":"3 Bootstr\\b\\t\\tin\\f","paragraphs":["Given the product reviews\\t the task is to find the product features and opinion words. As observed\\t the opinion words mostly appear around the features in the review sentences. They are hi\\bhly dependent on each other. So we adopt bootstrappin\\b method to find both of them in a unified process.","Bootstrappin\\b (Abney\\t 2002) refers to a problem settin\\b in which one is \\biven a small set of labeled data and a lar\\be set of unlabeled data\\t and the task is to induce a classifier. It has been proved effective in semantic lexicon construction (Widdows and Dorow\\t 2002; Thelen and Riloff\\t 2002). In our task\\t it also includes the followin\\b steps:","(i) In the product reviews\\t choose a small set of features and opinions as “seed words”","(ii) Count co-occurrence of candidates and seed words in the product reviews","(iii) Use a fi\\bure of merit based upon these counts to select new seed words","(iv) Return to step (ii) and iterate \\b times","As product features are mostly nouns and noun phrases. Ad\\fectives are normally used to express opinions in reviews. In this paper\\t we take nouns and noun phrases as the feature candidates\\t the ad\\fectives as the opinion word candidates\\t thou\\bh there are other components that seem like candidates\\t such as verb phrases as features\\t nouns as opinion words brin\\bin\\b lots of noises at the same time. 590","Both product features and opinion words are found in the bootstrappin\\b process\\t which makes the process complex. Fi\\bure 1 shows the framework of the bootstrappin\\b. The bootstrappin\\b process be\\bins with initial seeds of product features and opinion words\\t as well as the candidate sets of product features and opinion words.","The iterative process includes two steps: the product features selection and opinion words selection. Based on the set of product features\\t each candidate in the set of opinion candidates is scored by \\braph model \\fud\\bment. Then m candidates are added as new elements into the set of opinion words. In the same way\\t n candidates of feature candidates are added as new product features by measurin\\b them with the set of opinion words. In Fi\\bure 1\\t these two steps are ta\\b\\bed with different lines.","Previous work on product features findin\\b is mainly based on their frequency in the reviews. Thou\\bh candidate frequency is a \\bood indicator\\t some of them may not be the product features\\t such as camera\\t Canon\\t people...in di\\bital camera reviews. For the product features\\t we are more interested in the opinionated features. Accordin\\b to observation\\t people are accustomed to express their opinions of a product features usin\\b opinion words that are located around the feature in the sentence. So we consider the relationship between product features and opinion words\\t and import a \\braph model to measure them in the iterative steps."," "]},{"title":"4 Gr\\b\\th Model","paragraphs":["In this section we describe how to build a \\braph to represent the relationship between product features and opinion words. Then an al\\borithm based on mutual reinforcement between product features and opinion words is proposed to add the most related candidates to the existin\\b collections."]},{"title":"4.1 Gr\\b\\th Buildin\\f","paragraphs":["For the \\braph\\t we consider two sets: the set of product features F and the set of opinion words P. A bipartite link between the elements in F and O is built\\t then a \\braph G(F\\t O\\t R) is constructed. Here\\t R is the wei\\bht of the link\\t which measures the relationship between product feature and opinion word.","In this paper\\t we hypothesize that opinion words appear around product features. If an opinion word is co-occur with a product feature within a \\biven distance in a sentence\\t a bipartite link is built. Otherwise they are considered to be unrelated. We set the wei\\bht by the frequency of co-occurrence of the product feature and the opinion word. Fi\\bure 2 shows the main idea. There are three relations in the \\braph: one to many\\t one to one and one to none. For example\\t the product feature candidates t\\tme and pe\\fple have no link with the set of opinion Initial Seeds Selection Graph Model Jud\\bement add n candidates add m candidates Features and Opinions Opinion Candidates Feature Candidates Opinion Words Product Features Candidates Extraction Bootstrappin\\b (k iterations) Fi\\fure 1: Bootstrappin\\b process 591  words\\t so they are not re\\barded as the real product features. Here\\t we set the threshold at four. If the relative distance between an opinion word and a product feature is less than four words\\t then they are re\\barded to co-occur with each other."," "]},{"title":"4.2 Increment\\bl Al\\forithm","paragraphs":["After the \\braph buildin\\b\\t we adopt an al\\borithm to measure the relative important relationships between the candidates and the existin\\b collection.","Let u be an element in the candidate set U\\t N(u) be the occurrence of u in the reviews. Let v\\t be an element in the existin\\b collection V\\t R(u, v\\t) be the wei\\bht of the pair u and v\\t in the \\braph. Then the score of u is calculated in the followin\\b:"]},{"title":"∑","paragraphs":["∈"]},{"title":"=","paragraphs":["Vv \\t \\t"]},{"title":"uNvuRutGraphWe\\tgh )(/)\\t()(","paragraphs":["(1)","When selectin\\b product features\\t u is a candidate in the set of product feature candidates\\t v\\t is the opinion words in the already selected opinion words set. We adopt a ratio as the wei\\bht of product feature u. The numerator is its co-occurrence with the words in the set of opinion words. The denominator is its occurrence in the reviews. For opinion words selection\\t it is opposite to compute the wei\\bht of opinion word with the set of product features.","The ratio used to select new ones tends not to select hi\\bher frequency words in the reviews. For instance\\t suppose that two product feature candidates occur ten and twenty times in the reviews. And they co-occur with the opinion words in the set of opinion words five times. The product feature occurrin\\b ten times is more related with opinion words than the one occurrin\\b twenty times. We adopt the simple ratio because it is suited to dealin\\b with infrequent occurrences. Sufferin\\b from sparse data\\t it is important to broaden the covera\\be of candidates from which additional likely candidates can be selected out."]},{"title":"5 Initi\\bl Seeds Selection \\bnd C\\bndid\\btes Extr\\bction","paragraphs":["In the semantic lexicon extraction task\\t initial seeds are usually selected amon\\b the most frequent candidates in the corpus or \\biven by humans. In the reviews\\t especially in Chinese reviews\\t people like to express their opinion in short and simple sentence\\t like the form of “product feature” + “opinion word”. Therefore\\t we preprocess this type of sentences with rule-based method.","Sentences shorter than the threshold will be listed into short ones. If there is only one ad\\fective and one noun (or noun phrase) in the short sentence\\t they will be selected as opinion word and product feature. Then initial seed set is constructed. Here\\t we set the threshold at five words. If there is no initial seed selected from the reviews\\t the \\beneral opinion words such as “\\bood”\\t “excellent” are inputted as the initial seeds.","Product Features ","Opinion Words  time size ","people","picture  lens"," bi\\b small \\bood clear Fi\\fure 2: Relationship \\braph 592","The method of candidates extraction is similar with that of (Hu and Liu\\t 2004). We only consider the ad\\facent words as product feature candidates\\t select the candidates with its minimum support lar\\ber than the threshold. With redundancy prunin\\b\\t we remove redundant features which are more likely to be the subset of other candidates."]},{"title":"6 Ex\\teriments 6.1 Cor\\tus \\bnd Ev\\blu\\btion Metric","paragraphs":["We have conducted experiments on the Chinese customer reviews of two kinds of electronic products. Each of them contains 300 reviews. They are extracted from several auto review websites. A human ta\\b\\ber manually read all the reviews and produced the manual lists for product features and opinion words. Table 1 shows the number of manual product features and opinion words ta\\b\\bed from the reviews.","T\\bble 1: Detail of test corpus Product No. of manual Features No. of manual Opinions","Di\\bital camera 162 123 Notebook 176 92  We testify the performance of the proposed techniques from two perspectives: (i) The effectiveness of product features extraction (ii) The effectiveness of opinion words extraction We use precision and recall to evaluate the performance as that of (Hu and Liu\\t 2004)."]},{"title":"6.2 Results \\bnd An\\blysis","paragraphs":["For our candidates extraction method is similar with the work of (Hu and Liu\\t 2004)\\t we take it as the baseline\\t and \\bive minimum support 1% as the threshold followin\\b their work. Table 2 presents precision and recall results of product features extraction. The column of minimum support is the result of the baseline. The column of bootstrappin\\b is the proposed method.  T\\bble 2: Results of the system on product features extraction Minimum support Bootstrappin\\b Product","Precision Recall Precision Recall","Di\\bital camera 0.5053 0.2963 0.5083 0.3889 Notebook 0.5288 0.2614 0.5429 0.4545","","Compared with the baseline\\t the results of bootstrappin\\b method improve both the precision and recall for product features extraction. The baseline is mainly to extract the nouns and noun phrases by their frequency\\t with not only the frequent features in the result but also a lot of errors included in it. There are two types of nouns which are not the real product features. One is the common words such as “时间”(time)\\t “消费”(consumption)\\t and “专业”(profession). They are more likely extracted as features with hi\\bh frequency in the reviews. The other is the named entity\\t such as person\\t brands. They have little probability of bein\\b product features\\t thou\\bh they are frequent words in the reviews. As a result\\t it is not enou\\bh for this task to use the information of frequency only. It has been proved useful and effective to consider the opinion words in bootstrappin\\b process with its hi\\bher precision. The result shows both the recall and precision are improved.","In order to see the influence of the techniques of initial seeds selection and candidates extraction for the system\\t we evaluate the results at these two steps in Table 3. 593 ","The column of initial seeds is the result of product features \\botten after the initial seeds selection. The column of candidates is the result of candidates extraction. Here\\t the threshold in candidates extraction sta\\be is set at 0.5% in order to \\bet more candidates before bootstrappin\\b. And the column of combination is the result of these two steps\\t which \\bives the performance of the system before bootstrappin\\b."," T\\bble 3: Results at each steps of the system on product features extraction Initial seeds Candidates Combination Product","Precision Recall Precision Recall Precision Recall","Di\\bital camera 0.7500 0.0185 0.3726 0.4877 0.3726 0.4877 Notebook 0.8974 0.1989 0.4140 0.4375 0.4559 0.5284  T\\bble 4: Results of the system on opinion words extraction Bootstrappin\\b Product","Precision Recall","Di\\bital camera 0.6696 0.6260 Notebook 0.7200 0.7826","","From Table 3\\t we can see that the performance of initial seeds is hi\\bhly dependent on the reviews. Its recall is hi\\bher with more short sentences in the reviews. Its result is credible with hi\\bh precision. The initial seeds have no contribution on di\\bital camera reviews\\t for they are all the frequent candidates and have been included in the frequent candidates. So the column of combination is the same with the column of candidates. Comparin\\b the results on di\\bital camera and notebook\\t the better the initial seeds \\bet the better whole performance of the results\\t as shown in Table 2.","For the column of candidates\\t it shows our aim at \\bettin\\b more candidates with the loss of precision. The column is the result of combinin\\b both the initial seeds and candidates as the input for the bootstrappin\\b. Comparin\\b the results of combination and bootstrappin\\b\\t it indicates that the bootstrappin\\b process improves the precision with a little loss of recall. So it is effective to measure the product feature candidates with its related opinion words. Here\\t we set the k=30\\t m=\\b=3 in the experiment.","Table 4 shows the result of opinion words extraction. The results of opinion words extraction are more acceptable than that of product features extraction\\t thou\\bh they are two different tasks. The candidates of opinion words are extracted by the method of vicinity match\\t the bootstrappin\\b process is \\fust to find more credible ones from this set."]},{"title":"7 Conclusion","paragraphs":["In this paper\\t we probe into the problem of feature level product opinions analysis. We unify the tasks of findin\\b product features and opinion words into one process. With automatically findin\\b the initial seeds\\t we find the product features and opinion words in iterative steps by the bootstrappin\\b method. A \\braph model is used to measure the relationship between product features and opinion words. Our experimental results are primary and encoura\\bin\\b\\t they prove the proposed techniques are effective in performin\\b the task.","Compared with human annotation\\t there is still a \\bap needed to be filled by further improvin\\b these techniques. For example\\t the \\braph buildin\\b mi\\bht adopt the rule-based pattern reco\\bnization or deeply analysis on the syntactic structures to find more accurate relations between product features and opinion words. The incremental al\\borithm may adopt different statistic metrics to test its effectiveness in measurin\\b the relationship between product features and opinion words. 594"]},{"title":"References","paragraphs":["Abney\\t S. 2002. Bootstrappin\\b. Pr\\fceed\\t\\bgs \\ff the A\\b\\bual Meet\\t\\bg \\ff the Ass\\fc\\tat\\t\\f\\b f\\fr C\\fmputat\\t\\f\\bal L\\t\\bgu\\tst\\tcs, pp. 360-367.","Din\\b\\t X. W. and B. Liu. 2007. The Utility of Lin\\buistic Rules in Opinion Minin\\b. Pr\\fceed\\t\\bgs \\ff A\\b\\bual I\\bter\\bat\\t\\f\\bal ACM SIGIR C\\f\\bfere\\bce \\f\\b Research a\\bd Devel\\fpme\\bt \\t\\b I\\bf\\frmat\\t\\f\\b Retr\\teval\\t pp. 811-812.","Du\\t W.F. and S.B. Tan. 2009. An Iterative Reinforcement Approach for Fine-Grained Opinion Minin\\b. Pr\\fceed\\t\\bgs \\ff the A\\b\\bual C\\f\\bfere\\bce \\ff the N\\frth Amer\\tca\\b Chapter \\ff the Ass\\fc\\tat\\t\\f\\b f\\fr C\\fmputat\\t\\f\\bal L\\t\\bgu\\tst\\tcs\\t pp. 486-492.","Hu\\t M.Q. and B. Liu. 2004. Minin\\b and Summarizin\\b Customer Reviews. Pr\\fceed\\t\\bgs \\ff the I\\bter\\bat\\t\\f\\bal C\\f\\bfere\\bce \\f\\b K\\b\\fwledge D\\tsc\\fvery a\\bd Data M\\t\\b\\t\\bg\\t pp. 168-177.","Hu\\t M.Q. and B. Liu. 2004. Minin\\b Opinion Features in Customer Reviews. Pr\\fceed\\t\\bgs \\ff the Amer\\tca\\b Ass\\fc\\tat\\t\\f\\b f\\fr Art\\tf\\tc\\tal I\\btell\\tge\\bce\\t pp. 775-760.","Liu\\t B.\\t M.Q. Hu and J.S. Chen\\b. 2005. Opinion Observer: Analyzin\\b and Comparin\\b Opinions on the Web. Pr\\fceed\\t\\bgs \\ff the I\\bter\\bat\\t\\f\\bal W\\frld W\\tde Web C\\f\\bfere\\bce\\t pp. 342-351.","Popescu\\t A. and O. Etzioni. 2005. Extractin\\b Product Features and Opinions from Reviews. Pr\\fceed\\t\\bgs \\ff the C\\f\\bfere\\bce \\f\\b Emp\\tr\\tcal Meth\\fds \\t\\b Natural La\\bguage Pr\\fcess\\t\\bg\\t pp. 339-346.","Scaffidi\\t C.\\t K. Bierhoff\\t E. Chan\\b\\t M. Felker\\t H. N\\b and C. Jin. 2007. Red Opal: Product-Feature Scorin\\b from Reviews. Pr\\fceed\\t\\bgs \\ff the 8th ACM C\\f\\bfere\\bce \\f\\b Electr\\f\\b\\tc C\\fmmerce\\t pp.182-191.","Su\\t Q.\\t X. Xu\\t H. Guo\\t Z. Guo\\t X. Wu\\t X. Zhan\\b\\t B. Swen and Z. Su. 2008. Hidden Sentiment Association in Chinese Web Opinion Minin\\b. Pr\\fceed\\t\\bgs \\ff the Seve\\btee\\bth I\\bter\\bat\\t\\f\\bal C\\f\\bfere\\bce \\f\\b W\\frld W\\tde Web\\t pp. 959-968.","Thelen\\t M. and E. Riloff. 2002. A Bootstrappin\\b Method for Learnin\\b Semantic Lexicons usin\\b Extraction Pattern Contexts. Pr\\fceed\\t\\bgs \\ff the C\\f\\bfere\\bce \\f\\b Emp\\tr\\tcal Meth\\fds \\t\\b Natural La\\bguage Pr\\fcess\\t\\bg\\t pp. 214-221.","Widdows\\t D. and B. Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. Pr\\fceed\\t\\bgs \\ff the N\\t\\betee\\bth I\\bter\\bat\\t\\f\\bal C\\f\\bfere\\bce \\f\\b C\\fmputat\\t\\f\\bal L\\t\\bgu\\tst\\tcs\\t pp. 1093-1099.  595"]}]}