{"sections":[{"title":"Hybrid \\b\\tgra\\f Probability Esti\\fation in Morphologically Rich Languages","paragraphs":["∗ ∗ ∗ ∗ "," Hyopil \\b\\tin an\\f Hyunjo You","","Department of Linguistics, \\beoul National University,","\\billim-\\fong, Gwanak-gu, \\beoul, 151-745, Korea {\\tps\\tin, you\\tyunjo}@snu.ac.kr Abstract. N-gram language mo\\feling is essential in natural language processing an\\f speec\\t processing. In morp\\tologically ric\\t languages suc\\t as Korean, a wor\\f usually consists of at least one lemma (content morp\\teme) an\\f functional morp\\temes w\\tic\\t represent various grammatical. Most wor\\f forms in Korean, \\towever, \\tave problems of sparse \\fata an\\f zero probability, because of quite complex morp\\teme combinations. T\\tus morp\\teme-base\\f N-gram mo\\feling is wi\\fely use\\f instea\\f of a wor\\f sequence mo\\feling. In t\\tis paper, we conten\\f t\\tat a morp\\teme-base\\f N-gram is inefficient language mo\\feling in t\\tat it inevitably approximates t\\te probability of unnecessary morp\\teme sequences, so t\\te longer sequences we \\tave, t\\te lower probability estimates we get. We suggest a \\tybri\\f met\\to\\f t\\tat joins wor\\f-base\\f an\\f morp\\teme-base\\f language mo\\feling. T\\te new met\\to\\f can also be regar\\fe\\f as an extension of a class-base\\f measurement. Our experimental results s\\tow t\\tat t\\te met\\to\\f pro\\fuces better probability estimation t\\tan t\\te morp\\temebase\\f measurement. Keywords: \\tybri\\f N-gram estimation, Language Mo\\feling, \\bparse Data, \\bmoot\\ting"]},{"title":"1 Introduction","paragraphs":["An N-gram mo\\fel is usually n-token sequence of wor\\fs an\\f is essential in natural language processing an\\f speec\\t processing. In morp\\tologically simple language, like Englis\\t, a bigram is a two-wor\\f sequence like machine\\b \\trans\\fa\\tion, an\\f machine\\b \\fearning. N-gram probabilities are compute\\f in terms of t\\te maximum likeli\\too\\f estimation (MLE). One usually gets t\\te MLE for t\\te parameters of an N-gram mo\\fel by normalizing counts from a corpus. T\\te MLE, \\towever, confronts major problems of sparse \\fata an\\f eventually zero probability. T\\te longer n-grams one \\tas, t\\te \\tig\\ter c\\tances of sparse \\fata one \\tas. T\\tus smoot\\ting tec\\tniques \\tave been \\fevelope\\f to get better estimates for zero or low frequency sequences.","In morp\\tologically ric\\t languages or in agglutinative languages like Korean, sparse \\fata in language mo\\feling are even more serious. \\bince wor\\fs are forme\\f by combining lemmas an\\f various affixes toget\\ter. For example, t\\te bigram haksaying-i\\bka-n\\ta ‘stu\\fent-subject marker, go-sentence final en\\fing’ \\tas \\fifferent count from anot\\ter bigram haksaying-i\\bka-ss-\\ta ‘stu\\fent-subject marker, go-past-sentence’ 1",", even t\\toug\\t t\\tere is only one morp\\teme \\fifference. In typical N-gram mo\\feling, bigrams wit\\t t\\te same lemmas but wit\\t slig\\ttly \\fifferent affixes \\fo not get t\\te same counts.","Due to t\\te agglutinative c\\taracteristics in Korean, most N-gram mo\\feling is base\\f on morp\\temes not wor\\fs. T\\tus, instea\\f of t\\te bigram, haksayng-i\\bka\\ta ‘stu\\fent go’, t\\tree bigram sequences suc\\t as ‘haksaying\\bi’, ‘i\\bka’, an\\f ‘ka\\b\\ta’ are consi\\fere\\f. \\bince t\\te wor\\f forms were broken into morp\\temes, t\\te sequence of morp\\temes woul\\f \\tave \\tig\\ter counts t\\tan in t\\te wor\\f bigram. Morp\\teme-base\\f bigrams, \\towever, cannot \\firectly compute t\\te MLE between two \\u \\u  Copyrig\\tt 2009 by Hyopil \\b\\tin an\\f Hyunjo You 1 A \\typ\\ten is use\\f to separate a stem from affixes for a visual purpose. 511 23rd Pacific Asia Conference on Language, Information and Computation, pages 511–520 lemmas, except w\\ten two wor\\fs \\tave no affixes, like haksayng\\b yokeum ‘stu\\fent fare’. If one uses a trigram in haksayng-i\\b ka-n\\ta, instea\\f of a bigram, t\\te ‘haksayng i\\b ka’ sequence is consi\\fere\\f. But t\\te sequence woul\\f \\tave a lower frequency t\\tan t\\tat of bigrams. Anot\\ter problem originates from t\\tat morp\\teme-base\\f N-grams inevitably generates an unnecessary morp\\teme sequence like ‘i\\b ka’ in t\\te example above. T\\te sequence may assign an improper amount of probability to unseen N-grams.","Instea\\f of t\\te prevalent N-gram mo\\feling use\\f in Korean, we suggest a new \\tybri\\f met\\to\\f of wor\\f an\\f morp\\teme-base\\f N-grams. T\\te met\\to\\f takes a\\fvantage of t\\te agglutinative nature of t\\te Korean language an\\f utilizes class-base\\f N-gram mo\\feling. We make use of a variablelengt\\t N-gram mo\\fel in accor\\fance wit\\t t\\te structure of wor\\f sequences. We focus on lemmas in wor\\f sequences an\\f get probability estimates from lemma bigrams or functional morp\\teme-lemma combinations. T\\tis met\\to\\f also works well wit\\t unknown wor\\fs, since probabilities of unseen wor\\fs are also approximate\\f by variable-lengt\\t N-grams."]},{"title":"2 The Korean Morphology","paragraphs":["Korean is an agglutinative language w\\tose wor\\fs are forme\\f by joining morp\\temes toget\\ter. Two broa\\f classes of morp\\temes are usually \\fistinguis\\te\\f: stems an\\f affixes. T\\te stem is t\\te main morp\\teme of t\\te wor\\f, provi\\fing t\\te main meaning, w\\tile t\\te affixes a\\f\\f a\\f\\fitional grammatical or lexical meanings. \\btems or content morp\\temes in Korean usually represent major parts-of-speec\\t suc\\t as nouns, verbs, an\\f a\\fverbs. Affixes or functional morp\\temes contribute to various inflections an\\f \\ferivations. In morp\\tologically ric\\t languages, functional morp\\temes are notorious for t\\teir multiple combinations. For example, t\\te following sentence,","","(1) sal + a + ci + eo + o + ass + um + ey + to live+connective+auxverb+connective+auxverb+past+nominalize\\f+a\\fverbialparticle+a\\fverbial particle “even t\\toug\\t (somebo\\fy)\\tas been living”"," consists of one content morp\\teme, two auxiliary verbs an\\f six functional morp\\temes. Two auxiliary verbs increase t\\te size of t\\te morp\\teme sequences an\\f expan\\f t\\te meaning of t\\te main verb, sa\\f ‘live’, to passive an\\f progressive moo\\f.","Unlike Englis\\t, grammatical relations in Korean are realize\\f by various affixes, so wor\\f or\\fer of Korean is relatively free. T\\te subject in an Englis\\t sentence is \\fetermine\\f by t\\te position in a sentence an\\f a subject comes before a main verb. W\\tile in Korean, a subject is realize\\f as a stem wit\\t a subject marker, -i/-ka, t\\tus t\\te position is quite flexible. Verbs an\\f nouns in Korean can \\tave several \\tun\\fre\\fs of forms counting inflections an\\f \\ferivations.","Functional morp\\temes s\\tow certain or\\fers especially in verbal inflections. Pre-wor\\ffinal morp\\temes take prece\\fence over wor\\f-final morp\\temes. Insi\\fe a sequence of pre-wor\\ffinal morp\\temes, an \\tonorific morp\\teme prece\\fes tense morp\\temes, an\\f tense morp\\temes prece\\fe some mo\\fal morp\\temes. Functional morp\\temes can be combine\\f, but t\\tere are certain constraints on morp\\teme combinations in accor\\fance wit\\t grammatical functions an\\f categories."]},{"title":"3 Motivations","paragraphs":["Most Korean N-gram probabilities \\tave been estimate\\f not by wor\\f sequences but by morp\\teme sequences because wor\\f forms usually \\tave multiple morp\\temes, so unseen N-grams \\frastically increase. Let’s consi\\fer an example, haksayng-man\\bcwu-nun\\tey “stu\\fent-only give-connective en\\fing”. T\\te bigram probability of haksayng-man cwu-nun\\tey ‘give to only stu\\fent’ coul\\f not be estimate\\f by counting t\\te numbers of t\\te wor\\f sequence. Instea\\f, morp\\teme sequences from a morp\\tological analysis are use\\f for t\\te estimation base\\f on t\\te Markov assumptions. 512  (2)","Phaksay\\b\\t \\f cwu","P haksay\\b\\t P\\f|haksay\\b\\t","Pcwuhaksay\\b\\t \\f P|haksay\\b\\t \\f cwu","P haksay\\b\\t \\f|haksay\\b\\t | P|cwu ","","T\\te wor\\f form, haksaying-man cwu-nun\\tey, woul\\f \\tave very low frequency counts or zero counts, but morp\\teme sequences, on t\\te contrary, woul\\f \\tave more counts because t\\te wor\\f form splits into several morp\\temes.","T\\te wi\\fely use\\f morp\\teme-base\\f estimation, \\towever, \\tas several problems. Firstly, it intro\\fuces linguistically meaningless morp\\teme sequences. In (2), a con\\fitional probability P(cwu ‘give’\\b|man\\b‘only’) is compute\\f for a w\\tole probability, but it lacks linguistic significance. Also t\\te morp\\teme-base\\f probability may not be a real probability, because it woul\\f be a\\f\\fing extra probability mass into t\\te equation. T\\te above, P(cwu ‘give’\\b|man\\b‘only’) is one case. \\becon\\fly, t\\te morp\\teme-base\\f estimation requires linguistic knowle\\fge suc\\t as parts-of-speec\\t, grammatical relations an\\f so on. T\\tus a morp\\tological parsing is essential for N-gram estimation. T\\te results from t\\te morp\\tological analysis, \\towever, may vary. T\\te same sequence of morp\\temes can be \\fifferently segmente\\f off accor\\fing to t\\te morp\\tological parsers. In t\\tis case, t\\te probabilities of t\\te same bigram woul\\f be \\fifferent. Anot\\ter problem relate\\f to a morp\\tological analysis is t\\tat as state\\f in section 2, morp\\tological parsers generally restore an original morp\\teme of t\\te morp\\tologically contracte\\f an\\f transforme\\f forms, so t\\te probabilities of \\ti\\f\\fen morp\\teme sequences in t\\te surface form coul\\f be estimate\\f. Lastly, t\\te longer affixes we \\tave, t\\te lower t\\te probability we \\tave, since a long sequence of morp\\temes nee\\fs a longer c\\tain rule of probability an\\f more multiplications inevitably \\tave lower probabilities.","On t\\te contrary, we can consi\\fer anot\\ter extreme; lemmatization. A lemma is a set of lexical forms \\taving t\\te same stem. We strip off affixes to \\tave a lemma sequence suc\\t as haksaying\\b cwu ‘stu\\fent give’. T\\tis met\\to\\f is an extension of wor\\f-base\\f estimation. However, it oversimplifies bigram sequences. T\\tus, w\\tenever two lemmas are i\\fentical, we will \\tave t\\te same probabilities regar\\fless of t\\te fact t\\tat totally \\fifferent affixes were attac\\te\\f to t\\te sequences.","As preliminary work, we measure\\f perplexities of t\\te two language mo\\fels; wor\\f-base\\f an\\f morp\\teme-base\\f estimation. We took t\\te \\bejong morp\\tologically an\\f semantically tagge\\f corpus consisting of about 800K running wor\\fs2",". We c\\tose 10 sentences as a test set an\\f measure\\f bigram perplexities of t\\te two mo\\fels using t\\te \\bRILM Toolkit3",". We conclu\\fe\\f t\\tat t\\te number of unseen bigrams resulting from t\\te wor\\f-base\\f estimation grew constantly, t\\tus we coul\\fn’t possibly c\\toose t\\te estimation, so we woul\\f like a new mo\\fel to \\fo somet\\ting reasonable wit\\t unseen bigrams."]},{"title":"4 Related Work","paragraphs":["Most work on N-gram mo\\feling in Korean \\tas been \\fone wit\\t morp\\teme-base\\f met\\to\\fs. We will briefly review two relate\\f efforts.","Kwon (2000) compare\\f morp\\teme-base\\f recognition units wit\\t syllable-base\\f recognition units for t\\te performance of large vocabulary continuous speec\\t recognition (LVC\\bR). For t\\te morp\\teme-base\\f met\\to\\f, Kwon (2000) merge\\f morp\\teme units to re\\fuce recognition errors. T\\tus, s\\tort morp\\temes were merge\\f wit\\t one consonant, a stem an\\f en\\fings of auxiliary verbs were merge\\f, an\\f a suffix an\\f t\\te following particle were also merge\\f. For t\\te syllable-base\\f \\u \\u 2 T\\te \\bejong Corpus is one of 8 sub-projects of t\\te 21st","\\bejong Project fun\\fe\\f by t\\te Korean government an\\f \\fevelope\\f for 10 years. T\\te Corpus collecte\\f a wi\\fe range of unconstraine\\f materials an\\f en\\feavore\\f at annotating t\\te \\fata wit\\t part-of-speec\\t, morp\\tological, syntactic, an\\f semantic tags. 3 \\tttp://www.speec\\t.sri.com/projects/srilm/ 513 unit, only text corpus an\\f its pronunciation were use\\f. Any linguistic knowle\\fge suc\\t as parts-of-speec\\t information an\\f language-specific rules in t\\te morp\\teme-base\\f unit was not require\\f. Kwon (2000) conclu\\fe\\f t\\tat t\\te statistical merging met\\to\\f wit\\t appropriate linguistic constraints yiel\\fe\\f t\\te best recognition accuracy, an\\f t\\te syllable-base\\f approac\\t \\fi\\f not s\\tow comparable performance. T\\te syllable-base\\f met\\to\\f is t\\te same as t\\te wor\\f-base\\f met\\to\\f.","Park et al. (2007) suggeste\\f a met\\to\\f t\\tat a\\fjuste\\f t\\te improperly assigne\\f probabilities of unseen-N grams by taking a\\fvantage of t\\te agglutinative c\\taracteristics of t\\te Korean language. T\\tey argue\\f t\\tat t\\te grammatically proper class of a morp\\teme coul\\f be pre\\ficte\\f by knowing t\\te previous morp\\teme. By using t\\tis c\\taracteristic, t\\tey trie\\f to prevent grammatically improper N-grams from ac\\tieving relatively \\tig\\t probability an\\f to assign more probability mass to proper N-grams. T\\tat is, t\\te mo\\fel re\\fuce\\f t\\te probabilities of unseen N-grams t\\tat violate grammatical constraints w\\tile \\fistributing more probabilities to grammatically correct unseen N-grams. T\\tey use\\f a part-of-speec\\t tagge\\f morp\\teme as t\\te N-gram mo\\fel unit because some morp\\temes were ambiguous w\\ten morp\\temes were classifie\\f into content morp\\temes an\\f functional morp\\temes.","T\\te met\\to\\f is similar to our \\tybri\\f estimation in t\\tat it utilize\\f a morp\\teme sequence type suc\\t as content morp\\teme an\\f functional morp\\teme occurrences after a specific PO\\b. But t\\tey focuse\\f only on t\\te probabilities of PO\\b an\\f morp\\teme types, an\\f trie\\f to re\\fistribute more probabilities to grammatically correct unseen N-grams. T\\tis was \\fue to t\\te fact t\\tat pure morp\\teme base\\f approac\\t inevitably overestimates probabilities of morp\\teme sequences in t\\te case of unseen N-grams."]},{"title":"5 The Hybrid \\b\\tgra\\f Probability Esti\\fation 5.1 Le\\f\\fa\\tMorphe\\fe\\tbased Probability Measure\\fent","paragraphs":["We reviewe\\f two mo\\fels of N-gram probability estimation in section 4. T\\te wor\\f-base\\f estimation is a typical measurement in N-gram mo\\feling, but in morp\\tologically complex languages, it may not turn out to be true, because of large numbers of unseen N-grams. Also t\\te wor\\f-base\\f met\\to\\f \\tas a spacing problem in Korean. A wor\\f, eojeo\\f in a Korean term, is separate\\f from anot\\ter wor\\f by a space. But t\\te spacing is not strictly observe\\f. T\\te morp\\teme-base\\f measurement, on t\\te contrary, is common but intro\\fuces unnecessary morp\\teme sequences w\\tic\\t may overestimate t\\te w\\tole probability.","As a new measurement, we suggest a lemma-morp\\teme–base\\f N-gram probability. T\\te new met\\to\\f stan\\fs mainly on t\\te wor\\f or lemma-base\\f estimation an\\f incorporates benefits from morp\\teme-base\\f estimation. We can illustrate t\\te met\\to\\f as follows."," "," Figure 1\\ta: Morp\\teme-base\\f estimation    Figure 1\\tb: Hybri\\f estimation ","Let’s assume t\\tat a wor\\f consists of a lea\\fing lexical morp\\teme an\\f a following cluster of grammatical morp\\temes. T\\te new met\\to\\f is basically wor\\f-base\\f, but morp\\temebase\\f mo\\feling is also use\\f insi\\fe a wor\\f. As seen in figure 1-a, morp\\teme sequences form bigrams. We can now generalize insi\\fe-a-wor\\f estimation as follows. \\t, \\t, ... , \\t is a cluster of . 514 (3) ... ||... |","","In t\\te new met\\to\\f, \\towever as s\\town in figure 1-b, a lemma plays a barrier to t\\te functional morp\\temes-lemma sequence. Instea\\f of joining a functional morp\\teme an\\f a lemma, a Li-1-Li lemma sequence is forme\\f. In t\\te previous example, haksayng-man cwu-nun\\tey ‘give to only stu\\fent’, t\\te combination man ‘only’ wit\\t cwu ‘give’ is blocke\\f, an\\f t\\te lemma sequence, haksayng\\bcwu ‘stu\\fent give’ is forme\\f instea\\f. T\\ten t\\te P(cwu|haksayng) is approximate\\f. T\\tis lemma combination is a kin\\f of ‘variable-lengt\\t N-gram mo\\feling’ because functional morp\\temes between t\\te two lemmas can be any size, so t\\te lemma combination may be any N-grams.","We can furt\\ter generalize t\\te lemma sequences. First of all, let’s consi\\fer t\\te following \\fefinition of class-base\\f N-grams (Brown et al., 1992)","","(4) | || |   |  ∑"," We can apply t\\te class-base\\f N-gram to t\\te \\tybri\\f met\\to\\f. We exten\\f t\\te P(cwu|haksayng)","to P(cwu|haksayng*). Here haksayng* is a class w\\tic\\t inclu\\fes all t\\te wor\\f forms of haksayng,","suc\\t as haksayng ‘stu\\fent’, haksayng-i ‘stu\\fent-subject marker’, haksayng-eykey ‘to stu\\fent’,","\\taksayng-u\\f ‘stu\\fent-object marker’, an\\f haksayng-man-u\\f ‘only stu\\fent-object-marker’. We","can re\\fefine t\\te class-base\\f N-gram equations in example (4) as follows. Here","is any wor\\f","wit\\t t\\te lemma .","is t\\te class of , an\\f G is a sequence of grammatical morp\\temes.  (5)","| |","","| ","|    |","","|","",""," ∑"," ","∑  "," Now, our example haksayng-man cwu-nun\\tey ‘give to only stu\\fent’ is compute\\f as follows.  (6)","Pcwu \\bu\\btey|haksay\\b\\t ma\\b P cwu","|haksay\\b\\t","Pcwu \\bu\\btey|cwu","","P cwu","|haksay\\b\\t P\\bu\\btey|cwu","Pcwu\\bu\\btey|cwu P \\bu\\btey|cwu","","Pcwu","|haksay\\b\\t","Chaksay\\b\\t","cwu","","∑ Chaksay\\b\\t","L"," ","∑ Chaksay\\b\\t G cwu Chaksay\\b\\t   Finally we can reac\\t t\\te following generalization of t\\te \\tybri\\f estimation.  (7)","| ... | ...","","|","","| "," |","||... 515","We woul\\f like to point out t\\tat t\\tis approac\\t not only a\\fmits an importance of morp\\teme sequences in N-gram estimation, but also tries to minimize overestimation of t\\te morp\\teme sequence probabilities. T\\tis means t\\tat a c\\taining of morp\\teme sequences \\tas only local effects wit\\tin a wor\\f, t\\tus t\\te met\\to\\f prevents unnecessary morp\\teme sequences from being compute\\f an\\f expan\\fe\\f. An\\f, by putting two lemmas toget\\ter, we can take into account wor\\f sequences as well. "]},{"title":"5.2 Probability Esti\\fation for Unknown Words","paragraphs":["T\\te \\tybri\\f estimation also works well in unseen events w\\tere certain bigrams \\tave zero counts. Let’s consi\\fer a new example, haksaying-\\tu\\f-eykey-man cwu-si-ess-nun\\tey ‘give-\\tonorific-past \\to\\b on\\fy\\b s\\tuden\\ts-plural’. Assuming t\\tat eac\\t wor\\f form, suc\\t as haksaying-\\tu\\f-eykey-man an\\f cwu-si-ess-nun\\tey is unknown, we can approximate eac\\t wor\\f’s probability using t\\te equation s\\town in example (3).","","(8) a. Phaksay\\b\\t\\f","P haksay\\b\\tP|haksay\\b\\tP| P\\f|","b. Pcwu P cwuPsi|cwuPess|siP\\bu\\btey|ess  T\\te probability of an unknown wor\\f is estimate\\f wit\\t t\\te insi\\fe-a-wor\\f equation w\\tic\\t is","basically t\\te same as morp\\teme-base\\f estimation. Accor\\fing to our experiment, all t\\te","morp\\teme sequences \\tave counts in t\\te training corpus. Now consi\\fer t\\te case w\\tere only one wor\\f is unknown in t\\te bigram, haksaying-\\tu\\f-eykey-","man cwu-si-ess-nun\\tey ‘give-honorific-pas\\t to only stu\\fents-p\\fura\\f’. If haksaying-\\tu\\f-eykey-man","is unseen, we can estimate t\\te bigram probability as follows.  (9) Phaksay\\b\\t\\f cwu","= Phaksay\\b\\t\\f |  We estimate\\f t\\te probability of t\\te unknown \\taksaying-\\tu\\f-eykey-man by equation 8-a, so","we nee\\f to compute a con\\fitional probability bol\\fface\\f in equation (9). Below we can use t\\te","c\\tain rule to expan\\f t\\te sequence.  (10) Pcwu|haksay\\b\\t\\f = Pcwu|haksay\\b\\ttuleykeyma\\b Psi|haksay\\b\\ttuleykeyma\\b cwu Pess|haksay\\b\\ttuleykeyma\\b cwusiP\\bu\\btey|haksay\\b\\ttuleykeyma\\b cwusiess","P cwu|haksay\\b\\ttuleykeyma\\bPsi|cwuPess|siP\\bu\\btey|ess  T\\te problem \\tere is \\tow we can approximate t\\te Pcwu|haksay\\b\\ttuleykeyma\\b. T\\te","morp\\teme-base\\f met\\to\\f woul\\f \\tave t\\te bigram estimation as follows.  (11) Pcwu|haksay\\b\\ttuleykeyma\\b Pcwu|ma\\b  T\\tis results in a useless sequence. Instea\\f, we can apply our \\tybri\\f met\\to\\f an\\f \\tave t\\te","following estimation.  (12) Pcwu|haksay\\b\\ttuleykeyma\\b Pcwu|haksay\\b\\t"]},{"title":"6 Experi\\fents","paragraphs":["We performe\\f two main experiments. One experiment compare\\f t\\te morp\\teme-base\\f estimation an\\f t\\te \\tybri\\f N-gram estimation accor\\fing t\\te lengt\\t of a sentence. T\\te subsequent experiment compare\\f t\\te two met\\to\\fs accor\\fing to t\\te training size. We use\\f t\\te 21st","\\bejong 516 morp\\tologically an\\f semantically tagge\\f corpus. For t\\te \\tybri\\f N-gram computation, we converte\\f t\\te \\fata into ARPA format of t\\te \\bRILM Toolkit.","For t\\te first experiment, out of about 90K sentences, we ran\\fomly selecte\\f 941 test sentences wit\\t 0.01probability, an\\f we compute\\f bigram probabilities of t\\te sentences. For t\\te secon\\f experiment, we \\fivi\\fe\\f t\\te training \\fata into 10 sets w\\tose size range\\f from 1K to 512K. T\\ten we teste\\f 1,000 sentences from eac\\t set an\\f we got 10,000 results. T\\te following table s\\tows a fragment from t\\te first experiment."," Table 1: A part of results of t\\te experiment"," nnnn p1p1p1p1 p2p2p2p2 ppppp1p1p1p1 ppppp2p2p2p2 8888 -19.608 -20.413 282.50 356.17 35353535 -67.011 -53.534 82.15 33.85 43434343 -93.001 -79.231 145.49 69.60 22222222 -41.804 -38.846 79.46 58.30 55555555 -104.709 -86.221 80.13 36.95 38383838 -75.332 -62.034 96.03 42.90 46464646 -83.680 -68.661 65.94 31.09 13131313 -22.547 -23.737 54.25 66.97 5555 -12.024 -12.024 253.98 253.98 29292929 -53.523 -47.763 70.08 44.36 ","We only liste\\f 10 sentences out of a total of 941 sentences in Table 1. All probabilities are log probabilities. T\\te first column n means t\\te number of morp\\temes in a sentence. An\\f t\\te secon\\f an\\f t\\te t\\tir\\f column, p1 an\\f p2 specify morp\\teme-base\\f estimation an\\f \\tybri\\f estimation respectively. Pp1 an\\f pp2 s\\tow t\\te perplexities of t\\te p1 an\\f p2. Figure 2 s\\tows t\\te two perplexities."," Figure 2: Perplexity by lengt\\t of a sentence  T\\tis s\\tows t\\tat t\\te \\tybri\\f met\\to\\f gives us better estimation regar\\fless of t\\te lengt\\t of t\\te sentence. We observe\\f t\\tat t\\te longer t\\te sentence on w\\tic\\t we estimate t\\te probability, t\\te bigger \\fifference t\\te probability. T\\tis s\\tows t\\tat t\\te morp\\teme-base\\f met\\to\\f pro\\fuces poor 0 20 40 60 80 100 120 140 0 1 0 0 2 0 0 3 0 0 4 0 0 \\b\\tng\\fh of s\\tn\\f\\tnc\\t (numb\\tr of morph\\tm\\ts) P \\t r p \\b \\t x i \\f y p\\bain \\b\\tx\\tm-bas\\td 517 estimates w\\ten morp\\teme sequences get longer because unnecessary morp\\teme sequences occur more in a longer sentence t\\tan in a s\\torter sentence. Table 2 specifies t\\te results accor\\fing to t\\te size of t\\te training set."," Table 2: T\\te result from t\\te secon\\f experiment                  T\\te perplexities of pp1 from t\\te morp\\teme-base\\f estimation are \\tig\\ter t\\tan t\\te perplexities","of pp2 from t\\te \\tybri\\f estimation. T\\tus we can conclu\\fe t\\tat t\\te better mo\\fel is t\\te \\tybri\\f one","t\\tat \\tas a tig\\tter fit to t\\te test \\fata. Figure 3 s\\tows t\\te perplexities grap\\tically. ","Figure 3: Perplexities by t\\te training set size"," Figure 3, \\towever, \\foes not s\\tow t\\te probability \\fistributions at t\\te position of eac\\t training","\\fata size. T\\tus, we bean-plotte\\f t\\te \\fata as s\\town in Figure 4. T\\te beanplot s\\tows \\tow 1,000","\\fata from eac\\t training set are \\fistribute\\f (Kampstra 2008). 1 2 5 10 20 50 100 500 5 1 0 2 0 5 0 1 0 0 \\frainning s\\t\\f siz\\t P \\t r p \\b \\t x i \\f y morph\\tm\\t-bas\\td hybrid \\fraining s\\t\\f pp1 pp2 1K 26.82270 6.8\\b\\t\\b77 2K \\b2.05\\t48 8.742\\f55 4K \\b7.\\f88\\f6 \\t0.\\f\\b762 8K 45.\\f2444 \\t\\b.\\f\\f782 16K 5\\b.\\f0555 \\t7.86562 32K 6\\t.7206\\b 2\\b.477\\t2 64K 6\\f.05248 2\\f.\\b6778 128K 75.854\\t2 \\b6.\\t7477 256K 84.055\\f\\t 4\\t.7\\f22\\b 512K \\f4.\\t407\\f 46.\\f\\f\\b20 518"," Figure 4: Perplexity \\fistribution by t\\te training \\fata size  As s\\town in Figure 4, t\\te two met\\to\\fs \\tave largely \\fifferent \\fistributions, even t\\toug\\t t\\tere","are some overlaps in t\\te bigger size of t\\te training \\fata. T\\tis also vali\\fates our conclusions."]},{"title":"7 Discussion and Conclusions","paragraphs":["T\\tus far, we explaine\\f t\\te new met\\to\\f of N-gram probability estimation in morp\\tologically complex languages. \\bomeone may argue t\\tat t\\te \\tybri\\f met\\to\\f also \\tas a problem of unnecessary morp\\teme sequences in t\\tat insi\\fe a wor\\f, t\\te \\tybri\\f an\\f t\\te morp\\teme-base\\f met\\to\\f approximates t\\te same sequence of morp\\temes, t\\tus t\\tere will be no big \\fifferences between t\\te two.","T\\te \\tybri\\f met\\to\\f, \\towever, can pro\\fuce a better estimation of morp\\teme sequences in a wor\\f. Accor\\fing to our experiment, t\\te average number of morp\\temes in a Korean wor\\f is 2.5. T\\tis means t\\tat a wor\\f consists of about 2.5 morp\\temes inclu\\fing lemmas. Example (1) consisting of 9 morp\\temes is an extreme case. An\\f if we investigate t\\te structure of t\\te sequence, we can figure out t\\tat auxiliary verbs are require\\f for longer morp\\teme sequences. In t\\te case of (1), two auxiliary verbs, ci ‘passive morp\\teme’ an\\f o, ‘progressive morp\\teme’ are combine\\f. Wit\\tout auxiliary verbs, longer morp\\teme sequences are not usually permitte\\f. Also spacing is not quite strict in Korean. \\bomeone may separate auxiliary verbs from t\\te main verb, so t\\te sequence, sa\\facieoassumey\\to ‘even t\\toug\\t (somebo\\fy)\\tas been living’ can be split into t\\tree wor\\fs like sa\\fa\\bcie\\boassumey\\to ‘even t\\toug\\t (somebo\\fy)\\tas been living’.","We take only affixes as functional morp\\temes. \\bo auxiliary verbs are content morp\\temes an\\f can be a barrier to t\\te functional morp\\teme an\\f lemma combination. Even t\\toug\\t no spaces appear in t\\te structure, our \\tybri\\f met\\to\\f approximates t\\te probabilities of t\\te P(ci|sa\\f*) an\\f t\\te P(o|ci*), instea\\f of t\\te P(ci|a) an\\f t\\te P(o|a) respectively. T\\te lemma an\\f class-base\\f estimation can give us a better estimate of t\\te true probability of Korean.","Our next job is to incorporate t\\te language mo\\fel into a speec\\t recognition system an\\f see \\tow t\\te new met\\to\\f contributes to t\\te overall performance. We strongly believe t\\tat t\\te \\tybri\\f met\\to\\f can be a better N-gram estimator, bot\\t for natural language processing, an\\f for speec\\t processing in morp\\tologically ric\\t languages.","","",""," 1 2 5 1 0 2 0 5 0 1 0 0 1 2 4 8 16 32 64 2561 2 4 8 16 32 64 256 morph\\tm\\t-bas\\td hybrid 519"]},{"title":"References","paragraphs":["Brown, Peter F. an\\f Vincent J. Della Pietra. 1992. Class-Base\\f N-gram Mo\\fels of Natural Language. Compu\\ta\\tiona\\f\\bLinguis\\tics, 18(4):467-479. C\\ten, \\b. 1996. Bui\\fding\\b Probabi\\fis\\tic\\b Mode\\fs\\b for\\b Na\\tura\\f\\b Language, P\\tD. T\\tesis. Harvar\\f University. Jelinek, Fre\\ferick. 1998. S\\ta\\tis\\tica\\f\\bMe\\thods\\bfor\\bSpeech\\bRecogni\\tion. T\\te MIT Press. Jurafsky, Daniel an\\f James H. Martin. 2008. Speech\\band\\bLanguage\\bProcessing-An\\bIn\\troduc\\tion\\b \\to\\b Na\\tura\\f\\b Language\\b Processing,\\b Compu\\ta\\tiona\\f\\b Linguis\\tics,\\b and\\b Speech\\b Recogni\\tion, 2n\\f"," e\\fition. Pearson E\\fucation.","Justo, Raquel an\\f M. Ines Torres. 2007. Wor\\f \\begments in Category-Base\\f Language Mo\\fels from Automatic \\bpeec\\t Recognition, Lec\\ture\\b No\\tes\\b in\\b Compu\\ter\\b Science 4477:249-256. \\bpringer-Verlag.","Kampstra, Peter. 2008. Beanplot: A Boxplot Alternative for Visual Comparison of Distributions. Journa\\f\\bof\\bS\\ta\\tis\\tica\\f\\bSof\\tware, volume 28.","Kwon, O\\t-Wook. 2000. Performance of LVC\\bR wit\\t Morp\\teme-base\\f an\\f \\byllable-base\\f Recognition Units. 2000\\b IEEE\\b In\\terna\\tiona\\f\\b Conference\\b on\\b Acous\\tics,\\b Speech\\b and\\b Signa\\f\\b Processing, volume 3:1567-1570.","Manning, C\\tristop\\ter D. an\\f Hinric\\t \\bc\\tutze. 1999. Founda\\tions\\b of\\b S\\ta\\tis\\tica\\f\\b Na\\tura\\f\\b Language\\bProcessing. T\\te MIT Press.","Niesler, T.R. an\\f P.C. Woo\\flan\\f. 1996. A Variable-lengt\\t Category-base\\f N-gram Language Mo\\fel. 1996\\b In\\terna\\tiona\\f\\b Conference\\b on\\b \\the\\b Acous\\tics,\\b Speech,\\b and\\b Signa\\f\\b Processing, volume 3:164-167.","Park, Jae-Hyun, Young-In \\bong, an\\f Hae-C\\tang Rim. 2007. \\bmoot\\ting Algorit\\tm for N-Gram Mo\\fel Using Agglutinative C\\taracteristic of Korean. Proceedings\\b of\\b \\the\\b In\\terna\\tiona\\f\\b Conference\\bon\\bSeman\\tic\\bCompu\\ting:397-404.","Rosenfel\\f, R. an\\f X. Huang. 1991. Improvements in \\btoc\\tastic Language Mo\\feling, HLT\\b’91:\\b Proceedings\\bof\\b\\the\\bWorkshop\\bon\\bSpeech\\band\\bNa\\tura\\f\\bLanguage:107-111.  520"]}]}