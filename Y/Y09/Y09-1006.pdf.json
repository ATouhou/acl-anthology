{"sections":[{"title":"Generatin\\b\\tChine\\fe\\tCouplet\\f\\tand\\tQuatrain\\tU\\fin\\b\\ta\\t Stati\\ftical\\tApproach\\t∗∗∗∗\\t \\t","paragraphs":["Ming Zh\\b\\ta , L\\bng \\fianga",", and \\fing Heb ","a","Micr\\bsft Research Asia,","Sigma Centre, Haidian, Beijing, 100109, P.R. China","{mingzh\\b\\t, l\\bngj}@micr\\bs\\bft.c\\bm","b","Dept. \\bf C\\bmp\\tter Science, Tsingh\\ta University","hejing2929@gmail.c\\bm"," Ab\\ftract. We pr\\bp\\bse a n\\bvel statistical appr\\bach t\\b a\\tt\\bmatically generate Chinese c\\b\\tplets and Chinese p\\betry. F\\br Chinese c\\b\\tplets, the system takes as inp\\tt the first sentence and generates as \\b\\ttp\\tt an N-best list \\bf sec\\bnd sentences \\tsing a phrase-based SMT m\\bdel. A c\\bmprehensive eval\\tati\\bn \\tsing b\\bth h\\tman j\\tdgments and BLEU sc\\bres has been c\\bnd\\tcted and the res\\tlts dem\\bnstrate that this appr\\bach is very s\\tccessf\\tl. We then extended this appr\\bach t\\b generate classic Chinese p\\betry \\tsing the q\\tatrain as a case st\\tdy. Given a few keyw\\brds describing a \\tser's intenti\\bn, a statistical m\\bdel is \\tsed t\\b generate the first sentence. Then a phrase-based SMT m\\bdel is \\tsed t\\b generate the \\bther three q\\tatrain sentences \\bne by \\bne. Eval\\tati\\bn \\tsing h\\tman j\\tdgment \\bver individ\\tal lines as well as the q\\tality \\bf the generated p\\bem as a wh\\ble dem\\bnstrates pr\\bmising res\\tlts. Keyword\\f:\\tChinese c\\b\\tplets, Chinese classic p\\betry, a\\tt\\bmatic generati\\bn.","\\a \\a ∗ This paper s\\tmmarizes \\b\\tr w\\brk \\bn a\\tt\\bmatic generati\\bn \\bf Chinese c\\b\\tplets (\\fiang & Zh\\b\\t, 2008) and a\\tt\\bmatic generati\\bn \\bf p\\betry (He et al., t\\b appear), b\\bth were c\\bnd\\tcted in Nat\\tral Lang\\tage Gr\\b\\tp at Micr\\bs\\bft Research Asia.  C\\bpyright 2009 by Ming Zh\\b\\t, L\\bng \\fiang, and \\fing He"]},{"title":"1 Introduction\\t","paragraphs":["This paper presents a n\\bvel appr\\bach t\\b generate Chinese c\\b\\tplets and classic Chinese p\\betry with a Statistical Machine Translati\\bn (SMT) m\\bdel. Chinese antithetical c\\b\\tplets, called “d\\tìlián”, f\\brm a special type \\bf p\\betry c\\bmp\\bsed \\bf tw\\b sentences. The tw\\b sentences making \\tp a c\\b\\tplet are called the “first sentence” (FS) and the “sec\\bnd sentence” (SS) respectively. The q\\tatrain and reg\\tlated verse are the tw\\b m\\bst imp\\brtant f\\brms \\bf classic Chinese p\\betry. A q\\tatrain is c\\bmp\\bsed \\bf 4 lines and reg\\tlated verse is c\\bmp\\bsed \\bf 8 lines with additi\\bnal r\\tles g\\bverning str\\tct\\tre. Each line \\bf the q\\tatrain \\br reg\\tlated verse is c\\bmp\\bsed \\bf 5 Chinese characters (五言) \\br 7 characters (七言).","F\\br Chinese c\\b\\tplets, the general pr\\bcess \\bf generating a SS given a FS is like this: f\\br each w\\brd in the FS, find s\\bme w\\brds that can be \\tsed as the c\\b\\tnterparts in the SS; then fr\\bm the w\\brd lattice, select \\bne w\\brd at each p\\bsiti\\bn in the SS s\\b that the selected w\\brds f\\brm a fl\\tent sentence satisfying the c\\bnstraints \\bf Chinese c\\b\\tplets. This pr\\bcess is similar t\\b translating a s\\b\\trce lang\\tage sentence int\\b a target lang\\tage sentence with\\b\\tt w\\brd inserti\\bn, deleti\\bn and re\\brdering, b\\tt the target sentence sh\\b\\tld satisfy s\\bme ling\\tistic c\\bnstraints. Based \\bn this \\bbservati\\bn, we pr\\bp\\bse a m\\tlti-phase statistical machine translati\\bn appr\\bach t\\b generate the SS. First, a phrase-based statistical machine translati\\bn (SMT) m\\bdel is applied t\\b generate an 43 23rd Pacific Asia Conference on Language, Information and Computation, pages 43–52  N-best list \\bf SS candidates. Then, a set \\bf filters based \\bn ling\\tistic c\\bnstraints f\\br Chinese c\\b\\tplets is \\tsed t\\b rem\\bve l\\bw q\\tality candidates. Finally, a Ranking SVM is applied t\\b rerank the candidates. A c\\bmprehensive eval\\tati\\bn \\tsing b\\bth h\\tman j\\tdgments and BLEU sc\\bres has been c\\bnd\\tcted and the res\\tlts dem\\bnstrate that this appr\\bach is very s\\tccessf\\tl.","We extended this appr\\bach t\\b generate classic Chinese p\\betry \\tsing the q\\tatrain as a case st\\tdy. Given a few keyw\\brds describing a \\tser's intenti\\bn, a statistical m\\bdel is \\tsed t\\b generate the first sentence. Then a phrase-based SMT m\\bdel is \\tsed t\\b generate the \\bther three q\\tatrain sentences \\bne by \\bne. Preliminary eval\\tati\\bn \\tsing h\\tman j\\tdgment \\bver individ\\tal lines as well as the q\\tality \\bf the generated p\\bem as a wh\\ble dem\\bnstrates pr\\bmising res\\tlts."]},{"title":"2 Lin\\bui\\ftic\\tCon\\ftraint\\f\\tof\\tCouplet\\f\\t","paragraphs":["We \\tse an example \\bf a Chinese c\\b\\tplet t\\b explain the ling\\tistic c\\bnstraints \\bf Chinese c\\b\\tplets in Table 1 with the c\\brresp\\bndence between individ\\tal w\\brds \\bf the FS and SS. It means that the sea \\bs \\t\\bde so that \\f\\bsh jump at the\\br pleasure, and the sky \\bs h\\bgh so that b\\brd \\fl\\bes unrestr\\bctedly. \\t","Table\\t1:\\tAn example \\bf Chinese c\\b\\tplets\\t ","FS: 海(hai) 阔(k\\t\\b) 凭(pin) 鱼(y\\t) 跃(y\\te) sea wide all\\bw fish \\f\\tmp |\\t |\\t |\\t |\\t |\\t","SS: 天(tian) 高(ga\\b) 任(ren) 鸟(nia\\b) 飞(fei) sky high permit bird fly  A c\\b\\tplet m\\tst c\\bnf\\brm t\\b the f\\bll\\bwing ling\\tistic c\\bnstraints: 1) The tw\\b sentences \\bf a c\\b\\tplet agree in length and w\\brd segmentati\\bn. 2) In Chinese, every character is pr\\bn\\b\\tnced either level (平) \\br \\bbliq\\te (仄). The character at","the end \\bf the FS sh\\b\\tld be \\bbliq\\te (pr\\bn\\b\\tnced in a sharp d\\bwnward t\\bne); the character at","the end \\bf the SS sh\\b\\tld be level (pr\\bn\\b\\tnced in a level t\\bne). 3) C\\brresp\\bnding w\\brds in the tw\\b sentences sh\\b\\tld agree in their part \\bf speech and","characteristics. 4) The c\\bntents \\bf the tw\\b sentences sh\\b\\tld be related, b\\tt n\\bt d\\tplicated. 5) The tw\\b sentences sh\\b\\tld be identical in their writing styles. F\\br instance, if there is a","repetiti\\bn \\bf w\\brds, characters, \\br pr\\bn\\tnciati\\bns in the FS, the SS sh\\b\\tld c\\bntain an","identical repetiti\\bn. Given a FS, writing a g\\b\\bd SS is a diffic\\tlt task beca\\tse the SS m\\tst c\\bnf\\brm t\\b c\\bnstraints \\bn syntax, rhyme and semantics, as described ab\\bve. It req\\tires the writer t\\b inn\\bvatively \\tse extensive kn\\bwledge in m\\tltiple disciplines."]},{"title":"3 SMT-Ba\\fed\\tCouplet\\tGeneration\\tModel\\t","paragraphs":["In this paper, a m\\tlti-phase statistical machine translati\\bn (SMT) appr\\bach is designed, where an SMT system generates an N-best list \\bf candidates and then a ranking m\\bdel is \\tsed t\\b determine the new ranking \\bf the N-best res\\tlts \\tsing additi\\bnal feat\\tres. This appr\\bach is similar t\\b reranking appr\\baches \\bf SMT (Och and Ney, 2004). In \\b\\tr SMT system, a phrase-based l\\bg-linear m\\bdel is applied where tw\\b phrase translati\\bn m\\bdels, tw\\b lexical weights and a lang\\tage m\\bdel are \\tsed t\\b sc\\bre the \\b\\ttp\\tt sentences, and a m\\bn\\bt\\bne phrase-based dec\\bder is empl\\byed t\\b get the N-best res\\tlts. Then a set \\bf filters based \\bn ling\\tistic c\\bnstraints \\bf Chinese c\\b\\tplets are \\tsed t\\b rem\\bve l\\bw q\\tality candidates. Finally a Ranking SVM m\\bdel is \\tsed t\\b rerank the candidates \\tsing additi\\bnal feat\\tres like w\\brd ass\\bciati\\bns, etc.  44"]},{"title":"3.1 Phra\\fe-ba\\fed\\tSMT\\tModel\\t","paragraphs":["Given a FS den\\bted as },...,,{ 21 n\\f\\f\\fF= , \\b\\tr \\bbjective is t\\b seek a SS den\\bted as","},...,,{ 21 nsssS = , where \\f\\b and s\\b are Chinese characters, s\\b that p(S|F) is maximized. F\\bll\\bwing Och and Ney (2002) which departs fr\\bm the traditi\\bnal n\\bisy-channel appr\\bach and \\tses a m\\bre general l\\bg-linear m\\bdel, the S* that maximizes p(S|F) can be expressed as f\\bll\\bws:"]},{"title":"∑","paragraphs":["= = = M \\b \\b\\b S S FSh FSpS 1 ),(l\\bgmaxarg )|(maxarg* λ  (1)  where the h\\b(S,F) are feat\\tre f\\tncti\\bns and M is the n\\tmber \\bf feat\\tre f\\tncti\\bns. In \\b\\tr design, characters are \\tsed instead \\bf w\\brds as translati\\bn \\tnits t\\b f\\brm phrases. This is beca\\tse Chinese c\\b\\tplets \\tse dense lang\\tage m\\bstly f\\bll\\bwing the similar style \\bf ancient Chinese and m\\bst \\bf w\\brds c\\bntain \\bnly \\bne character. With this character based appr\\bach, we can av\\bid \\tnexpected err\\brs d\\te t\\b segmentati\\bn ambig\\tities and OOV (O\\tt \\bf V\\bcab\\tlary) w\\brds.","Am\\bng feat\\tres c\\bmm\\bnly \\tsed in phrase-based SMT, five feat\\tres, listed in Table 2, were selected f\\br \\b\\tr m\\bdel. T\\b apply phrase-based feat\\tres, S and F are segmented int\\b phrases","Iss ...1 and I\\f\\f...1 , respectively. We ass\\tme a \\tnif\\brm distrib\\tti\\bn \\bver all p\\bssible segmentati\\bns. Table\\t2: Feat\\tres in \\b\\tr SMT M\\bdel."]},{"title":"∏","paragraphs":["= = I \\b \\b\\bs\\fpFSh 11 )|(),( Phrase translati\\bn m\\bdel"]},{"title":"∏","paragraphs":["= = I \\b \\b\\b\\fspFSh 12 )|(),( Inverted phrase translati\\bn m\\bdel"]},{"title":"∏","paragraphs":["= = I \\b \\b\\b\\t s\\fpFSh 13 )|(),( Lexical weight"]},{"title":"∏","paragraphs":["= = I \\b \\b\\b\\t \\fspFSh 14 )|(),( Inverted lexical weight )(),(5 SpFSh = Lang\\tage m\\bdel  Phra\\fe\\ttran\\flation\\tmodel\\t(PTM)\\t In a phrase-based SMT m\\bdel, phrases can be any s\\tbstring that may n\\bt necessarily be ling\\tistically m\\btivated \\tnits. In \\b\\tr implementati\\bn, we extract phrases \\bf \\tp t\\b 4-character-grams. In a Chinese c\\b\\tplet, there is generally a direct \\bne-t\\b-\\bne mapping between w\\brds at same p\\bsiti\\bn in the FS and SS, respectively. As a res\\tlt, the ith","character/phrase in F is exactly “translated” int\\b the ith","character/phrase in S. Based \\bn this r\\tle, the phrase translati\\bn pr\\bbability )|( \\b\\bs\\fp can be estimated by relative freq\\tency in a training c\\brp\\ts:"]},{"title":"∑","paragraphs":["= = m r \\br \\b\\b \\b\\b s\\fcount s\\fcount s\\fp 1 ),(","),( )|( (2) where m is the n\\tmber \\bf distinct phrases that can be mapped t\\b the phrase \\b"]},{"title":"s","paragraphs":["and"]},{"title":"),(","paragraphs":["\\b\\b"]},{"title":"s\\fcount","paragraphs":["is the n\\tmber \\bf \\bcc\\trrences that \\b"]},{"title":"\\f","paragraphs":["and \\b"]},{"title":"s","paragraphs":["appear at the c\\brresp\\bnding p\\bsiti\\bns in a c\\b\\tplet. The inverted phrase translati\\bn m\\bdel )|( \\b\\b\\fsp has been pr\\bven \\tsef\\tl in previ\\b\\ts SMT research w\\brk (Och and Ney, 2002); s\\b we als\\b incl\\tde it in \\b\\tr phrase-based SMT m\\bdel. 45  Lexical\\twei\\bht\\t(LW)\\t Previ\\b\\ts research w\\brk \\bn phrase-based SMT has f\\b\\tnd that it is imp\\brtant t\\b validate the q\\tality \\bf a phrase translati\\bn pair (K\\behn et al., 2003). A g\\b\\bd way t\\b d\\b this is t\\b check its lexical weight )|( \\b\\b\\t s\\fp , which indicates h\\bw well its w\\brds translate t\\b each \\bther:"]},{"title":"∏","paragraphs":["= = N\\b j jj\\b\\b\\t s\\fps\\fp 1 )|()|( (3) where N\\b is the n\\tmber \\bf characters in \\b"]},{"title":"\\f","paragraphs":["\\br \\b"]},{"title":"s","paragraphs":[", j"]},{"title":"\\f","paragraphs":["and j"]},{"title":"s","paragraphs":["are characters in \\b"]},{"title":"\\f","paragraphs":["and \\b"]},{"title":"s","paragraphs":["respectively, and"]},{"title":")|(","paragraphs":["jj"]},{"title":"s\\fp","paragraphs":["is the character translati\\bn pr\\bbability \\bf j"]},{"title":"s","paragraphs":["int\\b j"]},{"title":"\\f","paragraphs":[". Like in phrase translati\\bn pr\\bbability estimati\\bn,"]},{"title":")|(","paragraphs":["jj"]},{"title":"s\\fp","paragraphs":["can be c\\bmp\\tted by relative freq\\tency:"]},{"title":"∑","paragraphs":["= = m r jr jj jj s\\fcount s\\fcount s\\fp 1 ),( ),( )|(  (4) where m is the n\\tmber \\bf distinct characters that can be mapped t\\b the character j"]},{"title":"s","paragraphs":["and"]},{"title":"),(","paragraphs":["jj"]},{"title":"s\\fcount","paragraphs":["is the n\\tmber \\bf \\bcc\\trrences that j"]},{"title":"s","paragraphs":["and j"]},{"title":"\\f","paragraphs":["appear at the c\\brresp\\bnding","p\\bsiti\\bns in a c\\b\\tplet. Like the phrase translati\\bn m\\bdel, we als\\b \\tse an inverted lexical weight )|( \\b\\b\\t \\fsp in additi\\bn t\\b the c\\bnventi\\bnal lexical weight )|( \\b\\b\\t s\\fp in \\b\\tr phrase-based SMT m\\bdel. Lan\\bua\\be\\tmodel\\t(LM)\\t A character-based trigram lang\\tage m\\bdel with Katz back-\\bff is c\\bnstr\\tcted fr\\bm the training data t\\b estimate the lang\\tage m\\bdel p(S) \\tsing Maxim\\tm Likelih\\b\\bd Estimati\\bn."]},{"title":"3.2 Data\\tCollection\\tand\\tModel\\tTrainin\\b\\t","paragraphs":["We \\tsed the meth\\bd pr\\bp\\bsed by (Fan et al., 2007) t\\b rec\\trsively mine th\\bse c\\b\\tplets with the help \\bf s\\bme seed c\\b\\tplets. The meth\\bd a\\tt\\bmatically learns patterns in a page c\\bntaining Chinese c\\b\\tplets and then applies the learned patterns t\\b extract m\\bre Chinese c\\b\\tplets. In additi\\bn, we f\\b\\tnd s\\bme \\bnline f\\br\\tms where Chinese c\\b\\tplet fans challenge each \\bther. When a new FS is p\\bsted \\bn the f\\br\\tms, many \\bther pe\\bple s\\tbmit their SSs in resp\\bnse, each with different meaning and w\\brd \\tsage. Using vari\\b\\ts web mining appr\\baches, we finally c\\bllected 670,000 c\\b\\tplets. We als\\b mined pairs \\bf sentences \\bf p\\betry which satisfied the c\\bnstraints \\bf c\\b\\tplets alth\\b\\tgh they were n\\bt \\briginally intended as c\\b\\tplets. F\\br instance, in eight-line Tang p\\betry, the third and f\\b\\trth sentences and the fifth and sixth sentences f\\brm pairs basically satisfying the c\\bnstraints \\bf Chinese c\\b\\tplets. Theref\\bre, an additi\\bnal 300,000 sentence pairs were \\bbtained, yielding a t\\btal \\bf 970,000 sentence pairs \\bf training data.","Beca\\tse the relati\\bnships between w\\brds and phrases in the FS and SS are \\ts\\tally reversible, t\\b alleviate the data sparseness, we reverse the FS and SS in the training c\\b\\tplets and merge them with \\briginal training data f\\br estimating translati\\bn pr\\bbabilities. T\\b sm\\b\\bth the lang\\tage m\\bdel, we add ab\\b\\tt 1,600,000 sentences which are n\\bt necessarily c\\b\\tplets, derived fr\\bm ancient Chinese p\\betry, f\\br lang\\tage m\\bdel training. T\\b estimate the weights λ\\b in f\\brm\\tla (1), we \\tse the Minim\\tm Err\\br Rate Training (MERT) alg\\brithm, which is widely \\tsed f\\br phrase-based SMT m\\bdel training (Och, 2003). The training data and criteria (BLEU) f\\br MERT will be explained in S\\tbsecti\\bn 4.1."]},{"title":"3.3 Lin\\bui\\ftic\\tFilter\\f\\t","paragraphs":["T\\b reflect the ling\\tistic c\\bnstrains, a set \\bf filters is \\tsed t\\b rem\\bve candidates that vi\\blate ling\\tistic c\\bnstraints. F\\br instance, Repetition\\tfilter rem\\bves candidates based \\bn vari\\b\\ts r\\tles related t\\b w\\brd \\br character repetiti\\bn. One s\\tch r\\tle req\\tires that if there are m\\tltiple characters that are identical in the FS, then the c\\brresp\\bnding characters in the SS sh\\b\\tld be identical t\\b\\b. C\\bnversely, if there are n\\b identical w\\brds in the FS, then the SS sh\\b\\tld have n\\b 46 identical w\\brds. Similarly, Pronunciation\\t repetition\\t filter,\\t Character\\t decompo\\fition\\t filter,\\t Phonetic\\tharmony\\tfilter\\tare \\tsed t\\b rem\\bve the candidates vi\\blating the req\\tirements \\bn the pr\\bn\\tnciati\\bn, character dec\\bmp\\bsiti\\bn and ph\\bnetic harm\\bny.\\t\\t\\t"]},{"title":"3.4 Rerankin\\b\\tBa\\fed\\ton\\tMultiple\\tFeature\\f\\t","paragraphs":["In many cases, l\\bng-distance c\\bnstraints are very helpf\\tl in selecting g\\b\\bd SSs, h\\bwever, it is diffic\\tlt t\\b inc\\brp\\brate them in the framew\\brk \\bf the dynamic pr\\bgramming dec\\bding alg\\brithm. T\\b s\\blve this iss\\te, we designed an SVM-based reranking m\\bdel inc\\brp\\brating l\\bng-distance feat\\tres t\\b select better candidates.","As sh\\bwn in f\\brm\\tla (5),"]},{"title":"xv","paragraphs":["is the feat\\tre vect\\br \\bf a SS candidate, and"]},{"title":"\\tv","paragraphs":["is the vect\\br \\bf weights."]},{"title":"⋅〉\\b⋅,","paragraphs":["stands f\\br an inner pr\\bd\\tct. \\f is the decisi\\bn f\\tncti\\bn with which we rank the candidates."]},{"title":"〉\\b= x\\tx\\f","paragraphs":["\\t"]},{"title":"vvv","paragraphs":["v"]},{"title":",)( ","paragraphs":["(5) Besides the five feat\\tres \\tsed in the phrase-based SMT m\\bdel, additi\\bnal feat\\tres f\\br","reranking can be added int\\b this framew\\brk s\\tch as the tw\\b as f\\bll\\bws (\\fiang & Zh\\b\\t, 2008). \\t M\\tt\\tal inf\\brmati\\bn (MI) sc\\bre: \\t MI-based str\\tct\\tral similarity (MISS) sc\\bre:"]},{"title":"4 Experimental\\tRe\\fult\\f\\t 4.1 Evaluation\\tMethod\\t","paragraphs":["A\\tt\\bmatic eval\\tati\\bn is very imp\\brtant f\\br parameter estimati\\bn and system t\\tning. An a\\tt\\bmatic eval\\tati\\bn needs a standard answer data set and a metric t\\b sh\\bw f\\br a given inp\\tt sentence the cl\\bseness \\bf the system \\b\\ttp\\tt t\\b the standard answers. Since generating the SS given the FS is viewed as a kind \\bf machine translati\\bn pr\\bcess, the widely accepted a\\tt\\bmatic SMT eval\\tati\\bn meth\\bds may be applied t\\b eval\\tate the generated SSs.","BLEU (Papineni et al., 2002) is widely \\tsed f\\br a\\tt\\bmatic eval\\tati\\bn \\bf machine translati\\bn systems. It meas\\tres the similarity between the MT system \\b\\ttp\\tt and h\\tman-made reference translati\\bns. The BLEU metric ranges fr\\bm 0 t\\b 1 and a higher BLEU sc\\bre indicates better translati\\bn q\\tality.",")l\\bgexp( 1"]},{"title":"∑","paragraphs":["= \\t= N n nn p\\tBPBLEU (6) S\\bme adaptati\\bn is necessary t\\b \\tse BLEU f\\br eval\\tati\\bn \\bf \\b\\tr c\\b\\tplet generat\\br. First, pn, the n-gram precisi\\bn, sh\\b\\tld be p\\bsiti\\bn-sensitive in the eval\\tati\\bn \\bf SSs. Sec\\bnd, BP, the brevity penalty, sh\\b\\tld be rem\\bved, beca\\tse all system \\b\\ttp\\tts have the same length and it has n\\b effect in eval\\tating SSs. M\\bre\\bver, beca\\tse the c\\b\\tplet sentences \\ts\\tally have fewer than 10 characters, we set n t\\b 3 f\\br the eval\\tati\\bn \\bf SSs, while in MT eval\\tati\\bn n is \\bften set t\\b 4.","It is imp\\brtant t\\b n\\bte that the m\\bre reference translati\\bns we have f\\br a testing sentence, the m\\bre reas\\bnable the eval\\tati\\bn sc\\bre is. Fr\\bm c\\b\\tplet f\\br\\tms menti\\bned in S\\tbsecti\\bn 3.2, we c\\bllected 1,051 FSs with diverse styles and each \\bf them has \\bver 20 \\tniq\\te SS references. After hand rem\\bval \\bf s\\bme n\\bisy references, each \\bf them has 24.3 references \\bn average. The minim\\tm and maxim\\tm n\\tmber \\bf references is 20 and 40. O\\tt \\bf these data, 600 were selected f\\br MERT training and the remaining 451 f\\br testing."]},{"title":"4.2 Feature\\tEvaluation\\t","paragraphs":["We c\\bnd\\tcted s\\bme experiments incrementally t\\b eval\\tate the feat\\tres \\tsed in \\b\\tr phrase-based SMT m\\bdel and reranking m\\bdel. All testing data are \\tsed. The res\\tlts are listed bel\\bw.   47  Table\\t4: Feat\\tre Eval\\tati\\bn. Feat\\tres BLEU Phrase TM(PTM) + LM 0.276 + Inverted PTM 0.282 + Lexical Weight (LW) 0.315 Phrase-based SMT M\\bdel + Inverted LW 0.348 + M\\tt\\tal inf\\brmati\\bn (MI) 0.356 Ranking SVM + MI-based str\\tct\\tral","similarity 0.361  As sh\\bwn in Table 4, with tw\\b feat\\tres: the phrase translati\\bn m\\bdel and the lang\\tage m\\bdel, the phrase-based SMT m\\bdel can achieve a 0.276 \\bf BLEU sc\\bre. When we add m\\bre feat\\tres incrementally, the BLEU sc\\bre is impr\\bved c\\bnsistently. 4.3 Human\\tEvaluation\\t In additi\\bn t\\b BLEU eval\\tati\\bn, we als\\b carried \\b\\tt h\\tman eval\\tati\\bn. We select 100 FSs fr\\bm the l\\bg data \\bf \\b\\tr c\\b\\tplet web service (http://d\\tilian.msra.cn) which was la\\tnched in 2005. F\\br each FS, 10 best SS candidates are generated \\tsing \\b\\tr best system. Then each SS candidate is labeled by h\\tman as acceptable \\br n\\bt. The eval\\tati\\bn is carried \\b\\tt \\tsing t\\bp-1 and t\\bp-10 res\\tlts based \\bn t\\bp-n incl\\tsi\\bn rate. T\\bp-n incl\\tsi\\bn rate is defined as the percentage \\bf the test sentences wh\\bse t\\bp-n \\b\\ttp\\tts c\\bntain at least \\bne acceptable SS. The T\\bp-1 incl\\tsi\\bn rate is 0.21 and the T\\bp-10 incl\\tsi\\bn rate is 0.73. The n\\tmbers are very enc\\b\\traging f\\br this diffic\\tlt task. An analysis \\bn the 27 FSs wh\\bse t\\bp-10 \\b\\ttp\\tts c\\bntain n\\b acceptable SS sh\\bws that the err\\brs mainly c\\bme fr\\bm three aspects: \\tnidentified named entity, c\\bmplicated character dec\\bmp\\bsiti\\bn and repetiti\\bn."]},{"title":"5 Model\\tof\\tChine\\fe\\tQuatrain\\t\\t","paragraphs":["We will describe \\b\\tr w\\brk \\bn extensi\\bn the m\\bdel \\bf Chinese c\\b\\tplets t\\b Chinese classic p\\betry \\tsing q\\tatrain that is c\\bmp\\bsed with 4 sentences as case st\\tdy. We believe that 8 sentence reg\\tlated verse can be generated with same appr\\bach.","Beca\\tse the Chinese c\\b\\tplet can be c\\bnsidered a special kind \\bf p\\betry with \\bnly tw\\b sentences, and since pairs \\bf adjacent sentences in q\\tatrains are q\\tite similar t\\b Chinese c\\b\\tplets, we are inspired t\\b apply the same statistical MT m\\bdel based appr\\bach. A necessary extensi\\bn is made f\\br generating the next sentence given a previ\\b\\ts sentence. Specifically, when generating the sec\\bnd sentence, we regard the first sentence as the s\\b\\trce lang\\tage sentence in a MT task and regard the sec\\bnd sentence as the target lang\\tage sentence. And similar appr\\bach is applied f\\br generating the third sentence and the f\\b\\trth sentence respectively. As in c\\b\\tplet generati\\bn, the SMT m\\bdel we \\tse here d\\bes n\\bt incl\\tde w\\brd deleti\\bn, inserti\\bn and re\\brdering, beca\\tse q\\tatrain sentences all have an identical n\\tmber \\bf characters. H\\bwever, t\\b meet the special req\\tirements \\bf p\\betic sentence generati\\bn, we made s\\bme necessary adaptati\\bns t\\b the c\\b\\tplet generati\\bn m\\bdel. First, t\\b impr\\bve the c\\bherence \\bf n\\bn-adjacent sentences, we add a m\\tt\\tal inf\\brmati\\bn feat\\tre which takes all previ\\b\\tsly generated sentences int\\b c\\bnsiderati\\bn and s\\b helps t\\b g\\tarantee the c\\bhesi\\bn between the sentence t\\b be generated and all previ\\b\\tsly generated sentences. Sec\\bnd, t\\b reflect the slightly different relati\\bnship between tw\\b adjacent sentences at different p\\bsiti\\bns \\bf a p\\bem, when generating a sentence at a certain p\\bsiti\\bn, we c\\bmbine the translati\\bn m\\bdel trained \\bn the data at the c\\brresp\\bnding p\\bsiti\\bn with a backgr\\b\\tnd m\\bdel trained \\bn all p\\betic sentence pairs.","Different with the c\\b\\tplet generati\\bn pr\\bcess which the first sentence is given as inp\\tt, the q\\tatrain system receives n\\b first sentence as inp\\tt b\\tt a few key w\\brds that describe the t\\bpic, 48 fr\\bm which the system generates the first sentence. As a preliminary experiment \\bf this paper, t\\b simplify the task, we limit the \\tser-inp\\tt key w\\brds f\\br specifying t\\bpic t\\b selecti\\bns fr\\bm a p\\betic phrase tax\\bn\\bmy “ShiX\\teHanYing” (诗学含英) which c\\bllected and classified phrases \\tsed in ancient Chinese p\\bems. In the tax\\bn\\bmy, 41,218 phrases (34,290 \\tniq\\te), with length fr\\bm 1 t\\b 5 characters, are classified int\\b 1,016 cl\\tsters. Each cl\\tster is named after a c\\bncept keyw\\brd, s\\tch as “spring”, \\br “m\\b\\tntaineering”, where w\\brds ass\\bciated t\\b a c\\bncept is p\\tt t\\bgether. The 1,016 keyw\\brds c\\bver m\\bst \\bf the freq\\tent t\\bpics in ancient Chinese p\\bems.","After the \\tser gives a few keyw\\brds, the q\\tatrain generat\\br will generate the first sentence. Then given the first sentence, it generates the sec\\bnd sentence. Then given the third sentence, it generates the f\\b\\trth sentence. Every step f\\bll\\bws a similar SMT m\\bdel t\\b c\\b\\tplet generati\\bn. In the next secti\\bn, we will describe the mechanism \\bf generati\\bn \\bf the first sentence. F\\br SMT m\\bdel and training data, as it is similar t\\b the c\\b\\tplet system, readers can refer t\\b Secti\\bn 3."]},{"title":"6 Generation\\tof\\tthe\\tFir\\ft\\tSentence\\t","paragraphs":["A template-based meth\\bd is ad\\bpted t\\b generate the first sentence with the inp\\tt keyw\\brds. Based \\bn the rhyme kn\\bwledge \\bf Chinese classic p\\betry, the template \\bf a 5-character p\\bem sentence m\\tst be \\bne \\bf the f\\bll\\bwing types: **|***, ***|**, *|****, ****|*, ..., *|**|**, **|**|*. Here “**|***” represents the j\\txtap\\bsiti\\bn \\bf a d\\b\\tble-syllable phrase and a triplesyllable phrase. Other n\\btati\\bns can be ded\\tced similarly. The templates \\bf 7-character sentences can be en\\tmerated similarly. Ass\\tming that all phrases in the first sentence \\bf the p\\bem are relevant t\\b the key w\\brds given by the \\tser, we can get a l\\bt \\bf first sentence candidates by filling all the pertinent phrases int\\b the ab\\bve templates.","Beca\\tse we have c\\bnstrained the \\tser t\\b select keyw\\brds fr\\bm the cl\\tster names in the tax\\bn\\bmy, it is straightf\\brward t\\b get the phrases related t\\b the selected keyw\\brds after the \\tser c\\bmpletes the selecti\\bn. Then we rand\\bmly c\\bmbine the phrases based \\bn the templates listed ab\\bve t\\b f\\brm candidates f\\br first sentences \\bf the \\tser-specified length. Let \\ts explain the generati\\bn pr\\bcess with an example in Table 5. S\\tpp\\bse a \\tser ch\\b\\bses three key w\\brds “春日” (spring day), “ 郊 行 ” (\\b\\tting) and “访 友 ” (visiting friends), then specifies a 5-character Q\\tatrain. O\\tr system will first c\\bnstr\\tct a phrase set fr\\bm the ShiX\\teHanYing tax\\bn\\bmy related t\\b the given key w\\brds, say {“明媚”, “寻芳草”, “水村”, “旧话”,...}, and p\\tt them at any p\\bssible p\\bsiti\\bn in the sentence t\\b generate the lattice bel\\bw:"," Table\\t5: An example lattice char 1 char 2 char 3 char 4 char 5 明媚 寻芳草 晴光 鱼 蝶飞 花变新红 绽 江山丽 迎门 ... ... ... ... ...  Then we search \\tsing the F\\brward-Viterbi-backward-A* alg\\brithm and find the N best first sentence candidates s\\tch as “晴光寻芳草”, “晴光鱼迎门”, “江山丽蝶飞”..."]},{"title":"7 Experimental\\tRe\\fult\\f\\t","paragraphs":["Finding an effective appr\\bach f\\br a\\tt\\bmatic eval\\tati\\bn is a big challenge f\\br p\\betry generati\\bn. It is diffic\\tlt t\\b \\tse BLEU f\\br the eval\\tati\\bn \\bf p\\bems beca\\tse BLEU req\\tires a reference set (i.e., sentence at (n+1)th","line) f\\br each sentence at nth","line. This req\\tires a h\\tman-a\\tth\\bred","standard answer. H\\bwever, given the same keyw\\brds, the p\\bems generated by h\\tman a\\tth\\brs","can be t\\b\\b diverse t\\b be able t\\b en\\tmerate. This kind \\bf answer data base is n\\bt presently 49  available. Theref\\bre, we ad\\bpted s\\tbjective eval\\tati\\bn \\bf the generated p\\bems. T\\b better \\tnderstand the perf\\brmance \\bf \\b\\tr meth\\bd, we designed the f\\bll\\bwing three experiments, each \\bf which has detailed criteria f\\br eval\\tating different parts \\bf the generated p\\bems."]},{"title":"7.1 Evaluation\\tof\\tthe\\tFir\\ft\\tSentence\\t","paragraphs":["In this experiment, we prepare 40 gr\\b\\tps \\bf key w\\brds. Each gr\\b\\tp c\\bnsists \\bf three key w\\brds which are rand\\bmly selected fr\\bm the tax\\bn\\bmy \\bf “ShiX\\teHanYing”. Fr\\bm the system generated sentences, tw\\b ann\\btat\\brs were asked t\\b select the best first sentence acc\\brding t\\b three criteria: the relatedness between this sentence and the inp\\tt keyw\\brds, the fl\\tency \\bf this sentence and the appr\\bpriateness \\bf this sentence t\\b be the first sentence \\bf a p\\bem. F\\br each criteri\\bn, the ann\\btat\\br will sc\\bre the sentence with \\bne \\bf the three n\\tmbers (g\\b\\bd=100, accept=50 and p\\b\\br=0). We set the weights f\\br the three criteria as 0.4, 0.4 and 0.2 respectively. F\\br example, if a sentence gets 50, 100 and 100 f\\br the three criteria respectively, this sentence will get a final sc\\bre \\bf 80. After the ann\\btat\\brs label all best first sentences, we c\\bmp\\tte the average sc\\bres f\\br 7-character and 5-character sentences separately, as sh\\bwn bel\\bw.  Table\\t6: Res\\tlts \\bf the first sentence eval\\tati\\bn F\\brm 5-character 7-character","Average sc\\bre 69.5 78.5"," As sh\\bwn in Table 6, b\\bth \\bf the sc\\bres are ab\\bve 60, which means that the \\bverall q\\tality \\bf the generated first sentences is acceptable. We als\\b n\\bte that the res\\tlt f\\br the 7-character sentences is better than 5-character sentences. This is beca\\tse a l\\bnger sentence is m\\bre likely t\\b c\\bver the meaning \\bf the inp\\tt key w\\brds."]},{"title":"7.2 Evaluation\\tof\\tthe\\tNext\\tSentence\\t","paragraphs":["We \\tsed the same keyw\\brds as in the previ\\b\\ts experiment and the best first sentences that h\\tman selected. Then we generated 10 best sec\\bnd, third and f\\b\\trth sentences in t\\trn with the pr\\bp\\bsed meth\\bd. After every generati\\bn, we man\\tally selected \\bne best sentence and treated it as inp\\tt t\\b generate the next sentence. Finally, we \\bbtained 40 gr\\b\\tps \\bf generated sec\\bnd, third and f\\b\\trth sentences. Then ann\\btat\\brs j\\tdge their q\\tality based \\bn tw\\b criteria: Fl\\tency and Relatedness, with eq\\tal weight \\bf 0.5. F\\br each criteri\\bn, the ann\\btat\\br will sc\\bre the sentence with \\bne \\bf the three n\\tmbers (g\\b\\bd=100, accept=50 and p\\b\\br=0). After the j\\tdgment, we calc\\tlate the sc\\bre \\bf each sentence, by which sentences are classified int\\b the f\\bll\\bwing three classes: Sentences with sc\\bre>=50 are Acceptable, sentences with sc\\bre=100 are Perfect and sentences with sc\\bre=0 are P\\b\\br. Then we calc\\tlate the percentage \\bf test cases wh\\bse the t\\bp N \\b\\ttp\\tts c\\bntain at least \\bne perfect sentence \\br acceptable sentence. The res\\tlts f\\br 5-character p\\bems and 7-character p\\bems are listed in Table 7.  Table\\t7: Res\\tlt f\\br (a) 5- and (b) 7-character sentences","(a)","Perfect Acceptable","T\\bp 1 T\\bp 5 T\\bp10 T\\bp 1 T\\bp 5 T\\bp10","1st","->2nd 15% 37% 50% 65% 87% 90%","2nd ->3rd","20% 43% 53% 65% 93% 97%","3rd ->4th 15% 37% 47% 35% 80% 87%","(b)","Perfect Acceptable","T\\bp 1 T\\bp 5 T\\bp10 T\\bp 1 T\\bp 5 T\\bp10","1st","->2nd 0 40% 53% 65% 80% 87%","2nd ->3rd","20% 44% 50% 50% 63% 75%","3rd ->4th 10% 39% 44% 45% 67% 78% 50"," Fr\\bm Table 7, we can see that the system is able t\\b get at least an acceptable sentence as a t\\bp 1 res\\tlt f\\br ab\\b\\tt 50% \\bf cases. When we check the t\\bp-5 and t\\bp-10 sentences, the acceptable rate rises t\\b 80%. H\\bwever, the \\bverall rate \\bf perfect sentences is m\\tch smaller than that \\bf the acceptable level, indicating that \\b\\tr system can generate p\\betic sentences that r\\b\\tghly meet the r\\tles b\\tt is still hard t\\b generate excellent \\bnes."]},{"title":"7.3 Evaluation\\tof\\tthe\\tWhole\\tPoem\\t","paragraphs":["T\\b eval\\tate the q\\tality \\bf the wh\\ble generated p\\bem, we rand\\bmly selected 20 gr\\b\\tps \\bf key w\\brds t\\b generate 20 p\\bems (ten 5-character and ten 7-character) with \\b\\tr system. When generating p\\bems with \\b\\tr system, we \\tse tw\\b different settings: \\bne is interactive m\\bde which means at each step \\bf the generati\\bn we man\\tally select \\bne best generated sentence and regard it as inp\\tt t\\b generate the next sentence; the \\bther is t\\btal a\\tt\\bmati\\bn which means at each step the system generates \\bnly \\bne best \\b\\ttp\\tt. After the p\\bems are generated, ann\\btat\\brs sc\\bre them with the f\\bll\\bwing criteria. Table\\t8: Criteria f\\br the wh\\ble p\\bem eval\\tati\\bn","Criteria Weight P\\b\\br Acceptable G\\b\\bd","Fl\\tency 5/15 0 50 100","Rhyme 5/15 0 50 100","Relatedness 3/15 0 50 100","Str\\tct\\tre 3/15 0 50 100","Artistic c\\bncepti\\bn 1/15 0 50 100","","After sc\\bring, we calc\\tlated the sc\\bre \\bf each p\\bem and get the average sc\\bre \\bf the 20 p\\bems","f\\br each system. The A\\tt\\bmatic M\\bde achieves 68.43% average sc\\bre and the Interactive M\\bde","achieves 77.83% average sc\\bre. This means with Interactive M\\bde the q\\tality can be largely","impr\\bved. T\\b get int\\titive \\tnderstanding, the f\\bll\\bwing is a p\\bem generated with the keyw\\brds","“踏青 赏花 游春” (\\b\\tting, enj\\by the bl\\bss\\bm, spring sightseeing) by \\b\\tr system, with the","setting \\bf t\\btal a\\tt\\bmati\\bn. 回舟一水香醉月/落日千山雪吟风/踏青寻花问柳春/人不在酒云","梦中."]},{"title":"8 Related\\tWork\\t","paragraphs":["In the area \\bf c\\bmp\\tter-assisted Chinese p\\betry generati\\bn research, (L\\b et al., 1999) has devel\\bped a t\\b\\bl which pr\\bvides the rhyme templates f\\br vari\\b\\ts styles \\bf ancient Chinese p\\betry and a dicti\\bnary f\\br t\\bne specificati\\bn \\bf any Chinese character. Users can write their \\bwn p\\bems with the help \\bf this t\\b\\bl. The rhyme templates and the dicti\\bnary were h\\tman-a\\tth\\bred and hand-c\\bmpiled.","A pri\\br attempt at a\\tt\\bmatic Chinese p\\bem generati\\bn, the “Da\\bxiang” p\\bem generat\\br (http://www.p\\beming.c\\bm/web/index.htm) system, directly fills classic rhyme templates (ch\\bsen fr\\bm a large invent\\bry) with w\\brds generally \\tsed in the ancient p\\bems. The p\\bems th\\ts generated, alth\\b\\tgh f\\brmally fitting the necessary rhythmic and metrical c\\bnstraints, generally cann\\bt s\\tpp\\brt a satisfact\\bry reader experience beca\\tse: 1) \\ts\\tally the generated sentences are n\\bt fl\\tent and have \\tnclear meanings; 2) the generated sentences seem independent \\bf each \\bther, which makes it diffic\\tlt f\\br readers t\\b derive a h\\blistic meaning fr\\bm the p\\bem.","F\\br \\bther lang\\tages, appr\\baches f\\br creating p\\betry with c\\bmp\\tters began in 1959 when The\\b L\\ttz created the first example \\bf “C\\bmp\\tter P\\betry” in Germany. Masterman (1971) and T\\bsa et al. (2008) describe tw\\b haik\\t pr\\bd\\tcers. Other systems incl\\tde RACTER and PROSE (Hartman, 1996). Appr\\baches t\\b p\\betry generati\\bn can r\\b\\tghly be classified int\\b template-based, ev\\bl\\tti\\bnary, and case-based reas\\bning. Typically, f\\br the template-based appr\\bach, the 51  generati\\bn pr\\bcess rand\\bmly ch\\b\\bses w\\brds fr\\bm a hand-crafted lexic\\bn and then fills in the sl\\bts pr\\bvided by a template-based grammar.","T\\b the best \\bf \\b\\tr kn\\bwledge, there is n\\b existing research w\\brk p\\tblished \\bn statistical meth\\bds f\\br generati\\bn \\bf Chinese p\\bems and c\\b\\tplets. O\\tr w\\brk can be regarded as the first attempt \\tsing statistical appr\\bach in this field."]},{"title":"9 Conclu\\fion\\f\\tand\\tFuture\\tWork\\t","paragraphs":["We pr\\bp\\bse a n\\bvel statistical appr\\bach t\\b a\\tt\\bmatically generate Chinese c\\b\\tplets (http://d\\tilian.msra.cn) and Chinese p\\betry. This design well inc\\brp\\brates the statistical appr\\bach and ling\\tistic c\\bnstraints \\bf Chinese c\\b\\tplets and p\\betry. A c\\bmprehensive eval\\tati\\bn has been c\\bnd\\tcted f\\br c\\b\\tplets generati\\bn and f\\br q\\tatrain generati\\bn and pr\\bmising res\\tlts were achieved.","In the f\\tt\\tre, we will expl\\bre an a\\tt\\bmatic eval\\tati\\bn appr\\bach \\bf each individ\\tal line and the wh\\ble p\\betry \\bf generated q\\tatrain. Besides, t\\b enhance thematic str\\tct\\tre \\bf a generated p\\betry, it is imp\\brtant t\\b st\\tdy t\\b a better meth\\bd t\\b inc\\brp\\brate the c\\bntents \\bf all \\bf the generated sentences f\\br the generati\\bn \\bf the c\\trrent line."]},{"title":"Reference\\f\\t","paragraphs":["Fan, C., L. \\fiang, M. Zh\\b\\t and S.-L. Wang. 2007. Mining C\\bllective Pair Data fr\\bm the Web. In Proc. o\\f the Internat\\bonal Con\\ference on Mach\\bne Learn\\bng and Cybernet\\bcs 2007, pages 3997-4002.","Hartman, Charles O. 1996. V\\brtual Muse: Exper\\bments \\bn Computer Poetry. Wesleyan University Press.","He, \\f., M. Zh\\b\\t and L. \\fiang. T\\b appear. Generating Chinese P\\bems \\tsing a Statistical MT Appr\\bach. Journal o\\f Ch\\bnese In\\format\\bon Process\\bng (in Chinese).","\\fiang, L. and M. Zh\\b\\t. 2008. Generating Chinese C\\b\\tplets \\tsing a Statistical MT Appr\\bach. In Proceed\\bngs o\\f the 22nd Internat\\bonal Con\\ference on Computat\\bonal L\\bngu\\bst\\bcs, Manchester, England.","K\\behn, P., F. \\f. Och and D. Marc\\t. 2003. Statistical phrase-based translati\\bn, In Proceed\\bngs o\\f HLT-NAACL 2003, pages 48-54.","Li\\t, Wenwei. 1735. Sh\\bXueHanY\\bng (诗学含英).","L\\b, Feng-\\f\\t, Y\\tan-Ping Lee and Wei-Cheng Tsa\\b. 1999. The F\\brmat A\\tt\\b-Checking and Database Indexing Teaching System \\bf Chinese P\\betry and Lyrics. Journal o\\f Ch\\bnese In\\format\\bon Process\\bng, V\\bl. 13, N\\b. 1, pp. 35-42.","Masterman, M. 1971. C\\bmp\\tterized Haik\\t. Cybernet\\bcs, pp. 175-183.","Och, F. \\f. 2003. Minim\\tm err\\br rate training in statistical machine translati\\bn. In Proc. o\\f the 41st","Annual Meet\\bng o\\f the Assoc\\bat\\bon \\for Computat\\bonal L\\bngu\\bst\\bcs.","Och, F. \\f. and H. Ney. 2002. Discriminative training and maxim\\tm entr\\bpy m\\bdels f\\br statistical machine translati\\bn. In Proc. o\\f the 40th","Annual Meet\\bng o\\f the Assoc\\bat\\bon \\for Computat\\bonal L\\bngu\\bst\\bcs.","Och, F. \\f. and H. Ney. 2004. The Alignment Template Appr\\bach t\\b Statistical Machine Translati\\bn. Computat\\bonal L\\bngu\\bst\\bcs, 30:417-449.","Papineni, K., S. R\\b\\tk\\bs, T. Ward and W.-\\f. Zh\\t. 2002. BLEU: A meth\\bd f\\br a\\tt\\bmatic eval\\tati\\bn \\bf machine translati\\bn. In Proc. o\\f the 40th","Annual Meet\\bng o\\f the Assoc\\bat\\bon \\for Computat\\bonal L\\bngu\\bst\\bcs.","St\\blcke, Andreas. 2002. SRILM -- An Extensible Lang\\tage M\\bdeling T\\b\\blkit. In Proc. o\\f Intl. Con\\f. on Spoken Language Process\\bng, v\\bl. 2, pp. 901-904.","T\\bsa, Na\\bk\\b, Hidet\\b Obara and Michihik\\b Min\\bh. 2008. Hitch Haik\\t: An Interactive S\\tpp\\brting System f\\br C\\bmp\\bsing Haik\\t P\\bem. ICEC 2008, pp. 209-216. 52"]}]}