{"sections":[{"title":"A Korean Homonym Disambiguation System Based on Statistical Model Using weights","paragraphs":["Jun-Su Kim Wang-Woo Lee Chang-Hwan Kim Cheol-young Ock","Dept. of Computer Engineering & Information Technology, University of Ulsan","680-749, San29, Mugeo-dong, Nam-gu","Ulsan, Korea {j skim, wwlee, edel}@cic.ulsan.ac.kr okcy@uou.ulsan.ac.kr Abstract A homonym could be disambiguated by another words in the context as nouns, predicates used with the homonym. This paper using semantic information (co-occurrence data) obtained from definitions of part of speech (POS) tagged UMRD-S 1 ). In this research, we have analyzed the result of an experiment on a homonym disambiguation system based on statistical model, to which Bayes' theorem is applied, and suggested a model established of the weight of sense rate and the weight of distance to the adjacent words to improve the accuracy. The result of applying the homonym disambiguation system using semantic information to disambiguating homonyms appearing on the dictionary definition sentences showed average accuracy of 98.32% with regard to the most frequent 200 homonyms. We selected 49 (31 substantives and 18 predicates) out of the 200 homonyms that were used in the experiment, and performed an experiment on 50,703 sentences extracted from Sejong Project tagged corpus (i.e. a corpus of morphologically analyzed words) of 3.5 million words that includes one of the 49 homonyms. The result of experimenting by assigning the weight of sense rate(prior probability) and the weight of distance concerning the 5 words at the front/behind the homonym to be disambiguated showed better accuracy than disambiguation systems based on existing statistical models by 2.93%. 1\\tIntroduction The ambiguity, which is the most difficult problem in natural language processing (NLP), occurs inevitably in every analysis process including morphological analysis and syntactic analysis. Ambiguity problems occurring at some parts have been resolved to some degree. As the studies on semantic and discourse analysis are becoming active, more effort is being made to research schemes for word sense disambiguation (WSD). WSD refers to disambiguating the sense of a word contextually suitable for the sentence when it is used with two or more different meanings in sentences. [1, 3, 4, 5, 8] Studies for solving ambiguity are largely grouped into methods using dictionaries according to the pattern of learning data and ones using a corpus. In terms of methodology, there are largely methods using rules, ones using probability statistics, and ones using semantic hierarchy structure. A method using dictionaries is disadvantageous in that it is difficult to reflect the dynamic characteristic of a language [10, 11], while advantageous in that it can abstract the detailed information of word senses [2, 3]. To disambiguate using a corpus, a large-sized semantic tagged corpus is required. However, a high-quality corpus is hard to find, and costly and time-consuming to build. But the method reflects the dynamic characteristic of a language. In the thesis, we abstract semantic information from a dictionary definition corpus based on the method suggested in J. Huh (2000)[3] research. We will study a plan to utilize the semantic information in the homonym disambiguation model based on Bayes' theorem. 1) UMRD-S : Ulsan university Machine Readable Dictionary (Semantic Tagged) 166 [geo-seon] • [yu-jo-seon] • [put-bae] of-1?-\\t[a-ju keun bae] (large ship) *"]},{"title":"Zs- A","paragraphs":["le.","\\t1111","-"]},{"title":"•","paragraphs":["o si-seor-eul • at-chun bae] (Oiler"]},{"title":")1 =0 blfi","paragraphs":["[a-jik deol ik-eun bae] (unripe pears) First type Second type 01171- I t]]\\t(11 *--6-- *-a- ,3,1- 1-1- i"]},{"title":"cl","paragraphs":["1 [bae-ga hang-hae jung-e pok-pung-u deung-eul man-na kicae-eo-*im Shi • wreck lt.1:31- [nan -p a] -27'4 [un-ha]"]},{"title":"-2- 4","paragraphs":["A-"]},{"title":"1","paragraphs":["7c)--&— 1-1157-- 14171- ul- LI 711 [yuk-ji-reul pa-seo gang-eul nae-go bae-ga da-ni-ge han su-ro (Canal 2\\tSemantic Information from definitions in Dictionary Semantic information extracted from definitions of part of speech (POS) tagged UMRD. We must classify definitions and titles according to the meaning before extracting semantic information. The structures of definitions are various, and are classified into 11 types by Cho(1999)[2]. The most frequent type is that head-word (hyponym), the title also is contained in semantic information."]},{"title":"UMRD Dictionary Definitions","paragraphs":["POS Tagger"]},{"title":"POS Tagged","paragraphs":["(Title:145,000 Eojeol:1,200,000) Revised POS tag","(by Hand) \\t"]},{"title":"• POS Tagged Corpus","paragraphs":["Sense Tagging (by hand) Semantic information Classified by sense Extracting Semantic Information"]},{"title":"Sense Tagged Corpus","paragraphs":["Figure 1. The Process to extract Semantic Information The semantic information is classified into two types. [2, 3] First has hyponym-hypernym relation between title-word and head-word (homonym) in definition. The other is extracted from definitions in which the homonym is used for defining other words. In other word, the homonym is located middle in the definitions. And the titles of 2 nd information are contained in the semantic information. Types merge semantic information. Table 1. Types of definitions for extracting semantic information"]},{"title":"167","paragraphs":["3\\tWord Sense Disambiguation Model Based on Statistics 3.1 Statistic Model Based on Bayes' Theorem In the WSD model utilizing the Semantic information of homonym sense abstracted from dictionary definitions as the prior probability in Bayes' theorem, a homonym H appearing in an arbitrary sentence C is disambiguated as one of senses Hs,, Hs2,\\tHsn W(H ,C) = arg max Hs, P(Hs „C)","P(Hsk ,C)= P(Hsk wi .1=1","P(w n Hs k) P(Hs k ( W j ) = \\t P(w n Hs i) In formula (3), Hsk is the k-th sense of homonym H, and w; appearing in sentence C is a word associated with the semantic information of Hs k and has its frequency information. In addition, w; may appear in other frequency with different frequency [2]. Formula (2) represents the sum of probability for each of appearing words extracted from formula (3) to be identified as sense Hs k. And formula (1) is for disambiguating the sense of homonym H for the sentence C with the maximum of the sums by senses calculated in formula (2) We experimented the statistical basic model (NB: Naïve Bayes Model) for the selected 31 nouns and 18 predicates among the homonyms frequently appearing in the dictionary definitions against the 3.5 million POS tagged corpus of the Sejong Project. As the result of applying to all the words of 50,703 sentences containing the selected 49 homonyms, the accuracy was 77.67% for the nouns and 61.73% for predicates on the average. When applying only to the 5 words front / behind the homonyms, the accuracy was 72.87% for the nouns and 43.79% for predicates. 3.2 Error Patterns in the Basic Model In this section, we will examine the cases of erroneous analysis in the statistical basic model (NB) and search for a method to resolve them."]},{"title":"[Example sentence","paragraphs":["1] a El i- r%I.Gii EIM\\tEIH ilia0 1\\t1:1-1zl 321-X1-","x l ELI-\\t'1-11112-1-71- -T-1\\tV0H H 111- .Y.21. 2-1 .acnic4c1-\\t1-[}11a1--ff (Thanks to that, I have to pay for sweets much more than what I tried to save. Since then, for the worse, my granddaughter goes nearly to burst into tears whenever seeing the grandpa holding a camera, and my wife threatens me to slam the camera.) In [example sentence 1] are two homonyms ' 8 [1 2) [bae]' and\\tc[3) [deul-da] '. Of the two,\\t[bae]' is used in the sense of ̀14 1(body part)'. Words in the semantic information for \\t[bae]' appearing in the [example sentence 1], namely the words in each sense, and their frequencies are as follows.","2) Senses of homonym 1:01 finer : 8H _1(body part), UH _3(vesse), 8H _4(fruit), 8H _6 (two times, double) (1) (2) (3) 168 ","1 bill 3 dH 4 01 6","d H .--11 [bae-kkop] (navel) 1.00 0.00 0.00 0.00 0.31 0.39 0.00 0.52 1.00 0.00 0.11 0.56 0.00 0.00 0.00 1.00 0.32 0.00 bi[ g [ba-ram] (as a result of) -11-1-x[Pcwa-ja](coolcie) 7tjA [kap](price)","Elikeu-dalibig, large) --E- E lideul-da] (stay, lift up , ...) 0.00 0.48 0.00 0.00 0.26 0.05","Table 2. Semantic information abstracted from [example sentence 1] (5 words front /behind the word)","1:111\\t1 (body part) noun 8 II 5(navel)(2) predicate a ERbig, large)(26) --2. E","[(stay, lift up , . • • •)(9) till"]},{"title":"j","paragraphs":["(vessel) noun b [ g(as a result of)(9) predicate a ERbig, large )(25) -R. ERstay, lift up , ....)(1) 1:1H\\t4 (fruit) noun El [ g(as a result of)(1) :P-I-X1-(coolcie)(2) predicate a ERbig, large)(1) -2- ERstay, lift up , ...)(1) 811\\t","6","(double) noun Kt(price)(1) predicate a ERbig, large)(5) The result of applying the extracted words and their frequencies to formula (3) and (2) is as [Table 3]. Consequently the basic model selects\\t_4(fruit)', so fails to disambiguate. Table 3. Probability drawn from NB Method The major reasons of disambiguation failure are, firstly, that the probability calculation for the frequency of semantic information used in disambiguation does not consider the use frequency of the homonym. For example, concerning the semantic information of -̀a- rildeul-dar extracted from dictionary definitions in [Table 2], the word appears once for each of A1_3(vessel)' and ̀1111_4(fruit)', but the numbers of words used in the definitions of ̀11_3(vessel)' and ̀1311_4(fruit)' are 24 and 513 respectively. Therefore, as ----a-r-t[deul-da]' is extracted once from 24 words and 513 words, the frequency should be normalized. Secondly, the present NB model does not analyze syntactic structure, and disambiguate a homonym simply according to what semantic information the sentence containing the homonym has. This is based on the assumption that, if the homonym is not used metaphorically or idiomatically, it is used with the words that are semantically related to the corresponding sense of the homonym, and if the homonym is used in a simple sentence, it is almost possible to disambiguate it without analyzing the syntactic structure. In complex sentences or compound sentences, however, the extracted semantic information 3) Senses of homonym\\tideul-dal' : 5 CE _1(stay, permeate), § C4 _4","(lift up, suggest {a fact or an example}), § _5 (receive the action represented by the front noun )"]},{"title":"169","paragraphs":["word sense","1 3 tli14 46 Noun 639 668 130 363 Pred. 307 283 67 82 Noun 2,323 1,593 164 676 Pred. 1,313 1,114 102 178 number of words","\\t Frequency sum 1,608 1,013 146 -a-ul-_1 _4 662 515 84 466 226 15 1,304 516 19 may not be determinant in disambiguating the corresponding homonym. Accordingly, in the present thesis, we extracted semantic information from the 5 words front / behind a homonym. [1, 4] Yet in this case, the semantic relevance may differ according to the distance from the homonym. Therefore, we should consider the location (the distance from the homonym) where the semantic information is found. In the thesis, we suggest a method to resolve the two problems. 4\\tApplication of Weight to the Probability Model Based on Statistics 4.1 Consideration of the Weight of Sense Rate Dictionary definition sense information, which is used as prior probability, varies greatly in the word types and frequencies according to the appearance frequencies of homonym (Hsi, Hs2 ,..., Hsn) senses.","Table 4. Sampled Number of words and Sums of Frequencies in Semantic information Extracted from Dictionary Definitions In case word wj (E Hs i n Hs2 n... n Hsk ) appearing in sentence C appears commonly in several semantic information sets, it is highly likely to be selected as it has high probability through formula (3) when the frequency sum of the word is small. According to [Table 4], if a same word appears with LI N 1(a body part)' and LI K 4(fruit)', the frequency of with d fi 1(a body part)' should be 15 times higher than Effi _4(fruit)' to be possibly selected as II _1(a body part)' . However, frequency 15 is quite high number as a frequency of appearance, and is enough to disambiguate the homonym. Accordingly, we need a method to consider the number of words in the semantic information and the frequency in the Bayes theorem of the basic statistic model, considering the peculiar feature of vocabularies. In this thesis, we assume that the word in the semantic information provides a solution, and will use words in the semantic information Using the number of words of nouns and predicates belonging to the senses of a homonym (Hsi, Hs2,\\tHs,,), we can obtain the weight of SR(Sense Rate) as formula (4)","a number of word in Fis k SR(Hs k) ="]},{"title":"E","paragraphs":["a number of word in Hsi J=1 Words in sense Hsk have the prior probability of P(wi n Hsk ) . By multiplying the weight of sense rate SR(Hsk ) to the existing probability, we obtain a new probability. (4) 70 1.00","dH 3 0.00","ti 11 4 0.001111 0.00 0.00 0.47 0.00 0.00 0.36 1.00 0.00 0.04 0.00 1.00 0.13.3_ Et Lill 6 0.00 A statistical model (SR : statistical model with Sense Rate) that considers the weight of sense use frequency is completed by applying the probability that reflects the weight to formula (1) and formula (2), which results formula (5). PSR k I )40=","P(wi n HSk )x SR(Hs k) P(wi n Hs; ) x SR(Hs i )\\t(5) i=1 To [example sentence 1] on which the statistical basic model has failed, we applied the statistical model (SR) that reflects the weight of sense use frequency on the words extracted from [Table 2], and found that the model disambiguated correctly as shown in [Table 5]. Table 5. Probability Considering the Weight of SR When comparing [Table 4] and [Table 5], the frequency of clicieul-dar belonging to the semantic information of ̀Lth_l[bae]' is 9, while that belonging to H _4[bae]' is 1. In the existing statistical model, the probability for LI N _1 [baer is 0.39, which is lower than the probability for HH_4[baer 0.56. According to [Table 5] reflecting the weight of sense rate, the probability of ' 14 _l[bae]' have risen to 0.70. Accordingly, the statistical model considering the weight of sense rate is advantageous in that it reflects frequency appropriately. 4.2 Consideration of the Distance between Words If we can utilize syntactic structure in disambiguating homonyms appearing in sentences, we may reduce unnecessary factors by selecting good-quality semantic information on the disambiguation. The present disambiguation model is based on a simple statistical model utilizing dictionary semantic information. Thus we attempt to resolve, using efficiently the information about the adjacent words. In the thesis, the disambiguation accuracy using the 5 words front / behind is not different significantly from that using the whole words. It is because the used semantic information is largely found in the adjacent words. In particular, it is even obvious that a word closer to the homonym among the 5 words front / behind it is more influential in disambiguation. Accordingly, we will apply the weight of distance appropriately. 171 2.1/NING+011/JKB"]},{"title":"1:11/NING-oms... V-7/1\\ING+31-/JKS... El","paragraphs":["Distance of Phrases\"' Figure 2. Distance between a homonym and a word of semantic information Considering the absolute distance d(H)— d(w j ) between a homonym and a word used as semantic information, we have derived a weight of formula (6). By applying the weight of distance Dis(H, w i) to formula (5), which is a new weight considering the weight of sense rate, we reflect the distance from the hononym. The longer the distance is, less influential the word is to the disambiguation. Accordingly, if a word is found near the homonym, it records high probability, and if it is found distantly from the homonym , it records low probability. Dis(H,wi ) = 1 (6) d(H)— d(w;)","P(Hs k ,C) =1Ps (Hs k I x'; ) x Dis(H, w; ) (7) f=i","Table 6. The result of applying weights of distance to probabilities with a deviation of 20% after applying weights of SR To improve the efficiency of the method, we experimented a method in which the weights of distance are applied only when the difference of disambiguaty is insignificant (the deviation is within 20%) just with the statistical model (SR) considering the weights of sense rate. According to [Table 5], the highest probability is 36.17%, followed by 23.83% and most are less than 20.%. Thus, when considering the weights of distance [Table 6], we found, the disambiguation is correct. 172 Number of Sentences All words 5 words Accuracy","rate (all words) Accuracy","rate (5 words)","Noun 30,451 23,652 22,189 77.67% 72.87%","Predicate 20,252 12,506 8,868 61.75% 43:79% 5 Experiment and Analysis 5.1 Selection if Ambiguous Homonyms and Extraction of Test Sentences After abstracting 200 homonyms appearing in dictionary definitions, we selects 49 words (31 nouns and 18 predicates). Which are used in balance among senses, and applies them to the disambiguation model [Table 7]. The analysis result by homonyms is as [Appendix 1] We abstracted 50,700 sentences that include the selected homonyms from the Sejong Project tagged corpus (around 3.5 million words), and performed automatic disambiguation using the statistical basic model (NB). After disambiguating the homonyms correctly through post-processing the automatically disambiguated sentences, we compared the accuracy rate. Table 7. Homonyms used in disambiguation experiments 7-1 2-1[geo-ri],\\t, -;1. [gyeol-jeong], ?-j, 7 1 [gyeon-ggi], •q [gulc], 71-[gi-gu], 7i ti[gi-won], 1--A-[nal], [̀nun], EH [dae], ---, [dok], -g-, [deung], ,T-,[mot],","Nouns LI N [bae],\\t-'ter .:;i [bu-jeong],\\tLI I [bi],\\t),̂1-[sang],\\t1,-; [seong],\\tRI A qui-sal ,","(31) RI x I [ui-ji],\\t0 I ).\"4 [i-sang],\\t,,,171[fang-gi],\\tT,,,1--'i-[jang-su],\\tN [jeol], -T-- oFrju-j and ,\\t[jung],\\tx l a Di-do],\\tXl[cha],\\tOF [chang],\\t[cheol], 121pan], Et [pyo] Predicates","E-1-[gal-da],\\tLI E- cligo-reu-dal,\\tA O[goe-da],\\t71Eilkici-dal, U-EF[dal-da], -2-- EF[deul-dal, It E l[mal-da], 514' c limat-da], '' Elimut-dat","(18) '-' Et[but-da], . --1E-F[swi-da,], \\tAF EE[ssa-da],\\tEF Elita-da],\\tL.\"' Ei[sseu-da], 01 ___Eqi-reu-da],\\t'1- Elicha-da], 71 Elikyeo-da], X"]},{"title":"I","paragraphs":["Et[ji-da], 5.2 Comparison of the Basic Model and the Model based on Weights of Sense Rate The average accuracy rate of the basic statistical model is as [Table 8]. In the experiment, we attempted to disambiguate using the whole words and the 5 words front / behind the homonyms. Table 8. Basic Statistical Model (NB, Accuracy Rate) In case of nouns at the existing statistical model (NB), the difference in accuracy rates between when disambiguating through the whole words and when doing through the 5 words is 4.8%. This indicates that lots of significant information is in the adjacent words and that, in a long sentence, words adjacent to homonym give semantic information enough to disambiguate. In addition, using adjacent words may exclude unnecessary information, which might occur when abstracting information from the whole words,"]},{"title":"173","paragraphs":["Number of Sentences All words 5 words Accuracy","rate all words Accuracy","rate 5 words) Noun","\\t 30,451","\\t 24,412 23,034","\\t 80.17%","\\t 75.64% Predicate","\\t 20,252","\\t 12,557","\\t 9,238","\\t 62.00%","\\t 45.62% Number of Sentences All words 5 words Accuracy rate (all words) Accuracy","rate (5 words) For Predicate, the semantic informationabstracted from dictionary definitions is often insufficient for disambiguating homonyms. For example,\\tt±' Et[but_da]' is closely associated with","Effssot_daRpour)', but EF[ssot_da]' is not included in the sense information. As the result of adding it to the semantic informationand analyzing, the accuracy rate increased by 6%. The reason is that dictionary definition techniques limit the words to be used. Accordingly, a more efficient method to add necessary semantic informaton should be researched further in the future. Table 9. The model considering the weights of SR [Table 9] is the analysis result using the model that obtains new probabilities considering the weights of sense rate (SR). As the result of considering the weights, the accurate rate increased by 1.7% for analysis on the whole words, and by 2.4% for analysis on the five words. When analyzing the whole words, accuracy rates increased in 29 homonyms, and when analyzing the five words, the rates increased in 31 homonyms. The result shows that applying the weights of sense rate to the basic statistical model makes disambiguation more efficient. In addtion, the weights for the 5 words front / behind homonyms are applied more effectively. Table 10. The model applying the weights of distance to probabilities with .a deviation of 20% after applying weights of SR The result in [Table 10] came from applying the weights of distance to the 5 words front / behind homonyms when the deviation of probability is 20% after the first disambiguation considering the weights of sense rate. The accuracy rate for the whole words has increased only by 0.61% from that of the basic statistical model (NB), and the increase is less than that when applying the weights of sense rate. For the five words front / behind homonyms, the analysis result shows the highest accuracy rate. Accordingly, a model that combines the two weights suggested in the thesis is most efficent. 5.3 Error Patterns According to the result of analyzing the cases that the accuracy rates fall in the model reflecting the weights of sense use frequency and the distance between words, the most significant cause appears to be lack of semantic information It is assumed that dictionaries restrain the use of extensive vocabularies and define the meanings of a word using a limited number of words. 174 6\\tConclusions and Further Researches First, according to the result of experimenting the statistical model reflecting the weights of sense rate and those of distances between words, which are suggested in the thesis, it is concluded that appropriate weights support disambiguation and that a further determinant weights should be explored for. Second, further researches are required for refining and expanding the semantic information abstracted from dictionary definitions. For refining, we should examine how nouns ( A} q[sa-ram], cO, [il],","[ttae], ...), verbs Cti- ri[ha-da],\\tr-l[dae-da], 91 r-I[it-sa],\\t...) and adverbs (21 q[eop-da], 91 [it-da], qlkeu-da], 3-1- r-tuak-da], V- [gat-da], cl[cla-reu-da], . . . ) which are highly frequency because of the peculiar characteristic of dictionary definitions, affect disambiguation, and prepare a method to exclude unnecessary semantic information appropriately. It is also required to study on methods to abstract semantic information and to expand information using semantic networks, along with establishing a large-sized semantic tagged corpus by creating semantic tagged program for expanding semantic information. References [1] D. Yarowsky(1992), \"Word-Sense Disambiguation Using Statical Model of Roget's Corpora\",COLING-92 [2] P.O. Cho and C.Y. Ock(1999), \"A Korean Noun Semantic Hirarchy based on Semantic Features\", Proceeding of the 18th ICCPOL Vol.1 . [3] J. Hur(2001), \"A Homonym Disambiguation System based on Semantic Information extracted from Definitions in dictionary\", ICCPOL-2001 [4] G. Rigau(2000), \"Naïve Bayes and Examplar-based Approaches to Word Sense Disambiguation Revisited\", ECAL [5] L. Marquez(2000), \"Machine Learning and Natural Language Processing\" [6] Alpha k, Luk(1995), \"Statistical Sense Disambiguation with Relatively Small Corpora Using Dictionary Definitions\", 33 rd Annual Meeting of the ACL [7] R. Bruce(1994),\"Word Sense Disambiguation Using Decomposable Models\"32rd Annual Meeting of the ACL, pp. 139-145 [8] P. Brown, V. Della Pietra, S. Della Pietra and R. Mercer(1991) Word sense disambiguation using statistical methods. In Proceedings of the 29th Annual Meeting of the ACL, pp.264-2'70 [9] G. Cottrell(1989) A Connectionist Approach to Word sense Disambiguation. Pitman, London [10] E. Brill(1993) A Corpus-Based Approach to Language Learning. Ph.D. thesis Computer and Information Science, University of Pennsylvania [11] S.B Park, B.T Zhang, Y.H Kim(2000) \"Word Sense Disambiguation From Unlabeled Data\", KISS '2000 Spring B', pp330 – 332 [12] Song, Young-bin, Choi, Gi-sun (2000) \"The use of thesaurus for disambiguating verbs and its limitation\", Treatise collection of the 12th Korean Alphabet and Korean Language Information Processing Conference, pp.255 - 261 [13] Lee, Chang-gi, Lee, Kun-bae (2000) \"WordNet automatic mapping using disambiguation, Treatise collection of the 12 th Korean Alphabet and Korean Language Information Processing Conference, pp.262 - 268 [14] Cho, Jung-mi(1998) \"Disambiguating verbs using corpus and dictionaries\", Ph.D. these, Korea Advanced Institute of Science and Technology 175 \\t ---Z-\";---------------.1.------------1-----4607-\\t'\\t46.21%\\tI\\tii..97%\\ti\\t46.21%\\tr\\t48.28%\\ti\\t46.21% \\t _......?....Virgila.......1._._...._.... 3\\t_ 1\\t.2\\t° \\t_.....\\t1\\t .. . \\t [jeol]\\t"]},{"title":"I\\ti","paragraphs":["--- i------a8foi-;------1-----WTOyo--T-----6634-----r---61:Wio-----t-------aiWo------1---.641162;-- -- t","j \\t 2f- .:,11.juang] I\\t3\\tti\\t94.65%\\tI81.99%\\t1\\t98.09%\\tI\\t87.93%\\tI\\t97.60%\\t1\\t87.59%:\\t :\\t........... ........... \\t g uungi\\t1\\t3\\t1\\t3ii4,;20\\t.1. ..:i6.iii\\t1\\t-6‘1.16;20\\t1\\t. i.4i(\\tL2.0\\t.\\ti.i6i0\\ti\\t3i.560Axi .E [ji-do]\\t1\\t2\\t1\\t82.90%\\t1 74.04% 1.\\t83.10%\\t1\\t74.45%\\t1\\t82.09% . 1: 77.67% \\t I[cha]\\t1\\t3\\t1\\t70.41%\\t1\\t68.65%\\t.1..\\t70.53%\\t1\\t68.77%\\t1 .\\t72.51%\\tI\\t69.71% \\t","g[chang]\\t1\\t2\\t1\\t78.25%\\tI\\t81.75%\\t1\\t78.60%\\t1\\t82.11% 1\\t85.61%\\tI\\t85.61%","•","I","\\t[Cheol]\\t1\\t3\\tI\\t65.64%\\tI\\t71.78%\\tI\\t62.58%\\t«̀.« \\tI\\t65.03%\\tI\\t65.64% \\t la. [pan] \\t3 \\t67.24%\\t1 63.22% i\\t66.09%\\tI 62.64% :\\t63.22%\\tI 60.92% \\t","11 [pyo] 1 \\t3\\t1\\t63.09%\\t1 59.78% 1\\t60.88%\\t1\\t57.58% 1\\t62.26%\\t1 58.68% .f. •\\t-1.- \\t","-c-l[gal-da] I\\t3\\tI\\t64.25%\\t1\\t41.90%\\t1\\t60.89%\\tI\\t45.81%\\t1\\t62.01%\\ti\\t45.81%","!\\t!\\t!","\\t3-1E-cilgo-reu-daji\\t3\\tI\\t57.86%\\tI\\t35.22%\\t:\\t52.83%\\t1\\t34.28%\\ti\\t48.11%\\t1 33.65%","•","","\\t-711 Ellgoe-da] I\\t2\\t1\\t77.97%\\tI\\t55.93% \\t79.66%\\ti\\t54.24%\\tI\\t72.88%\\t1 49.15%","-\\ti","\\t2\\ti\\t73.20%\\t:\\t53.10%\\tI\\t74.19%\\tI\\t53.60%\\t'\\t72.21%\\t1\\t53.85% \\t --.E4Eficiaf:dai I \\t-4\\t1\\t-64.i3;20\\tI -4164;20 :\\t67.iW20\\t1 46:4ii4 1\\t68.67%\\t1 46.48% \\t -2- Et[deul-da] 1\\t3\\t.I\\t61.92%\\t1 46.36%\\tI\\t63.00%\\t1\\t48.54% 1\\t59.77%\\tI 49.05%. _.\\t .71E-l[kki-da] \\t ca Eilmal,da] Ji...\\t3\\t1\\t48.51%\\tI\\t34.70%\\tI\\t49.25%\\ti 33.58%\\tI\\t45.90%\\ti\\ti3.58% \\t (kElirnat-da] I\\t3\\t1\\t55.45%\\tI\\t3&63%\\t1\\t55.38%\\t...i.. 40.44% ...1.\\t52.49%\\t.1\\t.\\t0 \\t -g c.E[mut-da] 1\\t3\\tI\\t64.86%\\t1\\t39.71%\\tI\\t66.85%\\t1\\t40.43%\\tI\\t60.71%\\ti\\t41.28.% .. \\t ì clibut-da] i\\t2\\t1\\t76.92%\\ti 57.69% I\\t76.92%\\t1\\t56.04% 1\\t75.27%\\t1 57.69% \\t -t-I cliswi-da] ..1-\\t3\\t-I\\t79.19%\\tI- 54.53%\\t1\\t80.92%\\tI\\t57.03%\\tI\\t76.69%\\t1 \\t 4a[ssa-da] 1\\t3\\tI\\t67 37%\\t1\\t40.40%\\t1\\t63.13%\\tI\\t44.15%\\t.\\t60.\\t0\\t,\\t.\\t0 \\t ±1\"- clisseu-da] 1\\t3\\ti\\t53.78%\\t1 \\t1\\t56.68%\\t1\\t44.45%\\tI.\\t52.18%\\t111... 43.86%. \\t - \\t","01 g.c1[1-reu-da].. I\\t3\\t1\\t60.50%\\tI\\t41.31%\\tI . 49.24%\\t1\\t37.97%\\t1\\t44.07%\\tI\\t35.21% xi cliji-daj\\tI\\t4\\t1\\t70.22%\\tI\\t46.22%\\tI\\t64.00%\\t•\\t45.78%\\tI\\t63.11%\\tI\\t46.96% . \\t","i[ cl[cha-da] I\\t4\\tI\\t60.74% -1- 36.60%\\t 1\\t62.20%\\t1 39.39% 1\\t58.36%\\tI 39.79% \\t4E-I-[kyeo-da] -.I..\\t2\\tI\\t83.82%\\t-I- 63.97% -1..\\t87.50%\\t1\\t70.59%\\ti.\\t83.82%\\tI \\t68.38% \\t El-c4ta-da] •\\t5\\tI\\t72.52%\\t•\\t50.23%\\t1\\t77.93%\\t1 \\tI\\t75.08%\\t1 57.81%","7121 [geoT:1 I\\t3\\t64.20%\\t1.1.. 62.96%\\t63.58%\\t1. 64.30% • •\\t2.. [guk] ! \\t","[nun]\\t 4 .2 \\t [41[dae]\\t2 \\t $[deung]\\t-.3_ \\t","[mot]\\t 3 4 1\\t94.63°4. .1\\t","I88.35%\\t1\\t94.83%\\t1\\t88.68°A\\t1\\t44.76%\\t:\\t88.48% 1---.1:703!?........... L. _78.25% .\\t77.95%\\t1\\t77.95% .1\\t80.09%\\t1\\t78.41% . 1------...•!!------4.-----ii.ii\"Will","....:-.--..° ,(1-----i--•-••••-• L.._ 974; -- -1- --Ei3;201..... 8- .76% .. 14_88.8 78:4 9% .... 1 ...81,12%..........L. 91,.15%_4_ ...i.!s..24,;/0±...1. -2/! 1 -iiiiiwi; 1 S...qi..... \\t\"--..iftl..: T---. - -1-***- i3.8........1:11i0.06.;/0... .......4 \\t77.d°4-11... 143g%.........-1.--- 82.28%\\t1\\t87.68%\\tIIll:- !:1::..65:28:...... :ll .....17it...........11.........._411...............f........_..6: \\tL\\t88. 0\\t1 ......3irifAL","i\\t64.89%\\t","-1.\\t5 .7 45%\\t1\\t56.38%\\t•\\t7.45%\\t1..._..._.59:f22...... 89.93%\\t1\\t82.01% \\t91.37%\\t1\\t584 .89°>O.. ..i\\t91.37% -.......--iiii;;0...........","I\\t83.66%\\t•\\t82.35%\\t1\\t87.15%\\tI\\t84.53%\\ti\\t86.711% - I.-..- itt....... 88.59% \\t86.47% 1\\t91.50%\\t1 89.57% I\\t92.44% I 91 63.38°4. 1 62.54% I642 12!)i.. 64.270.__L....66.!r-\\t","h......._1 39\\t","66. . --.46i\\t38.86%6% 1 38.86 0 IV2/--","4\\tI\\t87.09%\\tj_ 87.9?!......_.1............ ..290:92..............1..........9915!0.............i......_21:27f2.......\\tI ...... 92: 11%,........_...","4\\ti\\t62.37%\\t1\\t66.13%\\tI\\t6 1.","83% \\t","it\\t16.13%\\t71.51% -.1.- 71.51% i \\t4\\tI 70.71%\\t1 65.67% 4. 71.97%\\tI 6637% r 74.49% i 71.50% \\t 69.44%\\tI 66.03% I\\t65.48% 14[sang]\\t1 :1/4;i[seong]\\ti",";\\t .\\t. 9-1 A I[ui-sa]\\t1\\t2\\t,\\t =\\t63.71%\\tI\\t67.80%\\t1\\t66.17% \\t",", \\t-1-st xqui-ji]\\t1 \\t75.80%\\t1\\t71 28%\\t1\\t76.20%\\ti\\t71.81%\\tI\\t76.86%..............1.........73.54°A.......... \\t ----6 1 ): C[ I--....--.1........--------.....r------- .---- \\t'\\t32 90%\\tI\\t74.36%\\t1-------6-6--..6.-4-4.-----1--------ilii;A\\ti\\t6...f.i...i;i0..\\t.\\t............. [Appendix] 176"]}]}