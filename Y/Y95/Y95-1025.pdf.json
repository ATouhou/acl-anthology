{"sections":[{"title":"Using Brackets to Improve Search for Statistical Machine Translation Dekai WU\\tCindy NG","paragraphs":["HKUST"]},{"title":"Department of Computer Science Hong","paragraphs":["Kong University of Science & Technology"]},{"title":"Clear Water Bay, Hong Kong","paragraphs":["dekaiacs .ust shk Abstract","We propose a method to improve search time and space complexity in statistical machine translation architectures, by employing linguistic bracketing information on the source language sentence. It is one of the advantages of the probabilistic formulation that competing translations may be compared and ranked by a principled measure, but at the same time, optimizing likelihoods over the translation space dictates heavy search costs. To make statistical architectures practical, heuristics to reduce search computation must be incorporated. An experiment applying our method to a prototype Chinese-English translation system demonstrates substantial improvement."]},{"title":"1 Introduction","paragraphs":["The work we discuss here is embedded within the SILC project at HKUST (Wu 1994; Fung Wu 1994; Wu & Fung 1994; Wu & Xia 1995; Wu 1995a; Wu 1995b; Wu 1995c) which focuses on problems of machine translation learning. We are developing machine learning techniques to bear upon the shortage of adequate knowledge resources for natural language analysis, particularly for Chinese where there is relatively little previous computational linguistics research from which to draw. It is one of our objectives to investigate the suitability for Chinese of the statistical translation model originally proposed by IBM (Brown et al. 1990; Brown et al. 1993) for Indo-European languages. Henceforth we will therefore use \"Chinese\" to refer to the source language and \"English\" to refer to the target language, reflecting the . prototype SILC system.","An inherent characteristic of the basic IBM stochastic channel model is the large search space, due to the wide range of distortions that must be allowed in order to successfully transfer sentences of one language to the other. The underlying generative model maps target•language strings into source-language strings (i.e., in the reverse direction from translation). During translation, a maximum likelihood target•language string is sought for the 195 input source-language string, according to Bayes' formula:","(1)\\targmax Pr(elc) = argmax Pr(cle) Pr(e) e\\te The distortion operations in the channel model are chosen to permit sufficient flexibility to map English strings into Chinese translations that have greatly different word order. (It is a simplifying assumption of the model that the only sentence translations considered are those where the majority of words can be translated by lexical substitution.) The scheme admits many implausible mappings along with the legitimate translations, but thereby gains robustness. During the recognition process, legitimate translations will be selected so long as the implausible mappings have lower likelihoods.","The IBM model employs an A* search strategy on the space of translation hypotheses using incremental hypothesis expansion. The distance-to-goal heuristic is not admissible but reasonable estimates can be made yielding good performance. This approach arguably provides the highest possible accuracy assuming that no additional information is available.","In reality, however, additional information can usually be made available. The method we propose here exploits one such type of information, namely, that a preprocessing stage can be used to annotate the input source-language sentence with a syntactic bracketing. We will not dwell on the bracketing method here; numerous approaches for automatic bracketing have been developed, including strategies employing full grammars, local patterns, and information-theoretic metrics. Work on Chinese parsing (Jiang 1985; Zhou & Chang 1986; Lum & Pun 1988; Lee & Hsu 1991; Lee et al. 1992) would be particularly applicable here."]},{"title":"2 Baseline Translation Model","paragraphs":["The translation system employs two main sets of learned parameters corresponding to the two factors on the right side of Equation 1: the language model and the translation model.","Parameters for the translation model consist of (1) translation probabilities Pr(cle) which describe bilingual lexical correspondences in terms of the probability that a given English word e translates into a Chinese word c, and (2) alignment probabilities Pr(ai lj,l, m) which crudely describe word order variation in terms of the probability that a word in position j of a length-m Chinese sentence corresponds to a word in position a, of a corresponding length-/ English translation. The translation and alignment probabilities are automatically estimated by an iterative expectation-maximization algorithm (Wu & Xia 1995), using as training data a parallel bilingual corpus containing parliamentary transcripts from the Hong Kong Legislative Council which are available in both English and Chinese versions. The size of the training corpus was approximately 17.9Mb of raw English text and 9.6Mb of corresponding raw Chinese translation, or about 3 million English words, and approximately 3.2 million Chinese words (under certain Chinese segmentation assumptions). Since these proceedings were not originally available in machine-analyzable form, it was necessary to carry out data conversion and reformatting using manual and automatic processing, and then to perform automatic sentence alignment (Wu 1994). 196","Parameters for the English language model, on the other hand, were estimated from a much larger monolingual corpus to reduce sparse data problems. About 280Mb of text from the Wall Street Journal were used to to obtain a bigram model with the parameters are Pr(ei (ei _ i ), under a vocabulary restriction to match the translation lexicon.","Given the parameters, translation of a test sentence in Chinese is performed by a search to solve Equation 1. In our baseline system, we employ a beam search algorithm, a variation of A* with a thresholded agenda width."]},{"title":"3 Incorporating Bracketing Constraints","paragraphs":["In the baseline model, the coupling between words of the test sentence is ignored. The search process considers each of the input tokens as an individual word. In reality, however, often there exist known relations between individual words, as for example in \\tJA\\t"]},{"title":"p","paragraphs":["IN* o ), where\\tnx is a noun phrase in which\\tis a measuring element to describe A. Thus we would not expect the translations of these two tokens to be separated far apart in the target output. Again, in (ftill \\t"]},{"title":"Z","paragraphs":["tr, IJ o ), we consider (13E tJ f1.14) a phrase to be translated as a unit.","The search strategy we propose accepts any available bracketing information, full or partial. The bracketing information is used to partition the search in divide-and-conquer fashion. Innermost constituents are translated first, then assembled compositionally into larger constituents. Within any level of bracketing, an A* search is performed. The merits of the bracket-guided search strategy can be summarized as follows:","1. Use of divide-and-conquer. The problem of finding a complete English translation is recursively decomposed into sub-problems of finding translations of substrings.","2. Independence of syntactic knowledge. While it is true that the bracketing preprocessor may utilize syntactic knowledge, such knowledge is not used by the search algorithm itself. Moreover, the brackets do not carry syntactic category labels. Thus if alternative non-syntactic (e.g., statistical) bracketing strategies are available, the proposed algorithm can be deployed without any grammar.","3. Preservation of robustness. The spirit of the statistical approach with respect to robustness is preserved. At one extreme, given a complete bracketing of the input sentence, the solution of the sub-problems immediately yields the solution to the original problem. At the other extreme, if no brackets are given (or equivalently, each individual input token is bracketed by itself), the algorithm simply degenerates into the baseline model. In between the extremes, the search is guided heuristically as in the baseline model.","Our search algorithm dictates that nodes in the lower levels (those with higher level numbers) of the tree of c must be processed before nodes in the higher levels. In Figure 1, we have five subtrees labeled S1, S2 S3 S4, and S (which is the whole sentence). subtree 54 is processed first, followed arbitrarily by Si , S2 or S3. If we assume the subtrees Si and then 197 Level 0 Level 1 Level 2 Level 3 Figure 1: Example bracket structure of a test sentence c. S3 are processed next, the intermediate result will be as shown in Figure 2, where Pi, P2 and P3 hold English substrings. Thus at any point during the search, a subtree may consist of:","1. Chinese tokens only. In this case, the sub-search is identical to that in the baseline system.","2. English substrings only. All lexical translations have been made; it may still remain to align the English substrings.","3. A mixture of Chinese tokens and English substrings. This is analogous to a partial hypothesis in the baseline model where some of the English words have been translated. As above, the English substrings may still need to be aligned. In addition the Chinese tokens must still be translated and aligned. We impose an additional assumption: the available English substrings are aligned prior to continuing the search on Chinese token translations. The search algorithm follows the general schema below:","• While unprocessed nodes in the Chinese tree remain, choose an unprocessed subtree at the deepest remaining level, and replace Si with its translation computed as follows:","1. Create hypothesis nodes in the search tree representing alternative target lengths 1 for the output English phrases P that might be translations of Si. 198 Level 0 Level 1 Level 2 Figure 2: Bracket structure of an intermediate sentence translation hypothesis, where subtrees Sl, S3, and S4 of Figure 1 have been processed.","2. Arrange the search order of any previously computed English substrings under Si according to their length-normalized joint probability g = Pr(e) Pr(c, ale).","3. While any previously computed English substrings of the subtree remain to be processed: (a) Let"]},{"title":"p*","paragraphs":["be the remaining English substring with largest value of g. Expand the hypothesis space to include the set of hypotheses that include"]},{"title":"p*","paragraphs":["(each hypothesis corresponds to mapping"]},{"title":"p*","paragraphs":["to a different location in"]},{"title":"P).","paragraphs":["Calculate ft) for each hypothesis.","4. (At this point the subtree consists of Chinese tokens only.) Initialize a set of hypotheses using the translation probabilities: for each Chinese word c3 in Si, find all English words e such that Pr(c; le) is non-zero. Arrange their search order according to their Pr(ci le) value. 5. While any Chinese tokens remain to be processed:","(a) Expand the hypothesis with the maximum remaining Pr(c; le) value. Generate subhypotheses that associate alternative positions aj for the English word e. Calculate id, for each hypothesis. 6. (At this point all Chinese in the subtree has been eliminated.) For each hypothesis:","(a) While empty positions in the output string remain: i. Fill in the empty positions using the bigram probabilities Pr(e i lei _ i ) from","the language model, and calculate iv. 199"]},{"title":"4 Experiments","paragraphs":["We have tested our model with both natural test cases (from the Hong Kong Hansard) as well as synthetic ones. The synthetic cases are artifically constructed using the natural corpus vocabulary. Only noun phrases and verb phrases were bracketed, using the following simple pattern templates: • NP. 1. two consecutive nouns, e.g. I1 \\tA €; or 2. an adjective + a noun, e.g. WM SU; or 3. two nouns with the word n in between, e.g. 41 1:11 J 114; or 4. an adjective + a NP, e.g. Ma su BM; or 5. two NPs with the word riti in between, e.g.\\trftj\\t"]},{"title":"rog","paragraphs":["In addition, each of the above NP forms allows insertion of a measuring phrase of the form \"(specifier) + (number) + (unit)\" where the parentheses denote optionality. • VP. 1. a verb a noun, e.g. VIIP *V; or 2. a verb + a NP, e.g. m g! **. Erti","As a measure of efficiency, the average number of nodes in the search tree for each strategy was recorded. Table 1 shows the average number of nodes in the search tree expanded per test case for both the baseline and bracketing strategies, with a significant reduction in the search cost. Two example test sentences are shown in the Appendix. For both the cases with and without bracketing on each test sentence input, the top five output candidate translations are shown, along with their log probabilities."]},{"title":"Corpus Test Cases Synthetic Test Cases","paragraphs":["Baseline Bracketing % reduction Baseline Bracketing % reduction 443819 309860 30.2 434351 346702 20.2 Table 1: Average number of nodes in the search tree per bracketed test case","In addition to improving efficiency, the bracketing strategy simultaneously achieves higher accuracy as summarized in the tables below. The correctness criteria for the two sets of test cases are a bit different, as the outputs from the synthetic set do not have any reference translations to serve as an evaluation standard.","For the natural test cases from the corpus, a translation is considered: 200","1. Correct if it is exactly the same as the translation made in the bilingual corpus, or conveys the same meaning as that in the bilingual corpus;","2. Partially Correct if it conveys more or less the same meaning as that in the bilingual corpus and is grammatically incorrect; 3. Not Correct otherwise. Category Baseline Percent Bracketing Percent Correct 11 25.6 16 37.2 Partial Correct 18 41.9 14 32.6 Not Correct 14 32.5 13 30.2 Total 43 100.0 43 100.0 Table 2: Results with test cases from corpus For the synthetic test cases, a translation is considered: 1. Correct if it is an acceptable translation judged by a human evaluator; 2. Partially Correct if it conveys part of the meaning of the original sentence; 3. Not Correct otherwise. Category Baseline Percent Bracketing Percent Correct 10 25.7 13 33.3 Partial Correct 21 53.8 20 51.3 Not Correct 8 20.5 6 15.4 Total\\t","a 39 100.0 39 100.0 Table 3: Results with synthetic test cases"]},{"title":"5 Conclusion","paragraphs":["In most systems only partial bracketing information will be available since full-coverage grammars are not robust. The degree of bracketing affects performance as follows. A minimally-bracketed sentence, where there is only one pair of brackets enclosing the entire sentence, reduces to the original A* search. On the other hand, a fully-bracketed sentence offers the least room for variation in the translation hypotheses, and dictates clausal translation at every 201 level of the phrase structure. Thus speed will be maximally enhanced, but robustness will be minimized. Because of these properties, it is best to bias the bracketer conservatively, i.e., to commit to a pair of brackets only when certain.","This study underlines the effectiveness of combining linguistic analysis with statistical corpus-based techniques for practical applications such as machine translation. A conservative use of linguistic analysis improves both speed and accuracy, while maintaining the robustness and broad coverage of statistical methods."]},{"title":"References","paragraphs":["BROWN, PETER F., JOHN COCKE, STEPHEN A. DELLAPIETRA, VINCENT J. DELLAPI-ETRA, FREDERICK JELINEK, JOHN D. LAFFERTY, ROBERT L. MERCER, & PAUL S. ROOSSIN. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):29-85.","BROWN, PETER F., STEPHEN A. DELLAPIETRA, VINCENT J. DELLAPIETRA, ROBERT L. MERCER. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.","FLI NG, PASCALE & DEKAI WU. 1994. Statistical augmentation of a Chinese machine-readable dictionary. In Proceedings of the Second Annual Workshop on Very Large Corpora, 69-85, Kyoto.","JIANG, Y.P. 1985. Chinese Parsing: An Initial. Exploration at LRC. Computer Processing of Chinese and Oriental Languages, 2(2):127-138.","LEE, H.J., J.C. DAI,\\tY.S. CHANG. 1992. Parsing Chinese Nominalizations based on HPSG. Computer Processing of Chinese and Oriental Languages, 6(2):143-158.","LEE, H.J. & P.R. Hsu. 1991. Parsing Chinese Sentences in a Unification-based Grammar. Computer Processing of Chinese and Oriental Languages, 5(3-4):271-284.","Lum, B. & K.H. PUN. 1988. On Parsing Complex Noun Phrases in a Chinese Sentence. In 1988 International Conference on Computer Processing of Chinese and Oriental Languages. Proceedings, 470-474.","Wu, DEKAI. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics, 80-87, Las Cruces, New Mexico.","Wu, DEKAI. 1995a. An algorithm for simultaneously bracketing parallel texts by aligning words. In Proceedings of the 33rd Annual Conference of the Association for Computational Linguistics, 244-251, Cambridge, Massachusetts.","Wu, DEKAI. 1995b. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proceedings of IJCA 1-95, Fourteenth International Joint Conference on Artificial Intelligence, Montreal. To appear. 202","WU, DEKAI. 1995c. Trainable coarse bilingual grammars for parallel text bracketing. In Proceedings of the Third Annual Workshop on Very Large Corpora, 69-81, Cambridge, Massachusetts.","WU, DEKAI & PASCALE FUNG. 1994. Improving Chinese tokenization with linguistic filters on statistical lexical acquisition. In Proceedings of the Fourth Conference on Applied Natural Language Processing, 180-181, Stuttgart.","WU, DEKAI XUANYIN XIA. 1995. Large-scale automatic extraction of an English-Chinese lexicon. Machine Translation. To appear.","ZHOU, J.Y.\\tS.K. CHANG. 1986. A Methodology for Deterministic Chinese Parsing. Computer Processing of Chinese and Oriental Languages, 2(3):139-161."]},{"title":"Appendix Sentence","paragraphs":["1,"]},{"title":"unbracketed:,","paragraphs":["1. -1.95992 Sir(3M) ,(,) IM) )(NULL) These(itiL4) figures(a) exist(4)\\t) 2. -2.03140 Sir(5M) ,(NULL) my(ft) remarks(NULL) ,(,) were() these(ilLt ) figures(ft 3. -2.05858 Sir(5M) ,(NULL) my(R) remarks(NULL) ,(,) there() such(1-14-_t ) figures(a","4. -2.06033 Sir(tt.) ,(,) my(ft) main(NULL) duty(4) </s>(NULL) These(ZIA ) figures(n .(o 5. -2.06463 Sir(5M) ,(,) my(R) colleagues(NULL) have(4) .(NULL) These(tig) figures(n"]},{"title":"Sentence 1, bracketed: (\\t))( , )","paragraphs":["1. -1.40294 Mr(3M) Deputy(NULL) President(I g ) ,(,)"]},{"title":"I(T-1)","paragraphs":["have() these()11) figures(!( 2. -1.53207 Sir(tt.) ,(,) these(I lL) figures(n) I(R) have(4) 3. -1.63287 Mr(5M) Deputy(NULL) President(IPS)"]},{"title":"I(N)","paragraphs":["have() these(14,t ) figures* 4. -1.63503 Sir(IM) I(N) have(4) these(te,) figures(EZ)\\t)","5. -1.68973 Mr(5tt.) Deputy(NULL) President(IS) I(R) have(,) these(ta) figures(11/ )\\t) Sentence 2, unbracketed: (Main 1. -6.61285 tantamount(ga) no0k) need(NULL) deanOM and(NULL) environmental( \\t campaign(10)\\t) 2. -6.61400 tantamount(ga) clean(71X) and(NULL) not() contravene(NULL) environmental("]},{"title":"atm)","paragraphs":["campaign(ffi) )","3. -6.97496 does() not(*) to(NULL) clean( X) tantamount(ga) environmentalMa* iff) campaignOW )","4. -7.18585 does() no(*) need(NULL) clean(IN) tantamount(4a) environmental(Ma campaign(;16)\\t)","5. -7.20299 no(10 clean() and(NULL) not() tantamount(4a) environmentalMaig g() campaign(iffh)\\t) Sentence 2, bracketed: ( irta4;ra\\t Nth)) 1. -4.38424"]},{"title":"Eprortgint)","paragraphs":["should(NULL) not(*) tantamount(ga) to(NULL) clean(ig) campaigns(ffft) .(0","2. -4.40295 EPD(ilaig) should(NULL) not() tantamount(4a) to(NULL) clean(MX) campaignOW )","3. -4.41738 Protection(riagN) is(NULL) not(*) tantamount(4a) to(NULL) cleanOIX campaigns(ffth)\\t) 4. -4.42583 EPD(Olafggi) should(NULL) not() tantamount(4a) clean(Mg) up(NULL)"]},{"title":"campaigns(M) )","paragraphs":["5. -4.43609 Protection(AMM) is(NULL) not() tantamount(4a) to(NULL) cleanein campaign(10)\\t) 204"]}]}