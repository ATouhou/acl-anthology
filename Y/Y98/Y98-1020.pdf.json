{"sections":[{"title":"","paragraphs":["Language, Information and Computation (PACLIC12), 18-20 Feb, 1998, 206-211"]},{"title":"Extracting Recurrent Phrases and Terms from Texts Using a Purely Statistical Method Zhao-Ming Gao","paragraphs":["*"]},{"title":"and Harold Somers** Academia Sinica","paragraphs":["*"]},{"title":"and UMIST**","paragraphs":["Most statistical measures for extracting interesting word pairs such as MI and t-score require a large corpus to work well. This paper evaluates some of the most widely used statistical measures and introduces a method that can identify significant bigrams in relatively small texts by adapting Fung and Church's (1994) K-vec algorithm, which was originally designed to extract word correspondences from unaligned parallel corpora. The proposed method captures the linguistic generalisation abou lexical patterning in texts and can identify recurrent co-occurring word sequences, which might be phrases, terms, or unknown words. In addition, it has the potential of identifying key phrases and terms that reveal topicality in a text."]},{"title":"1. Introduction","paragraphs":["In recent years, there has been a growing interest in eliciting linguistic knowledge directly from corpora using statistical methods. Several quantitative measures have been proposed to identify significant lexical relations. These measures, however, are designed to work with large corpora with millions of words. Accordingly, they do not perform well in relatively small texts with a few thousand words. This paper presents a statistical method that is well-suited to extracting recurrent phrases and terms from relatively small texts. The method, a variant of Fung and Church's (1994) K-vec algorithm, is shown to be in line with linguistic generalisations about lexical cohesion in text structures."]},{"title":"2. Statistical Measures for Identifying Interesting Lexical Relations","paragraphs":["Church and Hanks (1990) and Church et al. (1991) use mutual information (MI) to estimate associations between two words. Mutual information is defined as follows. P(x, y) (1) 1(x, y) = log2 P(x)P(y) MI compares the joint probability P(x, y) (i.e. the probability of the co-occurrence of x and y) with P(x) and P(y), the independent probabilities of x and y (chance). If there is a strong association between word x and word y, then the joint probability P(x, y) will be much larger than chance P(x)P(y), and accordingly 1(x, y) > 0. If no significant relation holds between x and y, 1(x, y) will approximate to zero. If x is in complementary distribution with y, I(x, y) will be less than zero. Besides MI, Church and Hanks (1990) and Church et al. (1991) use t-score for testing the statistical significance of an co-occurrence. t-score can be approximated by (2). f (x, y ) -"]},{"title":"f (x)","paragraphs":["(Y) (2). t \\tN f (x, Y) where f(x), f(y), and f(x, y) are the number of occurrences of x, y, and x co-occurring with y, respectively; while N is the number of occurrences of all the tokens in the text. *Chinese Knowledge Information Processing Group, Institute of Information Science, Academia Sinica, Nankang, Taipei 115, Taiwan. E-mail: imgao@hp.iis.sinica.edu.tw"]},{"title":"206","paragraphs":["a=k(AB) b=k(--AB) c = k(A —B) d k(—A —B) Language, hiormation and Computation (PACLIC12), 18-20 Feb, 1998, 206-211 Several alternatives to MI and t-score have been proposed. These methods require the contingency table in (3). (3). where A and B are the words in question, and k is the count of the bigrams. The — sign means not; so for example c is the count of the bigram where A is followed by a word other than B.","One of the alternatives to MI is the association measure IM, which is very similar to MI. IM is calculated as in (4) (cf. Daille (1996)).","(4). IM = log2 a (a + b)(a + c) In addition, Gale and Church (1991) introduce the 0.2 coefficient using the formula in (5).","2 \\t(axd—bxc)2 (5). = (a + b)(a + c)(b + d)(c + d)","Dunning (1993) notes that MI is subject to overestimation when the counts are small and thus proposes using log likelihood ratio G2 as a significance test for estimating surprise and coincidence of a rare event. G 2 is computed by the formula in (6). (6). G2 = a log a + b log b + c log c + d log d • (a+b)log(a+b) — (a+c)log(a+c) — (b+d)log(b+d) — (c+d)log(c+d) + (a+b+c+d)log(a+b+c+d)","We conducted experiments testing all the statistical measures described above with a Chinese text of 5155 words. The Chinese text was preprocessed by the Chinese word segmentation program reported in Chen and Liu (1992). The results of the tests are shown in Table 1. Bigrams with a t-score lower than 1.65 have been left out. As can be seen in Table 1, the performance of MI and t-score is not satisfactory, for many uninteresting bigrams containing pronouns and determiners are incorrectly extracted. It is obvious in Table 1 that (1: 12 and G2 outperform MI and t-score. Nevertheless, (112 gives a zero value for word pairs which always co-occur with each other, since b + d in (6) is zero if word pairs always co-occur. Therefore, bigrams consisting of proper nouns such as"]},{"title":".1. I","paragraphs":["H̀siao Hung', Ni"]},{"title":"le","paragraphs":["'Ho Te',","'Te Fen' in Table 1 are given zero value, which is counterintuitive, because high values for rigid pairs are expected. Besides, icrs2 and G2 do not seem to be able to distinguish bigrams containing two content words from those containing one function word. For instance, G 2 gives a larger value to — 13.7. yi wei 'one CLASSIFIER' than the more interesting proper names"]},{"title":"/j\\ I","paragraphs":["lisiao Hung' and g . 0E7 L̀u Anni'. IM seems to outperform all the other statistical measures in small texts. By setting the threshold to —3, all the proper names together with some interesting terms such as"]},{"title":"tat","paragraphs":["f̀eminism',\\ttt 'equal right',"]},{"title":"fiec AA","paragraphs":["'administrative staffcan be extracted. However, IM has a serious defect: its threshold value is difficult to determine."]},{"title":"3. Modifying Fung and Church's (1994) K-vec Algorithm to Extract Recurrent Monolingual Terms","paragraphs":["Fung and Church (1994) propose a simple algorithm to find word correspondences from unaligned parallel texts. The basic idea is that a true word pair should have similar distributions in terms of the position of its occurrence in the text. To estimate the similarity of co-occurrence, the parallel texts are split into the same number of segments (K) and the distributions of each word are represented in a 1...K binary vector. For instance, suppose the Chinese and English texts are divided into ten segments. Suppose further that the Chinese word tig4t. daxue occurs ten times, with the first 3 occurrences in the fourth segment and the remaining 7 occurrences in the seventh segment and that the English word university appears twelve times, with the first 4 occurrences in the fourth segment and the remaining 8 occurrences in the"]},{"title":"207","paragraphs":["Language, Information and Computation (PACLIC12), 18-20 Feb, 1998, 206-211 seventh segment. Using the K binary vectors, the distributions of both the Chinese and English words in question can be represented as <0,0,0,1,0,0,1,0,0,0>. Mutual information (MI) and t-score are then used to estimate the correlation of a proposed word correspondence. Mutual information and t-score are computed using the formulas in (8) and (9). P(Vc,Ve)","MI(Vc ,Ve )= log g2 PV P(Vc)= a+b P(Ve )= a -11-cc* P (V c , V e ) = where a is the number of pieces of segments in which both the Chinese and the English word occur; b is the number of pieces of segment where only the Chinese word is found; c is the number of pieces of segment where only the English word is found. (8). t(Vc,Ve)= P(Vc ,Ve )– P(Vc)P(Ve) P(Vc,Ve)"]},{"title":"K","paragraphs":["The t-score in (8) is introduced to filter out word pairs with low frequency which happen to co-occur in the same segment by chance.' Fung and Church set the threshold value of MI to be 0 and t-score to be 1.65. Only word pairs with both MI and t-score higher than the predetermined threshold values and in the frequency range 3-10 are considered to be potential mutual translations.","The rationale behind the K-vec algorithm is that two words in parallel text associate strongly with each other if they co-occur more often than by chance in some text segments. The statistics of co-occurrence K-vec employs is actually grounded on a linguistic generalisation about lexical patterning in the text. Research by Halliday and Hasan (1976) and Hoey (1991) suggest that cohesion plays a very important role in the organisation of texts. They point out that the most straightforward form of cohesion is repetition. In addition, as each text has a topic, words or phrases closely related to the topic tend to recur in the text (cf. Salton and McGill (1983), Phillips (1985)).","K-vec can be easily applied to monolingual texts to identify recurrent noun phrases, collocations, or words not listed in the dictionary. The only necessary adaptation is that the source is the same as the target text. In addition, since sentences are the basic building blocks of a text, they are better units of a discourse segment than an ad hoc number of words as proposed by the original K-vec. As a result, a Word-Sentence Index (WSI) is required which records the position and the index of the sentence in which each word occurs. Based on WSI, adjacent word pairs that co-occur more often than by chance can be extracted.","Comparing Table 1 with Table 2 we can see that K-vec is better than MI, t-score, 0 2, and G2 in identifying collocations, recurrent proper names and phrases in a small text in terms of precision and recall. Like IM, K-vec can distinguish interesting bigrams from uninteresting ones. But unlike IM, the threshold value of K-value is predetermined (i.e. MI >=0 and t-score >= 1.65). K-value is thus more convenient than IM. In contrast with Smadja's (1993) Xtract, which was designed to extract collocations from large corpora, our proposed method is suitable for extracting recurrent rigid collocations in relatively small texts.","If two extracted bigrams are adjacent to each other, they are mostly likely to be phrases or proper nouns, as shown in Table 3. The proximity relation between two bigrams can be easily identified in the light of the WSI. It is interesting to note that many of the word pairs identified in Table 2 are key phrases that suggest topicality of the text, e.g."]},{"title":"rstif:mf","paragraphs":["L̀u Anni Incident',"]},{"title":"tcltIa","paragraphs":["'feminism',01M1,* leacher-student relationship', VanrIN. 'Campus Affairs Committee Conference'. 'The approximation.of t-score used by Fung and Church (1994) in (8) is slightly different from (2)."]},{"title":"K 208","paragraphs":["Language, Information and Computation (PACLIC12), 18-20 Feb, 1998, 206-211"]},{"title":"4. Conclusion","paragraphs":["This paper reconfirms the importance of selecting an appropriate unit of text in lexical knowledge acquisition, as emphasized by Church et al. (1991). The proposed method, a simple variant of MI, t-score, and K-vec, has a higher precision than most current statistical algorithms in extracting recurrent word sequences from relatively small texts. The algorithm can be used to identify Chinese unknown words or key phrases in any language."]},{"title":"Acknowledgement","paragraphs":["The first author would like to thank Prof. C.-R. Huang, Prof. K.-J. Chen, Dr. L.-F. Chien at Academia Sinica and anonymous PACLIC reviewers for their comments on an earlier draft of this paper."]},{"title":"References","paragraphs":["Chen, K.-J. and Liu, S.-H. (1992) \"Word Identification for Mandarin Chinese Sentences.\" In Proceedings of the","International Conference on Computational Linguistics, pp. 101-107. Church, K. and Hanks, P. (1990) \"Word Association Norms, Mutual Information, and Lexicography.\" Computational","Linguistics, Vol. 16, No. 1, pp. 22-29. Church, K, W. Gale, P. Hanks, and D. Hindle. (1991) 'Using Statistics in Lexical Analysis,' in Zernik (ed.) Lexical","Acquisition: Exploiting On-Line Resources to Build a Lexicon, pp. 115 - 164, Lawrence Erlbaum Associates","Publishers. Daille, B. (1996) \"Study and Implementation of Combined Techniques for Automatic Extraction of Terminology.\" In","Klavans and Resnik (eds.) The Balancing Act: Combining Symbolic and Statistical Approaches to Language, MIT","Press, pp. 49-66. Dunning, T. (1993) 'Accurate Methods for the Statistics of Surprise and Coincidences,' Computational Linguistics, Vol.","19, No. 1, pp. 61- 74. Fung, P. and Church, K. (1994) \"K-vec: A New Approach for Aligning Parallel Texts.\" Proceedings of the International","Conference of Computational Linguistics, pp. 1096-1102, Kyoto. Gale, W. and Church, K. (1991) \"Concordances for Parallel Texts.\" In Proceedings of the Seventh Annual Conference of","the UW Centre for the New OED and Text Research, Using Corpora, pp. 40-62, Oxford. Halliday, M. and Hasan, R. (1976) Cohesion in English. Longman Publishers. Hoey, M. (1991). Patterns of Lexis in Text. Oxford University Press. Phillips, M. (1985) Aspects of Text Structure: An Investigation of the Lexical Organisation of Text. Elsevier Science","Publishers. Salton, G. and McGill, M. (1983) Introduction to Modern Information Retrieval. McGraw-Hill. Smadja, F. (1993) 'Retrieving Collocations from Text: Xtract', Computational Linguistics, Vol. 19, No. 1, pp. 143 - 177. Table 1. Output of Different Statistical Measures for Identifying Interesting Bigrams Cl\\tC2\\tMI\\tt-score\\tIM\\t(1)"]},{"title":"2","paragraphs":["G2 TI\\t6.23\\t3.95\\t-5.90\\t33.19\\t57.20 flE\\t4.46\\t1.90\\t-7.67\\t1.45\\t8.03 \\t 3.33\\t1.80\\t-8.80\\t0.43\\t5.24"]},{"title":"1M\\t","paragraphs":["5.91\\t1.70\\t-6.22\\t4.01\\t9.21 EER\\t7.18\\t1.72\\t-4.95\\t0.09\\t13.12 43ZU\\t4.82\\t2.72\\t-7.31\\t3.85\\t18.24"]},{"title":"ft.\\t","paragraphs":["4.83\\t2.36\\t-7.30\\t1.25\\t16.04 tt\\tE4%\\t9.81\\t1.73\\t-2.32\\t0.59\\t18.74 /.1\\\\ttE\\t9.14\\t2.82\\t-3.00\\t0.00\\t50.97","Zit\\t6.41\\t1.97\\t-5.72\\t0.07\\t15.61 aF\\t11\\t3.55\\t3.03\\t-8.59\\t1.01\\t16.49"]},{"title":"aF\\t","paragraphs":["1M\\t4.99\\t1.67\\t-7.14\\t1.67\\t7.14 NA\\t6.41\\t1.71\\t-5.72\\t0.05\\t11.68"]},{"title":"T 4)\\tgo\\t","paragraphs":["5.84\\t2.40\\t-6.29\\t6.13\\t19.05 \\t","4.97\\t1.67\\t-7.16\\t1.75\\t7.02 ft\\t 4.85\\t2.15\\t-7.29\\t2.97\\t11.22 209 Language, Information and Computation (PACLIC12), 18-20 Feb, 1998, 206-211 g\\t9.81\\t1.99\\t-2.32\\t)609.00\\t25.72"]},{"title":"4S-\\tIR\\t","paragraphs":["7.91\\t1.72\\t-4.22\\t80.29\\t12.93 E\\t45Z0\\t3.36\\t2.21\\t-8.77\\t0.56\\t8.10 E\\tMRIt\\t5.87\\t2.19\\t-6.26\\t4.86\\t16.3"]},{"title":"P\\t{4\\t","paragraphs":["8.14\\t2.22\\t-4.00\\t175.80\\t23.29 1tZ\\tT\\t7.18\\t1.72\\t-4.95\\t15.50\\t13.12 \\t","3.00\\t1.75\\t-9.13\\t0.29\\t4.48 44.\\t"]},{"title":"Ili.\\t","paragraphs":["6.58\\t2.96\\t-5.55\\t0.18\\t36.54"]},{"title":"4\\tItl\\t","paragraphs":["6.64\\t2.61\\t-5.49\\t18.30\\t28.56"]},{"title":"4nfi\\tn\\t","paragraphs":["1.19\\t1.68\\t-10.90\\t0.02\\t2.64 ffff\\t:E\"\\t3.49\\t1.82\\t-8.64\\t0.47\\t5.65 -It--Ez\\t"]},{"title":").ja\\t","paragraphs":["9.33\\t172\\t-2.80\\t0.42\\t17.52","'62\\t55\\t5.94\\t1.70\\t-6.19\\t7.22\\t8.77","4JE\\t8.97\\t2.82\\t-3.16\\t0.88\\t48.24","55\\t"]},{"title":"4n\\t","paragraphs":["5.28\\t2.38\\t-6.85\\t3.65\\t16.06"]},{"title":"5\\tI;(d.q\\t","paragraphs":["5.19\\t1.94\\t-6.95\\t2.91\\t9.99","J:\\t6.11\\t1.70\\t-6.02\\t5.38\\t9.59","X\\t--\\t6.23\\t1.97\\t-5.90\\t6.58\\t13.80","22\\t:;\\t5.15\\t2.17\\t-6.99\\t3.09\\t12.55 \\t","6.85\\t1.98\\t-5.28\\t13.12\\t16.72 if:b\\t"]},{"title":"4n\\t","paragraphs":["5.31\\t2.17\\t-6.82\\t3.10\\t13.46 ii\\t1441\\t9.40\\t1.72\\t-2.73\\t676.60\\t16.79 ig\\t"]},{"title":"I \\t","paragraphs":["9.55\\t1.99\\t-2.58\\t0.66\\t24.57"]},{"title":"*4\\tizIR\\t","paragraphs":["7.18\\t1.72\\t-4.95\\t15.50 , 13.12 4\\tA$\\t10.55\\t1.73\\t-1.58\\t0.00\\t21.67"]},{"title":"J\\tItt\\t","paragraphs":["3.47\\t2.72\\t-8.67\\t0.34\\t15.50"]},{"title":"n\\t","paragraphs":["4Kfl\\t2.67\\t1.68\\t-9.46\\t0.07\\t4.33"]},{"title":"J\\t","paragraphs":["Oft.\\t1.80\\t1.89\\t-10.30\\t0.04\\t4.00"]},{"title":"n\\t","paragraphs":["MR\\t3.00\\t1.75\\t-9.13\\t0.09\\t5.26"]},{"title":"I \\t5k\\t","paragraphs":["10.14\\t1.99\\t-2.00\\t0.00\\t27.89 Igiw\\t"]},{"title":"1m\\t","paragraphs":["6.64\\t1.980\\t-5.49\\t9.69\\t16.20 fi\\t--J7\\t5.60\\t1.69\\t-6.53\\t4.15\\t8.18 1\\tA\\t6.31\\t1.71\\t-5.82\\t11.19\\t9.51","t.\\t","WI*\\t6.40\\t3.42\\t-5.73\\t35.92\\t42.24 tk-\\tP3\\t8.23\\t1.72\\t-3.90\\t112.60\\t13.77 Wig\\tltRi\\t8.09\\t3.30\\t-4.04\\t598.60\\t55.88 15ZIN\\t41\\t6.01\\t3.11\\t-6.12\\t16.82\\t32.10 bt . III\\t","J\\t1.45\\t1.67\\t-10.60\\t0.02\\t2.80 tZISI\\t","4\\t6.29\\t2.20\\t-5.84\\t10.27\\t17.10"]},{"title":"a\\tfU\\t","paragraphs":["10.55\\t1.73\\t-1.58\\t0.00\\t21.67 TrA\\tIg\\t9.81\\t1.73\\t-2.32\\t1353.00\\t18.74 3A\\t","/1\\\\t8.65\\t2.23\\t-3.48\\t402.30\\t25.67 IM\\tn\\t. 1.51\\t1.83\\t-10.60\\t0.03\\t3.42 gift\\t"]},{"title":"MN\\t","paragraphs":["5.36\\t1.69\\t-6.77\\t2.77\\t7.81 V\\tWt.\\t2.80\\t1.71\\t-9.33\\t0.14\\t4.21 PrF?-i.\\t"]},{"title":"4g\\t","paragraphs":["3.99\\t1.87\\t-8.14\\t0.78\\t6.88","..-\\tMIT\\t4.67\\t2.35\\t-7.46\\t2.21\\t13.12","74..1.\\t1 i","\\t5.49\\t1.69\\t-6.64\\t4.51\\t7.86"]},{"title":"a\\t","paragraphs":["IR\\t7.01\\t1.71\\t-5.12\\t23.98\\t11.01"]},{"title":"IR\\t","paragraphs":["J\\t3.73\\t2.06\\t-8.40\\t0.22\\t10.04","ii\\t.\\t","It\\t9.33\\t2.64\\t-2.80\\t0.00\\t45.41","Ag*g\\t"]},{"title":"4\\t","paragraphs":["8.23\\t1.72\\t-3.90\\t0.19\\t15.15 * \\t_h.\\t5.85\\t2.60\\t-6.28\\t10.15\\t21.13 210 Language, Ir1ormation and Computation (PACLIC12), 18-20 Feb, 1998, 206-211 -'.\\tVk\\t5.06\\t1.68\\t-7.08\\t2.58\\t7.05 111\\tA\\t10.14\\t1.73\\t-2.00\\t0.74\\t' 19.71"]},{"title":"te\\t","paragraphs":["4\\t9.33\\t2.64\\t-2.80\\t0.00\\t45.41 MS.\\tF.\\t6.59\\t2.21\\t-5.54\\t13.62\\t18.61 V/\\t5.05\\t1.67\\t-7.08\\t0.02\\t9.23 t.\\t- Oi\\t3.14\\t1.77\\t-8.99\\t0.20\\t5.04 \\t","fiffl\\t7.63\\t1.72\\t-4.50\\t39.62\\t12.84 ,"]},{"title":"0\\t","paragraphs":["ill \\t10.55\\t1.73\\t-1.58\\t0.00\\t21.67","Table 2. Chinese Bigrams Extracted Using a Variant of K-vec","A\\tB\\tMI\\tt-score","9.76\\t1.73","9.08\\t2.82 14\\t8.08\\t2.23 O'e\\t8.91\\t2.82","9.49\\t2.00","10.49\\t1.73","10.07\\t2.00","6.71\\t3.84","fSag\\t# \\t7.98\\t2.9","c\\trj\\t10.49\\t1.73","9.76\\t1.73","It\\t9.27\\t2.45","10.07\\t1.73",":.\\t9.27\\t2.64","iS\\t10.49\\t1.73 $4,"]},{"title":"Vi","paragraphs":["It"]},{"title":"E 1.*","paragraphs":["*1j Table 3. Proper Names Extracted On the Basis of Table 2 and Word-Sentence Index"]},{"title":"211","paragraphs":[]}]}