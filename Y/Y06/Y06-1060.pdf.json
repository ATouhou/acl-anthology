{"sections":[{"title":"Make Word Sense Disambiguation in EBMT Practical","paragraphs":["Feiliang REN Tianshun YAO School of Information Science & Engineering, Northeastern University, Shenyang 110004, China renfeiliang@ise.neu.edu.cn Abstract. In an EBMT system, we will meet the word sense disambiguation problem. The disambiguation methods used at present can’t be used easily in EBMT. We propose a new method for word sense disambiguation in EBMT: it is based on a language mode of the target language. Its main idea is that a proper word sense can make the whole sentence fluent. We use a language model to measure this fluency, and use dynamic programming method to compute the proper words sense sequence in EBMT. It has the virtues of easily being used, and doesn’t need extra lingual knowledge, besides, the general performance of it can be improved by using more target language resource to train. And experiment shows our method works well. Keywords: EBMT, word sense disambiguation, language model, N-gram, dynamic programming"]},{"title":"1. Introduction","paragraphs":["Example based machine translation (EBMT[1]",") is a method of translation by the principle of analogy. When given an input sentence, the EBMT system first retrieval the bilingual aligned corpora and select one or some example sentences as translation templates whose source sentence parts are similar to the given input sentences. And system then modifies the target parts of these translation templates to get finial translation result. These modification operations can be classified as: delete, replace, and insert.","There are many methods for retrieving some translation templates whose source parts are similar to the input sentences. When we have finished this step and begin to the second step: modify the translation templates, the ambiguity problem comes. For example, for a Chinese-Japanese EBMT system, the input Chinese sentence is and what’s we retrieved translation template is: , we can see there is a Chinese word doesn’t exist in the source part of the translation template, to get the right translation of the input sentence, we must insert the sense of in a proper place of the target part of the translation template (the proper insert place can be gotten by comparing two string). If from a bilingual dictionary we know, there are different senses for the Chinese word , which sense should we choose? The same situation will occur also in replacement operation. This is the problem of word sense disambiguation EBMT. 12345"]},{"title":"...","paragraphs":["m"]},{"title":"S ccccc c=","paragraphs":["1245 1234"]},{"title":"... ...","paragraphs":["n"]},{"title":"cc c c c j j j j j↔","paragraphs":["l 3"]},{"title":"c","paragraphs":["3"]},{"title":"c k","paragraphs":["3"]},{"title":"c","paragraphs":["414 Traditional word sense disambiguation methods don’t work here. They will neither meet a knowledge bottleneck [3-5]",", nor lower performance [2]",". And it can’t satisfy the requirement in an EBMT system. Here we propose a new word sense disambiguation method, which is based on the N-gram language model. We think a proper word sense must be the sense that makes the whole sentence looks frequent, and we use N-gram language model[6,7]","to measure this fluency. That is to say in our disambiguation method, we select the word sense that make the whole sentence fluent most. Our paper is organized as following: in section 2, we give the detail description of our disambiguation method, and its computation algorithm; in section 3, are our experiments; and at last, in section 4, we drew our conclusions."]},{"title":"2. Our Disambiguation Method","paragraphs":["Suppose we need to insert m words’ sense in the target part of a translation template, and every word has n different senses. Our aim is to select a proper sense sequence to be inserted that can make the target part of the translation template fluent most. Let’s suppose is a word in the word sequence of the target part of a translation template. is the place where a new word should be inserted. is the j-th sense of the i-th word to be inserted. And the disambiguation in EBMT can be shown in figure 1.Its task is to find a sense path that makes the whole sentence fluent most. i"]},{"title":"j","paragraphs":["i"]},{"title":"P","paragraphs":["ij"]},{"title":"s","paragraphs":["12 11 1 12 1 13 1"]},{"title":"... ... ... ...","paragraphs":["iik kl l"]},{"title":"jj j Pj j Pj j Pj","paragraphs":["−+ − +− + 11 21 31"]},{"title":"............. ............. ........sss","paragraphs":["12 22 32"]},{"title":"............. ............. ........sss","paragraphs":["...... 123"]},{"title":"............. ............. ........","paragraphs":["nnn"]},{"title":"ss s Fig. 1.","paragraphs":["Word Sense Disambiguation in EBMT Our disambiguation method can be denoted as the following formula:"]},{"title":"( ) arg max ( )","paragraphs":["ij"]},{"title":"BestSequence s P S= 1,[1,im ]j n= →∈","paragraphs":["(1) Where is the probability of sentence, and we use N-gram language model to compute it. Suppose we use trigram language model, then can be computed as following:"]},{"title":"()PS ()PS","paragraphs":["121312 21"]},{"title":"() ( )( | )( | ) ( | )","paragraphs":["nnn"]},{"title":"PS Pj Pj j Pj jj Pj j j","paragraphs":["−−"]},{"title":"=⋅⋅⋅","paragraphs":["(2) To make the time cost lower, we use dynamic programming method. 415 Let"]},{"title":"()","paragraphs":["t"]},{"title":"iδ","paragraphs":["denotes the max probability for the current sentence when take the i-th sense at place , we can write"]},{"title":"t ()","paragraphs":["t"]},{"title":"iδ","paragraphs":["as follow: 11"]},{"title":"() max ( , , , | )","paragraphs":["ttt"]},{"title":"iPjjjs","paragraphs":["ti"]},{"title":"Sδ","paragraphs":["−"]},{"title":"=⋅⋅⋅=","paragraphs":["(3) Let denotes the word sense we choose at place t-1 is . That is to say we use the variable to write down the path of the word sense selection sequence. Our disambiguation computation algorithm is in figure 2."]},{"title":"()","paragraphs":["t"]},{"title":"i∆ i ()","paragraphs":["t"]},{"title":"i∆ Initialize","paragraphs":[": 111112"]},{"title":"() ( | )iPsjj","paragraphs":["i"]},{"title":"δ","paragraphs":["−−"]},{"title":"= j","paragraphs":["12"]},{"title":"j","paragraphs":["− 11"]},{"title":"() max () ( | )","paragraphs":["tttjt"]},{"title":"iiPsj ","paragraphs":["1"]},{"title":"() 0i∆=","paragraphs":["( and are the two previous words of the word ) 1i"]},{"title":"s","paragraphs":["11− Recursion: 2t"]},{"title":"jδ δ","paragraphs":["−−"]},{"title":"= 1,","paragraphs":["−"]},{"title":"j in≤ ≤ ","paragraphs":["1"]},{"title":"()","paragraphs":["t"]},{"title":"jj","paragraphs":["−"]},{"title":"∆= Calculation","paragraphs":[":"]},{"title":"() argmax ()","paragraphs":["m"]},{"title":"PS iδ= Trace back","paragraphs":[": _"]},{"title":"arg max[ ( )]","paragraphs":["T T"]},{"title":"X iδ= ","paragraphs":["(__ 1","1"]},{"title":"()","paragraphs":["tt t"]},{"title":"XX","paragraphs":["+ +"]},{"title":"=∆","paragraphs":["_ t"]},{"title":"X","paragraphs":["denotes the inserted word sense at place .)"]},{"title":"t Fig. 2.","paragraphs":["Our computation algorithm for disambiguation Our disambiguation algorithm looks like the Viterbi algorithm, and its time complicity is . 2"]},{"title":"()On 3. Experiments","paragraphs":["We collected 1,400,000 words’ Japanese monolingual corpora from the Internet for the training of language model. In table 1 we give some basic information of the corpora. We use trigram language model for disambiguation. And we select 2,000 Chinese sentences as input for translation. We use the correct ratio to evaluate the performance of our disambiguation method. Denotes as the word number that should be inserted when translating. Denotes as the word number that has been correctly selected for sense. We denote the correct ration for disambiguation as following formula:"]},{"title":"N","paragraphs":["c"]},{"title":"N %100)/( ×= NNioCorrectRat","paragraphs":["c (4)","Our experiments results are in table 2. From the experiments results, it seems our disambiguation doesn’t show much power, but you know, we only use about 37,000 Japanese sentences to training the 416 language model, and if we increase the sentences of training corpora, the performance will increase accordingly. In our experiments, most of the errors for the disambiguation task are because of the data sparsely. We can improve the performance by increasing the scale of the training corpora. Table 1. Basic information of teh corpora used for training language model Sentence number Average words per sentence Average insert and replace words 36962 39.7 1.7 Table 2. Experiments result for our disambiguation method Input sentence number Average insert words per sentence Correct ratio 2,000 2.3 77.2%"]},{"title":"4. Conclusions","paragraphs":["In this paper, we propose a new disambiguation method for EBMT based on N-gram language model. Its main idea is to try to search a word sense sequence that can make the whole sentence fluent most. It doesn’t need large number of disambiguated training data, and is easily used. And experiments show our disambiguation method works well in a Chinese-to-Japanese EBMT system."]},{"title":"References","paragraphs":["[1]Harold Somers.2001. Review Article: Example-based Machine Translation. Machine Translation 14, pp.113-157 [2]Christopher D.Manning, Hinrich Schutze. 2005. Foundations of Statistical Natural Language Processing[M]. pp.143-163 [3]Gale, William A, Kenneth W.Church, David Yarowsky, 1992b. A method for disambiguating word senses in a large corpus[J]. Computers and Humanities 26: 415-439 [4]Brown, Peter F, Stephen A, et al. 1991b. Word sense disambiguation using statistical methods. ACL 29, pp.264-270 [5]Black, Ezra. 1988. An experiment in computational discrimination of English word sense. IBM Journal of Research and Development 32:185-194 [6]Eiji Aramaki, Sadao Kurohashi, Hideki Kashioka, Hideki Tanaka:Probabilistic Model for Example-based Machine Translation, MT Summit X, pp.219-226, 2005. [7]Michael Carl and Andy Way (editors), Recent Advances in Example-Based Machine Translation, Kluwer Academic Publishers, 2003. 417"]}]}