{"sections":[{"title":"Japanese Parser on the basis of the Lexical-Functional Grammar Formalism and its Evaluation","paragraphs":["Hiroshi MASUICHI","Corporate Research Group, Fuji Xerox Co., Ltd.","430 Sakai, Nakai-machi, Ashigarakami-gun,","Kanagawa, Japan hiroshi.masuichi@fujixerox.co.jp","Hiroki YOSHIMURA","Corporate Research Group, Fuji Xerox Co., Ltd.","430 Sakai, Nakai-machi, Ashigarakami-gun,","Kanagawa, Japan hiroki.yoshimura@fujixerox.co.jp","Tom oko OHKUMA","Corporate Research Group, Fuji Xerox Co., Ltd.","430 Sakai, Nakai-machi, Ashigarakami-gun,","Kanagawa, Japan ohkuma.tomoko@fujixerox.co.jp Yasunari HARADA","School of Law, Waseda University","1-6-1 Nishi-waseda, Shinjuku,","Tokyo, Japan harada@wasedajp Abstract We report a Japanese parsing system with a linguistically fine-grained grammar based on the Lexical-Functional Grammar (LFG) formalism. The system is the first Japanese LFG parser with over 97% coverage for real-world text. We evaluated the accuracy of the system comparing it with standard Japanese dependency parsers. The LFG parser shows roughly equivalent performance on the dependency accuracy to the standard parsers. It also provides reasonably accurate results of case detection. 1\\tIntroduction Deep grammatical analyses of input sentences based on theoretically sound grammar formalisms are essential for the further development of such NLP applications as machine translation, dialogue understanding, message extraction, etc. In this paper we report development and performance of a parser for the Japanese language based on the Lexical-Functional Grammar (LFG) formalism (Kaplan and Bresnan, 1982; Dalrymple, 2001),","The Japanese LFG grammar used for this parser is being developed in relation to the Parallel Grammar (ParGram) project (Butt et al., 1999, 2002). In this project, grammars for six languages, English, French, German, Japanese, Norwegian and Urdu', are under way, sharing various design decisions within the LFG formalism. LFG assumes two levels of syntactic representation for a sentence: a c(onstituent)-structure (a tree) and an functional)-structure (attribute value matrices: AVMs). Within LFG, an f-structure is meant to encode a language universal level of analysis, allowing for cross-linguistic parallel ism.2","Our research goal is to construct a practical Japanese LFG parsing system with broad coverage and deep analysis for real-world text. In this paper we describe the details of the system, and show its coverage and accuracy. For the accuracy evaluation, we compared outputs of the Japanese LFG parsing system with outputs of standard bunsetsu3 dependency parsers for Japanese. I Korean has recently been added to this list. The Japanese LFG grammar is used as the basis for a grammar of Korean (Kim et al., 2003). 2 Butt et al. (2002) reported how far the parallelism of f-structures can be maintained across languages in the ParGram project. Frank (1999) reported on a machine translation system which took advantage of the f-structure parallelism. 3 A bunsetsu is a widely accepted unit of syntactic and phonological phrase structures in Japanese. It consists of one content word plus optionally following function words. 298 V XLE input Sentence","Morphological Analyzer","Morpheme Sequence\\t","Lexical Entry","Generator\\t","Generator","Morpheme Sequence\\t","Sentence","for Input Sentence\\t","Lexical Entries Core Lexical Entries Verb Lexical Entries Adjective Lexical Entries Adjectival Noun Lexical Entries Grammar Rules","c-structure\\t f-structure Figure 1: Diagram of the Japanese LFG system","This paper is organized as follows. Section 2 describes the architecture of the Japanese LFG system. In Section 3 we present the Japanese grammar written in the LFG formalism for the system described in Section 2. We show the coverage of the system in 4.1, and explain our approach to evaluating the accuracy of the system and report experimental results in 4.2."]},{"title":"2 Japanese LFG system","paragraphs":["The ParGram project employs the XLE parser and grammar development platform (Maxwell and Kaplan, 1993). XLE produces packed representations, specifying all possible grammar analyses of the input. Japanese is the first Asian language that XLE was applied to (Masuichi and Ohkuma, 2003).","Figure 1 shows the diagram of the Japanese LFG system. An input Japanese sentence is segmented and tagged by the Chasen morphological analyzer (Matsumoto et al., 1999). Then the lexical entry for each word in the input is automatically created (Sentence Lexical Entry: SLE). We implemented 40 templates for lexical entries and SLEs are produced by selecting an appropriate template for each word on the basis of information of the word and the words around it such as part-of-speech, surface form, conjugation, etc. Unknown words are currently treated as nouns.","Verb Lexical Entries (VLEs), Adjective Lexical Entries (ALEs) and Adjectival Noun Lexical Entries (ANLEs) were written based on the case frame information in the Japanese IPAL dictionary (IPA, 1987) and have been manually enhanced. VLEs consist of 10,387 entries and 41,115 functional annotations for 2,366 verbs. ALEs and ANLEs consist of 947 entries and 2,197 functional annotations for 369 words in total. Core Lexical Entries (CLEs) include entries for basic words such as auxiliary verbs, postpositional particles and so forth, plus syntactically important nouns such as"]},{"title":"toki","paragraphs":["(time) and"]},{"title":"aida","paragraphs":["(interval). CLEs consist of 1,252 entries and 1,913 functional annotations for 675 words.","SLEs have the lowest preference; an SLE for a word is overwritten if an entry for the same word already exists in another set of entries. This is intended to recover from erroneous analyses by Chasen. For instance, it is impossible to distinguish correctly the Japanese case marker (postpositional particle)"]},{"title":"de","paragraphs":["from the conjugated form"]},{"title":"de4","paragraphs":["of the auxiliary verb"]},{"title":"da","paragraphs":["at the level of morphological analysis. 4"]},{"title":"This form behaves like a sentential conjunction. 299","paragraphs":["Therefore we ignore the SLE created for de regardless of the output of Chaser. Instead we wrote the CLEs for de, which represent the behaviors both for the case marker de and for the conjugated form de of the auxiliary verb"]},{"title":"da.","paragraphs":["XLE parses the input keeping both possibilities and selects the one that matches the syntactic structure of the whole input.","Grammar Rules (GRs) include 2,468 terms in their disjunctive normal forms and 1,223 functional annotations. GRs have been designed to meet the specifications of f-structures described in Butt et al. (1999). We explain GRs in detail in Section 3. The words in the input are delimited by spaces and passed on to XLE, and XLE outputs c-structures and f-structures.","XLE implements a SKIMMING mechanism (Riezler et al., 2002) to increase the coverage of a grammar. XLE goes into the skimming mode when the amount of time or memory used on the input exceeds a specified threshold. In this mode, XLE does a limited amount of work per sub-tree for the constituents whose processing has not been completed. This mechanism enables the parser to avoid time-outs and memory-shortage problems and can improve the robustness of the system."]},{"title":"3 Japanese LFG grammar 3.1 Basic Rules","paragraphs":["Some of the most noticeable syntactic characteristics of Japanese sentences actually used are relatively free word order, rampant pro-drop and extensive use of complex predication. To account for the free word order, we adopted the following (1) as the most basic rule of our grammar."]},{"title":"(1)","paragraphs":["—>\\t"]},{"title":"PP*\\t","paragraphs":["V GF)= 1 T = I","For every sentence in Japanese, native speaker judgements seem to converge on an 'optimal' word order (Shibatani,"]},{"title":"1990).","paragraphs":["For instance, (2a) is an intuitively strange Japanese sentence, while (2b) is felt to be more natural. It is possible to write a grammar that does not allow for the sentences like (2a). Otani et al. (2000) proposed a Japanese Head-driven Phrase Structure Grammar that is very sensitive to the word order. We, however, adopt (I) to achieve broader coverage because sentences like (2a) frequently appear in real-world text.","(2a) Singaporu e Zyon ga\\tTokyo kara itta. Singapore to John SUBJ Tokyo from went. John went from Tokyo to Singapore. (2b) Zyon ga Tokyo kara Singaporu e itta We use the following type of lexical entry to handle pro-drop. (3) is the (simplified) entry for the verb"]},{"title":"yomu","paragraphs":["(read).","(3) yomu V ( T PRED) = 'yomu<( SUBJ)( OBJ)>' @(PD SUBJ) @(PD OBJ).","PD(GF)=@(DEFAULT (T GF PRED) 'pro'","(T GF PRON-TYPE) null) ProDrop: OT.","DEFAULT(ATT I VALI ATT2 VAL2)= ATT I =VAL I ATT2=VAL2. P̀D' and 'DEFAULT' are macro definitions, and '@' indicates a macro call. 'ProDrop: OT' indicates that the Optimality Theory (Bresnan, 2000) mark TroDrop' is added. We set the preference of"]},{"title":"300","paragraphs":["P̀roDrop' at the lowest level. Therefore '@(PD SUBJ)' and '@(PD OBJ)' work only if no constituent that can be subcategorized for by the verb"]},{"title":"yomu","paragraphs":["exists in the input. Because pro-drop frequently occurs in Japanese, lexical entries like (3) are important for achieving broad coverage. (Frank et al., 2001)","We adopted relatively loosely constrained mono-clausal analyses for verbs to deal with various kinds of complex predications. However, we employed multi-clausal analyses when it is reasonable to consider that a verb includes multiple predicate-argument relations (PARs). This treatment is essential for such NLP applications as question answering, dialog understanding, machine translation, etc. We will describe the multi-clausal analyses in 3.2. 3.2 Predicate-Argument Relations and Cases Japanese case markers (e.g."]},{"title":"ga, o)","paragraphs":["are frequently omitted when a particular particle, a topic particle (e.g."]},{"title":"wa, mo, koso)","paragraphs":["or a focus particle (e.g."]},{"title":"made, bakari, sae),","paragraphs":["is added to a noun phrase. Moreover a case marker can mark a different case than it usually does, when a sentence has a particular syntactic construction. In addition to these problems, the problem of the rampant pro-drop makes it a difficult task to capture the correct PARs and to detect the correct cases in Japanese. Our grammar pays maximum attention to this task.","(4a) Zyon ga\\tyonda hon John SUBJ read book","(4b) Zyon no\\tyonda hon","SUBJ The book John read","(4c) Zyon no hon o\\tyonda.","GEN\\tOBJ (Someone) read John's book.","(4d) Zyon ga hon o yonda.","(4e) Zyon wa\\thon o yonda.","TOPIC/SUBJ John read the book.","(40 Zyon wa\\tyonda hon o nagesuteta.","TOPIC/SUBJ\\tthrew-away. John threw away the book (someone) read. For instance,"]},{"title":"no","paragraphs":["can be used as a SUBJ marker as in (4b) instead of"]},{"title":"ga","paragraphs":["as in (4a). However"]},{"title":"no","paragraphs":["in (4c) cannot be interpreted as a SUBJ marker."]},{"title":"Wa","paragraphs":["in (4e) causes the omission of a SUBJ marker"]},{"title":"ga","paragraphs":["in (4d). On the other hand,"]},{"title":"wa","paragraphs":["in (40 also causes the omission of a SUBJ marker but 'John' is the SUBJ of t̀hrew away' and the SUBJ of 'read' is dropped, that is,"]},{"title":"'Zyon wa'","paragraphs":["modifies"]},{"title":"nagesuteta","paragraphs":["rather than"]},{"title":"yonda.","paragraphs":["The generalized rules for these grammatical phenomena are: (I)"]},{"title":"no","paragraphs":["can be a SUBJ marker only in a relative clause, and (II)"]},{"title":"wa","paragraphs":["cannot cause a topicalization and an omission of a SUBJ marker in a relative clause. Although these rules have widely been discussed, there is no Japanese syntactic parser that has formal rules to treat (I) and (II). We can write the following simple rule (5) based on the LFG formalism, which represents (I) and (II). '13Psubj-no' in (5) represents a postpositional phrase with"]},{"title":"no,","paragraphs":["and '13Psubj' represents a postpositional phrase with a SUBJ marker other than"]},{"title":"no. (5)","paragraphs":["Srel\\t{PPsubj PPsubj-no}\\tPP*\\tV \\t","T SUBJ)=\\tT GF)= T = ( TOPICALIZATION-FORM)* 'wa'","Another type of construction that relates to PAR capturing and case detection is a verb (a bunsetsu) which includes multiple PARs."]},{"title":"301","paragraphs":["p ED • I:.","eizo:1?1>• .30P1TS ;11","VECE FORLI [_trrype ED \\t","•14.4:","\\:a* .,L,R3 I SZ,","40 PASSTkIC -","• ni n , (40 AZ., j ,","10:7cor (32\\tj'","0N 4,","CASE act, MCHD-SE10 incl., HUH cy, PERS 3 PROH-StIMPI . PRott-rtre",":MrED OBL","7","\\t 911 \\t","\"C•","\\t","106 ti..6","kare ga\\t9gatu\\tni Tokyo dc yuki ni huru reru\\ta.","he SUB' Septemer in Tokyo in snow by fall PASSIVE PAST","PRED • 3 m •","ECK LAIN-EQUI? 41","10 V-TYPE :p-ad:. PERS \"","'SUED , *e","\\t",":it(*)",".0 Ng ,","GRADA preps c","*DJ UWE","MEM pc...axon-ripe u..nap.cit led PROPER locatic.","2 0","ace [...rm.. di .12friDjUVET-TIPE pcontpooitioaisl. WORN 'e","•111S-ASP 1,00\\tTOME ?sot]","2 VAZSIVE , .57ca-rt pa cis c TTYPE mai. Figure 2: F-structure for sentence (6)","(6) kare ga\\t9gatu\\tni Tokyo de yuki ni hurareta. he SUBJ Septemer in Tokyo in snow by fell-PASSIVE. *He was fallen by snow in Tokyo in September.","(7) zyoo ga\\tSirayukihime ni ringo o\\ttabesaseta. queen SUBJ Snow-White by apple OBJ ate-CAUSATIVE. The queen made Snow White eat an apple. (The queen tempted Snow White to eat an apple.) Sentence (6) is an example of the indirect passive (Masuoka et al., 1997). In this case, the intransitive verb"]},{"title":"huru","paragraphs":["(fall) is passivized. Sentence (7) is an example of a causative sentence. We adopted multi-clausal analyses for the indirect passives and the causative sentences, by regarding auxiliary verbs (fragments of bunsetsus) that cause passives (e.g."]},{"title":"reru","paragraphs":["and"]},{"title":"rareru)","paragraphs":["and causatives (e.g."]},{"title":"seru, saseru, (te)morau","paragraphs":["and"]},{"title":"(te)itadaku)","paragraphs":["as having PREDs. ' These analyses enable us to capture the PARs"]},{"title":"huru-yuki(SUBJ)'","paragraphs":["in (6) and"]},{"title":"taberu-Sirayukihime(SUBJ)-ringo(OBJ)'","paragraphs":["in (7) as well as the main PARs"]},{"title":"reru-kare(SUBJ)-yuki(OBL)-huru(XCOMP)'","paragraphs":["in (6) and"]},{"title":"saseru-2yoo(SUBJ)-Sirayukihime(OBL)- taberu(XCOMP)'","paragraphs":["in (7). Figure 2 shows the f-structure generated by the LFG system for (6). The f-structure for (7) can be seen in Fig. 5. The standard bunsetsu dependency parsers for Japanese cannot capture 'snow falls in Tokyo in September' or 'Snow White eats an apple' because they consider a bunsetsu the unit of analysis. -A case marker shift caused by potential verbs A case marker shift caused by"]},{"title":"te-aru","paragraphs":["sentences -Behavior of particular nouns that a verb in a relative clause cannot subcategorize for Case disambiguation in causative-and-passive sentences -Case disambiguation in coordinated sentences with topic particles Case disambiguation in relative and embedded clauses with topic particles -Case marker shifts caused by relative and embedded clauses Figure 3: Examples of Japanese grammatical issues related to PAR and case detection 5 Within LFG, although the analysis for Japanese causative sentences adopted here has already been proposed (Sells, 1985), the debate has not yet been settled as to whether Japanese causative structures are multi-clausal or mono-clausal (Matsumoto, 1996 ; Yokota, 2001). JUI1C1."]},{"title":"302 \\t","paragraphs":["a< i\\t*\\tr\\t","ate \"7: 06 II'\\t","t..IttgAtz.„","watasi wa","\\t","kare ga\\t","kaku to","\\t","hon\\t","o","\\t","zutto yomu de-iru masu","\\t","tinamini.","I\\t","TOPIC/SUBJ he SUBJ write PAST book OBI always read PROG POLITE F.Y.I.","I am always reading the","book he wrote, F, Y.I."]},{"title":"C.....","paragraphs":["1:\\t P RACHEI ITS"]},{"title":"\\t","paragraphs":["Sod,\\t FIVakT.NTS","S\\t","AOVP FRAMIEHTS","•","IIP\\t ADVP\\t","'sorb","\\tACO,\\t","PERIOD","•","Hadj PPoubj","\\tHad j","\\tPP obj :+13, \\t, ePent kW7","\\t","n","1\\t[\\t /\\t11\\t1","PP53(1\\t","rik 1\\t11.o co frt i ei a mg'\\tvv..-"]},{"title":"/\\ I","paragraphs":["Iladj PPoubj- rel \\tAtri.poot","4 1"]},{"title":"\\t","paragraphs":["PFAU\\t<","fprzo\\t'SEA' bitio"]},{"title":"\\t","paragraphs":["vi,t110",",","\\tmean;","trAS  .9, MI. 1, Ple041,01,14 ,","PAO.,TXP",",r","cat","re\" rcAza iza-zou-ron, •","rot:AMY"]},{"title":"\\t","paragraphs":["Fsirao\\ta•","tqRio\\t4< is\\t11","\\t","'",".• irri0 wce 1F.A.\\t. P7.0.1•TZPI ea •liodutc.T\\t"]},{"title":"fJ6J","paragraphs":["\\t.0` ","ore, ii .111t3 +7,","C.:a wan,","a\\ttutuo-...a\\t1.1(04\\tsrca-ronts , \"MU ,711M 1a-OAT: pool 14.0\\tic.Ot\\t?Awl","Ilich.namrncrs\\tz-n2c--ryos Seal, ',rt. nisi."]},{"title":"\\t","paragraphs":["wee,\\t3","-TYPO r̂wdv r.zrz","1.00 imd.iicativ, 1 . 740","\\t","2111.0Z","deo 1, ;'T'S,. main zr.t *Z0 ' r"]},{"title":"Figure 4: Examples of Fragment c-/f-structures There are several grammatical issues concerning PAR capturing and case detection. Figure 3 shows the list of examples of the grammatical issues for which we have already written LFG rules. 3.3 Robustness Techniques As mentioned in 3.1, we use Optimality Theory (OT) marks to delete dispreferred parses. We also use OT marks for efficiency. When OT marks are divided into groups that are ranked according to their preference, XLE processes the input in multiple passes. The core grammar that consists of the rules with OT marks in the highest preference group is used for the first pass. If a valid parse is found, then XLE will stop. Otherwise, XLE will process the input again with the core grammar plus the rules with OT marks in the second highest preference group for the second pass. This multiple-pass-parsing mechanism is useful for writing rules for rare grammatical phenomena without increasing unintended ambiguity. (8a) aruku koto ga itibanda. walk NOMINALIZER SUBJ is-best. To walk is best. (8b) *aruku ga\\titibanda. walk SUBJ is-best.","paragraphs":["2169 ucr"]},{"title":"303","paragraphs":["(8c) makeru ga\\tkatida. lose\\tSUBJ is-victory. To lose is a victory. For instance, the SUBJ marker"]},{"title":"ga","paragraphs":["ordinarily does not follow the canonical form of verbs as shown in (8a) and (8b). However, it exceptionally occurs in some idiomatic expressions as in (8c). We divided the 42 OT marks in our grammar into four groups, and we put the OT mark for the rule for the sentences like (8c) in the lowest preference group. The OT marks in the lowest preference group are added to the rules for rare idiomatic expressions or colloquial expressions so that those rules do not affect the core grammar.","When the STANDARD grammar, which consists of all the rules described above, does not produce a complete parse, a FRAGMENT grammar (Riezler et al., 2002) we wrote for Japanese is used. This grammar parses the input as a sequence of well-formed chunks. These chunks have both c-structures and f-structures. The set of fragment parses is then chosen on the basis of a fewest-chunk method.","The ungrammatical constituents in Japanese sentences that the STANDARD grammar does not cover tend to appear in sentence-final position. Therefore the fragment grammar is likely to output a meaningful chunk for the major part of the input sentence. Examples of fragment c-structures and f-structures are shown in Fig. 4. In this case, the adverb"]},{"title":"tinamini","paragraphs":["(for your information) in sentence-final position is regarded as an ungrammatical constituent; the STANDARD grammar assumes that a Japanese adverb cannot modify a verb to the left of it. 6 Figure 4 shows that the whole sentence except"]},{"title":"tinamini","paragraphs":["is correctly analyzed. 4\\tExperimental Evaluation 4.1 Coverage We prepared 3 types of Japanese text for evaluating the coverage of the Japanese LFG system: (A) 10,000 sentences in the Japanese EDR corpus (EDR, 1996), mainly composed of newspaper text, (B) 460 sentences in a copier manual (Fuji Xerox, 2000), (C) 9,637 sentences of eCRM text.","The 'coverage' in this paper refers to the percentage of the sentences for which the system returns at least one f-structure. All of (A)-(C) consist of randomly selected unseen sentences. Most sentences in (A) and (B) are grammatical. On the other hand, (C) includes a lot of ungrammatical and colloquial sentences, because the sentences in (C) are the transcriptions of telephone calls from customers to a customer service center.","Table 1 lists the coverage results. The Total coverage for (C) is almost the same as that of (A) and (B). On the other hand, the STANDARD coverage for (C) is lower than (A) and (B). This is because (C) includes a lot of ungrammatical sentences, and the ratio of the sentences that could be covered by the STANDARD grammar was low; the fragment grammar covers the ungrammatical sentences. The main Table 1: Results of the coverage tests Num. of sentences Ave. num. of words in a sentence","Coverage (%) Ave. time. to analyze a sentence \"without Skimming* (sec) Ave. num. of STANDARD","resultsTotal STANDARD FRAGMENT Skimmed","STANDARD","Skimmed","FRAGMENT (A) 10.000 22.6 97.3 91.6 5.7 4.7 1.2 1.9 14.6 (B) 460 21.3 98.7 94.4 4.3 2.0 0.7 1.1 10.1 (C) 9,637 16.3 97.9 87.0 10.9 1.9 0.9 4.8 46.2 .2.8G1-17. CPU 2G13 m ono ry"]},{"title":"304","paragraphs":["reason for failure of analysis for (A)-(C) was time-outs caused by the sentences that even the skimming mode could not cover. The table shows that the system has over 97% coverage in total and 87% coverage with the STANDARD grammar even for (C). 4.2 Accuracy"]},{"title":"4.2.1 Approach","paragraphs":["Outputs of standard Japanese parsers such as KNP (Kurohashi and Nagao, 1994) and Cabocha (Kudo and Matsumoto, 2002) are trees that express bunsetsu dependencies (Bunsetsu Tree). It is quite natural and reasonable to compare a Bunsetsu Tree with an f-structure, because a Bunsetsu Tree is essentially a simplified f-structure. We can change an f-structure into a dependency tree by regarding a PRED in the f-structure as a node in the tree, and the outside-inside relation of the nested AVMs of the f-structure as the parent-child relation between the nodes (Pred Tree).","A problem is that one bunsetsu unit can correspond to more than one PRED. For instance, (9a) is one bunsetsu, but the f-structure for (9a) includes two PREDs as described in 3.2. (9b) is also one bunsetsu but its f-structure includes two PREDs: one for the verb"]},{"title":"kaku","paragraphs":["(write) and the other for the auxiliary verb"]},{"title":"nai","paragraphs":["(not). A compound noun is also regarded as one bunsetsu as in (9c), but its f-structure can include multiple PREDs corresponding to the nouns that compose the compound noun.","(9a) kakaseru (one bunsetsu) kaku (write) seru (make) (two morphemes)","(9b) kakanai (one bunsetsu) kaku (write) nai (not) (two morphemes)","(9c) sekisyokuzetuenwaiya (one bunsetsu) sekisyoku zetuen\\twaiya (three nouns) (red-color) (insulation) (wire)","Another problem is that a bunsetsu is a language-dependent and phrase structural constituent. As described in Section 1, an f-structure is meant to encode a more language universal level of analysis. On the other hand, a c-structure encodes language particular differences in syntactic structures and constituency. Thus the information concerning bunsetsus should appear in a c-structure and not in an f-structure. This means that we cannot detect which part of an f-structure corresponds to a bunsetsu. However, it is easy in general to list the grammatical categories in c-structures for Japanese, and these correspond to bunsetsus. We created a Bunsetsu Tree from a c-structure and an f-structure as follows: 1. List the grammatical categories that correspond to bunsetsus. 2. Specify the nodes in a c-structure, which correspond to bunsetsus referring to the category list in 1. 3. Specify the f-structure AVMs, which correspond to the nodes specified in 2 (Bunsetsu AVMs). 4. Create a Bunsetsu Tree by regarding 'a set of PREDs' in a Bunsetsu AVM as a node in the Bunsetsu","Tree, and the outside-inside relation of the nested Bunsetsu AVMs as the parent-child relation","between the nodes.","Figure 5 shows the c-/f-structures and the Pred/Bunsetsu Trees for sentence (7). 'NP', ǸPobl' and V̀verb' in the c-structure are the grammatical categories that correspond to bunsetsus. The numbers in Fig. 5 indicate the correspondence of the grammatical categories in the c-structure to the AVMs in the f-structure. We can specify the Bunsetus AVM by referring to the correspondence and create the Bunsetsu Tree.' The Bunsetsu Trees are comparable to the trees generated by the standard bunsetsu dependency parsers for Japanese."]},{"title":"6 It is one of the principles of Japanese syntax that a bunsetsu should modify another bunsetsu to the right of it. 7 When we created the Pred Trees and the Bunsetsu Trees for our tests, we did not make a node that corresponds to a PRED of an OBL as in Fig. 5. Instead we included the PRED information in the link labels. 305","paragraphs":["no.:8‘5 Sal Putior.:34 .: :33 Ca 1:","urobl:12,9 up:1032 t.,7:14:13:","758","..,","Mad j : S8 PPxubj:"]},{"title":"g","paragraphs":["11.1j:213 PPo1,1 :1' llaclj: 3,2 PP0 :25\\tV,,45","1\\t ,'''''","tmerc.; 72\\tI,' :2 \\t","mi.*: 21/\\ti"]},{"title":":","paragraphs":[".: :12\\tliter*: 3"]},{"title":",","paragraphs":["0"]},{"title":"4....","paragraphs":[":22 V:2,\\t:30 k.","I\\tI\\ti","n:1\\t0:11\\tn:21\\t.*,,:s :26","\\t",":5"]},{"title":".","paragraphs":["tf: 4: :","28","1 I.\\tIi","t±: : 0\\tc.1114 :10\\t') A, 7 : 20","MED \\t","•i:.;(!0: 11̀m) 10 PACO '","08.3\\tI /TYPE\\tT\\tunap.ci if,. PROPER 341.0 213 et1LK t,","PERS 3","CHEM\\tfrPTYPC","POST-PERM","PRE0 \\t.1.4\"tt,","S1.15.7\\t/ 22 PREP 257...0 arc.","PERS 20 21 370 ,2 1032 27 Aar: i414.700","\\t","Tell= ,",". TI4T-T7PE dtcl. ',TYPE 12 1.7 1:70","Ea. DBJ 5t 34 ZS '0 '4 3 ,S3 a23 031 let eat (SUBJ) (OBL_ni SUBJ\\tOBL ni\\tXCOMP Pred Tree Bunsetsu Tree trT\\t(134S4E\\tI.: (9","\\t","*--<6\\tttu zyoo ga\\tSirayukihime ni ringo\\to\\ttaberu saseru ta. queen SUB) Snow-White by apple OBI eat\\tmake PAST. The queen made Snow White eat an apple.","PEED","el . t","\\t","0•4C\\t• /MED cAr.e 1GRb PER3 i2 Bunsetsu AVM"]},{"title":"Figure 5: C-/f-structure and Pred/Bunsetsu Trees for sentence (7) 4.2.2 Experiments and Results We randomly selected 200 unseen sentences from the EDR corpus for which the Japanese LFG system returns at least one f-structure. The 200 sentences were analyzed with KNP, Cabocha and the Japanese LFG system. The outputs of KNP and Cabocha are Bunsetsu Trees, and Pred Trees and Bunsetus Trees were created from the outputs of the LFG system. In this experiment, the link labels in the trees are grammatical functions as in Fig. 5, and we omitted other attributes and values in f-structures such as TENSE and MOOD. A Japanese linguist and a Japanese native speaker created gold standard Pred Trees and Bunsetsu Trees for the 200 sentences consulting the outputs of KNP, Cabocha and the LFG system. 8 We compared the gold standard Bunsetsu Trees with the Bunsetsu Trees generated by KNP, Cabocha and the LFG system (Bunsetsu Comparison). We also compared the gold standard Pred Trees with the Pred Trees generated by the LFG system (Pred Comparison). Table 2 shows the results of the Bunsetsu Comparison. The precision and recall are based on the number of correct bunsetsu-bunsetsu (B-B) dependencies. The precision and recall of B̀-G-B' in the table are based on the number of the correct triplets of a bunsetsu, its parent bunsetsu and a link label (a grammatical function) between them. The LFG system outputs all possible grammar analyses (as in Section 2). The Upper Bound is the average for 200 parses each of which is the best parse for a sentence. The Upper Bound is the ideal value we can obtain if the system could always select the best parse for each sentence. The Average is the average of the 200 averages for all parses for each sentence. The BASELINE is for the Bunsetsu Trees obtained assuming all bunsetsus modify their right-hand neighbors. 8","paragraphs":["We did not refer to the syntactic annotations the EDR corpus has. Comparing our gold standard with these annotations is future work."]},{"title":"306","paragraphs":["Table 2: Results of the Bunsetsu Comparison","Prec. (%) Rec.\\t(\"D) KNP (B-B) 77. 2 78. 9 Cabocha\\t(B-B) 82. 6 82. 0 LFG (B-B) Upper Bound 89. 1 87. 7 Average 81. 3 79. 5 LFG (B-G-B) Upper Bound 86. 9 85. 4 Average 77. 9 76. 2","BASELINE\\t(B-B) 58. 6 56. 0 Table 3: Result of the Pred Comparison Prec. (%) Rec. (%) LFG (P-G-P) Upper Bound 87. 5 87. 8 Average 79. 7 80. 4 LFG (P-P) Upper Bound 90. 0 90. 3 Average 83. 3 84. 0","Table 3 shows results of the Pred Comparison. The 'P-G-P' is based on the number of correct triplets of a PRED, its parent PRED and a link label between them. The '13-P' is based on the number of the correct parent-child PRED pairs."]},{"title":"4.2.3 Discussion","paragraphs":["Table 2 shows that the Average '13-B' of the LFG system is roughly equivalent to that of KNP and Cabocha. The results of B̀-G-B' are encouraging, because the 1̀13-G-B' results are not substantially worse than the 'B-B' results of the LFG system. This means the case detection by the LFG system works well.","We examined the Bunsetsu Trees to check the difference among the three systems, and found the LFG system based on the linguistically fine-grained grammar advantageous in some points. For example, KNP and Cabocha output the parses in which a noun phrase which ends with no modifies a verb that is not in a relative clause. This never happens with the LFG system as described in 3.2. As another example, the dependencies of noun phrases topicalized by wa were more correctly analyzed by the LFG system than the other two because of the precise grammar rules for the case detection described in 3.2. On the other hand, rules for coordination in our grammar are not sophisticated enough, so the analyses for the sentences including coordination structures on the basis of the non-linguistic methods of KNP and Cabocha were better.","The results in Table 3 show that both precision and recall of the LFG system in the Pred Comparison are higher than those in the Bunsetsu Comparison. This means the accuracy of the bunsetsu-internal analyses by the LFG system is reliable. Examples of the bunsetsu-internal analyses are the analyses for (9a)-(9c). For instance, the LFG system is required to determine the dependencies of the nouns in the compound noun (9c), that is, both"]},{"title":"sekisyoku","paragraphs":["(red-color) and zetuen (insulation) modify waiya (wire).","The LFG system is based on a hand-coded grammar. We think we will be able to increase the accuracy of our system by continuing the grammar development for the grammatical issues that have not yet been considered. In addition, by using a statistical method for disambiguation (e.g. the method proposed in Riezler et al. (2002)), it will be possible to select better parses from among the parses XLE generates for a sentence than random selection."]},{"title":"307 5\\tConclusion","paragraphs":["We have reported a Japanese parsing system based on the LFG formalism. The system is the first Japanese LFG parser with over 97% coverage (91% on average without FRAGMENT analyses) for real-world text. We evaluated the accuracy of the system compared with the standard Japanese dependency parsers. The LFG parser shows roughly equivalent performance on the bunsetsu dependency accuracy to the standard Japanese parsers. It also provides reasonably accurate results for case detection and bunsetsu-internal analyses.","We are incorporating the LFG system in some off-line NLP applications. However the time performance of the system is not enough for real-time applications, so we will customize XLE for the Japanese grammar to achieve better performance."]},{"title":"Acknowledgements","paragraphs":["We acknowledge our indebtedness to all the members of the ParGram project, especially Ronald Kaplan, Tracy Holloway King and John Maxwell at Palo Alto Research Center, and Mary Dalrymple at King's College London for their comments and discussions on early versions of the Japanese LFG grammar and system. We would also like to thank Tracy Holloway King at PARC, along with Martin van den Berg and Lorenzo Thione at FX Palo Alto Laboratory for their helpful comments on an earlier draft of this paper. References"]},{"title":"Joan Bresnan. 2000. Optimal Syntax. In Optimality Theory: Phonology, Syntax and Acquisition, (Joost Dekkers, Frank van der Leeuw, and Jeroen van de Weijer, eds.) pages 334-385. Oxford University Press, Oxford. Miriam Butt, Helge Dyvik, Tracy H. King, Hiroshi Masuichi, and Christian Rohrer. 2002. The Parallel Grammar Project. In Proceedings of the 19th International Conference on Computational Linguistics (COLING '02) Workshop on Grammar Engineering and Evaluation, pages 1-7. Miriam Butt, Tracy H. King, Maria-Eugenia Nino, and Fr6d6rique Segond. 1999. A Grammar Writer's Cookbook. Number 95 in CSLI Lecture Notes. CSLI Publications, Stanford, California. Mary Dalrymple. 2001. Lexical-Functional Grammar. Academic Press, New York. Syntax and Semantics, volume 34. EDR (Japanese Electronic Dictionary Research Institute, Ltd.). 1996. EDR electronic dictionary version 1.5 technical guide. (in Japanese) Anette Frank, Tracy H. King, Jonas Kuhn, and John T. Maxwell III. 2001. Optimality Theory Style Constraint Ranking in Large-scale LFG grammars. pages 367-397. In Formal and Empirical Issues in Optimality Theoretic Syntax, Peter Sells, ed. CSLI Publication, Stanford, California. Anette Frank. 1999. From Parallel Grammar Development towards Machine Translation. In Proceedings of NIT Summit VII. \"MT in the Great Translation Era\", Singapore, pages 134-142. Fuji Xerox Co., Ltd. 2000. Document Gate operation manual first edition, DE-1006. (in Japanese) IPA (Information-technology Promotion Agency). 1987. IPA Lexicon of the Japanese Language for Computers IP.4L. (in Japanese) Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-Functional Grammar: A formal system for grammatical representation. In The Mental Representation of Grammatical Relations (Joan Bresnan, ed.), pages 173-281. The MIT Press, Cambridge, Massachusetts Roger Kim, Mary Dalrymple, Ronald M. Kaplan, Tracy H. King, Hiroshi Masuich, and Tomoko Ohkuma. 2003. Multilingual Grammar Development via Grammar Porting, In Proceedings of the 2003 European Summer 308 School in Logic, Language, and Information (ESSLLI2003) workshop \"Ideas and Strategies for Iviultilingual Grammar Development\". To appear. Taku Kudo and Yuji Matsumoto. 2002. Japanese Dependency Analysis using Cascaded Chunking. In Proceedings","paragraphs":["of"]},{"title":"Sixth Conference on Natural Language Learning (CoNLL '02), Taipei, Taiwan, pages 63-69. Sadao Kurohashi and Makoto Nagao. 1994. A Syntactic Analysis Method of Long Japanese Sentences Based on the Detection of Conjunctive Structures. Computational Linguistics, 20(4):507-534. Hiroshi Masuichi and Tomoko Ohkuma. 2003.\\tConstructing a practical Japanese parser based on Lexical-Functioncal Grammar. Journal","paragraphs":["of"]},{"title":"Natural Language Processing, 10(2):79-109. (in Japanese) Takashi Masuoka, Yoshio Nitta, Takao Gunji, and Satoshi Kinsui. 1997. Grammar. Iwanami Press, Tokyo, Japan. (in Japanese) Yo Matsumoto. 1996. Complex Predicates in Japanese: A Syntactic and Semantic Study","paragraphs":["of"]},{"title":"the Notion 'Word'. CSLI Publications, Stanford, California and Kurosio Publishers, Tokyo. Studies in Japanese Linguistics volume 7. Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Hroshi Matsuda, Kazuma Takaoka, and Masayuki Asahara. 1999. Japanese Morphological Analysis System ChaSen version 2.0 Manual 2nd edition. Technical report, Nara Institute of Science and Technology, Japan. (in Japanese) John T. Maxwell III and Ronald M. Kaplan. 1993. The interface between phrasal and functional constraints. Computational Linguistics, 19(4):571-589. Akira Ohtani, Takashi Miyata, and Yuji Matsumoto. 2000. On HPSG-Based Japanese Grammar -Refinement and Extension for Implementation-. Journal","paragraphs":["of"]},{"title":"Natural Language Processing, 7(5):19-49. (in Japanese) Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell HI, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques. In Proceedings","paragraphs":["of"]},{"title":"the 40th .4nnual Meeting","paragraphs":["of"]},{"title":"the Association for Computational Linguistics (ACL'02), Philadelphia, PA, Pages 271-278. Peter Sells. 1985. Lectures on Contemporary Syntactic Theories: An Introduction to Government-Binding Theory, Generalized-Phrase Structure Grammar, and Lexical-Functional Grammar. CSLI Publications, Stanford, California. Masayoshi Shibatani. 1990.\\tThe languages","paragraphs":["of"]},{"title":"Japan. Cambridge University Press, Cambridge, United Kingdom. Kenji Yokota. 2001. Complex-predicate Formation and Some Consequences in Japanese. In On-line Proceedings","paragraphs":["of"]},{"title":"the LFG01 Conference (Miriam Butt and Tracy H. King eds.). URL: http://csli-publications. stanford.edu/. 309","paragraphs":[]}]}