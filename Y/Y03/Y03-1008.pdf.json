{"sections":[{"title":"A Simplified Latent Semantic Indexing Approach for Multi-Linguistic Information Retrieval 1","paragraphs":["Liu Yi, Lu Haiming, Lu Zengxiang, Wang Pu","Department of Automation, Tsinghua University","Beijing 100084","P.R.China yiliu00@mails.tsinghua.edu.cn Abstract Latent Semantic Indexing (LSI) approach provides a promising solution to overcome the language barrier between queries and documents, but unfortunately the high dimensions of the training matrix is computationally prohibitive for its key step of Singular Value Decomposition (SVD). Based on the semantic parallelism of the multi-linguistic training corpus we prove in this paper that, theoretically if the training term-by-document matrix can appear in either of two symmetry forms, strong or weak, the dimension of the matrix under decomposition can be reduced to the size of a monolingual matrix. The retrieval accuracy will not deteriorate in such a simplification. And we also discuss what these two forms of symmetry mean in the context of multi-linguistic information retrieval. Although in real world data the term-by-document matrices are not naturally in either symmetry form, we suggest a way to make them appear more symmetric in the strong form by means of word clustering and term weighting. A real data experiment is also given to support our method of simplification. 1\\tIntroduction Multi-linguistic Information Retrieval (MLIR for short, also \"translingual\" or \"cross-language\" IR) enables a query in one language to search document collection in another one or more languages. Many monolingual IR approaches can be extended to multi-linguistic environment and among them Latent Semantic Indexing (LSI for short, Deerwester et al., 1990) has proved effective (Y. Yang et al., 1997, Douglas William Oard, 1996).","The particular technique used in LSI is singular-value decomposition (SVD for short), in which a large term-by-document matrix is decomposed into a set of orthogonal factors from which the original matrix can be approximated by linear combination. However, SVD on the large term-by-document matrix whose size increases with the size of the training corpus brings huge computation costs. This situation becomes even worse when we use LSI for MLIR because the training matrix consisting of various languages is always several times larger. To reduce the cost of SVD in LSI and thus make it feasible in MLIR, we try to exploit the semantic symmetry hidden in the training corpus. We find that theoretically if the term-by-document matrices of multi-linguistic training set have either a weak symmetry form or a strong symmetry form, the SVD step of LSI in multi-linguistic environment can be simplified. Both symmetry forms have clear meanings in the context of MLIR. Further, though we can never reach precisely either of two symmetry forms from real world data, two possible methods are raised to enhance the strong form symmetry of the term-by-document matrices. Our small-scale experiment gives a satisfying result though we only roughly keep the strong symmetry.","In section 2 we will briefly introduce how LSI approach can be extended to multi-linguistic environment. In section 3 we will prove some theorems for the LSI simplification in the two symmetry forms, then discuss symmetry enhancement issues for real world data. Experiments and results will be 1 Supported by the National Natural Science Foundation of China (No. 60003004) 69 given in section 4. Finally in section 5 we will draw our conclusions with some problems for future work. 2\\tLSI for MLIR LSI is based on the vector space mode (VSM), in which both queries and documents are represented as vectors of term weights","= (qi, q2","d = (di,d2,•••,dm)t where q is the query vector, d is the document vector, m is the number of unique terms (words, phrases or word clusters) in the corpus after stop-word removal and stemming, qi and di are term weights in the query and the document respectively. Terms are usually weighted by term frequency, term frequency inverted document frequency (TFIDF), information gain or other weighting schemes. In monolingual IR, the similarity between a query and a document is defined as sim(q, d) = cos(q, d) = 2 ÷1 't ni=1 qi","6.di 2","LSI is a one-step extension of VSM. The claim is that neither terms nor documents are the optimal choice for the orthogonal basis of a semantic space, and a reduced vector space consisting of the most meaningful linear combinations of documents would be a better representative basis for the documents content.","In monolingual IR, let W be the term-by-document matrix of the training corpus consisting of only one language. By SVD analysis W = USV [Z, 01","where matrices U and V are unitary matrixes, which means that LY= U 4, NT1= V'. And S =\\tin 0 0 which E is a diagonal matrix with r nonzero diagonal entries. These r nonzero values are called singular values of the matrix W. If we take the first k biggest nonzero diagonal entries of S and the corresponding columns in matrices U and V, the bilingual term-by-document matrix W can be approximated as W UkSkVif where matrices Uk and Vk contain a set of k orthogonal singular vectors each (one for the representation of terms and the other for the representation of documents). Matrix Sk is k-diagonal, containing the singular values indicating the importance of the corresponding singular vectors in matrices Uk and Vk. The monolingual retrieval criterion of LSI approach is defined to be","sim(q,d) = cos(Wkci , Ikd)","LSI approach can be extended to multi-linguistic environment. Let us take bilingual case for example. We define Amxn be a term-by-document matrix for the training documents in the source language (also the language of queries), B .. be a term-by-document matrix for the training documents in the target language. The corresponding columns of A and B are the matching pairs of documents in the bilingual corpus.","Now, W is defined as an (m + s)xn term-by-document matrix of the entire corpus, representing the bilingual document pairs 70 Utk"]},{"title":"d 0 [A] W= B","paragraphs":["Then we do the SVD analysis and k singular value approximation on the bilingual term-by-document matrix W just as in the monolingual case. Let q be a query in the source language,"]},{"title":"d","paragraphs":["be a document in the target language. The retrieval criterion of LSI approach in bilingual environment is (Y. Yang et al., 1997) sim(q,"]},{"title":"d) = cos(U","paragraphs":["kt [coi"]},{"title":"l","paragraphs":[","]},{"title":"rd])","paragraphs":["The bilingual case above can be easily extended to multi-linguistic cases. When the training corpus has more than one target language, the matrix W will be composed of all the term-by-document matrices of the languages involved. Suppose that the training corpus consists of 1 languages, and for each language Li we choose mi terms and the same n matching documents. We define Ai as the term-by-document matrix for the training documents in the i-th language. That the documents of different languages are \"matching\" means out of n training documents of any language the i-th one has the same content but in different languages. Accordingly we align the corresponding columns representing those matching documents to the same position in A. Now the term-by-document matrix W representing the entire training corpus is defined as"]},{"title":"A1 =","paragraphs":["A2 A1 The query q can be in any one of the 1 languages, for example L1 . And the document"]},{"title":"d","paragraphs":["in search can be in any other /- 1 languages. After SVD analysis on the matrix W, the retrieval criterion is"]},{"title":"q\\t0","paragraphs":["0 sim(q,"]},{"title":"d) = cosark 0","paragraphs":["However, when the size of the matrix W increases the SVD becomes more and more time-consuming and even infeasible in practical use. This is one of the major problems of LSI approach and it is intensified in the field of MLIR. 3\\tSimplified LSI approach LSI for MLIR is based on parallel corpus training. In parallel corpus, each documents in one language has its counterparts in all other languages. This kind of parallel structure makes the corpus documents semantically symmetric. If the symmetry can be embodied in text representation, we could use it to simplify computation with mathematical methods. That is the basic idea of our simplified LSI approach. In this section we will show that in two particular circumstances the dimension of the matrix under decomposition in MLIR can be reduced to the size of a monolingual matrix. In either of the two circumstances, the term-by-document matrix of each language has some kind of symmetry, which we call weak and strong symmetry form respectively. Theoretically we prove that our simplification will not deteriorate the accuracy of information retrieval. For practical use, we will also discuss how to enhance the symmetry of real world data to approximate the conditions required in theoretical conclusions. 71 3.1 Simplified LSI for the Weak Symmetry form The simplification of LSI under the weak symmetry form is based on the following theorem.","Theorem 1 If two matrices B e 0 \"z\",C e 0 s\" (m, s ̂n) satisfi, that I313 ="]},{"title":"CC ,","paragraphs":["then (1) B and C have the same singular values (2) Let oi , a2, • • • or be the singular values ofB and C. If all these singular values are different, i.e. oi > 62 > • • • > cr, > 0 , and we define E = diag(ai , 62 , • • • , 6r ) , then there must exist SVD B = i'01-1! and C"]},{"title":"= kit'","paragraphs":["satisfying that I 0 . [ L Ol e 0 mxn 9"]},{"title":"i, =","paragraphs":["[ E l e 0 sxn , 1 00\\t00 II. Ifwe define the first r columns of"]},{"title":"it","paragraphs":["and"]},{"title":"d","paragraphs":["as iii\\t and"]},{"title":"d","paragraphs":["i respectively, then Before proving Theorem 1 we first give a lemma with proof.","Lemma Define unit matrix Wm, k) = diag(6i,a2,••• • ani) ai = —1, i = k of =1, i = others B = PQRt is an arbitrary SVD on a matrix BEO ' If we define is = PM(m,k), ii = R1V1(n, k) k 5_ min(m, n) then"]},{"title":"B","paragraphs":["= PQRt is also a SVD on the matrix B."]},{"title":"iii. = d1.","paragraphs":["Proof of Lemma According to the definition of SVD, P"]},{"title":"and","paragraphs":["Q are both orthogonal matrices."]},{"title":"And","paragraphs":["the unit"]},{"title":"matrix M(n,","paragraphs":["k)"]},{"title":"is an symmetric orthogonal matrix. So","paragraphs":["-"]},{"title":"1","paragraphs":["3"]},{"title":"is also an orthogonal matrix. Hence PQRt = PM(m, k)QM(n,","paragraphs":["14"]},{"title":"le = PM(m, k)QM(n, k)Rt = P [M(m, k)QM(n,","paragraphs":["k)]"]},{"title":"itt","paragraphs":["For any matrix C e D \"1\", M(m, k)C equals to multiplying the k-th row of matrix C by -1, and CM(m, k) equals to multiplying the k-th column of matrix C by -1. According to the definition of SVD, the matrix Q is composed of a diagonal matrix E and three zero"]},{"title":"matrices, i.e. Ql. [E o o o So M(m, k)QM(n,","paragraphs":["k) ="]},{"title":"Q~ ~","paragraphs":["Hence B = PQRt holds true, i.e. B = PQR t is also a SVD on the matrix B. 72 _\\t -"]},{"title":"k","paragraphs":["r\\t• • •\\t11,n -' • • • • 2"]},{"title":"0","paragraphs":["• r ,n","•• •\\t1 r,n","•• •\\t1 r,r 62r 2 r"]},{"title":"000","paragraphs":["1n,1\\tn,r • • •\\t1 n,n 1n,1\\t*\\t1n,r •• •\\t1","n,n 11,1\\t• .. ir • • •\\t1r ,r 2","1'1,14727\\t • • 6rlr3 1̀,n • • •"]},{"title":"0 0","paragraphs":["0.27\\tn.2i 1̀,r\\t• •\\tr'r,r a"]},{"title":"l","paragraphs":["1 n,1 0.2 ,r Then we prove Theorem 1."]},{"title":"Proof of Theorem 1","paragraphs":["(1) According to the definition of SVD, the singular values of matrix"]},{"title":"B","paragraphs":["are the square roots of the positive eigenvalues of matrix"]},{"title":"B1̀3 ,","paragraphs":["and the singular values of matrix"]},{"title":"C","paragraphs":["are the square roots of the positive eigenvalues of matrix"]},{"title":"C","paragraphs":["t"]},{"title":"C .","paragraphs":["Because of"]},{"title":"13","paragraphs":["`"]},{"title":"B = C","paragraphs":["t"]},{"title":"C , B","paragraphs":["and"]},{"title":"C","paragraphs":["have the same singular values. (2) Let"]},{"title":"B = PQR","paragraphs":["t and"]},{"title":"C = EFG","paragraphs":["t be two arbitrary SVDs of matrices"]},{"title":"B","paragraphs":["and"]},{"title":"C.","paragraphs":["Here"]},{"title":"P, R, E","paragraphs":["and"]},{"title":"G","paragraphs":["are all orthogonal matrices."]},{"title":"[E 01","paragraphs":["According to (1) and the definition of SVD, we have"]},{"title":"Q = F = 0 0 S = 4:YQ = F","paragraphs":["t"]},{"title":"F = diag(6","paragraphs":["2"]},{"title":", 6","paragraphs":["2 ,- ,"]},{"title":"6","paragraphs":["2 ,"]},{"title":"0,— , 0) .","paragraphs":["Because of"]},{"title":"B","paragraphs":["I"]},{"title":"B = C","paragraphs":["t"]},{"title":"C ,","paragraphs":["i.e."]},{"title":"(PQR","paragraphs":["t"]},{"title":")","paragraphs":["t"]},{"title":"PQR! = (EFG","paragraphs":["t"]},{"title":")EFG RI:YQR! = GFtFGt RSR","paragraphs":["t"]},{"title":"= GSGt SR","paragraphs":["t"]},{"title":"G = RIGS","paragraphs":["Let"]},{"title":"L =\\t=","paragraphs":["(1 j ) nxn , and"]},{"title":"L","paragraphs":["is also an orthogonal matrix. So"]},{"title":"SL = LS i","paragraphs":[".e. . Let us define"]},{"title":"0","paragraphs":[".2/\\t.„2/ 1 \"1,1\\t• •\\t1̀r 6r2 r,1","\\t0. r2 r • 'rr̀,n"]},{"title":"0 0 Compare the corresponding entries of both sides","paragraphs":["2","(72/ = cr./ li"]},{"title":"d","paragraphs":[". 0"]},{"title":".2","paragraphs":["/."]},{"title":"= 0","paragraphs":["aA J ="]},{"title":"0 1\\t","paragraphs":["j r 15_i ̂r, r+1<_ j5.n r+1 ̂i ̂n,1 ̂j ̂r"]},{"title":"Considering that","paragraphs":["al > 62 > • • • > a,. >"]},{"title":"0 , we get","paragraphs":["/id"]},{"title":"=0\\t","paragraphs":["1 ̂i ̂r"]},{"title":"or 15j ̂r and","paragraphs":["i"]},{"title":"73 =","paragraphs":["... ] , and"]},{"title":"L","paragraphs":["i"]},{"title":"= diag(43,","paragraphs":["12,2 , • • • 91r,r) • L2 L is an orthogonal matrix, i.e."]},{"title":"L","paragraphs":["t"]},{"title":"L = (R","paragraphs":["t"]},{"title":"G)","paragraphs":["t"]},{"title":"IVG = G","paragraphs":["t"]},{"title":"RR","paragraphs":["t"]},{"title":"G = I ,","paragraphs":["or","... __\\t....\\t_","/2","11,1\\t'1,1 So L can be represented in the form of"]},{"title":"[Li 0","paragraphs":["ir,2 ="]},{"title":"I 0","paragraphs":["1r,rlr,r"]},{"title":"0 \\tLt","paragraphs":["2"]},{"title":"L","paragraphs":["2"]},{"title":"0 0","paragraphs":["L2"]},{"title":"0","paragraphs":["1.12 _ hence li,i ="]},{"title":"±1, 1 <","paragraphs":["i < r . We define the first r columns of R as matrix"]},{"title":"R","paragraphs":["1"]},{"title":",","paragraphs":["and the rest as matrix R2 ; similarly we define the first r columns of G as G1 , and the rest as G2 , i.e."]},{"title":"R = [R","paragraphs":["I R2 j ,"]},{"title":"G = [G,","paragraphs":["G2]"]},{"title":"R","paragraphs":["t"]},{"title":"G = L G = RL","paragraphs":["[G, G2"]},{"title":"1 =","paragraphs":["[R, R2 1[1.'1\\t 2 i ="]},{"title":"[R1L1 R","paragraphs":["2"]},{"title":"L","paragraphs":["2"]},{"title":"]","paragraphs":["L2 So G"]},{"title":"1","paragraphs":["="]},{"title":"R1L1","paragraphs":["The corresponding column of R i and G1 are either equal or opposite, because L 1 is an diagonal matrix with either 1 or -1 as its diagonal entries. Supposing that 1„ ,„ ="]},{"title":"–1,","paragraphs":["according to the Lemma, when the k-th columns of R and P are multiplied by a factor –1 we get a new form of SVD on matrix"]},{"title":"B.","paragraphs":["Repeat such transformation for each -. 1 entries of"]},{"title":"L","paragraphs":["1 and we will reach a SVD in the following form"]},{"title":"B = i'QiZt","paragraphs":["with the matrix iii, the first r column °fit , satisfying G 1 = AlR. Finally, let E = È ,"]},{"title":"F=i","paragraphs":["1"]},{"title":",G=d,Q=0,","paragraphs":["and we find SVDs"]},{"title":"B = i'451","paragraphs":["-"]},{"title":"i","paragraphs":["t and c = Eitt with it, and di ,the first r columns off?. and d respectively, satisfying k, = d,G.","The above theorem gives a possible way of reducing SVD computation in a particular case. In the context of multi-linguistic IR the prerequisite condition can be formally written as","Weak Symmetry Form AtiAi","= A2 A2 2 A = • • = At/A, ( Ai is defined as the term-by-document matrix for the training documents in the i-th language ).","If the term-by-document matrices are in the symmetry form above, according to Theorem 1 there exist SVDs Ai = UiSiVi ......- UikSikVik k ̂min(rank(Ai )) , i =1,2,• • • ,/ satisfying that 74 S = S"]},{"title":"ik","paragraphs":["Vik = V"]},{"title":"ik\\t","paragraphs":["15i,j51 Note that the condition in Theorem 1 that all singular values of"]},{"title":"Ai","paragraphs":["are different is always satisfied","because SVD is calculated numerically in practice. Hence"]},{"title":"Ut A =S Vt = \\t=Ut","paragraphs":["ik ik\\tik ik\\tjk jk\\tjk"]},{"title":"A","paragraphs":["jk 1 1, j 5_1 which means that in two different LSI-generated reduced vector spaces for two different languages, the matching documents have the same vector representations. Therefore if the query q is in the language L1 and the document"]},{"title":"d","paragraphs":["is in the language Ld, the criterion of LSI for weak symmetry form can be simplified as sim(q, d)"]},{"title":"= cos(U kci, Wad)","paragraphs":["3.2 Simplified LSI for the Strong Symmetry Form Hongxing Zou, Dianjun Wang et al. have reached some useful conclusions on the SVD for unitary symmetric matrix (Hongxing Zou et al., 2000 & 2002), which we call strong symmetry form in this paper. Their conclusions are recapitulated below as Theorem 2.","Theorem 2 We define = A2"]},{"title":"A","paragraphs":["_ 1 _ where"]},{"title":"Ai E 0 m\",","paragraphs":["i = 1,2,-"]},{"title":"• • ,1","paragraphs":["satisfying A"]},{"title":"2","paragraphs":["= P"]},{"title":"i","paragraphs":["A"]},{"title":"i","paragraphs":[", A"]},{"title":"3","paragraphs":["= P"]},{"title":"2","paragraphs":["A"]},{"title":"1","paragraphs":[", • • • ,"]},{"title":"A1 = Pi_lAi ,","paragraphs":["and P"]},{"title":"1","paragraphs":[", P"]},{"title":"2","paragraphs":[", • • • , P"]},{"title":"1","paragraphs":["_"]},{"title":"1","paragraphs":["are all permutation matrices. A"]},{"title":"1","paragraphs":["= U"]},{"title":"i","paragraphs":["S"]},{"title":"i","paragraphs":["V: is an SVD on A1 , where"]},{"title":"S = /1 ° e 0","paragraphs":["m\" and"]},{"title":"Ei = diag(cri ,","paragraphs":["Ci"]},{"title":"2","paragraphs":[", • • • ,"]},{"title":"a","paragraphs":["r ) . Then there must exist SVD 1\\t0 0[ W =USV , satisfying"]},{"title":"[E l e 0","paragraphs":["lmxn ,"]},{"title":"E = diag(licri, ,,fic.2,• • • ,.,ficrir)","paragraphs":["I. S = 0 0","1\\t11\\tTT","II. U =[ U\\tPiUi • • •\\tn","-, ri_i u 11","V1 HI. V =Vi It is not difficult to find that Theorem 2 is a special case of Theorem 1. That is why we call their corresponding symmetry form \"weak\" and \"strong\" respectively. For this reason we do not prove Theorem 2 here and you can find a wonderful proof for that in Hongxing Zou et al., 2000 & 2002. Accordingly, Theorem 2 gives basis of LSI simplification for the following strong symmetry form.","Strong Symmetry Form A"]},{"title":"2","paragraphs":["=\\t, A"]},{"title":"3","paragraphs":["= P"]},{"title":"2","paragraphs":["A"]},{"title":"1","paragraphs":[", • • • ,\\t=\\tHere"]},{"title":"P1 ,","paragraphs":["P"]},{"title":"2","paragraphs":["• • • , P"]},{"title":"l","paragraphs":["_"]},{"title":"i","paragraphs":["are all permutation matrices. ("]},{"title":"Ai","paragraphs":["is defined as the term-by-document matrix for the training documents in the i-th language )."]},{"title":"75","paragraphs":["0 Utk d 0 0 0 d 0 Supposing that the query q is in the language"]},{"title":"L1","paragraphs":["and the document d is in the language Ld, when","the term-by-document matrices in strong symmetry form take A i = UiSiVi UikS ikVik as their","SVDs, the criterion of LSI for the strong symmetry form can be simplified by Theorem 2 as below.","0"]},{"title":"sim(q,","paragraphs":["d) = cos(Vk •••","lr","= COS(---LUik UikPi •","• •","Urkl)t\\t q 0/1 0 [utik utikPit Vl • • • utik pit","1\\tt\\t1 , Tt nt = COS(-"]},{"title":"'","paragraphs":["J"]},{"title":"AY-","paragraphs":["u ik r c1-1U) = cos(UkcbUad) 3.3 Discussion on the Two Symmetry Forms In both symmetry forms when we make SVD analysis we can only decompose matrices of the query language and the target language instead of the several times larger matrix W of all the languages. Therefore the LSI approach is simplified.","Both symmetry forms have clear meanings in the context of MLIR. They embody in deferent levels the parallel structure of the training corpus. When we normalize all the term-by-document matrices Ai by column, each entry of AitAi represents a similarity between two documents in the same i-th language. So in weak symmetry form, the similarity between documents in one language is the same with those of the matching documents in another language. That is, though corresponding documents in different language may be quantified differently, for example in vectors of different dimensions, we can simplify LSI approach as long as the consistency of similarity relationships between documents of the same language are preserved across languages. In strong symmetry form, all term-by-document matrices in different languages have the same row vectors but can be arranged in random order. It requires corresponding documents in different languages have the same quantified representations but need no alignment work. The strong symmetry form is actually a special case of the weak symmetry form. 3.4 Enhancement of Symmetry for Real Data Naturally the term-by-document matrix from real data does not precisely conform to either of the two symmetry forms. But we find at least two ways that can help to enhance the strong form of symmetry. As we know documents are represented as vectors of term weights. Firstly we can change the conventional ways of term selection. When we use word clusters for synonyms rather than words or phrases as terms, the term-by-document matrices appear more symmetric. This is because different languages will have various amounts of synonyms to refer to the same thing or similar things, which diversify the word frequency distribution. However when we gather all these synonyms into a cluster, the frequency of the cluster appearance tends to be consistent. 76","Also we can choose appropriate weighting scheme, for example binary weighting, to promote the symmetry. Binary weighting ignores the details of term frequency but only cares about whether a term appears or not. Despite of being almost the simplest weighting scheme binary weighting gives a reasonable IR performance in many occasions, for example matching similar documents of different languages in MLIR.","The weak points of the above two ways are very clear. Though statistical method can help us to do word clustering job, it usually brings a lot of calculation and hence counteract with our motive of computation reduction. More precise clustering should be on a semantic basis, which practically depends on a good synonym thesaurus or even manual labor. Binary weighting leads to a loss in IR performance because it ignores details of term distribution. In fact the \"strong\" symmetry form has a rather strict requirement for term-by-document matrix. We hope to find easy ways for weak form symmetry enhancement. Unfortunately, we haven't so far found any effective way that can give a satisfactory result. The major problem lies in the difficulty to preserve documents similarity across different languages. Ideally if the vectors are proper semantic representation of documents and quantified properly, the similarity of a pair of documents in one language should be identical to that of a matching pair of any other language. But currently all term-frequency-based weighting schemes cannot be a good quantification of the original documents. How to represent a document on a semantic basis rather than on a pure statistical basis is one of the chief goals of our further research.","Symmetry enhancement still does not provide a precise symmetry form. However our experiment suggest that small perturbations will not harm the precision of IR greatly. In Hongxing Zou et al., 2000 & 2002, a perturbation analysis is also given on Theorem 2. 4\\tExperiments on Strong Symmetry Form 4.1 Test Collection Our experiment is based on a bilingual corpus consisting of 352 Chinese-English document pairs. All the documents are passages adopted from a bilingual column of an IT weekly newspaper China Computer World in a period of about six years. And all passages are introduction or comment on various new IT technologies, about 400-500 English words or 800-900 Chinese characters long. We make 2/3 of the corpus documents as training set and the other 1/3 as test set. Also we manually generate 38 queries for IR test, 19 in Chinese and another corresponding 19 in English. 4.2 Experiment Result Chinese Word Clusters\\t!\\tEnglish Word Clusters PE Ak","\\t","1 company. enterprise: corporation on","V-3,,111+\\tpassword",".5.E\\t change. alter, transform, shift","iN*,45)111.4t,t)10\\tI increment. add, addition, increase","1 router Table 1 Word Clusters To enhance symmetry, we use word clusters as terms. For weak symmetry form, we manually generated different amount of word clusters for Chinese and English documents. But we find the symmetry is not good enough to give a reasonable performance on IR test. Further, from all the word clusters we choose 361 pairs for the two languages and now they are in strong symmetry form. Table 1 gives some examples for word cluster pairs. In fact the term-by-document matrices of the two languages has a difference approximately 9.1% compared to their own, which shows the extent to which the strong symmetry form is violated. The difference is calculated as follows 77 D IIA, —A,112 IIAcilliAell where Ac and Ae representing the training matrix in Chinese and in English respectively.","We use 19 Chinese queries to search for English documents and then use 19 English queries to search for Chinese documents. For comparison, we use two kinds of weighting scheme -- term frequency (TF) weighting and binary weighting. For either weighting scheme, we compare two retrieval criterions -- LSI criterion and simplified LSI criterion. The average precision-recall curves are shown in Figure I.","From Figure 1, we find the best precision comes from our LSI criterion with TF weighting. Binary weighting give a better recall but a worse precision, because binary weighting lost much of the detail about term distribution and hence cannot discriminate as well as TF weighting. Simplified LSI (SLSI for short in Figure 1) criterions with both weighting schemes suffer a loss of about 5% in both precision and recall because the symmetry is not precisely kept. However the reduction in calculation and time cost is clear, as shown in Figure 2. We simulate the SVD process involved in the LSI approach and the simplified LSI approach for bilingual training corpus and compare the CPU time used for SVD on an m x m randomly generated matrix and for SVDs on two halves of the same matrix. The result shows that for bilingual cases the simplified LSI approach can save half of the SVD cost approximately. All the calculations are made on a PC with a P4 1.7G CPU and 256M RAM and the CPU time is given by a Matlab function.","The experiment results show that by manually clustering words as terms and binary weighting scheme, we can enhance the symmetry of term-by-document matrices of different languages. Although the simplified LSI approach under strong symmetry form suffers a 5% loss or so in IR performance, we approximately reduce the SVD cost to half, which is more desired in some circumstances. Precision-Recall in MIR","o","-ri\\t,v*.w.„ I","N Ma. •","\\t","tb.","00.9 \\t.........6,_\\t","..,,,...\\t..... .., ,r_„, \\t","-t I\\t I","-.4","i_ –L.... . rii. *. I0\\t 4,,,","0...\\t,\\tI\\tI\\t I","a,\\t 4. MN NM WM","we 41. :11:1411611.,,,,,,,,,..","__, .- ._.,$),_ .. ......,,\\t","I--\\t__0.8 ... \\t",",,\\t--....-......„,,,","''''r**-4,-, \\t-....,„,, r","LSI (binary weighting)","----- Simplified LSI (binary weighting)","..,,,, LSI (TF weighting)","---- Simplified LSI (TF weighting)","0.1\\t0.2\\t0.3\\t0.4\\t0.5\\t0.6\\t0.7\\t0.8\\t0.9\\t1.0","Recall Figure 1 Precision-Recall for Multi-linguistic IR 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 78 12 10 8 Comparison of CPU Time Cost in SVD — Simplified LSI (bilingual case) \\t LSI 6 4 2 Ar*. L\\ti1\"\\t",".t.e\"","....,\"̀'w\\tI","L,. ....A.---T--","I\\t.......*•.** \\tI \\t ........__L___,......r - — •\\t","1 0 0\\t100\\t200\\t300\\t400\\t500\\t600\\t700\\t800\\t900\\t1000","size of matrix Figure 2 Reduction of CPU Time Using Simplified LSI 5\\tConclusions and Future Work How to avoid or reduce the SVD cost is the major problem of LSI approach, especially for multi-linguistic IR. Theoretically when the term-by-document matrices of training corpus are in either of two symmetry forms (1) weak form: the similarity between documents in one language is the same with those of the","matching documents in other languages (2) strong form: the vectors representing matching documents in different languages are the same","but they can be arranged in arbitrary order to compose the term-by-document matrix LSI approach can be simplified by reducing the sizes of matrices for SVD analysis. To enhance the symmetry for real data, we propose two ways — word clustering and binary weighting. Our experiment suggests they are two feasible ways to enhance the strong form symmetry to some extent for a reasonable IR performance. But how to reach the weak symmetry form, which is more generalized and hence seems more promising in practical use, needs further research. References Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6), 391-407. Douglas William Oard. 1996. Adaptive Vector Space Text Filtering for Monolingual and Cross-Language Applications. PhD thesis, University of Maryland, College Park, August 1996. Hongxing Zou, Dianjun Wang, Qionghai Dai, and Yanda Li. 2000. SVD for row or column symmetric matrix. Chinese Science Bulletin, Vol.45, No.22, pp.2042-2044. Hongxing Zou, Dianjun Wang, Qionghai Dai, and Yanda Li. 2002. Singular value decomposition for unitary symmetric matrix. Chinese Journal of Electronics. (Tracking number: 02-730; Accepted for publication). Y. Yang, J. Carbonell, R. Brown, and R. Frederking. 1997. Translingual information retrieval: Learning from bilingual corpora. Artificial Intelligence Journal special issue: Best of IJCAI-97. 79"]}]}