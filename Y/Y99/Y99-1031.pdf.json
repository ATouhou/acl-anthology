{"sections":[{"title":"A STUDY OF PERFORMANCE EVALUATION FOR GA-ILMT USING TRAVEL ENGLISH","paragraphs":["Hiroshi Echizen-yat,Kenji Arakitt, Yoshio Momouchitand Koji Tochinaitt","t Department of Electronics and Information Engineering, Hokkai-Gakuen University S 26-Jo, W 11-Chome, Chuo-ku, Sapporo, 064-0926 Japan","Email:fechi,momouchil@eli.hokkai-s-u.ac.jp","ttDivision of Electronics and Information Engineering, Hokkaido University N 13-Jo, W 8-Chome, Kita-ku, Sapporo, 060-8628 Japan","Email: { araki,to chinai} @media. eng .hokudai. ac. jp Abstract","Recently, many machine translation systems have been developed. However, for translation of conversation, correct translation rates and quality of translation are particularly low. This is due to machine translation systems not being able to generate translation results which fit the context of the conversation . We previously proposed a method of Machine Translation Using Inductive Learning with Genetic Algorithms(GA-ILMT). We compare this system's results to two others that use rule-based translation method, and evaluate the results of experiments done with GA-ILMT, measuring it's performance when applied to travel English. As a result of the evaluation experiments, we confirmed that GA-ILMT can generate translation results which are more appropriate to the context of the conversation."]},{"title":"1 INTRODUCTION","paragraphs":["Recently, the internet has come to be used by many people around the world. As a result, it is necessary to communicate correctly and quickly, information that is expressed in different languages. Machine translation is a very effective method of responding this need. Therefore, substantial research has been carried out in the machine translation field, and many machine translation systems have been developed. However, their correct translation rates and quality of translation are not sufficient to date. In the translation of conversation, the correct translation rate and the quality of translation are particularly low. This is due to machine translation systems not being able to generate translation results which fit the context of the conversation. Machine translation systems must be able to translate well to generate translation results which will also fit the context of the conversation taking place..","To date, machine translation systems have been rule-based machine translation[1]. However, this method has several problems. Rule-based machine translation cannot deal adequately with various linguistic phenomena due to its use of limited rules. Moreover, it also has difficulty in dealing with words that it does not recognize due to the use of it's non-dynamic dictionary. It is difficult for the rule-based machine translation method to translate conversational sentences which have a certain context. To resolve these problems, an example-based machine translation method is being researched [2, 3, 4]. This method can improve the correct translation rate and the quality of translation by incorporating any new translation examples that it is given. It becomes possible to translate conversational sentences through having many translation examples. However, serious obstacles remain before the realization of a practical translation system, as the example-based machine translation method requires both a large base of translation examples and a lot of CPU time. As a result, it is difficult to produce a machine translation system based on this method. -285-Source Sentence Translation Process Translation Result \\t (Proofreading) - Proofread Translation Result I Feedback Process","Dictionary for Translation Rules .1 Learning Process","We previously proposed a method of Machine Translation Using Inductive Learning with Genetic Algorithms toward the realization of a practical machine translation system. We call this method GA-ILMT. In GA-ILMT, translation rules are inductively acquired from only translation examples. Thereafter, many translation rules which apply to specific fields are acquired. Translation results are then generated by using these translation rules. As a result, GA-ILMT can improve the correct translation rate and the quality of translation by incorporating new translation examples. Moreover, a greater variation of translation examples is generated from the initial small number of translation examples, by applying genetic algorithms[6] to machine translation using inductive learning.","Our research goal is to design a computer system with the same capability of language and knowledge acquisition as found in human beings [7]. We believe that GA-ILMT can imitate the learning process which human beings have. GA-ILMT can generate variety translation examples because it uses only given translation examples without any analytical knowledge. For example, the system automatically generate the translation example \"He likes tennis.\" ; Kare wa tenisu ga suki desu. 1 \" by replacing the word \"tea; ocha\" in the given translation example \"He likes tea.;Kare wa ocha ga suki desu.\" with the word \"tennis;tenisu.\" This process corresponds to the process in which a little child replaces words in sentences with other words.","hi order to show the practical effectiveness of GA-ILMT, we performed experiments that give a practical evaluation of GA-ILMT. We used translation examples of travel conversations[8]. In the experiments, we used travel conversations as data to confirm that GA-ILMT can generate translation results which fit the context of the conversation. These results showed that GA-ILMT is a more practical machine translation system than machine translation systems that are rule-based machine translations. However, consideration of the experiment's results was still not sufficient. Therefore, we performed further experiments using more data, evaluating the effectiveness of GA-ILMT in more detail. In this paper, we describe the consideration of these experiment's results in detail. Moreover, we confirm GA-ILMT can generate translation results which fit a certain context, and can adapt to various filed data through its learning capability. 1.1 Outline of the GA-ILMT Figure 1: Outline of GA-ILMT 1 Japanese words are written in italics","GA-ILMT can translate many different languages by simply changing to the language of the translation examples, as this method aims to acquire translation rules from character strings in each of the languages it is translating. Figure 1 shows the outline of GA-ILMT.","First, in the case of English-to-Japanese, the user inputs a source sentence in English. Second, in the translation process, the system generates several candidates as translation results using translation rules extracted from the learning process. Third, the user proofreads the translated sentences if they include any errors. Fourth, in the feedback process, the system determines the fitness value of translation rules used in the translation process and performs a selection process removing erroneous translation rules. In the learning process, new translation examples are automatically produced through crossover and mutation, and various translation rules are acquired from the translation examples through inductive learning. Repetition of the abovementioned process is the equivalent of generation replacement within the system, thus the system is continuously evolving to a higher-quality translation system.","In the feedback process, the system evaluates the translation results using translated sentences which have been proofread. The system determines the fitness value of the translation rules used in the translation process through the use of correct and erroneous translation frequencies. The fitness value is calculated by the fitness function as follows: Fitness value(%) = The correct translation frequency x 100\\t(1)","The number of uses The system performs the selection process using the fitness value.","(1)Selection form, giving translation examples ENGLISH\\tJAPANESE (He likes tennis .;Kare wa tenisu ga suki desu .) (She likes tea .;Kanojyo wa ocha ga suki desu .)","(2)One-point crossover of English sentence He likes\\ttennis.\\tHe likes tea. She likes tea.\\tShe likes tennis. tenisu ga suki desu. ocha ga suki desu. \\t I\\tâ€¢ Kare wa ocha ga suki desu.","Kanojyo wa tenisu ga suki desu.","(4)Generated translation examples \\t ENGLISH\\tJAPANESE (He likes tea .;Kare wa ocha ga suki desu .) (She likes tennis .;Kanojyo wa tenisu ga suki desu .) Figure 2: Example of one-point crossover","In the learning process, new translation examples are automatically generated through crossover and mutation. In crossover, two translation examples which have common parts are selected, the crossover position occurring at the common part. Crossover uses the common parts from the English and Japanese sentences for translation examples. Figure 2 shows examples of a one-point crossover. In Figure 2, \"likes\" is the common part of the two English sentences, and \" wa\" and \"ga suki desu\" are the common parts of the two Japanese sentences. Therefore, \"likes\" and \"wa\" are the crossover positions. New translation examples are generated using the one-point crossover. Next, one-point crossover is performed for \"likes\" and \"ga suki desu\". However, the generated translation examples have the same character strings as the source sentences. Therefore, these translation examples are not input into the dictionary. The system extracts the common and (3)One-point crossover of Japanese sentence Kare wa Kanojyo wa -287-","Table 1: The number of source sentences and the average number of words per source sentence. I Scene 1 (Scene 2 I Scene 3 I Scene 4 number of source sentences \\t345","\\t 607","\\t 456","\\t 296 average number of words per source sentence \\t5.3","\\t 5.6","\\t 5.8","\\t 5.7 different parts from the character strings of all translation examples which incorporating translation examples and generated translation examples inputed into the dictionary. These become translation rules. There are two kinds of translation rules: those for sentences and those for words. The former are called sentence translation rules and the latter word translation rules.","In the translation process, the system generates several candidates as translation results for a source sentence using extracted translation rules. This process also uses genetic algorithm. The system substitutes words using word translation rules, for the variables in the sentence translation rules. The system can generate the Japanese sentence for the English sentence when the English sentence has the same character string as the source sentence. The Japanese sentence generated is the translation result. When there are several translation results, the system selects the correct translation result according to two criteria: the one that uses the translation rule more similar to the source sentence, and the one that uses the translation rule with a higher fitness value."]},{"title":"2 EXPERIMENTS","paragraphs":["We evaluated the performance of GA-ILMT using translation examples from travel conversations in four different scenes \"on the plane\", \"at the airport\", \"on the check-in\" and \"on the telephone\". We call \"on the plane\" Scene 1, \"at the airport\" Scene 2, \"on the check-in\" Scene 3 and \"on the telephone\" Scene 4, respectively. 2.1 Data","Table 1 shows the number of source sentences and the average number of words in each source sentence. These source sentences were taken from 10 English travel books written for Japanese people. These books are [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]. We call these books No.1, No.2, \\t, No.10, respectively. 2.2 Procedure","We used two other machine translation systems, comparing them with the GA-ILMT. We call these two machine translation systems A and 13, respectively. We performed four experiments for data of Scenel, Scene 2, Scene 3 and Scene 4. The initial dictionary of GA-ILMT was empty for each experiment. In the learning process, one-point crossover and two-point crossover were used, and the translation examples were randomly changed by mutation at a rate of 2.0%. In the selection process, the number of uses would be over five and the fitness value would be under 25.0%. The thresholds were determined by a preliminary experiment. The experiment's procedure is shown in Figure 1.","In GA-ILMT, the correct translation rate and the quality of translation may change when the sequence of text books because GA-ILMT is a machine translation method which has learning capability. Therefore, we changed the sequence of text books while performing the evaluation experiments. In the first experiment, the sequence is No.1, No.2, \\t, No.10. In the second experiment, the sequence is No.10, No.9, \\t, No.1. In the third experiment, the sequence is No.1, No.10, No.2, No.9, No.3, No.8, No.4, No \\t7, No.5, No.6. We call these three experiments a, 3 and 7, respectively."]},{"title":"2.3 Standards for Evaluation","paragraphs":["In these experiments, a correct translation result is defined as an exact same character string or a very similar expression as that of a human translator. These evaluation standards are very -288-70 1-\"1 6Â°a ti 50 C 70; 40 A 30 u20 ( 10","la\\tI-\\t.\\t__,.. Art.__ z I\\tâ€¢ I -Jaz on razes. System Scene 1 Scene 2 Scene 3 Scene 4","GA-ILMT(a) 22.3% 25.4% 17.8% 18.9%","GA-ILMT(fi) 22.0% 24.2% 18.0% 20.3%","GA-ILMT(-y) 22.6% 26.0% 18.4% 18.9%","A 6.7% 16.8% 10.7% 16.9%","B 22.0% 20.1% 18.6% 19.6% high. However, translations like these are required as the user needs translation results which fit the context and translation results which use natural conversation sentences. 2.4 Results","Table 2 shows the total correct translation rates of GA-ILMT, A and B, respectively. Figure 3 shows the changes in correct translation rate as the number of source sentences available increases for Scene 1 in experiment a.","50\\t100\\t150\\t200\\t250\\t300\\t345 Number of source sentences available Figure 3: Changes in the correct translation rate as the number of source sentences available increases for Scene 1 in experiment a. 2.5 Discussion 2.5.1 Correct translation rates","In Table 2, all the correct translation rates of GA-ILMT are nearly equal to those of B. However, the changes in correct translation rates are different as shown in Figure 3. The reason for this is that GA-ILMT is a machine translation method based on learning capability. GA-ILMT needs a number of translation examples to acquire translation rules in order to begin learning. Therefore, correct translation rates at the beginning of the learning process are low. In Figure 3, the correct translation rates for GA-ILMT in the first half are generally lower than B. However, the correct translation rates for GA-ILMT in the second half are higher than A and B.","Table 3 shows the number of correct translation results which are generated when combining translation rules from the first half to the second half, respectively. In Table 3, there are few translation results generated by combining the translation rules of the first half. However, in the second half, the number of correct translation results generated by combining extracted translation rules increases. These results show that GA-ILMT has a higher learning capability. In experiments # and y using GA-ILMT, the same evaluation results are achieved. 2.5.2 Quality of translation","The evaluation experiments show that GA-ILMT is a machine translation system which can perform a contextual translation through learning capability. For example, in the experiment of -289-Table 3: Details of the numbers of correct translation results generated.","Scene 4 Second","Scene 1 First I Second","Scene 2 First 1 Second","Scene 3 First 1 Second First 1 Number of correct translation \\t0","\\t 9 [ 10\\t21 1\\t2 \\t","9","\\t 5","\\t 9 Scene 3, GA-ILMT translated \"I have a reservation.\" into \"Yoyaku wo ire te ari masu.\" This translation result nuance is \"I have reserved it.\" A translated it into \"Watashi ni wa yoyaku ga aru.\" B translated it into \"Watashi wa yoyaku wo motte i masu.\" The translation results of A and B have the nuance \"I am the holder of the reservation.\" Generally, \"reservation\" is not an object. Therefore, the translation result of GA-ILMT is a more natural sentence. This shows that GA-ILMT can generate a translation result which better fits the context of a sentence. Figure 4 shows the generation process for the correct translation result of Scene 3. Source sentence:I have a reservation. (1)Translation examples","ENGLISH\\tJAPANESE"]},{"title":"Oa","paragraphs":["made a reservation .; Yoyaku shi te ari masu .)","ENGLISH","("]},{"title":"I","paragraphs":["am Tanaka and I have a reservation for 3 nights. JAPANESE ;Kyou kara san paku no yoyaku wo ire te iru Tanaka desu .)","(2)Replacement of words \"made;shz\" in translation exampleQ and words \"have;wo ire\" in translation example by two-point crossover ENGLISH\\tJAPANESE"]},{"title":"Â©","paragraphs":["(I have a reservation .; Yoyaku wo ire te ari masu .) (3)Translation result: Yoyaku wo ire te ari masu. Figure 4: Examples of generation process in correct translation results for Scene 3.","In Figure 4, the translation example is generated by a two-point crossover between given translation examplesD and"]},{"title":"0.","paragraphs":["In this case, \"reservation; yoyaku\" is one of the crossover positions. Therefore, the translation example whose object is \"reservation;yoyaku\" is selected, and the crossover is performed. As a result, a correct translation result is achieved because the English sentence from the generated translation example has the same character strings as the source sentence. 2.5.3 Effectiveness of GA-ILMT","In GA-ILMT, various translation examples are generated by applying genetic algorithms. The system uses given translation examples and generates a greater variety of translation examples using it's learning capability. Through this process, the system does not use any analytical knowledge like grammatical rules. As a result, system based on GA-ILMT can generate translation rules which better fit the context of translatable subjects. It is difficult for rule-based machine translation to generate translation rules which fit the context. Figure 5 shows the effectiveness of GA-ILMT in Scene 3.","In Figure 5, GA-ILMT translated \"Is there anything cheaper?\" into \"Motto yasui heya wa ari masuka?\" This translation result is correct. A translated it into \" Yori yasui donna mono mo aruka?\" This translation result is grammatically erroneous. B translated it into \"Motto yasui nanika ga ari masuka?\" This translation result means \"Is there anything cheaper?\" In the context of Scene 3, \"cheaper\" corresponds to \"cheaper room.\" Therefore, the translation result of GA-ILMT is correct. In Figure 5, the correct translation result is generated using translation rule6\"anything cheaper;motto yasui heya.\" The Japanese means \"a cheaper room.\" Source sentence:Ls there anything cheaper? (1) Translation examples","ENGLISH","0(Could you tell me where a good restaurant is near here ? JAPANESE ; Chikaku ni yoi resutoran wa ari masenka ?) ENGLISH","(2)(Do you have a map of the city? JAPANESE",";Shinai no chizu wa ari masuka ?)","ENGLISH","o (Do you have anything cheaper? JAPANESE",";Motto yasui heya wa ari masenka ?)","(2)Translation example which was generated by one-point crossover between translation examplesT and e","ENGLISH","Â®(Do you have a good restaurant is near here?","JAPANESE",";Shinai no chizu wa ari masenka ?)","(3)Translation rule which was gotten by extracting different parts between translation examplese and e","ENGLISH\\tJAPANESE","Â®(anything cheaper; motto yasui heya) Figure 5: Effectiveness of GA-ILMT in Scene 3. Translation ruleÂ® is acquired by extracting different parts from the given translation examplee and the generated translation exampleÂ®. The generated translation exampleÂ® is an erroneous translation example. However, the local part \"Do you have \"-r- wa ari masenka?\" is correct. Therefore, the system has acquired translation ruleÂ® from the given translation exampleÂ°. By applying genetic algorithms, GA-ILMT has succeeded in attaining correct contextual application.."]},{"title":"3 CONCLUSION","paragraphs":["We confirmed that GA-ILMT can generate translation results which fit the context. System based on GA-ILMT is a more practical machine translation system than the systems based on rule-based machine translation for travel conversation. Moreover, we confirmed that GA-ILMT can improve the correct translation rate because it has learning capability.","In GA-ILMT, the translation examples and translation rules are generated using only given translation examples without any analytical knowledge. As a result, GA-ILMT can generate translation results which better fit the context of translatable subjects. It is difficult for rule-based machine translation to generate translation results which fit the context. The reason being that rule-based machine translation needs too much knowledge to generate translation results which fit the context. Also, in GA-ILMT, the generation process corresponds to the process where a little child acquires a lot of knowledge by changing words in given sentences. We consider that GA-ILMT imitates the process of knowledge and language acquisition which human beings have. As a result, a system based on GA-ILMT is a machine translation system which through its learning capability has both the higher correct translation rate and the quality of translation.","In the future, we plan to experiment using various data for many other fields where translation is required, and improve GA-ILMT to get the ever higher correct translation rate and quality of translation. -291-"]},{"title":"4 ACKNOWLEDGMENTS","paragraphs":["This work is partially supported by the Grants from the Government subsidy for aiding scientific researches (No. 10680367 and No. 09878070) of the Ministry of Education of Japan and High-Tech Research Center of Hokkai-Gakuen University."]},{"title":"References","paragraphs":["[1] W. John Hutchins. and Harold L. Somers. 1992. An Introduction to Machine Translation. ACADEMIC PRESS. (London)","[2] Nagao, M. 1984. A Framework of a Mechanical Translation between Japanese and English by Analogy Principle, Elithorn, A. and Baneriji, R.(eds). Artificial and Human Intelligence, NATO.","[3] Stanfill C. and Waltz D. 1986. Toward Memory-Based Reasoning. Communications of the ACM, Vol.29, No.12, pages 1213-1228.","[4] Sumita, E., 0i, K., Furuse, 0., lida, H., Higuchi, T., Takahashi, N. and Kitano. H. 1993. Example-Based Machine Translation on Massively Parallel Processors. In Proceedings of IJCAI-93, pages 1283-1288, Chambery, France, August-September.","[5] Echizen-ya, H., Araki, K., Momouchi, Y. and Tochinai, K. 1996. Machine Translation Method Using Inductive Learning with Genetic Algorithms. In Proceedings of Coling'96, pages 1020-1023, Copenhagen, Denmark, August.","[6] Goldberg, D.E. 1989. Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley. (Massachusetts)","[7] Araki, K., Momouchi, Y. and Tochinai, K. 1995. Evaluation for Adaptability of Kana-Kanji Translation of Non-Segmented Japanese Kana Sentences using Inductive Learning. In Proceedings of PACLING-II, pages 1-7, Brisbane, Australia, April.","[8] Araki, K., Echizen-ya, H. and Tochinai, K. 1997. Performance Evaluation in Travel English for GA-ILMT. In Proceedings of the LASTED International Conference, pages 117-120, Banff, Canada, July-August. [9] Araki, Y., and Lee, J. 1995. Travel English pocket book. Nihon Bungei Sha. (Tokyo). [10] Ryokou Kaiwa Kenkyuukai. 1980. Kaigai Ryokou Eikaiwa. Jitugyou no Nihon Sha. (Tokyo). [11] Gilbert, K. 1995. Kent no Travel Eikaiwa. Jitugyou no Nihon Sha. (Tokyo).","[12] Ishikawa, Y. and Travel Communication Kenkyuukai. 1995. A Timely Handbook for Single Travelers Travel English. Jitugyou no Nihon Sha. (Tokyo). [13] Maekawa, Y. 1994. America o Jiyuu ni Aruku Tabi no Beikaiwa. Ikeda Shoten. (Tokyo). [14] Reed, W. 1995. Komatta Toki no Travel Eikaiwa Nyuumon. Nihon Bungei Sha. (Tokyo). [15] Book Maker. 1996. Kaigai Ryokou Kantan Eikaiwa Hand Book. Ikeda Shoten. (Tokyo).","[16] Junko Kai. 1991. Hitori Aruki no Eigo Jiyuujizai. Nihon Koutuu Kousha Shuppan Jigyou Kyoku. (Tokyo).","[17] Tikyuu no Arukikata Hensyuusitu. 1993. Tabi no Kaiwasyuu 2 Beigo/Eigo. Diamond Sha. (Tokyo).","[18] Saito, A. 1960. Rokkakokugo Kaiwa I Pocket Interpreter. Nihon Koutuu Kousha Shuppan Jigyou Kyoku. (Tokyo)."]}]}