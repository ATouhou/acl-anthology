{"sections":[{"title":"Parsing Some Constrained Grammar Formalisms K. Vijay-Shanker*","paragraphs":["University of Delaware"]},{"title":"David J. Weir*","paragraphs":["University of Sussex"]},{"title":"In this paper we present a scheme to extend a recognition algorithm for Context-Free Grammars (CFG) that can be used to derive polynomial-time recognition algorithms for a set of formalisms that generate a superset of languages generated by CFG. We describe the scheme by developing a Cocke-Kasami-Younger (CKY)-like pure bottom-up recognition algorithm for Linear Indexed Grammars and show how it can be adapted to give algorithms for Tree Adjoining Grammars and Combinatory Categorial Grammars. This is the only polynomial-time recognition algorithm for Combinatory Categorial Grammars that we are aware of. The main contribution of this paper is the general scheme we propose for parsing a variety of formalisms whose derivation process is controlled by an explicit or implicit stack. The ideas presented here can be suitably modified for other parsing styles or used in the generalized framework set out by Lang (1990). 1. Introduction","paragraphs":["This paper presents a scheme to extend known recognition algorithms for Context-Free Grammars (CFG) in order to obtain recognition algorithms for a class of grammatical formalisms that generate a strict superset of the set of languages generated by CFG. In particular, we use this scheme to give recognition algorithms for Linear Indexed Grammars (LIG), Tree Adjoining Grammars (TAG), and a version of Combinatory Categorial Grammars (CCG). These formalisms belong to the class of"]},{"title":"mildly contextsensitive grammar formalisms","paragraphs":["identified by Joshi (1985) on the basis of some properties of their generative capacity. The parsing strategy that we propose can be applied to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993).","There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of \"states\" the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is important because it allows one to keep only a limited amount of context during the recognition process, * Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716. E-mail: vijay@udel.edu. School of Cognitive and Computing Sciences, University of Sussex, Brighton BN1 9QH, U.K. E-mail: davidw@cogs.susx,ac.uk. © 1994 Association for Computational Linguistics Computational Linguistics Volume 19, Number 4 which results in polynomial time algorithms. In the recognition algorithms mentioned above for CFG, HG, and TAG this is reflected in the fact that the recognizer can encode intermediate stages of the derivation with a bounded number of states. An array is used whose entries are associated with a given component of the input. In the case of the CKY algorithm, the presence of a particular nonterminal in an array entry is used to encode the fact that the nonterminal derives the associated substring of the input. The context-freeness of CFG has the consequence that there is no need to encode the way, or ways, in which a nonterminal came to be placed in an array entry.","In this respect, the derivation processes of CCG and LIG would appear to differ from that of CFG. In these systems unbounded stacklike structures replace the role played by nonterminals in controlling derivation choices. This would seem to suggest that the context-freeness property of CFG, HG, and TAG derivations no longer holds. Unbounded stacks can encode an unbounded number of earlier derivation choices. In fact, while the path sets 1 of CFG, HG, and TAG derivation trees are regular languages, the path sets of CCG and LIG are context-free languages. With respect to recognition algorithms, this suggests that the array (whose entries contain nonterminals in the case of CFG) would need to contain complete encodings of unbounded stacks giving an exponential time algorithm.","However, in LIG and CCG, the use of stacks to control derivations is limited in that different branches of a derivation cannot share stacks. Thus, despite the above observations, the context-freeness property does in fact hold. A detailed explanation of why this is so will be presented below. We propose a method to extend the CKY algorithm to handle the limited use of stacks found in CCG and LIG. We have chosen to adapt the CKY algorithm since it is the simplest form of bottom-up parsing. A similar approach using Earley algorithm is also possible, although not considered here. Since the use of the stacks is most explicit in the LIG formalism we describe our approach in detail by developing a recognition algorithm for LIG (Sections 2 and 3). We then show how the general approach suggested in the parser for LIG can be tailored to CCG (in Section 4). In the above discussion TAG has been grouped with HG. However, TAG can also be viewed as making use of stacks in the same way as LIG and CCG. In Section 5 we show how the LIG algorithm presented in Section 3 can be adapted for TAG. 2. Linear Indexed Grammars An Indexed Grammar (Aho 1968) can be viewed as a CFG in which objects are nonterminals with an associated stack of symbols. In addition to rewriting nonterminals, the rules of the grammar can have the effect of pushing or popping symbols on top of the stacks that are associated with each nonterminal. Gazdar (1988) discussed a restricted form of Indexed Grammars in which the stack associated with the nonterminal on the left of each production can only be associated with one of the occurrences of nonterminals on the right of the production. Stacks of bounded size are associated with other occurrences of nonterminals on the right of the production. We call this Linear Indexed Grammars (LIG). 2 1 The path set of a tree is the set of strings labeling paths from the root to the frontier of the tree. The path set of a tree set is the union of path sets of trees in the set. 2 The name Linear Indexed Grammars is used by Duske and Parchmann (1984) to refer to a different restriction on Indexed Grammars in which production was restricted to have only a single nonterminal on their right-hand side. 592 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Definition 2.1 A LIG, G, is denoted by"]},{"title":"(VN, VT, VI, S, P)","paragraphs":["where"]},{"title":"VN","paragraphs":["is a finite set of nonterminals,"]},{"title":"VT","paragraphs":["is a finite set of terminals,"]},{"title":"VI","paragraphs":["is a finite set of indices (stack symbols), S c VN is the start symbol, and P is a finite set of productions.","We adopt the convention that (~, fl (with or without subscripts and primes) denote members of V~, and ~ denotes a stack symbol. As usual, A, B, C will denote nonterminals, a, b, c will denote terminals, and u, v, w will denote members of V~. Definition 2.2 A pair consisting of a nonterminal, say A, and a string of stack symbols, say (~, will be called an object of the grammar and will be written as A (c~). Given a grammar, G, we define the set of objects Vc(G) = { A ((~) I A C VN, (~ E V~ }.","We use T to denote strings in (Vc(G) U VT)*. We write A(-.~) to denote the nonterminal A associated with an arbitrary stack (~ with the string on top. Also, we use A () to denote that an empty stack is associated with A. The general form of a production in a LIG is:"]},{"title":"a (.. (~)","paragraphs":["--+ Wlal"]},{"title":"(oL1)w2...","paragraphs":["ai-1"]},{"title":"(oq-1)","paragraphs":["wiai (.. oq) Wi+lai+ 1"]},{"title":"(oq+ 1)...","paragraphs":["A n (o@) Wn+ 1 for n"]},{"title":">","paragraphs":["0 and wl ..., W,+l are members of V~-. Definition 2.3 The derivation relation, ~, is defined below. If the above production is used then for any fl ~ V{, T1, T2 E (Vc(G) U Wv) *: T1A (rico"]},{"title":"T2 ~ TlWlA1 (o~1)W2... Ai-1 (oq-1) wiAi (tic, i)Wi+lAi+l (Oq+l) • .. An (oln)","paragraphs":["wn+IT2. We use ~ as the reflexive, transitive closure of ~. As a result of the"]},{"title":"linearity","paragraphs":["in the general form of the rules, we can observe that the stack flc~ associated with the object in the left-hand side of the derivation and"]},{"title":"flc~i","paragraphs":["associated with one object in the right-hand side have the initial part fl in common. In the derivation above, we will say that this object a i (flOq) is the distinguished child of A (flo0. Given a derivation, the distinguished descendant relation is the reflexive, transitive closure of the distinguished child relation.","The language generated by a LIG, G,"]},{"title":"L(G) = { w I S() ~ w }. Example 2.1","paragraphs":["The LIG, G = ({ S, T }, { a, b, c }, { \")/a~ \"Yb )~ S~/9) generates ("]},{"title":"wcw ] w C","paragraphs":["{a, b} + } where P contains the following productions."]},{"title":"S(..)-*aS(..%) S(..)--~bS(..q/b) S(..)---~ T(..) T(..%)--, T(..)a T(..',/b)-+ T(..)b T()--*c","paragraphs":["A derivation tree for the string"]},{"title":"abbcabb","paragraphs":["is given in Figure 1. 593 Computational Linguistics Volume 19, Number 4 sf ) b s%v \"~aV b T(~_ ) b T() a I ¢ Figure 1 Derivation tree for LIG.","In this paper rather than adopting the general form of rules as given above, we restrict our attention to grammars whose rules have the following form. In fact, this can be easily seen to constitute a normal form for LIG. 1. A (c0 ~ c where ~ C"]},{"title":"VT U","paragraphs":["{c} and length of c~,"]},{"title":"len (,9<) >>_ 1. 2. A (..","paragraphs":["\"/1... Q/m) ---->"]},{"title":"Ap (.. Vp) As (O<s)","paragraphs":["where m > 0. 3. a ('\" '71\". \"Ym) --\" As (OLs) ap (.."]},{"title":"~p)","paragraphs":["where m > 0."]},{"title":"4. A (\"71... 7m) \"--+ Ap (.. 7p)","paragraphs":["where m > 0. We allow at most two symbols in the right-hand side of productions because we intend to develop CKY-style algorithms. In the above rules we say that"]},{"title":"AF (.. \"yp)","paragraphs":["is the primary constituent and"]},{"title":"As","paragraphs":["(c~s) is the secondary constituent. Notice also that in a derivation using such a rule, the primary constituent yields the distinguished child. (In grammatical theories that use a stack of subcategorized arguments, the top of the stack in the primary constituent determines which secondary constituent it can combine with.) 2.1 Terminators Let us consider how we may extend the CKY algorithm for the recognition of LIG. Given a fixed grammar G and an input al • .. an, the recognition algorithm will complete an n x n array P such that an encoding of A (cO is stored in P [i, d] if and only if A (oQ"]},{"title":"ai... ai+d-1.","paragraphs":["The algorithm will operate bottom-up. For example, if G contains the rule a ('\" \")11... \"Ym) ---+ ap (.. \"~p) A s (O~s) and we find an encoding of"]},{"title":"Ap (O<p'yp)","paragraphs":["in"]},{"title":"P Ii, dp]","paragraphs":["and an encoding of As (C~s) in"]},{"title":"P Ii + dp~ ds]","paragraphs":["then an encoding of A (C~p'yl... \"Ym) will be stored 594 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms in"]},{"title":"P Ii, dp + dsl.","paragraphs":["What encoding scheme should be used? The most straightforward possibility would be to store a complete encoding of A (c~p3,~... 3,m) in P [i, dp + ds]. However, in general, if an object A (~) derives a string of length d then the length of o~ is (,.9(d). 3 Hence there can be O(/d) objects that derive a substring of the input (of length d), for some constant k. Hence, the space and time complexity of this algorithm is exponential in the worst case. 4","The inefficiency of this approach can be seen by drawing an analogy with the following algorithm for CFG. Suppose rather than storing sets of nonterminals in each array entry, we store a set of trees containing all derivation subtrees that yield the corresponding substring. The problem with this is that the number of derivation trees is exponential with respect to the length of the string spanned. However, there is no need to store derivation trees since in considering the combination of subderivation trees in the CFG, only the nonterminals at the root of the tree are relevant in determining whether there is a production that licenses the combination.","Likewise because of the last-in first-out behavior in the manipulation of stacks in LIG, we will argue that it is not necessary to store the entire stack. For instance, consider the derivation (depicted by the tree shown in Figure 2) from the point of view of recording the derivation in a bottom-up parser (such as CKY). Let a node ~?1 labeled B (fl3,1 ... 3,k... 3,m) be a"]},{"title":"distinguished descendant","paragraphs":["of a node ~1 labeled A (fl3,1 ... 3,k) as shown in the figure. Viewing the tree bottom-up, let the node ~], labeled A (fl3,1 • •. 3,k), be the first node above the node ~71, labeled B (fl3,1 •. • 3,k. • • 3,m), where 3,k gets exposed as the top of the stack. Because of the last-in first-out behavior, every"]},{"title":"distinguished","paragraphs":["descendant of ~] above 711 will have a label of the form A I (fl3,1 ... 3,k~) where"]},{"title":"len (~) > 1.","paragraphs":["In order to record the derivation from A (fl3,1 ... 3,k) it would be sufficient to store A and 3'1 .. • 3,k if we could also access the entry that records the derivation from"]},{"title":"At (fl3,t).","paragraphs":["In the entry for ~?, using a pointer to the entry for"]},{"title":"At (fl3,t)","paragraphs":["would enable the recovery of the stack below the top k symbols, 3,1 • .. \"Yk. However, this scheme works well only when k _> 2. For instance, when k = 1, suppose we recorded only A, 3,1, and a pointer to entry for"]},{"title":"At (fl3,t).","paragraphs":["Suppose that we are looking for the symbol below 3,1, i.e., the top of ft. Then it is possible that in a similar way the latter entry could also record just"]},{"title":"At~ 3,t,","paragraphs":["and a pointer to some other entry to retrieve ft. This situation can occur arbitrarily many times.","Consider the derivation depicted in Figure 3. In this derivation we have indicated the branch containing only the distinguished descendants. We will assume that the node labeled D (f13,, ..-3,k-13,~"]},{"title":".-.","paragraphs":["3/~n ,) is the closest distinguished descendant of"]},{"title":"C (fl3,1..-3,k-13,~)","paragraphs":["such that every node between them will have a label of the form C' (fl\"Yl-., 3,k-13,~ O/) where"]},{"title":"len","paragraphs":["(~') > 1. Therefore, any node between that labeled"]},{"title":"C (fl3,1..-3,k-13,~)","paragraphs":["and"]},{"title":"B(fl3,1...3,rn)","paragraphs":["will have a label of the form"]},{"title":"C\" (fl3,1..-\"~k-10/')","paragraphs":["where"]},{"title":"fen","paragraphs":["(c~\") > 1. Now the entries representing derivations from both"]},{"title":"A(fl3,1... 3,k-13,k)","paragraphs":["and C (fl3,1... 3,k-13,~) could point back to the entry for the derivation from"]},{"title":"At","paragraphs":["(fl3,t), whereas the entry for C' (fl3,1 ...3,k-13,~c~') will point back to the entry for A We shall now formalize these notions by defining a terminator. 3 For instance, consider the grammar in Example 2.1 and the derivation in Figure 1. In general we can have derivations of the form"]},{"title":"T (q'a3\"~) ~ cab n.","paragraphs":["However, if there exists productions of the form A (c~) --~ ~ then the length of the stack in objects is not even bounded by the length of strings they derive. 4 The CCG parsing algorithms that have been proposed so far follow this strategy (Pareschi and Steedman 1987; Tomita 1988). 595 Computational Linguistics Volume 19, Number 4 H 1 Figure 2 Recovering the rest of stack-1."]},{"title":")m Z Figure 3","paragraphs":["Recovering the rest of stack-2. v v Figure 4 Definition of a Terminator. 596 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Definition 2.4 Suppose that we have the derivation tree in Figure 4 that depicts the following derivation:","A (fl3q ... %-17) ~ uB (fl'Ya ... q/k--lq/k.'' q/m) W uAt (flq/t) As (O~s) w uvw or similarly:"]},{"title":"A (flq/1\"'- q/k-lq/)","paragraphs":["uB"]},{"title":"(flq/1...","paragraphs":["q/k-lq/i.., q/m) W uAs (Ols) At (flq/t) w l~ V W where the following conditions hold 2<k<m","The nodes labeled B (flq/1 ...","q/k-lq/k . .",".","q/m) and At (flq/t) are distinguished","descendants of the node labeled A (flq/1 ... q/k-lq/) in the respective trees. For any distinguished descendent labeled C (c~') between the nodes labeled"]},{"title":"A (flq/1...","paragraphs":["\")~k-lq/) and B (flq/1.-. q/k-lq/k'''"]},{"title":"q/m), O/ is of","paragraphs":["the form flq/1 • • • q/kC~ where len (c~) > 1. Note that the nodes labeled"]},{"title":"a (flq/1...","paragraphs":["q/k-lq/) and B (flq/1... q/k-lq/k''\" q/m) need not be different. The node labeled At (flq/t) is the k-terminator of the node labeled A (flq/1 ... q/k-lq/).","When it is clear from context, rather than saying that a node is a terminator of another we will assume that terminators have been defined on objects that participate in a derivation as well. For instance, in the above derivations, we will say that At (flq/t) is the k-terminator of A (fl71 ... 7k-l\"Y). Also when the derivation is clear from context, we will omit the mention of the derivation (or derivation tree). Additionally, we will say that a node (object) has a terminator, if it has a k-terminator for some k.","We will now state some properties of terminators that influence the design of our recognition algorithm. Definition 2.5 Given a grammar, G, define MCL(G) (Maximum Change in Length) as: MCL(G) = max { m ] A (.. q/1. . . q/m) --* T1Ap ('\" ~p) T2 is a production of G } Henceforth, we will write MCL since the grammar in question will always be known from context. Observation 2.1 In a derivation tree, if a node (say ~) has a k-terminator (say ~t) then ~t is a distinguished descendant of ~/. If the node ~/is labeled A (flc~) (where len (c~) = k) then the node 7/t must be labeled A t (flq/t) for"]},{"title":"some","paragraphs":["At C VN and q/t ff VI. Furthermore, 2 < k < MCL. Observation 2.2 In a derivation tree, if a node has a k-terminator then it has a unique terminator. 597 Computational Linguistics Volume 19, Number 4","If ~/is the node in question then we are claiming here that not only does it have a unique k-terminator but also that there does not exist k ~ with k' ~ k such that ~ has a kMerminator. To see why this is the case, let some node ~? have a k-terminator (for some k), say ~t. Using Observation 2.1 we can assume that they are labeled A (fl~l ... ~k-l\"Y) and"]},{"title":"At (flq/t),","paragraphs":["respectively, where we have (k-1) > 1. From the definition of terminators we can assume that the parent of the terminator, ~/t, is a node (say ~') that has a label of the form B (fl3'1 ..."]},{"title":"\"/k-l\"~k... \"Ym).","paragraphs":["Since (from the definition of terminators) every node between ~ and 7/~ (inclusive) must have a label of the form C (fl'Yl -.. \")'k-la ~) where"]},{"title":"len (a ~) >_","paragraphs":["1, it immediately follows that Tit is the closest"]},{"title":"distinguished","paragraphs":["descendant of such that the length of the stack in the object labeling ~]t is strictly less than the length of the stack in the object labeling ~/. From this, the uniqueness of terminators follows. Observation 2.3 Consider the derivation A (fl\"Yl ... \"Yk-l\"~) ~"]},{"title":"uAt (fl\"Yt) w ~ uvw","paragraphs":["where"]},{"title":"At (fl'~t)","paragraphs":["is the k-terminator of A (fl~/1---'Tk-l\"Y). Then for any fl' and v', if"]},{"title":"At","paragraphs":["(fl'~'t) ~ v' then we have the derivation A (fl'~l ..."]},{"title":"\"/k-~\"/) ~ uAt","paragraphs":["(fl\"Yt)"]},{"title":"w ~ uv'w","paragraphs":["where"]},{"title":"At","paragraphs":["(fl\"~t) is the","k-terminator of A (fl\"~l ... 3~k-~'Y). This follows from the fact that the derivation of"]},{"title":"uAt (fl\"yt) w","paragraphs":["from A (fl'Yl ... \"Yk-l\"7) is independent of ft. Therefore we can replace"]},{"title":"At (fl')'t) ~ v","paragraphs":["by"]},{"title":"At","paragraphs":["(fl'\"/t) =~ v'. This is a very important property that is crucial for obtaining polynomial-time algorithm. Note that not all nodes have terminators. For example, if a node labeled A (a) is the parent of a node labeled a (i.e., corresponding to the use of the production A (a) --* a where a is a terminal symbol) then obviously this node does not have a terminator. Definition 2.6 Given a grammar, G, we define MTL(G) (Maximum Length in, Terminal production) as: MTL(G) ="]},{"title":"max { len","paragraphs":["(a) ] A (a) --* c is a production of G where ~ c"]},{"title":"VT","paragraphs":["(_J{¢} }. As in the case of MCL, we will use MTL rather than MTL(G). Observation 2.4 In the derivation A (a) ~ w if"]},{"title":"len (a) >","paragraphs":["MTL then A (a) has a terminator. There must be at least two steps in the above derivation since"]},{"title":"len","paragraphs":["(a) > MTL. However, we can assume that the node (say 7) in question labeled by the object","1 A (a) has a distinguished descendant, say ~/~, with label B (fl) such that B (fl) ~ ¢. Therefore,"]},{"title":"len (fl) <_","paragraphs":["MTL and we may rewrite w as"]},{"title":"u¢v.","paragraphs":["Since"]},{"title":"fen (a) > len (fl)","paragraphs":["we can find the closest distinguished descendant of ~/labeled C (a ~) for some C, a ~ such that"]},{"title":"len (a ~) < fen (a).","paragraphs":["That node is the terminator of ~] from the arguments made in Observation 2.2.","The above observations will be used in the following sections to explain the way in which we represent derivations in the parsing table. We conclude this section with an observation that has a bearing on the steps of the recognition algorithm. 598 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Observation 2.5 Consider the following derivation."]},{"title":"a (fl'~l\"\"\" \"/k-l) TIA. (fl~/l... ')'k-l\"Yk) T2 l~llap (fl71... 7k-17k) U2 Ul Vl At (fl'/t) v2u2 Ul Vl WV2U2","paragraphs":["where"]},{"title":"Ap (fiVe... \"Yk-l'Yk) is","paragraphs":["the distinguished child of A (f17~..."]},{"title":"')'k-l) and At (flVt)","paragraphs":["is the k-terminator of"]},{"title":"Ap","paragraphs":["(fl71..-"]},{"title":"7k-l\"Yk). At (fl~t)","paragraphs":["is the (k - 1)-terminator of"]},{"title":"A (fl71 .-- \")'k-l)","paragraphs":["if and only if k > 2. If k = 2 then A (fl71) has a terminator if and only if"]},{"title":"At (fl'Tt)","paragraphs":["does. In fact, in this case, if"]},{"title":"At (fl'Tt)","paragraphs":["has a kMerminator then that terminator is also the","k~-terminator of A (flVt).","This can be seen by considering the derivation shown in Figure 3 and noting the","sharing of the terminator of C (fl3'1.-."]},{"title":"7k-17~)","paragraphs":["and"]},{"title":"A (fl\"/1-.. 7k-l\")/k) • 3. Recognition Algorithms","paragraphs":["As in the CKY algorithm we will use a two-dimensional array, P, such that if A (c~)"]},{"title":"ai.. • ai+d-1","paragraphs":["then a representation of this derivation will be recorded with an encoding of A ((~) in P [i, d]. Here we assume that the given input is al ..."]},{"title":"an.","paragraphs":["We start our discussion by considering the data structures we use to record such objects and derivations from them. 3.1 Anatomy of an Entry We mentioned earlier that the stack in an object can be unboundedly large. We must first find a compact way to store encodings of such objects whose size is not bounded by the grammar. In this section we provide some motivation for the encoding scheme used in the recognition algorithm by considering the bottom-up application of the rule and the encoding of the primary constituent: A (..'y1. . .'~m) --* Ap (\"~/p) As (,~s) The Head. An object with nonterminal"]},{"title":"Ap","paragraphs":["and top of stack \"Tp will match the primary category of this rule. Thus, the first requirement is that at least this much of the object must be included in every entry since it is needed to determine if the rule can apply. This component is denoted lap, vp/and called the head of the entry. Thus, in general, an entry in"]},{"title":"P Ii, d I","paragraphs":["with the head {A,'~/ encodes derivations of"]},{"title":"ai...ai+cl-1","paragraphs":["from an object of the form A (fl'y) for some fl¢ V 7. Terminator-pointer. An encoding of the object"]},{"title":"Ap (fl'Tp)","paragraphs":["(the primary constituent) that derives the substring"]},{"title":"a i ...","paragraphs":["ai+dp_ 1 (of the input string al • •. an) will be stored in the array element"]},{"title":"P {i, dp]","paragraphs":["in our CKY-style recognition algorithms. Now consider the encoding of"]},{"title":"Ap (fl'yp)","paragraphs":["for some sufficiently long fl-yp. While the head,"]},{"title":"lAp, ~p),","paragraphs":["of the entry is sufficient to determine whether the object in question can match the primary category of the rule, we will need to store more information in order that we can determine the content of the rest of the stack. In the above production, if m = 0 then the combination of Ap (fl~/p) and As (~s) results in A (fl). In order to record the derivation from A (fl), we need to know the top symbol in the stack fl, i.e., the symbol below the top of the stack associated with the primary constituent. We need to recover the identity of 599 Computational Linguistics Volume 19, Number 4 this symbol from the encoding of the primary category. This is why we introduced the notion of terminators. As mentioned in Section 2.1, terminators can be used to access information about the rest of the stack. In the encoding of Ap (fl'yp), we will store information that allows us to access the encoding of its terminator. The part of the entry encoding the terminator will be called terminator pointer. The Middle. Note that the object Ap (fl,yp) (in the derivation Ap (fl3'p) =~ ai... ai+dp-1) can have a k-terminator where k is between 2 and MCL. Therefore, from Observation 2.1 it follows that the terminator-pointer can only be used to determine the (k+l) st symbol from the top. Therefore, assuming that fl = fl\"yl • .. \"Yk-1, the terminator-pointer will allow us to access fl~. (Recall from the definition, a k-terminator of A (fl\"yl ... \"Yk-13'p) will have the form At (fl\"Yt). Thus the (k + 1) st symbol from the top in A (fl-yp) is the same as the symbol below the top of the stack of the terminator.) Thus, we will need to record the string \"yl -'' \"Yk-1 in the encoding of Ap (fl'q/1 ... 3'k-1~'p) as well. This part of the entry will be called the middle.","To summarize, the entry stored in P [i, dp] (where fl\"yl... \"Yk-l\"Yp is assumed to be sufficiently long that we know A m (fl'71-.. 7k-l\"Yp) is guaranteed to have a terminator) will have a head, (Ap,-yp); and a tail comprised of a middle, \"Yl..-'Yk-1; and a terminator-pointer. Note that the length of the middle must be at least one, but at most MCL - 1, since from Observation 2.1, we know 2 < k < MCL. We will call an entry of this kind a terminator-type entry.","We will now discuss what we need to store in order to point to the terminator. Suppose we would like to record in P[i,d] the derivation of ai...ai+d-1 from A (fl'Yl... 7k-l\"Y) as shown below. We assume that At (fl'yt) is the terminator in this derivation.","a (fl\"/1...'Yk-l\"Y) ai. .. at_lAt (fl')'t) at+dt . . . ai+d-1","ai • • • at-l at • • • at+dt-l at q-dt • • • ai+d-1 ~- ai • •. ai+d-1 From Observation 2.3, it follows that it would be sufficient to use"]},{"title":"((at~ \"Ytl~ [t~","paragraphs":["dt]) as the terminator-pointer. This is because any entry with the head (At~ \")'tl in P It, dt] will represent in general a derivation At (fl\"Yt) ~ at... at+dr-1. This not only matches the above case, but even if fl' ~ fl, from the Observation 2.1, we have A (fl'\"/1... \"Yk-lq/) ~ ai... at-lAt (fl'\"/t)"]},{"title":"at+dr.., aiq-d--1 ~","paragraphs":["ai... ai+d-1. Thus, the use of the head information (plus the two indices) in the terminator-pointer captures the essence of Observation 2.3. It is this structure-sharing that allows us to achieve polynomial bounds for space and time. Note that the string derived from the terminator, at...at+dr-i, is a substring of ai...ai+d-1. In such a case, i.e., when i G t and i+ t >>_ t + dr, we will say that /t, dt/ <_ {i~dl. We define {t, dt/ < {i, dl if {t, dtl <_ {i, dl and {t, dtl # {i, dl. Since any terminator-type entry in P[i,d] can only have terminator-pointers of the form ({At, \"Ytl ~ {t, dtl ) where It, dtl <_ {i, dl, the number of terminator-type entries in P [i, d] is O(d2). Definition 3.1 Given a grammar, G, define MSL(G) (Maximum Secondary constituent's stack Length) as MSL(G) = max { len (as) I As (e~s) is the secondary constituent of a production } Henceforth we will use MSL rather than MSL(G). 600 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms","We now consider the question of when a terminator-type entry is appropriate. Of course, if A (~) ~ ai... ai+d-1 we could store such an entry in P Ii, dpl only when A (c~) has a terminator in this derivation. From Observation 2.4 we know that if len (c~) > MTL then there exists a terminator of A (~) in this derivation. However, it is possible that for some grammar MSL > MTL. Therefore even when len (c~) > MTL (i.e., the object has a terminator) A (~) can still match the secondary category of a rule if len (c~) G MSL. In order to verify that an object matches the secondary category of a rule we need to consider the entire stack in the object. When A (~) ~ ai... ai+d-1 and length of ~ does not exceed MSL, it would be convenient to store A as well as the entire stack c~ because such an object can potentially match a secondary category of a rule. To be certain that such an object is stored in its entirety when len (~) < MSL, the terminator-type entry can only be used when len (c~) > max(MSL~ MTL). However, we prefer to use the terminator-type entry for representing a derivation from A (~) only when its terminator, say At (fl), is such that len (fl) >_ max(MSL~ MTL) rather than when len (c~) > max(MSL~ MTL). Again, we point out that this choice is made only for convenience and because we feel it leads to a simpler algorithm. The alternate choice could also be made, which would lead to a slightly different algorithm."]},{"title":"Definition 3.2","paragraphs":["Define the constant TTC (Terminal-Type Case) as TTC = max(MSL MTL). In a derivation A (fl71 ... 7k) ~ W we will say that A (flY1 -.. Vk) has the TC-property iff it has a k-terminator, say At (flTt), such that len (flVt) _> TTC. If A"]},{"title":"(fl31","paragraphs":["... 3k) ~ ai... ai+d_l, where A (fl31 .-. 3k) does not have the TC-property then we record the object in its entirety in P Ii~ d]. In order for such an entry to have the same format as the terminator-type entry, we say that the entry has a head /A~ 3k); a tail with a middle 31.-. 7k-1 and a nil terminator-pointer. Note that in this case the middle can be an empty string; for instance, when we encode A (V) ~ ai.. • ai+d-1. In general, if c~ = f13 then we say top (~) = 3 and rest (c~) = ft. If o~ = ¢ then we say that top (c~) = rest (c~) ~- ~.","To summarize, the structure of an entry in P Ii, d I is described by the following rules. • An entry consists of a head and a tail. • A head consists of a nonterminal and a stack symbol.","• A tail consists of a middle and a terminator-pointer. The exact nature of the middle and the terminator-pointer are as given below.","-- The terminator-pointer may be of the form (IAt~ 7tl~ [t~dtl) where At E VN~ 3t E W I and It~ dtl <_ li~ d). In this case, the middle is a string of stack symbols of length at least one. This form of a terminator pointer is used in the encoding of a derivation from an object if its terminator has a stack length greater than or equal to TTC. Recall that we had called this type of an entry a"]},{"title":"terminator-type entry.","paragraphs":["A terminator-pointer can be a nil. Then the middle is a (possibly empty) string of stack symbols. However, the length of the middle is less than TTC + MCL - 1. This form of a terminator pointer is used in the encoding of a derivation from an object if it does not satisfy the TC-property; i.e., either it has no 601 Computational Linguistics Volume 19, Number 4 terminator or if the terminator exists then its stack length is less than TTC."]},{"title":"3.2 Recognition Algorithms for LIG","paragraphs":["Since the full algorithm involves a number of cases, we develop it in stages by restrict-ing the forms of productions. The first algorithm that considers the most restricted form of productions introduces much of what lies at the core of our approach. Next we relax these restrictions to some degree. After giving the algorithm at this stage, we switch to discuss how this algorithm can be adapted to yield one for CCG. Later, in Section 5, we consider further relaxation of the restrictions on the form of LIG productions, which can help us produce an algorithm for TAG.","Regardless of which set of restrictions we consider, in every algorithm we shall establish that the following proposition holds."]},{"title":"Proposition 3.1","paragraphs":["• ((at ~k) (')'1.-. \"Yk-1,"]},{"title":"((at,,,/t), [t, dt]))) E P[i,d]","paragraphs":["if and only if for some fl c v~,"]},{"title":"a (fl~'l...","paragraphs":["\"Yk-l\"/k) ~ ai... at-la (fl\"/t) at+dt-1...ai+d-1 ai. . .ai+a-1 where"]},{"title":"At (fl\"/t)","paragraphs":["is the k-terminator of A (fl\"/1 ''' \"Yk) and"]},{"title":"len (fl'Yt) >_","paragraphs":["TTC."]},{"title":"• ((A,'yk) (3'1...Tk-1,nil)) E P[i,d]","paragraphs":["ifand only if a (')'1..."]},{"title":"q/k-lq/k) ~ ai...ai+d-1","paragraphs":["where in this derivation A (\"/1 ..."]},{"title":"\"Yk-l'Yk)","paragraphs":["does not have the TC-property. 3.2.1 Algorithm 1. Recall that the general form of rules that are to be considered are as follows. 1. A (c~) --* c where e ¢ {e} U"]},{"title":"VT,","paragraphs":["and"]},{"title":"len (c~) > 1.","paragraphs":["2. A(..'y~. .'rm)~ Ap(..'~p)As(~s)"]},{"title":"3. a(..~l...\"ym)----~ as(ozs)ap (..\"fp). 4. a(\"~l...3'm)~Ap(\"3'p).","paragraphs":["At this stage we assume that the following restrictions hold of the above rules. In the first type of production we assume that e c"]},{"title":"VT","paragraphs":["and"]},{"title":"len (c~) > 1.","paragraphs":["Thus MTL > 1."]},{"title":"len","paragraphs":["(C~s) _> 1 in productions of type 2 and type 3, i.e., MSL > 1. There are no productions of type 4.","We will now give the following rules that specify how entries get added in the parsing array. The control structure of the algorithm (a CKY-style dynamic programming structure) will be added later. We assume that the input given is"]},{"title":"al ... an,","paragraphs":["where n>l. 602 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Initialization Phase In the initialization phase of the algorithm we store lexical objects (objects deriving a terminal symbol in one step) entirely in a single entry. In other words, Rule 1.L"]},{"title":"A (c~) ---~ a a = ai","paragraphs":["l<i<n"]},{"title":"( {A, top","paragraphs":["(c~)}"]},{"title":"(rest (ee), nil)) ¢ P[ i,","paragraphs":["1] Inductive phase Here productions of type 2 and type 3 will be considered. Let us assume the presence of the following production in the grammar:"]},{"title":"A (..","paragraphs":["\"Yl ... \"/m) --+"]},{"title":"Ap (.. %) As (o~s). 5","paragraphs":["Suppose that while considering which entries are to be included in P [i, d] we find the following for some"]},{"title":"dp, ds","paragraphs":["such that"]},{"title":"dp + ds = d.","paragraphs":["• The entry"]},{"title":"((Ap,,ypl (flp,tpp))E P[i, dp].","paragraphs":["This is consistent with the rule's primary constituent. Regardless of whether"]},{"title":"tpp = nil","paragraphs":["or not, for some"]},{"title":"fl E V~: Ap (flflp'yp) ~ ai...ai+dp-1.","paragraphs":["That is, when"]},{"title":"tpp = nil","paragraphs":["we have • The entry ((As,"]},{"title":"top (o~s)) (rest (c~s), nil)) E P [i + dp, ds].","paragraphs":["This is consistent with the rule's secondary object. Thus if d ="]},{"title":"dp + ds","paragraphs":["we may assume"]},{"title":"As (Ols) ~ ai+dp . . . ai+d_l.","paragraphs":["From the presence of the two entries specified above (and the derivations they represent) we have"]},{"title":"A (flflp'Yl ... 7m) ~ Ap (flflp'yp) As (c~s) ~ ai... ai+d-1.","paragraphs":["This derivation must be recorded with an entry in P [i, d]. The content of the entry depends on several factors: the value of m; whether or not the terminator-pointer in the entry for the primary constituent (i.e.,"]},{"title":"tpp)","paragraphs":["is"]},{"title":"nil;","paragraphs":["and the length of the middle in this entry (i.e.,"]},{"title":"tip).","paragraphs":["These determine whether or not the new entry will be a terminator-type entry. We have cases for m = 0, m = 1 and m _> 2. CASE WHEN m = 0 The new object to be stored is"]},{"title":"A (flflp).","paragraphs":["The top of the stack in this object can be obtained from the stack associated with the primary constituent. How this is done depends on whether the entry encoding the primary constituent is of terminator type or not. When m = 0 and"]},{"title":"tpp = nil","paragraphs":["This means that the primary constituent has been represented in its entirety; i.e., the primary constituent is"]},{"title":"Ap (flpTp).","paragraphs":["Since"]},{"title":"tpp = nil","paragraphs":["the primary constituent does not satisfy the TC-property (i.e., it does not have a terminator with a stack of length greater than or equal to TTC), the new constituent too cannot be encoded using a terminator-type entry. Therefore, Rule 2.ps.L"]},{"title":"(lAp, 3'pl (tip, nil)) ¢ P[i, dp] ((As, top(c~s)) (rest(o~s),nil) ) C P[i +dp, d -dp] ( IA, t°P (flp) l (rest (flp),nil) ) E P[i,d]","paragraphs":["5 Similar arguments can be used when we consider the production: A (.. 3'1 ... 7m) --* As (C~s) Ap (\" 3'p). 603 Computational Linguistics Volume 19, Number 4 The following rule is the counterpart of Rule2.ps.L 6 that corresponds to the use of the production"]},{"title":"A (..) ~ As (C~s) Ap (..","paragraphs":["7p). Rule 2.sp.L"]},{"title":"( (As, top(c~s)) (rest(c~s),nil) ) • P[i, ds] ((Ap, Vp ) (tip,nil)) • P[i + ds,d- ds] ((A, top (tip)) (rest (tip),nil) ) • P[i,d] When m = 0 and tpp ~ nil","paragraphs":["Let the entry for the primary constituent be"]},{"title":"((Ap, 7p) (tip, ((At, 7t), It, dr]))).","paragraphs":["Since the primary constituent is"]},{"title":"Ap (flflpTp)","paragraphs":["we will assume that its terminator is"]},{"title":"At (fl'Yt)","paragraphs":["where"]},{"title":"len (flVt) >","paragraphs":["TTC. Note also that"]},{"title":"len","paragraphs":["(flp'yp) > 2. The entry for the new object (A (flflp)) is determined based on whether"]},{"title":"len (tip)","paragraphs":["= 1 or"]},{"title":"len (tip)","paragraphs":["> 1. In the latter case the"]},{"title":"len","paragraphs":["(flpVp)-terminator of the primary constituent is the"]},{"title":"len","paragraphs":["(&)-terminator of the new","object. This is not so in the former case, as noted in Observation 2.5. Considering the latter case first, i.e.,"]},{"title":"len (tip)","paragraphs":["> 1, we may write tip as 71...'Yk-lVk where k > 2. Since in this case the new object and the primary constituent have the same terminator and since the primary constituent has the TC-property"]},{"title":"(tpp ~ nil),","paragraphs":["the new object must also be encoded with a terminator-type entry. Thus we have the following rule: Rule 3.ps.L"]},{"title":"((Ap,~/p)('y1...3%tpp)) •P[i, dp] tpp = ((At,vt), [t, dt]) ,k >_ 2 ((As, top","paragraphs":["(c~s))"]},{"title":"(rest","paragraphs":["(c~s),"]},{"title":"nil)) • P [i + dp, d - dp]","paragraphs":["((A,\"Ykl (\"Y1...Tk-l,tpp)) cP[i,d 1 Henceforth we shall give the"]},{"title":"ps","paragraphs":["versions of the rules only and omit"]},{"title":"sp","paragraphs":["versions. Now let us consider the case when"]},{"title":"len","paragraphs":["(tip) = 1. Rewriting tip as 71, the entries represent derivation for"]},{"title":"fl E V~ (len","paragraphs":["(fl'yl) ="]},{"title":"len (fl\"/t) >","paragraphs":["TTC). a (ti\"/1) ~ ap (ti\"/l\"/p) As (0@)"]},{"title":"ai . . . at- l At (tiTt ) at+a,.., ai+ap- l As","paragraphs":["(C~s) ai . . . at_ lat . . . at+d t_ lat+dt •.. ai+dp_ laiq-d p ... ai+d_l where"]},{"title":"At","paragraphs":["(ti'Yt) is the 2-terminator of"]},{"title":"Ap","paragraphs":["(ti\"/l\"Yp)- From Observation 2.5 it follows that if"]},{"title":"At(tiVt)","paragraphs":["has a terminator then the terminator of"]},{"title":"A(ti'yl)","paragraphs":["in this derivation is the same as the terminator of"]},{"title":"At","paragraphs":["(fl'Yt); and if"]},{"title":"At (fl'Yt)","paragraphs":["has no terminator then neither does A (ti'Yl). Additionally, in this derivation A (ti'yl) satisfies the TC-property if and only if"]},{"title":"At (ti'Yt)","paragraphs":["has the TC-property. That is, we should use a terminator-type entry to record this derivation from A (ti'Yl) if and only if a terminator-type entry has been used for"]},{"title":"At (tiTt).","paragraphs":["Since these two objects share the same terminator (if it exists) the terminator-pointer must be the same when we record derivations from them. Therefore, suppose we use the terminator-pointer of"]},{"title":"((Ap, 1⁄2) (tip, ((At, \"Yt), [t, dt])))","paragraphs":["to locate an entry"]},{"title":"((At,","paragraphs":["\"Yt)"]},{"title":"(tit, tPt)) • P It, dr].","paragraphs":["This would suggest the addition of the entry 6 Here L indicates a rule we use in LIG parsing; ps indicates that the primary constituent appears before the secondary constituent. Similarly, sp will be used to indicate that the secondary constituent appears before the primary constituent. 604 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms"]},{"title":"(IA, \"/1)(tit, tpt) )","paragraphs":["to"]},{"title":"P[i,d],","paragraphs":["regardless of whether or not"]},{"title":"tPt = nil.","paragraphs":["However, we give the two cases"]},{"title":"(tpt = nil","paragraphs":["or"]},{"title":"tPt = (IAr, %1, [r, dr])","paragraphs":["for some"]},{"title":"At, %, r, dr)","paragraphs":["in the form of two different rules. This is because (as we shall see later) these two rules will have to appear in different points of the control-structure of the parsing algorithm. Rule 4.ps.L"]},{"title":"((Ap, 7p) (71, ((At, 7t), [t, dt]) ) ) cP[i, dp] ((As, top (c~s) ) (rest (c~s), nil)) ¢ P[i+dp,d-dp] ( (At, Tt) (fit,nil)) E P[t, dt] ( (A, 71) (fit,nil)) E P[i,d]","paragraphs":["Rule 5.ps.L"]},{"title":"( (Ap, vp) (71, ( (At, 7t) , [t, dt] ) ) ) ¢ P[i,d~,] ((As, top (c~s) ) (rest (C~s), nil)) ¢ P[i+dp,d-dp] ((At, \"~/t) (fit, tPt) ) P[t, at] tpt = ((Ar,'Yr), Jr, dr])","paragraphs":["((A,"]},{"title":"7\"/1) (flt,tpt) ) E P[i,d] CASE WHEN m -- 1","paragraphs":["The length of the stack in the new object is equal to that of the primary object. In fact, the terminator of the primary object (if it exists) is the same as the terminator of the new object, and when the primary object has no terminator neither does the new object. Therefore the encoding of the new object can easily be derived from that of the primary object by simply modifying the head (to change the top of the stack symbol). Thus we have: Rule 6.ps.L"]},{"title":"((Ap,'Tp) (tip, nil)) ¢ P[i, dp] ((As, top(c~s)) (rest(c~s),nil)) c P[i+dp,d-dp]","paragraphs":["((A,\")'I)"]},{"title":"(tip, nil)) ¢ P[i,d]","paragraphs":["Rule 7.ps.L"]},{"title":"( (Ap, 7p) (tip, ((At, 7t) , [t, dt] ) ) ) ¢ P","paragraphs":["[i, dp]"]},{"title":"( ( A~ , top ( c~s ) ) (rest(as), nil)) E P [ i + dp , d - dp ] CASE WHEN m > 2","paragraphs":["If the primary constituent is"]},{"title":"Ap (titip,,/p)","paragraphs":["then the new constituent is"]},{"title":"A (tiflp'Yl... \"/m).","paragraphs":["In fact, in this case, we have the primary constituent being the m-terminator of"]},{"title":"A (fltip3'l... \"Ym).","paragraphs":["Of course, this does not mean that the derivation from the new object should be recorded with the use of a terminator-type entry. We use the terminator-type entry only when"]},{"title":"len","paragraphs":["(tip3'p) ~ TTC. In order to determine the length of this stack we have to use the entry for the primary constituent (i.e.,"]},{"title":"(IAp,.Tp)(tip, tpp)lE PIi, dp])","paragraphs":["and consider whether this is a terminator-type entry or not (i.e., whether"]},{"title":"tpp = nil","paragraphs":["or not). 605 Computational Linguistics Volume 19, Number 4 When m _> 2 and"]},{"title":"tpp ~ nil","paragraphs":["Therefore the length of the stack of the terminator of the primary constituent is greater than or equal to TTC. This means that stack length of the primary constituent (the terminator of the new object) exceeds TTC. Thus we have the following rule: Rule 8.ps.L"]},{"title":"({Ap,'Tp) (tip, tpp) ) ¢ P[i, dp] tpp = (Iat,'~t), It, dr])","paragraphs":["({As,"]},{"title":"top (C~s)) (rest","paragraphs":["(as),"]},{"title":"nil)) E P[i +dp, d - dpl (/A, ~m) (\")/1.-. \"/m-l, ( {Ap, \"yp) , [i, dp]) ) ) c P[i,d] When m _> 2 and tpp = nil","paragraphs":["The primary constituent (which is the terminator of the new object) should be represented in its entirety. Therefore, in order to determine whether we have to encode the new object with a terminator-type entry or not, we have to look at the entry for the primary constituent. Thus we obtain the following rules: Rule 9.ps.L"]},{"title":"len (tip'yp)","paragraphs":["< TTC"]},{"title":"( IAp, 3,pl (tip, nil)) E P[i, dp] ((As, top (as)) (rest (as), nil)) E P [i +","paragraphs":["dp,"]},{"title":"d - dp] ( {A, ~m) (tipVl . . . Tm-l,nil) ) E P[i,d]","paragraphs":["Rule 10.ps.L"]},{"title":"len","paragraphs":["(tip-yp) > TTC"]},{"title":"( (Ap, 7p) (tip, nil)) c P[i, dp] ( IAs, top (as)) (rest (~s), nil)) E P [i + dp, d - dp] ((A, \"Ym) ('Yl... \"/m-l, (lAp, \"/p) , [i, dp] ) ) ) C P[i,d]","paragraphs":["In the discussions that follow, we find it convenient to refer to the entries mentioned in the above rules as either antecedent entries (or entries that appear in the antecedent) of a rule or consequent entry (or entry that appears in the consequent) of a rule. For example,"]},{"title":"(lAp, q/pl (tip, nil))","paragraphs":["in"]},{"title":"PIi, dpl","paragraphs":["and"]},{"title":"( IAs,top(e~s)l (rest (c~s),nil) )","paragraphs":["in"]},{"title":"PIi + dp,d - dpl","paragraphs":["are the antecedent entries of Rule 10.ps.L and"]},{"title":"( IA, \"Yml ('Y'\"\" \"Ym--l~ ( I ap~","paragraphs":["\"~p"]},{"title":"I' [i, dp] ) ) )","paragraphs":["that is added to P [i, d I is the entry in the consequent of Rule 10.ps.L. 3.3 The Control Structure We will start by giving a simple control structure for the recognition algorithm that follows the dynamic programming style used in the CKY algorithm. 606 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms","In this section we modify the notation for entries slightly. In the above discussion, the terminator-pointer of a terminator-type entry contains a pair of indices representing input positions. Thus, in effect, P is a four-dimensional array. As an alternative to saying that (( A, 3,) (fl, (CA', 3\"), [t,d;]))) is in"]},{"title":"P[i,d]","paragraphs":["we will sometimes say (C A, -y)(fl, (A', 3/))) is in"]},{"title":"P[i, d] [t, dt].","paragraphs":["Also as an alternative to saying"]},{"title":"(C A, oe)(fl, nil))","paragraphs":["is in"]},{"title":"P[i,d]","paragraphs":["we will sometimes say ((A, c,) (fl,"]},{"title":"nil))","paragraphs":["is in"]},{"title":"P[i,d][O,O].","paragraphs":["Thus P can be considered to be an array of size n x n x (n + 1) x (n + 1).","In the specification of the algorithm (Figure 5) we will not restate all the rules we discussed in the previous section. Instead we will only indicate where in the control structure each rule fits. As an example, when we state \"Use Rule 2.ps.L with"]},{"title":"dp = d\"","paragraphs":["within the i, d, and d' loops we mean the following: for current values of i, d, and d' (and hence"]},{"title":"dp, ds)","paragraphs":["consider every production of the form A (.. \"Yl ... 7m) --+"]},{"title":"Ap (..","paragraphs":["7p) As (as) with m = 0. For each such production, look for entries of the form ((Ap,"]},{"title":"7p) (tip, nil)) E P [i, dp]","paragraphs":["[0,0] for some"]},{"title":"tip","paragraphs":["and"]},{"title":"( (As, top (as) ) (rest (as), nil) ) E P [i + dp, d - dp]","paragraphs":["[0,0]. In the event we find such entries, we add"]},{"title":"((A, top (tip)) (rest (flp),nil))","paragraphs":["to"]},{"title":"P[i,d]","paragraphs":["[0,0] if it is","not already there. Since the entries in"]},{"title":"P[i, d]","paragraphs":["have the form"]},{"title":"( (A, -,/) (fl, (CAt, 3q) , It, dt] ) ) )","paragraphs":["(where"]},{"title":"(t, dt) G (i,d))","paragraphs":["or the form (CA,')')(fl,"]},{"title":"nil)),","paragraphs":["there are O(d 2) many entries in"]},{"title":"P[i,d]","paragraphs":["(where 1 G i < n and 1 G d G n - d). Thus space complexity of this algorithm is O(nd). Note that within the body within the r loop will be attempted for all possible values of"]},{"title":"i, d, d', t, dt, r, dr.","paragraphs":["Since the range of each loop is O(n), the time complexity is O(n7).","The asymptotic complexity of the above algorithm can be improved to O(n 6) with a simple rearrangement of the control structure. The key point here is that the steps involving the use of rules 5.ps.L and 5.sp.L can be split into two parts each. Consider, for example, the use of the Rule 5.ps.L, which is repeated below. Rule 5.ps.L"]},{"title":"((Ap,',/p) (~I, ((At, q:t) , [t, dt]) ) ) EP[i, dp] ((A,, top ( o~, ) ) (rest","paragraphs":["(c~),"]},{"title":"nil)) GP[i+dp,d-dp]","paragraphs":["((At,"]},{"title":"\")'t) (fit, tpt) ) tpt = ((Ar,\")'r), [r, dr]) ((A, q'l) (fit, ((Ar,'Yr),","paragraphs":["[r, dr]))) E"]},{"title":"P[i,d]","paragraphs":["This rule corresponds to the use of the production"]},{"title":"A (..) ~ Ap (.. q/p) As (as).","paragraphs":["The values of"]},{"title":"i, d, d', t, d t","paragraphs":["are necessary to determine the span of the substrings derived from the primary constituent and the secondary constituent, and the values of"]},{"title":"i, d, t, dt, r, dr","paragraphs":["are needed to locate the entry for the terminator, i.e., (CAt,,),t)(fit,"]},{"title":"(CAr,\"/r), [r, dr])))","paragraphs":["and to place the new entry in the appropriate parsing table element. That is, the values of r and"]},{"title":"dr","paragraphs":["are not required for the first part and the value of d' need not be known for the second part. This indicates that the second part need not be done within the loop for dq Therefore, we can modify the control structure in the following way. Within the t loop (which appears within the loops for"]},{"title":"d, i, d',dt)","paragraphs":["we find the entries for the primary and secondary constituents. Having found the two relevant entries, we must record the head of the new entry"]},{"title":"(A, tip)","paragraphs":["and the terminator-pointer of the primary constituent, i.e.,"]},{"title":"(CAt, ~t), [t, dt]).","paragraphs":["We can do this by using a two-dimensional array called TEMP where we store"]},{"title":"CA, q'l, At, 7t).","paragraphs":["Outside the d' loop (and hence outside the loops for t and"]},{"title":"dt","paragraphs":["as well), but within the loops for i and d, we can have the loops that vary"]},{"title":"t, dt, r, dr","paragraphs":["(note"]},{"title":"(r, dr) < (t, dr))","paragraphs":["in order to locate the entry for the terminator by using the information recorded in TEMP. Finally, having found the entry for the 607 Computational Linguistics Volume 19, Number 4","Algorithm 1","begin","for i:= 2 to n do","Initialization phase","Use Rule 1","for d := 2 to n do % d loop","fori:=lton-d+ldo%iloop","begin","ford':=ltod-ldo %d'loop","begin","Use Rule 2.ps.L, 6.ps.L, 9.ps.L, 10.ps.L with dp = d'.","for dt := (d' - 1) to 1 do % dt loop for t := i to (i + d' - dt) do % t' loop","begin","Use Rule 3.ps.L, 4.ps.L, 7.ps.L, 8.ps.L with dp = d'","for dr :-- dt to 1 do","for r := t to t + dt - dr do","begin","Use Rule 5.ps.L with dp = d'","end","% end of dr loop","% end of r loop","end % end of t loop","% end of dt loop","for dt := (d - d' - 1) to 1 do % dt loop for t := (i + d') to (i + d - dr) do % t' loop","begin","Use Rule 3.ps.L, 4.ps.L, 7.ps.L, 8.ps.L with ds = d'","for dr := dt - 1 to 1 do","for r := t to (t + dt - dr) do","begin","Use Rule 5.sp.L with ds = d'","end","% end of r loop","% end of dr loop","end % end of t loop","% end of dt loop","end","% end of d' loop","end","% end of i loop","% end of d loop Figure 5 Algorithm 1. 608 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms terminator we then store the resulting entry in P [i, d]. These steps are captured by the following rules. For a specific value of (i, d) we have Rule 5.i.ps.L"]},{"title":"( (Ap, q/p) (71, ( (At, q/t) , It, dr]))) C P[i, dp] ((As, top","paragraphs":["(C~s))"]},{"title":"(rest (C~s), nil)) e[i + clp, cl - G] (A, q/1,At, 7t) E","paragraphs":["TEMP[t, dt] Rule 5.ii.ps.L"]},{"title":"(a~\"/l~.,z~t,q/t) E","paragraphs":["TEMP[t, dt]"]},{"title":"((At, q/t) (fit, ((Ar, q/r), [r, dr]))) C P[t, dt]","paragraphs":["((A,q/1) (fit,"]},{"title":"((Ar, q/r), [r, dr]))) C P[i,d]","paragraphs":["Similarly, we assume we have the pair Rule 5.i.sp.L and Rule 5.ii.sp.L corresponding to Rule 5.sp.L. This leads to the algorithm given in Figure 6. In this algorithm we drop the"]},{"title":"sp","paragraphs":["rules and specify the"]},{"title":"ps","paragraphs":["rules only for the sake of simplicity. The correctness of Algorithm 2 can be established from the correctness of Algo-","rithm 1 (which is established in Appendix A) and the following Lemma. Lemma 3.1 Given a grammar G and an input al...an an entry ((A, q/}"]},{"title":"(fl, tp))","paragraphs":["is added to"]},{"title":"P[i,d]","paragraphs":["by Algorithm 1 if and only if"]},{"title":"((A,q/) (fl, tp))","paragraphs":["is added to"]},{"title":"P[i,d]","paragraphs":["by Algorithm 2. Outline of Proof: Using induction on d. The base case corresponding to d = 1 involves only the initialization step, which is the same in the two algorithms. The only difference between the two algorithms (apart from the control structure) is the use of Rule 5.ps.L (and Rule 5.sp.L) by Algorithm 1 versus the use of Rule 5.i.ps.L and Rule 5.ii.ps.L (Rule 5.i.sp.L and Rule 5.i.sp.L) in Algorithm 2. Rule 5.ps.L is used to add entries of the form ((A,"]},{"title":"~Yl)(fit, ((ar~ q/r)~ Jr, dr]))). We","paragraphs":["can establish that"]},{"title":"((A, ,/̀1) (fit, ((Ar, q/r), [r, dr])))","paragraphs":["is added to P [i, d] due to the application of Rule 5.ps.L if and only if there exist entries of the form"]},{"title":"((Ap, q/p)","paragraphs":["('71,"]},{"title":"((At, q/t), [t, dt])))","paragraphs":["in"]},{"title":"P[i, dp]; ((As, top(c~s))(rest(c~s),nil))","paragraphs":["in"]},{"title":"P[i+dp,d-dp]; ((at, q/t)(flt~((ar, q/r)~[Y~dr])) )","paragraphs":["in"]},{"title":"P It, dt];","paragraphs":["and the production"]},{"title":"A (..) --* Ap","paragraphs":["(.. q/p) As (~s). Using induction, we can establish that these entries exist if and only if"]},{"title":"(A, q/1,At~ q/t)","paragraphs":["is added to TEMP[t, dt] using Rule 5.ps.i.L (or Rule 5.sp.i.L) and ((A, q/1) (fit,"]},{"title":"((Ar, q/r), Jr, dr])))","paragraphs":["is added to"]},{"title":"P[i,d]","paragraphs":["using Rule 5.ii.ps.L. 4. Combinatory Categorial Grammars Combinatory Categorial Grammars (CCG) (Steedman 1985, 1986) are extensions of Classical Categorial Grammars in which both function composition and function application are allowed. In addition, forward and backward slashes are used to place conditions concerning the relative ordering of adjacent categories that are to be combined. Definition 4.1 The set of categories generated from a set, VN, of atomic categories is defined as the smallest set such that all members of VN are categories, and if cl, c2 are categories then so are"]},{"title":"(Cl/C2)","paragraphs":["and (el\\e2). 609 Computational Linguistics Volume 19, Number 4","Algorithm 2","begin","for i:= 1 to n do","Initialization phase","Use Rule 1","ford:=2tondo%dloop","for i := 1 to n- d + 1 do % i loop","begin Initialize TEMP It, dt] to ~ for all (t~ dt) ~ (i~ d) fordp:=ltod-ldo %dploop","Use Rule 2.ps.L, 6.ps.L, 9.ps.L, 10.ps.L","for dt := dp - 1 to 1 do % dt loop","for t := i to i + dp - dt do % t loop","Use Rule 3.ps.L, 4.ps.L, 5.i.ps.L, 7.ps.L, 8.ps.L","% end of t loop","% end of dt loop % end of dp loop for dt := d - 1 to 1 do % dt loop","for t := i to i + d - dt do % t loop","for dr := dt - 1 to 1 do","for r := t to t + dt - dr do","begin","Use Rule 5.ii.ps.L","end","% end of r loop","% end of dr loop","% end of dt loop % end of t loop","end","% end of i loop","% end of d loop Figure 6 Algorithm 2. Definition 4.2 A CCG, G, is denoted by (VT, VN, S~f~ R) where VT is a finite set of terminals (lexical items), VN is a finite set of nonterminals (atomic categories), S is a distinguished member of VN,"]},{"title":"f","paragraphs":["is a function that maps each element of VT to a finite set of categories, R is a finite set of combinatory rules, where combinatory rules have the following"]},{"title":"form.","paragraphs":["1. A forward rule has the following form where m > 0."]},{"title":"(x/y) (yllZl[2... ImZm) ---4. (XllZll2... [mZm)","paragraphs":["2. A backward rule has the following form where m > 0."]},{"title":"(y11Zl12... ImZm)","paragraphs":["(x\\y)"]},{"title":"---+ (XIIZl12... ]mZm)","paragraphs":["610 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Here x,y, zl,... ,Zm are meta-variables and h,..-, [m E {\\, /}. For m -- 0 these rules correspond to function application and for m > 0 to function composition. Note that the set R contains a finite subset of these possible forward and backward rules; i.e., for a given CCG only some of the combinatory rules will be available. Definition 4.3 In the forward and backward rules given above, we say that (x/y) (resp. (x\\y)) is the primary constituent of the forward (resp. backward) rules and (y]1zl[2... ]mZm) is the secondary constituent of the rule. The notion of a distinguished child is defined as in the case of LIG, i.e., a category is the distinguished child of its parent if it corresponds to the primary constituent of the rule used. As before, the distinguished descendant is the reflexive, transitive closure of the distinguished child relation.","In discussing CCG we use the notational conventions that the variables ] and c (when used with or without primes and subscripts) range over the forward and backward slashes and categories, respectively. We use x,y, z for meta-variables; a, fl for strings of directional categories (i.e., a string of the form ]1Cl]2.,. ]nOn from some n ~ 0); and A, B, C for atomic categories (i.e., members of VN).","Derivations in a CCG, G = (VT, VN~ S,f, R), involve the use of the combinatory rules in R. Let ~ be defined as follows, where T1 and T2 are strings of categories","G and terminal symbols. If ClC 2 ---+ C is an instance of a rule in R, then TlCT 2 ~ \"~1ClC2T2 . G If c C f(a) for some a c VT and c is a category, then TlCT2 ~ TlaT2. The string languages generated by a CCG, G, L(G) = { w ] S ~ w ] w E V~ }. G Example 4.1 The following CCG generates { wcw ] w E {a, b} + }. Let G = ({at b, c}, {S, T, A, B}, S,f, R) where f(a) = (A, T\\A/T, T\\A} f(b) = {B, T\\B/T, T\\B} f(c) = (S/T} The set of rules R includes the following three rules. y (x\\y) ~ x (x/y) (y\\zl/z2) ---+ (y\\zl/z2) (x/y) (y\\zl) ~ (y\\zl) In each of these rules, the target of the category matched with x must be S. 7 Figure 7 shows a derivation of the string abbcabb.","We find it convenient to represent categories in a minimally parenthesized form (i.e., without parentheses unless they are needed to override the left associativity of the slashes), where minimally parenthesized form is defined as follows. 7 Following Steedman (1985), we allow certain very limited restrictions on the substitutions of variables in the combinatory rules. A discussion on the use of such restrictions is given in Vijay-Shanker and Weir (in press). However, we have not included this in the formal definition since it does not have a significant impact on the algorithm presented. 611 Computational Linguistics Volume 19, Number 4 A S~A S~A~B B b S~AkBkB S~B/T S'~JF SIT T'Odr"]},{"title":"I I","paragraphs":["a c T~B/F"]},{"title":"I","paragraphs":["b T~B"]},{"title":"I","paragraphs":["b Figure 7 CCG example derivation tree.","Definition 4.4 • A is the minimally parenthesized form of A where A C VN.","• If cl,...,c, are the minimally parenthesized forms of categories c~,..., c\" respectively, then"]},{"title":"(allcll2... Incn)","paragraphs":["is the minimally parenthesized form of","(('-' (allc~)12'' \")l~c').","A category c is in minimally parenthesized form if c is the minimally parenthesized","form of itself. Definition 4.5 Let a category c = AllC112... [nCn be in minimally parenthesized such that n > 0, A E VN, and Cl,..., Cn are minimally parenthesized categories. • The target category of c"]},{"title":"= allClI2... InCn","paragraphs":["denoted by tar(c) is A. • The arity of c = AllC112... InCn, denoted as arity (c), is n. • The argument categories of c"]},{"title":"= AllC 112... Inch","paragraphs":["denoted by args (c) = { ci ] 1 < i < n }. 4.1 CCG and LIG Before showing how the general parsing scheme illustrated by the LIG recognition algorithm can be instantiated as a recognition algorithm for CCG, we show that CCG and LIG are very closely related. The details of the examination of the relationship between CCG and LIG may be found in Weir and Joshi (1988) and Weir (1988).","A minimally parenthesized category"]},{"title":"(AIlc 112...","paragraphs":["InCn) can be viewed as the atomic category, A, associated with a stack of directional argument categories, ILC112... Inc,. The rule (x/y) (yllZll2... ImZm) ~ (XI1Zll2... ImZm)","! ! ! !","has as an instance (ApieCe... InCh~As) (AsliCl}2... ImCm) --~ (apieCe... InCnllCll2... ImCm) I I I I I I","as well as"]},{"title":"(A~L~c~... InCn/(Asl c )) (AI'c'L~c~I2...","paragraphs":["I,~cm) ~"]},{"title":"(&l~c'~...","paragraphs":["I~Cnh~lL2... Imam) 612 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms ! ! as an instance. Thus x matches the category"]},{"title":"(Apl~c ~","paragraphs":["... [nCn),"]},{"title":"y","paragraphs":["matches an atomic category As in the first example and a nonatomic category"]},{"title":"(As]~C ')","paragraphs":["in the second, and each"]},{"title":"zi","paragraphs":["matches"]},{"title":"ci","paragraphs":["for 1 ( i ( m. A derivation involving the second instance (viewed bottom-up) can be seen as popping the top directional argument"]},{"title":"/(Asl'c')","paragraphs":["from the primary category and pushing the m directional arguments"]},{"title":"IlCl]2.-.","paragraphs":["ImCm • Thus, each instance of the combinatory rule appears to closely resemble a LIG production. For example, in case of the second instance we have ap (.."]},{"title":"ILC112...","paragraphs":["ImCm) ---+ ap (../(As]'c')) As"]},{"title":"(['c'11c112...","paragraphs":["ImCm) . We now show that, like the set of stack symbols of a LIG, the set of directional argument categories that we need to be concerned with is finite. Definition 4.6 Let c be a useful category with respect to a grammar G if and only if c ~ w for some w E V~. The set of argument categories,"]},{"title":"args (G)","paragraphs":["of a CCG, G ="]},{"title":"(VT~ VN~ S,f,","paragraphs":["R), is defined as"]},{"title":"args (G) = Uc~f(a) args (c). Observation 4.1","paragraphs":["If c is a useful category then"]},{"title":"args (c) c args","paragraphs":["(G), a finite set determined by the grammar, G.","This observation can be shown by an induction on the length of the derivation of some string from c. The base case corresponds to a lexical assignment and hence trivially"]},{"title":"args (c) C args (G).","paragraphs":["The inductive step corresponds to the use of a combination using a rule of the form (x/y)"]},{"title":"(yllZll2...","paragraphs":["ImZm) ---+"]},{"title":"(XIlZll2... ImZm ) or (yllZll2...]mZm)","paragraphs":["(x\\y) ~"]},{"title":"(XIlZll2...ImZm)","paragraphs":["By inductive hypothesis, any useful category matching either"]},{"title":"(x/y)} (x\\y)","paragraphs":["or"]},{"title":"(y]lZll2... ImZm)","paragraphs":["must take its arguments from"]},{"title":"args (G)","paragraphs":["(a finite set) and therefore the resulting useful category also shares this property.","The above property makes it possible to adapt the LIG algorithm for CCG. Note that in the CKY-style CCG recognition we only need to record the derivations from useful categories. From Observation 4.1 it follows that the lexical category assignment, f, determines the number of \"stack\" symbols we need to be concerned with. Therefore, only one of the variables (x) in a combinatory rule is essential in the sense that the number of categories that it can usefully match is not bound by the grammar. Therefore, it would be possible to map each combinatory rule to an equivalent finite set of instances in which ground categories (from"]},{"title":"args (G))","paragraphs":["were substituted for all variables other than x; i.e.,"]},{"title":"y, zl}...Zm","paragraphs":["in the combinatory rule above. This would result in a grammar that was a slight notational variant of a LIG where the CCG variable x and the LIG notation .- perform similar roles. However, for the purpose of constructing a recognition algorithm it is both unnecessary and undesirable to expand the number of rules in this way. We adapt the LIG algorithm so that it, in effect, constructs appropriate instances of the combinatory rules as needed during the recognition process. 613 Computational Linguistics Volume 19, Number 4 4.2 Recognition of CCG The first step in modifying the LIG algorithm is to define the constants MSL and MTL for the case of CCG. Let G -- (VT, VN, S,f, R) be a CCG. These definitions follow immediately from the similarities between CCG combinatory rules and LIG productions. Observation 4.2 If we were to express a combinatory rule (x/y)"]},{"title":"(yIlZl...ImZm)","paragraphs":["~"]},{"title":"(X]IZ1...]mZm)","paragraphs":["in terms of LIG production"]},{"title":"A (\"\")'1...\"/m)","paragraphs":["\"-'+ Ap (\"\"yp)"]},{"title":"As(o@)","paragraphs":["then we have the following correspondences: • % with/y. • \"~i with"]},{"title":"=","paragraphs":["]izi for 1 < i < m, i.e., \"Y1"]},{"title":"'''","paragraphs":["\"Ym with IlZl ... ]mZm. • A = Ap. • As (as) with yhz~... ImZm . Given such a direct correspondence between combinatory rules and LIG productions, we will define the following constants to be used in the the CCG algorithm with minimal explanation.","• MTL is the maximum arity of a lexical category. Thus, MTL = max { arity (c) ] c c f(a), a C VT }.","• MSL should be the maximum arity of a useful category that can match the secondary category of a rule. Note that a category matching"]},{"title":"(y]lZ1]2-.. ImZm)","paragraphs":["will have an arity that is the sum of m and the arity of the category matching y. Furthermore, note that since y is an argument of the primary category it must be bound to a member of args (G). Thus, MSL = max { m ] (y[lz112..."]},{"title":"]mZm) }","paragraphs":["is the secondary category of a rule in R + max { arity (c) ] c E args (G) }.","• Note that in the case of CCG, MCL need not be defined independently of MSL. • As before, we define TTC as TTC = max { MSL, MTL }.","Since directional categories play the same role that stack symbols have in LIG, we revise the notions of length top ( ) and rest ( ) as follows. We say that the string of directional arguments categories"]},{"title":"]lCl I2\"'\"","paragraphs":["InCh has a length n, i.e., len"]},{"title":"(IlCl 12.-.","paragraphs":["]nCn) = n. Note that arity"]},{"title":"((A]1c112...","paragraphs":["InCn)) = len ([1Cl]2... ]nCn) = n. We define top"]},{"title":"((IlCl]2...","paragraphs":["InCn)) = [nCn and rest ((]1c112... ]nCn)) = ]1Cl ]2.-. ]n--lCn--1 • Additionally, top (¢) = rest (~) = ¢. 4.2.1 Terminators in CCG. We can define a k-terminator in essentially the same way as in the case for LIG. Note that a category shares its target category with all of its distinguished descendants. 614 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Definition 4.7 Suppose that we have the following derivation:","aflllCl . . . Ik_lCk_llC ~ u aflllCl .. . ]k_lCk_llkCk. . . ImCm ==~ U afl/Cp CpllCl . . . IkCk. . . ImCm","ldVW W W or similarly aflllCl...Ik_lCk-llC ~ U aflllCl...Ik_lCk_llkCk...ImC m W"]},{"title":"u CpllCl...[kCk...ImCm Aft\\c, w","paragraphs":["UVW where the following conditions hold • fl is a string of direction categories, i.e., fl E ({/~"]},{"title":"\\}args (G))*. • k-l>l,","paragraphs":["• AflllCl..."]},{"title":"Ik-lCk-llkCk... ImCm","paragraphs":["and"]},{"title":"Afl/c","paragraphs":["are distinguished descendents of aflllCl . . . Ik_lCk_l [C • any distinguished descendent between"]},{"title":"Afl/Cp","paragraphs":["and"]},{"title":"AflllCl ... ]k-lCk-llC","paragraphs":["can be expressed in the form"]},{"title":"afll~cl ... Ik-lCk-1 le~","paragraphs":["where"]},{"title":"len (c~) >_ 1","paragraphs":["We say that"]},{"title":"Aft/@","paragraphs":["is the"]},{"title":"len","paragraphs":["(11Cl ..."]},{"title":"Ik_lCk_llc)-terminator","paragraphs":["of"]},{"title":"Aflllcl... Ik_lCk_l[C.","paragraphs":["Note that"]},{"title":"cp","paragraphs":["need not be atomic. Hence if we write the secondary category as"]},{"title":"cpllcl ... ImCm","paragraphs":["we are not necessarily expressing it in minimal parenthesis form. 4.2.2 Anatomy of a CCG Entry. In the CCG algorithm we will use entries that have a form similar to that of the entries in the LIG algorithm. The choices we make are based on Observation 4.2. For a derivation"]},{"title":"AllCl ... Ijcj ~ al ... ai+d-1","paragraphs":["(where the input is"]},{"title":"al... an),","paragraphs":["we will have an entry in P Ii~ d] with a head"]},{"title":"IA~ IjCjl,","paragraphs":["where A E VN, Ij C {\\~/}, and cj E"]},{"title":"args (G).","paragraphs":["First consider the case when a terminator-type entry is used. The terminator-type entry is applicable when"]},{"title":"Aflllcl ... Ik-lCk-11c","paragraphs":["has a k-terminator, say Aflltct where len"]},{"title":"(flltCt) ~_","paragraphs":["TTC. As before we say that in such a case AflllCl... ]k_lCk_ 11c satisfies the TC-property. Assuming the terminator derives the substring"]},{"title":"at... at+d~-l,","paragraphs":["we can use the terminator-pointer"]},{"title":"(llctl~ [t~ dtl)","paragraphs":["and a middle I1c1..."]},{"title":"Ik_lCk_l .","paragraphs":["Notice that since the target of the category"]},{"title":"Alfl]lcl... Ik-lCk-11C","paragraphs":["as well as the target of its terminator is A","and since A is already noted in the head, it is not recorded in the terminator-pointer.","For entries that are not terminator-pointer, the entire category is noted in the","entry. Such an entry has the form"]},{"title":"(IA~ Ijcj~ (11cl... Ij-lCj-l, nil))","paragraphs":["assuming that j > 1. However, it is possible that j = 0. In this case the category being represented is A, and the entry will be written as"]},{"title":"(IA~ c I (~ nil)).","paragraphs":["In general, we use the non-terminator-type entry for recording a derivation from As when it has no terminator or when the terminator, say aflltCt (rewriting c~ as"]},{"title":"flllCl... Ik-lCk-1","paragraphs":["[C) is such that len"]},{"title":"(flltCt) ~__","paragraphs":["TTC; i.e., when the category As does not satisfy the TC-property. 4.2.3 CCG Algorithm. It is straightforward to derive the rules for the CCG recognition algorithm from those used in LIG algorithm. Using Observation 4.2, we can now give the rules for the CCG algorithm with no explanation. 615"]},{"title":"Computational Linguistics Volume 19, Number 4 Rule 1.C Ac~cf(a) a=ai l<i<n Assume the combinatory rule (x/y) When m = 0 and tpp = nil Rule 2.ps.C ((Ap,/Cp) (tip, nil)) C P[i, dp] ((Ap, top (~)) (rest (c~), nil)) E P[i, 1] (YllZl --. ImZm) ---+ (XIlZl ... ImZm). Asc~s = Cp ((As, top (C~s)) (rest (C~s), nil)) E P [i + de, d - dp] ((Ap, top (tip)) (rest (tip),nil) ) E P[i,d] When m = 0 and tpp ¢ nil Rule 3.ps.C tp e = ((I,c,), [t,d~]) k>2 Asc~s = Cp ((As, top(as)) (rest(o~s),nil) ) C P[i + de,d-de] ( (Ap, IkCk) (llCl... Ik_lCk_i, tpp) ) ~ P[i,d] Rule 4.ps.C ((ap,/ep) (lie1, ((]tct), [t, dt]) ) ) E P[i, dp] Asc~s = Cp ((As, top (cts)) (rest (C~s), nil) ) ~ P[i+de, a-de] ( ( )) (A e, ]tct) tit, nil * P[t, dt] ((Ap, I1c,) (tit,nil)) C P[i,d] Rule 5.ps.C ((Ae,/ce) O~c,, ((Ltc,), [t,d,]))) C P[i, de] Asc~s","paragraphs":["="]},{"title":"Cp ((At, top (C~s)) (rest (C~s), nil)) cP[i+dp,d-de] tpt-~ ((IrCr), [r, dr]) ( (Ae, ltct) (ti,, tp,) ) ((A e, Ilcl) (tir, tPt) ) C P[i,d] When m -- 1 Rule 6.ps.C ( (Ap, /Cp) (tip, nil)) C P[i, dp] Asc~s","paragraphs":["="]},{"title":"CpllCl ((As, top(c~s)) (rest(c~s),nil) ) C P[i + dp,d-d,] ((A e, ]1c,) (tie,nil)) E P[i,d] 616","paragraphs":["K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Rule 7.ps.C"]},{"title":"( (Ap, /Cp) (flp, ( (Itct) , [t, dt] ) ) ) c Pfi, dp] As~s","paragraphs":["= Cp[lCl"]},{"title":"((As,top(c~s)l (rest(c~s),nil)) C P[i +dp~d-dp]","paragraphs":["((Ap, llCl ) (tip, ((ItCt),"]},{"title":"[t, dt]) ) )","paragraphs":["C P[i,d] When m > 2 and"]},{"title":"tpp ~ nil","paragraphs":["Rule 8.ps.C"]},{"title":"tpp","paragraphs":["= (lltctl,"]},{"title":"[t~dt]) (IAp, /Cp> (tip, tpp) ) E PIi, dp] As~s","paragraphs":["= CpllCl ... ImCm"]},{"title":"( IA~, top (c~s) l (rest (c~s), nil)) E P [i + dp, d - dp] ( (Ap, ImCm}","paragraphs":["(11Cl..."]},{"title":"Im--lCm--l~ ( (/Cp) , [i, dp] ) ) ) C P[i,d]","paragraphs":["When m > 2 and"]},{"title":"tpp = nil","paragraphs":["Rule 9.ps.C"]},{"title":"len (tip/@) <","paragraphs":["TTC"]},{"title":"((Ap,/Cp) (tip,nil)) E PIi, dp]","paragraphs":["asoz s = CpllCl . . . ]mCm"]},{"title":"( IAs, top (c~s) ) (rest (c~s), nil) ) E P [i + dp, d - dp]","paragraphs":["(lap,"]},{"title":"Imem) (tip]lCl . . . Im-lCm--l,nil) ) E P[i,d]","paragraphs":["Rule 10.ps.C"]},{"title":"len (tip/ep) >","paragraphs":["TTC"]},{"title":"((Ap,/Cp) (tip, nil)) E P[i, dp]","paragraphs":["aso @ = CpllCl . . . [mCm"]},{"title":"((As, top(c~s)) (rest(c~s),nil) ) c P[i + dp,d -dp]","paragraphs":["((ap, IrnCrn} (11Cl... Irn--lCm--l~ ((/Cp} , Ii, dp]) ) ) C P[i,d] Proposition 4.1 The CCG recognition algorithm can be seen to establish the following."]},{"title":"• (lAp, Ic~ (t3, (lltctl, [t~ dt])))","paragraphs":["C"]},{"title":"P [i~ d]","paragraphs":["if and only if there is some c~ such that"]},{"title":"Ac~tilc ~ ai... ai+d-1","paragraphs":["and the"]},{"title":"(len (/3) +","paragraphs":["1)-terminator (Aozltct) of"]},{"title":"Ac~tilc","paragraphs":["derives the string"]},{"title":"at... at+dr-1","paragraphs":["and len"]},{"title":"(O~ltCt) ~","paragraphs":["TTC."]},{"title":"• ((Ap, top(cO~, (rest(cO, nil,)) E P[i,d]","paragraphs":["if and only if"]},{"title":"Ac~ ~ ai...ai+d-1","paragraphs":["and either Ac~ has no terminator or its terminator, say Ac~ t is such that"]},{"title":"len","paragraphs":["(c~') < TTC. 5. TAG Recognition We begin this section by first considering how to extend our algorithm for LIG to handle unary productions. This will be needed to show we can instantiate our scheme to give a recognition algorithm for TAG. 5.1 Handling Unary Productions and Epsilon Productions We will now show how the LIG algorithm given earlier can be extended to consider unary productions of the form A (.. ~1... \"Ym) -'-9 Ap (.. 3'p) as well as e productions of 617 Computational Linguistics Volume 19, Number 4 the form: A (c~) --+ e. However, we will now assume that m G 2 in productions of the form"]},{"title":"A (.. 2/1 ... q/m) --+ T1ap ('\" q/p)","paragraphs":["T2. Thus, henceforth MCL < 2. Note that this refers to both unary and binary productions. This additional restriction does not change the generative power. We have introduced these restrictions in order to reduce the number of cases we have to consider and also because we can restrict our attention to the productions that are used in the TAG to LIG construction.","Consider the processing of a"]},{"title":"binary","paragraphs":["production A (.. 2/1 ... 2/m) ---+ Ap (.. \"yp) As (as). Since CKY-style parsers work bottom-up, we check to see if the primary and secondary categories derive adjacent strings (say"]},{"title":"ai...ai+dp-1","paragraphs":["and"]},{"title":"ai+dp...ai+dp+&,","paragraphs":["respectively) and then we store an encoding for the new object that results from the combination. The processing of unary productions is similar except that we do not have to consider a secondary constituent. The rules that express the processing of such productions will be very similar to those for the binary productions. For example, consider Rule 2.ps.L for the binary production"]},{"title":"A (..) ---+ Ap (.. \",/p) As (C~s).","paragraphs":["Rule 2.ps.L"]},{"title":"((Ap,,yp) (tip, nil)) E P[i, dp] ((As,top(e~s)) (rest(e~s),nil)) E P[i+dp,d-dp] ( ( A, top (tip)) (rest (tip), nil) ) E P [ i, d]","paragraphs":["Given a unary production"]},{"title":"A (..) --+ Ap (.. 2/p)","paragraphs":["we have the Rule 2.u.L (where u stands for unary). Rule 2.u.L"]},{"title":"((Ap,',/p) (tip,nil)) c P[i, dp] ((A, top (G) ) (rest (&),nil)) E P[i,d]","paragraphs":["In addition, with the introduction of e productions, we have to consider derivations of strings of length d -- 0. We shall assume that if A (c~) ~ e then an encoding of A (~) will be stored in"]},{"title":"P[i,","paragraphs":["0] (for all i). We must also consider the possibility that the primary constituent or the secondary constituent derive the empty string, i.e.,"]},{"title":"dp = 0","paragraphs":["or ds = 0. Processing of such cases becomes similar to that of unary productions.","To indicate the additional processing required due to the introduction of unary productions and the possibility of the derivation of the empty string, let us consider Rule 8.ps.L. \"Use Rule 8.ps.U' can be paraphrased as follows. If there exists a production"]},{"title":"A (.. 71 ... 2/m) -+ A m ('\" 7p) As (C~s)","paragraphs":["where m >__ 2, el ="]},{"title":"((Ap,q'p)(flp,(at,'Tt)))","paragraphs":["belongs to"]},{"title":"P[i,d][t, dt]","paragraphs":["and e2 ="]},{"title":"((As, top (C~s))(rest (c~s), nil))","paragraphs":["belongs to"]},{"title":"P[i +dp, d - dp]","paragraphs":["[0, 0] then add"]},{"title":"e3 = ((A~'ym)(qrl...~/m-l,(Ap~2/p)))","paragraphs":["to"]},{"title":"P[i,d][i~dp]","paragraphs":["if e3 is not already present in this array element. If we allow ~ productions it is possible that"]},{"title":"ds --- d- dp","paragraphs":["-- 0. Consider the case where we have"]},{"title":"As","paragraphs":["(as) ~ c. That is, we expect the entry e2 to be present in P [i + d, 0] [0, 0]. This means that the resulting entry e3 must be added to P [i, d] [i, d] since we now have"]},{"title":"dp = d.","paragraphs":["Note that the addition of e3 ="]},{"title":"((A,\"/m)(\"yl...\"Yrn--l~ZZ~p~p~))","paragraphs":["(that encodes the derivation from A (fl'yl ... %) for some fl) can result in more entries being added to the same array element P [i, d] [i, d] (for instance, when we have the production B (--2/~ ... 2/~) --+ A (-. \"Ym)). This is similar to the"]},{"title":"prediction","paragraphs":["phase in Earley's algorithm and the state construction in LR parsing. Based on this analogy, we will define our 618 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms notion of closure. Closure"]},{"title":"(e,i,d,t, dt)","paragraphs":["will add entries to"]},{"title":"PIi, d] It, dr]","paragraphs":["or"]},{"title":"PIi, d] Ii, d]","paragraphs":["that result from the inclusion of the entry"]},{"title":"P Ii, d] It, dt]","paragraphs":["by considering unary productions (or binary productions when the primary or secondary constituent derives the empty string). Before we define Closure () we note that for each occurrence in the algorithm of \"use Rule X\" is replaced by \"use closure of Rule X.\" For example, \"Use closure of Rule 8.ps.U' stands for If we have the production"]},{"title":"A (\"~1. . . ?m) ~ Ap (.. %) As (C~s)","paragraphs":["where m ~ 2,"]},{"title":"el","paragraphs":["="]},{"title":"(IAp, ~pl (flp,At~',/t) )","paragraphs":["belongs to"]},{"title":"PIi, d] It, dt]","paragraphs":["and"]},{"title":"e2 = ( IAs~ top (C~s) l (rest (C~s)~ nil) )","paragraphs":["belongs to"]},{"title":"P Ii + dp~ d - dp] IO~","paragraphs":["0] and"]},{"title":"e3 ~- (/a~ '~m/(~/1-..","paragraphs":["\"Ym-l~ap~"]},{"title":"\"Yp))","paragraphs":["does not belong to"]},{"title":"P[i,d] Ii, dp] then","paragraphs":["add e3 to"]},{"title":"PIi, d] Ii~dp]","paragraphs":["and then invoke Closure"]},{"title":"(e3,i~d~i~dp).","paragraphs":["Closure is defined as follows: Closure"]},{"title":"( e, il, dl , t, dr)","paragraphs":["begin","use closure of Rule 2.ps.L, 6.ps.L, 7.ps.L, 8.ps.L, 9.ps.L, 10.ps.L","with d = dp and the entry e as the primary constituent in the antecedent.","use closure of Rule 2.sp.L, 6.sp.L, 7.sp.L, 8.sp.L, 9.sp.L, 10.sp.L","with ds -- d and the entry e as the secondary constituent in the antecedent.","use closure of Rule 2.u.L, 6.u.L, 7.u.L, 8.u.L, 9.u.L, 10.u.L","with d = dp and the entry e as the primary constituent in the antecedent. end.","Note Rule 3 does not apply since we have to assume MCL _< 2 (hence any terminator is a 2-terminator and the length of the middle in a terminator-type entry is always one). We have not included Rule 4 and Rule 5 while computing the closure. These correspond directly to the completor step in Earley's algorithm and to the popping of stack elements and hence are not considered a part of the closure. They have to be applied later in the control structure.","We will now consider the effect of including unary rules on the control structure of the algorithm. Let//il, dl/~/i2, d2//G//i3, d3/~ lid, dd//if and only if (1) (il, dl/ < (i3, d3/ or (2) /il~ dl/ --- /i3~ d3/and/i2~ d2/ < (i4~ dd/. The simplicity of the loop structure in the algorithms seen thus far stems from the fact that for any parsing rule if the entry in the consequent is to be added to P Ii3,"]},{"title":"d3] Iid~","paragraphs":["dd] based on the existence of an antecedent entry in"]},{"title":"P Iil, dl] I/'2,","paragraphs":["d2], then"]},{"title":"Ilil, dll, lid, d211","paragraphs":["~"]},{"title":"Ili3, d31~ lid, dd}l.","paragraphs":["This no longer holds when we consider Rule 5.u.L or Rule 5.ps.L when the secondary constituent derives the empty string. Consider the following derivation (and the presence of the productions assumed) for a sufficiently long fl:"]},{"title":"a (flo~) ~ a 1 (fl\"~l) ~ A2 (fl~l~Y2) ~ A3 (fl'y) ~","paragraphs":["ai... ai+d-1 Consider the addition of an entry e3 to"]},{"title":"P[i,d] It, dr]","paragraphs":["(for some"]},{"title":"It, dtl)","paragraphs":["to record the derivation from A3 (fl'y). Closure (e3~ i, d, t, dr) is invoked, resulting in the addition of e2 (corresponding to A2 (fl'Y13'2)) to"]},{"title":"P[i, d] Ii, d].","paragraphs":["From Rule 5.u.L and the presence of entry e2 and e3"]},{"title":"we","paragraphs":["would add el (corresponding to"]},{"title":"al (fl3/1)","paragraphs":["to"]},{"title":"P Ii, d] It, dt]).","paragraphs":["This could result in the need to add more entries to P Ii, d] Ii, d], which in turn could cause new entries being added back to"]},{"title":"PIi, d] It, dt],","paragraphs":["and so on. Thus we have a situation where 619 Computational Linguistics Volume 19, Number 4"]},{"title":"initialization phase","paragraphs":["for loops for d, i, d' as before begin","consider closure of Rules in Rule set I"]},{"title":"for dt","paragraphs":[":-- d' - 1 to 1 do for t := i to"]},{"title":"i + d'","paragraphs":["- dt do","repeat consider closure of Rules in Rule set II for"]},{"title":"dr :=","paragraphs":["dt"]},{"title":"-","paragraphs":["1 to 1 do for r := t to"]},{"title":"t +","paragraphs":["dt - dr do consider closure of Rules 5.ps.L and Rule 5.u.L","until no new entries are added to"]},{"title":"P[i,d] [t~dt] Figure 8","paragraphs":["Control structure with unary productions. an antecedent entry in"]},{"title":"P[i,d]","paragraphs":["It, dr]"]},{"title":"((t, dt) < (i,d))","paragraphs":["causes an entry to be added to P [i, d] [i, d], which, acting as an antecedent entry, causes a new entry to be added to"]},{"title":"P[i,d] It, dr].","paragraphs":["A simple strategy to take care of this situation would be to add another loop within the t loop (as shown in the partial control structure given in Figure 8) that is repeated until no new entries are added to P [i, d] It,"]},{"title":"dt].","paragraphs":["It is straightforward to prove the correctness of the algorithm with this additional loop and also that the asymptotic complexity remains the same. The latter is the case because only a bounded number of entries can belong to"]},{"title":"P[i,d] It, dr]","paragraphs":["for any fixed value of i, d, t,"]},{"title":"dr,","paragraphs":["and hence the repeat loop can be iterated only a bounded number of times (as determined by the grammar). In the partially specified control structure given in Figure 8, we have not considered the"]},{"title":"sp","paragraphs":["rules. Also we only consider the changes that need to be made to Algorithm 1; the changes to Algorithm 2 can be made in a similar fashion. Finally, for purposes of abbreviation, we have grouped Rules 2.ps.L, 6.ps.L, 9.ps.L, and 10.ps.L together and called it the Rule set I, and Rules 3.ps.L, 4.ps.L, 7.ps.L, and 8.ps.L the Rule set II.","The repeat loop shown in Figure 8 is not needed in some situations. Consider the derivation and the sequence of addition of entries, e3~ e2~ el, as discussed above. Viewing this derivation as a bottom-up recognizer would, we have a \"prediction\" from entry e3 followed by a \"completion\" that results in the entry el. In this case the two entries both encode objects with the same stack length. We generalize this situation and call such derivations auxiliary derivations (named after auxiliary trees in TAG). a"]},{"title":"(fl\"Yl) ~","paragraphs":["TIA1 (fl~/l\"/2)T2 ~"]},{"title":"TlUAt (fl'Tt)wT2","paragraphs":["~"]},{"title":"UlUAt","paragraphs":["(fl\"Yt)WWl where"]},{"title":"At (fl'Tt)","paragraphs":["is the 2-terminator of A1"]},{"title":"(fl\"/1\"/2).","paragraphs":["We will say that this auxiliary derivation spans at least one terminal if"]},{"title":"len (UlUWWl) _>","paragraphs":["1. Notice that if for a particular grammar every auxiliary derivation spans at least one terminal, then the extra repeat loop added becomes unnecessary. This is because now, with this assumption, for every parsing rule if the entry in the consequent is to be added to"]},{"title":"P [/3~ d3] [i4~","paragraphs":["dd] based on the existence of an antecedent entry"]},{"title":"in P[/1, dl] [12~","paragraphs":["d2] then ((il, dl), (12~ d2))"]},{"title":"-<","paragraphs":["((i3, d3), (i4~ dd)). 620 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms","We end this section by noting that in the case of a lexicalized TAG, we can verify that every auxiliary derivation spans at least one terminal, and hence in the TAG algorithm we do not have to include this additional repeat loop."]},{"title":"5.2 Tree Adjoining Grammars","paragraphs":["Tree Adjoining Grammars (TAG) is a tree generating formalism introduced by Joshi, Levy, and Takahashi (1975). A TAG is defined by a finite set of trees composed by means of the operation of tree adjunction."]},{"title":"Definition 5.1","paragraphs":["A TAG, G, is denoted by (VN,"]},{"title":"VT~ 57 Iv A)","paragraphs":["where VN is a finite set of nonterminals symbols,"]},{"title":"VT","paragraphs":["is a finite set of terminal symbols, S E VN is the start symbol, I is a finite set of initial trees, A is a finite set of auxiliary trees. An"]},{"title":"initial","paragraphs":["tree is a tree with root labeled by S and internal nodes and leaf nodes labeled by nonterminal and terminal symbols, respectively. An auxiliary tree is a tree that has a leaf node (the"]},{"title":"foot","paragraphs":["node) that is labeled by the same nonterminal that labels the root node. The remaining leaf nodes are labeled by terminals and all internal nodes labeled by nonterminals. The path from the root node to the foot node of an auxiliary tree is called the"]},{"title":"spine","paragraphs":["of the auxiliary tree. An elementary tree is either an initial tree or an auxiliary tree. We will use c~ to refer to an initial tree, and fl to refer to an auxiliary tree. \"y may be used to refer to either an elementary tree or a tree that is derived from an elementary tree.","We will call a node in an elementary tree an"]},{"title":"elementary node.","paragraphs":["We can give a unique name to each elementary node by using an"]},{"title":"elementary node address.","paragraphs":["An elementary node address is a pair composed of the name of the elementary tree to which the node belongs and the address of the node within that tree. We will assume the standard addressing scheme where the root node has an address c. If a node addressed # has k children then the k children (in left to right order) have addresses # • 1,..., # • k. Thus, if dV\" is the set of natural numbers then # E W'*. In this section we will use # to refer to addresses and ~/to refer to elementary node addresses. In general, we can write ~ = IV, #/ where 3̀is an elementary tree and # E Domain (3'). We will use Domain (3') for the set of addresses of the nodes in %"]},{"title":"Definition 5.2","paragraphs":["Let 3̀be a tree with internal node labeled by a nonterminal A. Let fl be an auxiliary tree with root and foot node labeled by the same nonterminal A. The tree, 3~̀, that results from the"]},{"title":"adjunction","paragraphs":["of fl at the node in 3̀labeled A (as shown in Figure 9) is formed by removing the subtree of 3̀rooted at this node, inserting fl in its place, and substituting it at the foot node of ft. Each elementary node is associated with a"]},{"title":"selective adjoining","paragraphs":["(SA) constraint that determines the set of auxiliary trees that can be adjoined at that node. In addition, when adjunction is mandatory at a node it is said to have an obligatory"]},{"title":"adjoining","paragraphs":["(OA) constraint. Figure 9 shows how constraints are associated with nodes in trees derived from adjunctions. Whether fl can be adjoined at the node (labeled by A) in 3̀ is determined by c, the SA constraint of the node. In 3r̀ the nodes contributed by fl 621 Computational Linguistics Volume 19, Number 4 Acl A c2 A c2"]},{"title":"aN Figure 9","paragraphs":["The operation of adjoining. have the same constraints as those associated with the corresponding nodes in ft. The remaining nodes in 7' have the constraints of the corresponding nodes in 3.̀","Given/* E Domain (3)̀, by LABEL(% #/we refer to the label of the node addressed /* in 7. If the tree in question is clear from context, we will simply use LABELI#/. Similarly, we will use SA(%/*) (or SA(#)) and OA(% #) (or OA(/*)) to refer to the SA and OA constraints of a node addressed /* in a tree 3.̀ Finally, we will use ft (/3) to refer to the address of the foot node of an auxiliary tree/3.","To be precise, we define the adjunction of/3 at a node in 7 with address /* as follows. This operation is defined when/3 is included in the SA constraints of node addressed # in 3.̀ If the operation is defined, we will use ADJ (3,̀ #,/3) to refer to the tree that results. Let 3'' = AD3 (%/*,/3). Then the nodes in 3'' and their labels and adjoining constraints are defined as follows.","• Domain (-y') = {/.1 I /.1 E Domain(7),/.1 ~/*'/*2, for some/*2 E Af*} U {/* \",1 [/.1 E Domain (fl)} U {#. ft (fl)'/~l I/* \"/.1 C Domain (3'), and/.1 e}","• When/.1 C Domain (3)̀ such that/.1 #/* •/.1 for some/.1 E d~ f*}, i.e., the node in ~ with address/.1 is not equal to or dominated by the node addressed/* in 3,:","LABEL(-/',/*~) = LABEL('),,/.1), -- SA(3\",/.1) = SA(3,̀/.1), --"]},{"title":"OA(7',/*1) = OA(-y,/.1),","paragraphs":["• when/*'>1 E Domain (7') such that #1 C Domain(fl):","LABEL(7',/*. >1) = LABEL(fl,/*I), -- SA(\"/t,/*\"/.1) ="]},{"title":"SA(fl,/.1), -- OA(3'̀,/*'/*1) ~- OA(fl,/.1),","paragraphs":["• when/*, ft (fl)./.1 E Domain (3\") such that/*./.1 C Domain (7) and/.1 # e: LABEL ('T t,/*- ft (/3). /.1) = LABEL (-y, #./xl ), 622 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms {z S {IBt,IB2}"]},{"title":"s b s {131,132} b s¢ a Figure 10","paragraphs":["Example of a TAG G.","-- SA(3/, # • ft (fl) - it1 ) = SA('y,# - #1}, OA(~/', # - ft (fl). #~) = OA(%#. #1), In general, if # is the address of a node in \"~ then by (% #) we refer to the elementary node address of the node that contributes to its presence, and hence its label and constraints.","The tree language, T(G), generated by a TAG, G, is the set of trees derived starting from an initial tree such that no node in the resulting tree has an OA constraint. The (string) language, L(G), generated by a TAG, G, is the set of strings that appear on the frontier of trees in T(G). Example 5.1 Figure 10 gives a TAG, G, which generates the language {wcw I w E {a, b}+}. The constraints associated with the root and foot of fl specify that no auxiliary trees can be adjoined at these nodes. This is indicated in Figure 10 by associating the empty set, G with these nodes. An example derivation of the strings aca and abcab is shown in Figure 11. 5.3 TAG and LIG In this section, we examine bottom-up recognition of a TAG. In doing so, we construct a LIG that simulates the derivations of the TAG. Based on this construction, we derive a recognizer for TAG from the algorithms given earlier.","Consider bottom-up TAG recognition. Having recognized the substring dominated by an elementary node there are two possible actions: (1) move up the tree by combin-ing this node with its siblings; or (2) consider adjunction at that node. In bottom-up recognition, the second action (i.e., adjunction) must be considered before the first. Therefore, there are two phases involved in the consideration of each node. On entering the bottom phase of a node, having just combined the derivations of its children, we predict an adjunction. On entering the top phase, having just finished adjunction at that node, we must now combine with any siblings in order to move up the tree. Note that in the bottom phase we may also predict that there is no adjunction at the node (if there is no OA constraint on that node) and hence move to its top phase directly.","Figure 12 shows why, because of the nature of the adjoining operation, TAG can be seen to involve stacking. Suppose, during recognition, the bottom phase of a node, 7], 623 Computational Linguistics Volume 19, Number 4"]},{"title":"3,1","paragraphs":["c~ S {[31,[52}"]},{"title":"I","paragraphs":["c Figure 11 Sample derivations in G."]},{"title":"s~ a S {61,~2} Soo a I","paragraphs":["c"]},{"title":"s~ a S~","paragraphs":["b"]},{"title":"S","paragraphs":["{[51,[52} Sq~ b"]},{"title":"S~ a I","paragraphs":["c Figure 12 Stacking in a TAG. has been reached. When adjunction by the auxiliary tree fl is predicted, control shifts to the bottom phase of fl's foot node. As we move up the spine of fl it is necessary to remember that fl was adjoined at 7. On reaching the top phase of fl's root we must return to (the top phase of) 7. Therefore, the adjunction point, ~7, must be propagated up the spine of ft. In general, we may need to propagate a stack of adjunction points as we move up the spine as shown in Figure 12 where 3'2 is obtained by adjoining 131 at a node ~11 on the spine of ft. From this figure, it can be seen that the information about the adjunction points (that must be propagated along the spine of an auxiliary tree) follows the stack (last-in first-out) discipline. Notice also that only the nodes on the spine participate in the propagation of adjunction points.","Consider how a LIG that simulates this process can be constructed. The details of the equivalence between LIG and TAG can be found in Vijay-Shanker (1987). In the 624 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms LIG, we use two nonterminals, t and b to capture the differences between the top and bottom phases associated with a node. The stack holds an appropriate sequence of adjunction points in the form of elementary node addresses. The top of the stack is the elementary node address of the node that is currently being visited (thus all objects have at least one element on the stack). Nodes that are not on the spine, or belong to an initial tree, do not participate in the propagation of adjunction points. Therefore, the objects for such nodes will have stacks that contain only their elementary node address.","The set of LIG productions is determined as follows. We assume that internal elementary nodes have either a single child labeled by a terminal symbol (or ~), or exactly two children labeled by nonterminals. In this discussion below, we will use ~] for a node and its elementary node address interchangeably. 1. If ?7 is a node that is labeled e where ~ c V T"]},{"title":"U{~}","paragraphs":["then we will include t 07) ---+ c.","2. If ?Tp and qs are the children of a node ?7 such that the left sibling ~/p (and hence ?7) is on the spine then the following holds: (1) the object corresponding to ~/p can have an unboundedly large stack, whereas the object for ?7s will have a stack of size one; (2) the top of the stack in these objects will be ?Tp and ~Ts; (3) combination of these two sibling nodes is possible only after the top parts of these nodes are reached; (4) the stack in the object for ~]p must be propagated to object for 77, except that the top symbol ?7p is replaced by ?7; (5) when the two sibling nodes are combined we reach the bottom part of ?7. Hence, we include the production b (.. ?7) ---+ t (.. ?Tp) t 07s).","3. If ?7p, ~]s are children of ~ as in the previous case except that ?7p is the right sibling and is on the spine, then we include the production b ('\" n) -~ t(ns) t (..'qp).","4. If ?Tp, qs, and 77 are as before except that neither sibling is on the spine of an auxiliary tree then we include the production b (.. ?/) ~ t (.- ?7p) t 07s). 5. If ?7p is the only child of 77 we have b (.. ?7) --+ t (-- 77p ).","6. If ?7 is a node where fl can be adjoined and we are at the bottom of ~7, then, by predicting adjunction by fl, control moves to the bottom part of 771 (the foot node of fl). This is illustrated in Figure 13. In this case we add the production b (.. 77771) --+ b (.. 77). When there is no OA constraint at 77 then we can predict that no adjunction takes place. This is captured with the production t (.. 7/) --+ b (-- ?7).","7. Suppose we have reached the top part of the root node, 772, of the auxiliary tree ft. The corresponding object has the nonterminal t with 772 on top of the stack and the node at which fl was adjoined is immediately below 772. Having reached the top of the root node of fl we must return to the top of the node where fl was adjoined. This is accomplished with the production t (..) ---+ t (.. 72) (see Figure 13).","Figure 13 captures the essence of the connection between TAG and LIG--in particular the way the adjoining operation in TAG can be simulated in LIG. This figure is also useful in order to understand the notion of terminators. As in the case of CCG, the construction of the LIG equivalent of the given grammar is unnecessary. However, as in the case of CCG, this discussion of the connection between TAG and LIG can be 625 Computational Linguistics Volume 19, Number 4 A","7 J n 1 t (I~l) I b(r'n)"]},{"title":"/\\ Figure 13","paragraphs":["TAG/LIG relationship. used to motivate the choices we make in the form of entries in TAG parser as well as the rules in the algorithm. 5.4 Recognition of TAG We now give a CKY-style recognition algorithm for TAG. But first we shall consider the LIG constructed from a given TAG as described in Section 5.3. Given this LIG grammar, consider the objects derived and the form of entries that will be used by the LIG algorithm.","• If ~ is an elementary node address of a node on the spine of an auxiliary tree, say fl, then any object that has ~ as the top symbol of its stack must be of the form"]},{"title":"A (~h ... 7]k~]t~)","paragraphs":["where k > 0, A C {t, b}, and/It is the elementary node address of a node where fl can be adjoined. Furthermore, in any derivation, the terminator of A (~t77) will be b (~rlt).","• For this LIG, MSL = MTL = TTC = 1 and MCL = 2. Hence it follows that any terminator is a 2-terminator. From the discussion above, an object A (9~) (where A C {t, b} and"]},{"title":"len","paragraphs":["(~) > 0) has a terminator if and only if is an elementary node address of a node on the spine of an auxiliary tree.","• Consider the forms of entries for a LIG in this form. First, the length of the middle in a terminator-type entry will be one always, since any terminator is a 2-terminator. Note that the terminator of A (9~t~) will be b (~t). Thus, a terminator type entry in a parsing array entry, say P [i, d] will have the form"]},{"title":"({A,~)O]t, ({b,~lt}, [t, dt])))","paragraphs":["where A c {t,b} and"]},{"title":"It, dr} < li, d}.","paragraphs":["Note that {b, ~Tt) in the terminator-pointer is redundant.","• From the discussion above, a non-terminator-type entry wilt be used to record derivations from A (7) where A E {t, b} and r/is the elementary node address of a node that belongs to an initial tree or of a node that is 626 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms not on the spine of an auxiliary tree. To record this object the entry"]},{"title":"((A, 71, nil)","paragraphs":["would have been used.","From the above discussion it makes sense that terminator-type entries in the TAG parser have the form ((A, T]/ (7t,"]},{"title":"t, dt))","paragraphs":["where A E {t,b}, 7 is an elementary node address of a node on the spine of an auxiliary tree, say fl, and /'It is the elementary node address of a node where fl can be adjoined in. A non-terminator-type entry has the form ((A, 7),"]},{"title":"nil)","paragraphs":["where A E {t, b}, and 7 is an elementary node address of a node","that is not on the spine of an auxiliary tree.","Finally, consider an auxiliary derivation in the LIG obtained from a TAG as de-","scribed in Section 5.3. Recall that an auxiliary derivation has the form A (~71) ~ TIA1 (~')'172) T2"]},{"title":"T~uAt (9~\"yt)","paragraphs":["wT 2 In this case we would have: • 3/1 = \"Yt, • A = A1 = t,"]},{"title":"• At","paragraphs":["= b, and","• \"72 is the root of an auxiliary tree that can be adjoined at the node whose elementary node address is given by ~?t. Since every auxiliary tree in a lexicalized TAG has at least one terminal node in its frontier, every auxiliary derivation spans at least one terminal in the LIG we have constructed. 5.5 Recognition Algorithm We begin with a description of the cases involved in TAG recognition algorithm. Predicting adjunction: During the recognition phase, on reaching the bottom part of a node 7, we predict adjunction by each auxiliary tree, fl that can be adjoined at 7 as determined by its SA constraints. As given in Case 6 of the construction in Section 5.3, this prediction is captured with the LIG production b (.. ~/~]1) -~ b (.. 7) where 71 is the foot node of the auxiliary tree, ft. Depending on whether 7 is on the spine of an auxiliary tree or not, we have the following counterparts of Rule 8.u.L and Rule 10.u.L: Rule 8.u.T 71 = (fl, ft(fl)) fl c SA(7) ((b,7)"]},{"title":"(~h,t, dt)) E P[i,d]","paragraphs":["((b,"]},{"title":"111) (7, i,d) ) C P[i,d]","paragraphs":["Rule 10.u.T 7, = (fl~ft(fl)) fl C SA(7) ((b~7)~nil)"]},{"title":"E P[i~d]","paragraphs":["((b, 71)"]},{"title":"(7, i,d)) E P[i,d]","paragraphs":["627 Computational Linguistics Volume 19, Number 4 As in the second part of Case 6 of the LIG construction (i.e., when there is no OA constraint at the node ~7) we have the following counterparts of Rule 6.u.L and Rule 7.u.L: Rule 6.i.u.T OA(rl)"]},{"title":"=false","paragraphs":["((b,r/)(r]t,t,"]},{"title":"dt)) E P[i,d]","paragraphs":["((t,)"]},{"title":"(,t,t, dt)) E P[i,d]","paragraphs":["Rule 7.i.u.T OA(~))"]},{"title":"=false","paragraphs":["((b,~7),nil)"]},{"title":"E P[i,d] ((t, rl),nil) E P[i,d]","paragraphs":["Left sibling on the spine: This corresponds to Case 2 of the LIG construction. The following rule that captures this situation corresponds to Rule 7.ps.L. Rule 7.ps.T ~p is left child of ~?p is on the spine of an auxiliary tree ((t,~/p/(,,,t,"]},{"title":"dt)) E P[i, dp]","paragraphs":["~/p is right child of 7/ ((t, ~/s),"]},{"title":"nil) C P [i + dp, d - dp]","paragraphs":["((b,,)"]},{"title":"01t, t, dt) ) C P[i,d]","paragraphs":["The following covers Case 4 of LIG construction where the two siblings are not on the spine or belong to an initial tree and corresponds to Rule 6.ps.L (or Rule 6.sp.L). Rule 6.ps.T r/p is left child of z/ r/is not on the spine of any auxiliary tree ((t,,p}"]},{"title":",nil) C P[i, dp] ~p","paragraphs":["is right child of (t, rls},"]},{"title":"nil) E P [i + dp, d -- dp]","paragraphs":["((b, rl}"]},{"title":",nil) E P[i,d]","paragraphs":["Right sibling on the spine: Corresponding to Case 3 of LIG construction and Rule 7.sp.L we have Rule 7.sp.T rls is left child of ~1 ((t,,s}"]},{"title":",nil) E P[i, ds]","paragraphs":["~/p is right child of Wp is on the spine of an auxiliary tree"]},{"title":"((t,,p} (Zlt, t, dt)) E P[i+ds,d-ds]","paragraphs":["((b, rl)"]},{"title":"(rlt, t, dt)) C P[i,d]","paragraphs":["Single child case: Corresponding to Case 5 of LIG construction, Rule 7.u.L and Rule 6.u.L. Rule 7.ii.u.T ~p is only child of rlp ison the spine of some auxiliary tree ((t,~/p}"]},{"title":"(rlt, t, dt)) E e[i,d]","paragraphs":["((b,,)"]},{"title":"(rlt, t, dt)) C P[i,d]","paragraphs":["628 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Rule 6.ii.u.T","~/p is only child of ~/ is not on the spine of any auxiliary tree"]},{"title":"((t,%),nil) E P[i,d]","paragraphs":["((b,r/)"]},{"title":",nil) E P[i,d]","paragraphs":["Completing an adjunction: Corresponding to Case 7 of the construction and depending on whether the node of adjunction is on the spine of an auxiliary tree, we have the following counterparts of Rule 4.u.L, Rule 5.u.L. Rule 4.u.T ((t,~lp)"]},{"title":"(zlt, t, dt)) E P[i,d]","paragraphs":["~?t is not on the spine of any auxiliary tree"]},{"title":"((b,z]t),nil) E Pit, dr]","paragraphs":["((t,,t)"]},{"title":",nil) E P[i,d]","paragraphs":["Rule 5.u.T ((t,e)"]},{"title":"(,t,t, dt)) E P[i,d]","paragraphs":["7/t is on the spine of an auxiliary tree ((b,r]t)"]},{"title":"(l']r, Gdr) ) E P[t, dt]","paragraphs":["((t,~]t)"]},{"title":"(~lr, r, dr)) E P[i,d]","paragraphs":["From the nature of entries being created it will follow that if ~]p = (fl, e), for some auxiliary tree fl, then fl is adjoinable at z]t. Similarly, if ~t ~- (fl', #) for some auxiliary tree fl', then fl' is adjoinable at/'Jr. Scanning a terminal symbol: If z/is a node labeled by a terminal matching the"]},{"title":"i th","paragraphs":["input symbol,"]},{"title":"ai,","paragraphs":["then we have (corresponding to Rule 1.L): Rule 1.T LABEL(7])=ai"]},{"title":"l < i < n","paragraphs":["((t, r/),"]},{"title":"nil) E P[i,","paragraphs":["1] Scanning empty string: If ~ is a node labeled by e, then we have (corresponding to Rule lx.L): Rule 1.e.T","LABEL(z]) = e (It, n)"]},{"title":",nil)","paragraphs":["E P[i,0]","This concludes our discussion of the parsing rules for TAG. With the correspondences with the LIG parsing rules given (via the numbering of rules), these rules may be placed in the control structure as suggested in Section 5.1. As noted earlier, in the case of a lexicalized TAG, since every auxiliary derivation spans at least one terminal we do not require the repeat loop discussed in Section 5.1. 629 Computational Linguistics Volume 19, Number 4 6. Conclusion In this paper we have presented a general scheme for parsing a set of grammar formalisms whose derivation process is controlled by (explicit or implicit) stacking machinery. We have shown how this scheme can be instantiated to give polynomial time algorithms for LIG, CCG, and TAG. In the case of CCG, this provides the only polynomial parsing algorithm (apart from a slight variant of this scheme given in Vijay-Shanker and Weir (1990)) we are aware of.","The main contribution of this paper is the general recognition scheme and definitions of some notions (e.g., terminators, data structures sharing of stacks) crucial to this scheme. We believe that these ideas can be suitably adapted in order to produce parsing schemes based on other CFG parsing algorithms (such as Earley's algorithm). For instance, the definition of terminator given here was tailored for pure bottom-up parsing. In the case of Earley's algorithm, a bottom-up parser with top-down prediction, an additional notion of terminator for the top-down prediction component can be obtained in a straightforward manner.","We have also introduced a new method of representing derivations in a TAG, one that we believe is appropriate in capturing the stacking that occurs during a TAG derivation. The derivations themselves represented can be in another TAG that we call the derivation grammar (see Vijay-Shanker and Weir (1993)).","We have not discussed the extraction of parses after the recognition is complete because of space considerations. However, an algorithm to extract the parses and build a shared forest representation of all parses for CCG was proposed in Vijay-Shanker and Weir (1990). This scheme was based on the approach we have taken in our general scheme. The method of extracting parses and representing them using a shared forest given in Vijay-Shanker and Weir (1990) can be generalized in a straightforward manner to be compatible with the generalized recognition scheme given here. Acknowledgments This work has been partially supported by NSF Grants IRI-8909810 and IRI-9016591. We would like to thank A. K. Joshi, B. Lang, Y. Schabes, S. M. Shieber, and M. J. Steedman for many discussions. We are grateful to the anonymous reviewers for their numerous suggestions.","References","Aho, A. V. (1968). \"Indexed grammars--An extension to context free grammars.\" J. ACM, 15, 647-671.","Duske, J., and Parchmann, R. (1984). \"Linear indexed languages.\" Theoretical Comput. Sci., 32, 47-60.","Gazdar, G. (1988). \"Applicability of indexed grammars to natural languages.\" In Natural Language Parsing and Linguistic Theories, edited by U. Reyle and C. Rohrer. D. Reidel, 69-94.","Joshi, A. K. (1985). \"How much context-sensitivity is necessary for characterizing structural descriptions--tree adjoining grammars.\" In Natural Language Processing--Theoretical, Computational and Psychological Perspective, edited by D. Dowty, L. Karttunen, and A. Zwicky. Cambridge University Press, 206-250.","Joshi, A. K.; Levy, L. S.; and Takahashi, M. (1975). \"Tree adjunct grammars.\" ]. Comput. Syst. Sci., 10(1), 136-163.","Kasami, T. (1965). \"An efficient recognition and syntax algorithm for context-free languages.\" Technical Report AF-CRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.","Lang, B. (1990). \"Towards a uniform formal framework for parsing.\" In Current Issues in Parsing Technology, edited by M. Tomita. Kluwer Academic Publishers, 153-171.","Pareschi, R., and Steedman, M. J. (1987). \"A lazy way to chart-parse with categorial grammars.\" In Proceedings, 25th Meeting of the Association for Computational Linguistics, 81-88.","Pollard, C. (1984). Generalized Phrase Structure Grammars, Head Grammars and Natural Language. Doctoral dissertation, Stanford University.","Steedman, M. (1986). \"Combinators and grammars.\" In Categorial Grammars and 630 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms"]},{"title":"Natural Language Structures,","paragraphs":["edited by R. Oehrle, E. Bach, and D. Wheeler. Foris, 417-442.","Steedman, M. J. (1985). \"Dependency and coordination in the grammar of Dutch and English.\""]},{"title":"Language,","paragraphs":["61:523-568.","Tomita, M. (1988). \"Graph-structured stack and natural language parsing.\" In"]},{"title":"Proceedings, 26th Meeting of the Association for Computational Linguistics,","paragraphs":["248-257. Vijay-Shanker, K. (1987)."]},{"title":"A study of tree adjoining grammars.","paragraphs":["Doctoral dissertation, University of Pennsylvania, Philadelphia, PA.","Vijay-Shanker, K., and Joshi, A. K. (1985). \"Some computational properties of tree adjoining grammars.\" In"]},{"title":"Proceedings, 23rd Meeting of the Association for Computational Linguistics,","paragraphs":["82-93.","Vijay-Shanker, K., and Weir, D. J. (In press). \"The equivalence of four extensions of context-free grammars.\""]},{"title":"Mathematical Systems Theory.","paragraphs":["Vijay-Shanker, K., and Weir, D. J. (1990). \"Polynomial parsing of combinatory categorial grammars.\" In"]},{"title":"Proceedings, 28th Meeting of the Association for Computational Linguistics,","paragraphs":["Pittsburgh, PA, 1-8.","Vijay-Shanker, K., and Weir, D. J. (1993). \"The use of shared forests in TAG parsing.\" In"]},{"title":"Proceedings, 6th Meeting of the European Association for Computational Linguistics,","paragraphs":["Utrecht, The Netherlands, 384-393.","Weir, D. J. (1988)."]},{"title":"Characterizing mildly context-sensitive grammar formalisms.","paragraphs":["Doctoral dissertation, University of Pennsylvania, Philadelphia, PA.","Weir, D. J., and Joshi, A. K. (1988). \"Combinatory categorial grammars: Generative power and relationship to linear context-free rewriting systems.\" In"]},{"title":"Proceedings, 26th Meeting of the Association for Computational Linguistics,","paragraphs":["278-285.","Younger, D. H. (1967). \"Recognition and parsing of context-free languages in time n3. \""]},{"title":"Inf. Control,","paragraphs":["10(2), 189-208. Appendix A: Correctness of Algorithm 1 We will now prove the correctness of Algorithm 1. In doing so, we will start by observing some properties of the rules and the control structure used.","Firstly, given an input is al... an, we can note that every entry added by a rule (i.e., consequents of rules) satisfies the requirements for the terminator-type and non-terminator-type entries; viz., if"]},{"title":"((A,'y)(/3, ((At, \"Yt), It, dr])))","paragraphs":["is added to an array element P [i, d] then"]},{"title":"• A, At E VN, • %'YtEVI,","paragraphs":["• fl E V + where 1 <"]},{"title":"len(fl) <_","paragraphs":["MCL- 1 and • (t,"]},{"title":"dt) < li, d) <","paragraphs":["(1, n) where d _> 2. We can also note that if"]},{"title":"((A,'71 (fl, nil))","paragraphs":["is added to"]},{"title":"P[i,d]","paragraphs":["then"]},{"title":"• AEVN, • \"yEVI,","paragraphs":["• flEV 7 where0~len(fl) KTTC+MCL-1 and • d>_l.","These can be verified from noting the form of the rules and by simple induction on (i, d/. We can also observe from the control structure given that entries to P [il, dl ][/2, d21 are added before entries are added to"]},{"title":"PIi3,d3]","paragraphs":["[id,dd] if and only if"]},{"title":"(il,dll < (i3,d31","paragraphs":["or (/1, dl) = (/3, d3) and (/2, d2) > (/4, dd). This observation can be used to show that when 631 Computational Linguistics Volume 19, Number 4 a rule is considered for the purposes of adding an entry to"]},{"title":"P[il,dl]","paragraphs":["[/2,d2] then the array elements specified in the antecedent of that rule would have already been filled. Verifying these properties of the algorithm enables us to establish the correctness of the algorithm more easily. Theorem A.1 if and only if ((A, 7) (c~,"]},{"title":"((At,","paragraphs":["7t), [t, dt]))) C"]},{"title":"P[i,d]","paragraphs":["A (flo~\"y) ~ ai. . .at-lAt (fl3/t) at. . .ai+d_l ai . • • ai+d-1 for some fl such that"]},{"title":"At (fl3/t)","paragraphs":["is the"]},{"title":"len","paragraphs":["(aT)-terminator of A (fla'~) in this derivation and"]},{"title":"len","paragraphs":["(flVt) k TTC."]},{"title":"((A,7) (o~,nil)) E P[i,d]","paragraphs":["if and only if"]},{"title":"A (oc'y) ~ ai...ai+d-1","paragraphs":["where A (a7) does not have the TC-property, i.e., A (aT) has no terminator in this derivation or the terminator, say"]},{"title":"At (fl3q),","paragraphs":["is such that"]},{"title":"len (flTt)","paragraphs":["< TTC. Proof of Soundness: We prove the soundness by inducting on d. The base case corresponds to d = 1. We have to consider only entries of the form"]},{"title":"((A,'y)(c~,nil))","paragraphs":["in"]},{"title":"P[i,","paragraphs":["1]. Such entries are added only by the application of Rule 1. Therefore, we have A (~/) --+ a and a ="]},{"title":"ai.","paragraphs":["Hence"]},{"title":"A (c~,y) ~ ai","paragraphs":["as required. Now, for the inductive step, let d > 2. Any entry ((A,3'/(a,"]},{"title":"tp))","paragraphs":["added to"]},{"title":"P[i,d]","paragraphs":["where d > 2 must be due to a rule other than Rule 1.L. This means that we have either a production"]},{"title":"A (.. \"/1... 3'm) --* Ap (.. 3'p) As (C~s)","paragraphs":["or"]},{"title":"A (.. 71... \"Ym) --+ As (O~s) A m ('\" q/p).","paragraphs":["Let us assume that the first production was used. We will discuss the cases for m = 0, m = 1, and m _> 2 separately. Let m = 0. In this case the production is"]},{"title":"A (..) --* Ap (.. \",/p) As","paragraphs":["(as). Then the entry ((A, \"~)"]},{"title":"(a, tp))","paragraphs":["should have been added by using one of rules 1.ps.L through 5.ps.L. We take Rule 4.ps.L as a representative. If"]},{"title":"((A, \"Yl) (fit, nil))","paragraphs":["were to be added as a result of this rule, then we have to show that A"]},{"title":"(fit\"Y1) :~ ai... ai+d-1","paragraphs":["where A"]},{"title":"(fit\"Y1)","paragraphs":["does not meet the TC-property. Since"]},{"title":"(i, dp) < (i, d I, (i +dp,","paragraphs":["d - ds) < (i, d), and"]},{"title":"(t, tit)","paragraphs":["( (i, d) the inductive hypothesis applies to the three entries in the antecedent. Thus, we have for some a the following derivations: At (fit'}q)"]},{"title":"A~ (o~s)","paragraphs":["at... at+dt-1 ai+dp •.. ai+d_l ai. • • at-lAt (oz\"/t) at+dr. • • ai+dp-1 ai •. • ai+dp-1 632 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms such that"]},{"title":"At (fltTt)","paragraphs":["does not meet the TC-property. However,"]},{"title":"Ap","paragraphs":["(ol'/l'/p) satisfies the TC-property and furthermore"]},{"title":"At (o~'/t)","paragraphs":["is the 2-terminator of"]},{"title":"Ap","paragraphs":["(c~'/l'/p). From Observation 2.1, we can also infer the existence of the following derivation."]},{"title":"Ap (flt'/l'/p) ~ ai. . .at-lAt (flt'/t)at+dt . . .ai+dp-1 ai... ai+dp-1","paragraphs":["Combining this derivation with the derivation from As (c~s) we have a(flt'/1)~ ai • •. at_l At (flt'Tt ) at+dr. • • ai+dp-l ai+dp • • • ai+d-1 a i ... ai+d_l From Observation 2.5, we know that the terminator of"]},{"title":"At (fit'~t)","paragraphs":["in this derivation is also the terminator of"]},{"title":"A (fit'~l)","paragraphs":["(and if"]},{"title":"At (fit'~t)","paragraphs":["has no terminator then neither does A (fit'/I)). Since"]},{"title":"At (fit'~t)","paragraphs":["does not satisfy the TC-property (i.e., it does not have a terminator with stack length greater than or equal to TTC),"]},{"title":"A (fit'~l)","paragraphs":["does not satisfy the TC-property either. Thus we have shown the existence of the required derivation. Let m = 1. Therefore the production may be written as"]},{"title":"A (.. \"/1) --+ Ap (.. %) As (oes).","paragraphs":["This time we will take Rule 6.ps.L as a representative. Hence, we can assume that the entry added to P [i, d] has the form ((A,'/1)(tip,"]},{"title":"nil)).","paragraphs":["Since"]},{"title":"(i, dp) < (i,d),","paragraphs":["and"]},{"title":"(i + dp, d -ds) < (i,","paragraphs":["d), the inductive hypothesis applies to the two entries in the antecedent. Thus, we have the following derivations:"]},{"title":"Ap (gp'/,) a, ai+ -i As (Oes) ~ ai+G.., ai+d-1","paragraphs":["Therefore we have the derivation:"]},{"title":"A (tip'/l) ~","paragraphs":["ap (tip'~p)"]},{"title":"As (c~s)","paragraphs":["ai ...... ai+dp-l ai+dp • • • ai+d-1 = ai • •. ai+d-1 Note that any terminator of"]},{"title":"Ap (tip'~p)","paragraphs":["is also the terminator of"]},{"title":"A (tip'~l)","paragraphs":["(and if"]},{"title":"Ap (tip'~p)","paragraphs":["has no terminator then neither has A (flpVl)). Since"]},{"title":"Ap (tip'~p)","paragraphs":["does not meet the TC-property in this derivation (from inductive hypothesis), neither does"]},{"title":"A (tip'~p).","paragraphs":["Thus we have shown the existence of the required derivation. Let m > 2. We will consider the application of Rule 10.ps.L as a representative. Again, applying the inductive hypothesis we have the following derivations: Ap (tip')p) ~ ai...ai+dp-1 As(o@) ~ ai+dp...ai+d_ 1 633 Computational Linguistics Volume 19, Number 4 where"]},{"title":"len (flpTp) >","paragraphs":["TTC. Combining the two derivations, we have: A (tip\"Y1... \"Ym)"]},{"title":"ap (~p-yp) As (as)","paragraphs":["ai ...... ¢li+dp_ l ai-bdp •.. ai+d_ l ai • • • aiq-d-I Since m > 2,"]},{"title":"Ap (tip'/p)","paragraphs":["is the m-terminator of A (flp\"Yl..."]},{"title":"\"Ym)","paragraphs":["in the above derivation. Since"]},{"title":"fen (flp'yp) >_","paragraphs":["TTC, we have shown the existence of the required derivation and that A (tip71...'Ym) satisfies the TC-property. In a similar manner we can consider other rules (including those that assume a production of the form"]},{"title":"A (..","paragraphs":["\"Yl'.. \"Ym) ---+"]},{"title":"As (c~s) Ap (.. 7p))","paragraphs":["as well. Proof of Completeness: We will now show the completeness of Algorithm 1. This time we use induction on the number of steps in a derivation. Suppose"]},{"title":"A (fl) ~ ai...ai+d-1;","paragraphs":["we have to show that there is a corresponding entry (as specified in Theorem A.1) in"]},{"title":"P[i, d].","paragraphs":["The base case corresponds to l = 1. From the form of the productions being considered we can assume that d = 1 and that there exists a production A (~) ~"]},{"title":"ai.","paragraphs":["Rule 1 would apply and thus we have the required entry. Let"]},{"title":"A (fl) ~ ai... ai+d-i","paragraphs":["where l >__ 1. The first production used in this derivation must have the form"]},{"title":"A (..","paragraphs":["\"rl . . . \"Ym)--+ Ap (\" \")'p)"]},{"title":"As (C~s)","paragraphs":["or A (.. 3Zl..."]},{"title":"\"ym)---+ As (ees) Ap (.. \"yp).","paragraphs":["We will only assume that the production is"]},{"title":"A (..q,~ ... q,,,) ~ Ap (.. 7p)As","paragraphs":["(c~s). Arguments similar to the one given below can be used when the production of the form"]},{"title":"A (.. \"~1 ... 7m) --+ As (%) Ap (.. %)","paragraphs":["is involved as the first step of the derivation. Case m = 0: We begin by considering the case when m = 0. Since the first production used in"]},{"title":"A (fl) =~ ai. . . ai+d-1","paragraphs":["is"]},{"title":"A (..) ---+ Ap (.. \"yp) As","paragraphs":["(C~s), we can write the derivation as"]},{"title":"A (fl) ~ Ap (9\"Yp) As (c~s) ai . . .ai+dp-lAs (o~s)","paragraphs":["ai . . . ai+dp-lai+4 . . . ai+d-1 for some I <"]},{"title":"dp< d","paragraphs":["and"]},{"title":"lp + ls = I.","paragraphs":["Applying the inductive hypothesis to the derivation As (o@) ~ ai+clp ... ai+d-1, we can assume the existence of the entry"]},{"title":"(IAs, tOp(~s)) (rest(C~s),nil) )","paragraphs":["in"]},{"title":"P [i + dp, d -dp].","paragraphs":["In order to show the existence of the appropriate type of entry corresponding to the derivation of"]},{"title":"ai... ai+cl-1","paragraphs":["from A (fl), we need to consider whether A (fl) satisfies the TC-property in this derivation. This could depend on whether the primary constituent A, (flpVp) does. Since the inductive hypothesis applies for the derivation Ap (flVp) G"]},{"title":"ai... ai+dp-1.","paragraphs":["Let us start by assuming that A (fl) satisfies the TC-property. This means that it has a (say) (k + 1)-terminator whose stack length is greater than or equal to TTC. Expressing fl as"]},{"title":"flt71 ... 7k,","paragraphs":["we can then rewrite the derivation from A (fl) as follows. a (flt\"Yl... ~k) ~ ap (fitlY1... ~k\"Yp) As (o@) ai . . . at- l At (flt\"Yt )"]},{"title":"at+at","paragraphs":["... ai+dr- l ai +d p ... ai+d-1 ai . • • at-lat • •. at+dt-lat+d t •. • ai+d-1 634 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms where"]},{"title":"At","paragraphs":["(fltq/t) is the terminator of"]},{"title":"Ap (fltq/~... q/kq/p).","paragraphs":["Thus,"]},{"title":"len","paragraphs":["(fltq/t)"]},{"title":"~","paragraphs":["TTC and k _> 1."]},{"title":"Now,","paragraphs":["At (fltq/t) is the terminator of"]},{"title":"A","paragraphs":["(fltq/1 .'' q/k) if and only if k > 1 (from Observation 2.5). Let k > 1."]},{"title":"At","paragraphs":["(fltq/t) is the terminator of"]},{"title":"A (fltq/1 ... q/k)","paragraphs":["and"]},{"title":"len (fit'~t) >_","paragraphs":["TTC. Thus,"]},{"title":"A","paragraphs":["(fltq/1 ..\" q/k) satisfies the TC-property. Therefore we must show that the entry ((A, q/k) (q/1 ... q/k-l,"]},{"title":"((At,","paragraphs":["q/t)~"]},{"title":"[t, dt])))","paragraphs":["belongs to"]},{"title":"P[i,d].","paragraphs":["By inductive hypothesis we may assume"]},{"title":"( (Ap, q/p) (\"/1. . . q/k, ((At,","paragraphs":["q/t) , [t, dt]))) belongs to"]},{"title":"P[i, dp].","paragraphs":["Now all the conditions in the antecedent of Rule 3.ps.L have been met and thus we have shown the existence of the appropriate entry to record the derivation of ai..."]},{"title":"ai+d_l","paragraphs":["from A (fl). Let k -- 1. From Observation 2.5 it follows that the k'-terminator of At (fltq/t) (if it exists) is also the k~-terminator of A (fltq/1), and if"]},{"title":"At","paragraphs":["(fltq/t) has no terminator then neither does A (fltq/1). Therefore"]},{"title":"A (fl) = A (flt'yl)","paragraphs":["satisfies the TC-property if and only if"]},{"title":"At (fit'~t)","paragraphs":["does. Suppose"]},{"title":"At (fltq/t)","paragraphs":["satisfies the TC-property; then all conditions stated in the antecedent of Rule 5.ps.L are met and the appropriate entry is added to record the derivation from A (fl). On the other hand, if"]},{"title":"At (fit'/t)","paragraphs":["does not satisfy the TC-property then all conditions stated in the antecedent of Rule 4.ps.L are met and the appropriate entry is added to record the derivation from A (fl). Case m = 1\" Here we are concerned with the situation where A (.. \")/1) ---+ Ap (.- q/p) As (as) is the first production used in the derivation of ai..."]},{"title":"ai+d-1","paragraphs":["from A (fl). Rewriting fl as flpq/1 we have"]},{"title":"A (flpq/1) ~ Ap (flpq/p) As(~s)","paragraphs":["a i ...... ai+dp-lai+dp •.. ai+d_l Applying the inductive hypothesis we have"]},{"title":"((As, top (C~s) ) (rest (o~s), nil)) c P [i +dp, d - dp] .","paragraphs":["Now any k-terminator of"]},{"title":"Ap (tipq/p)","paragraphs":["is also the k-terminator of"]},{"title":"A (tipq/1)","paragraphs":["(and if"]},{"title":"Ap (tipq/,)","paragraphs":["has no terminator then neither does"]},{"title":"A (flpq/1)).","paragraphs":["That is, A (flpq/1) satisfies the WC-property in this derivation if and only if Ap (fl/~p) does. If"]},{"title":"Ap (flpq/p)","paragraphs":["does not satisfy the TC-property, then, by inductive hypothesis, we have ((Am,"]},{"title":"q/p)(tip, nil)) c P[i, dp].","paragraphs":["Thus the entries corresponding to the antecedents of Rule 6.ps.L exist and the algorithm would have added the entry ((A,q/1)(tip,nil))"]},{"title":"c P[i,d]","paragraphs":["as desired. If"]},{"title":"A m (flpq/p)","paragraphs":["does satisfy the TC-property then Rule 7.ps.L would add the required entry to record the derivation from A (fl). Case m > 2\" Finally, consider that case when m > 2. The given derivation may be expressed as"]},{"title":"A (flpq/,...q/m) =~ Ap (flpq/p) As (as)","paragraphs":["ai ...... ai+dp-l ai+dp • • • ai+d-1 = ai • • .ai+d-1 Applying the inductive hypothesis we have"]},{"title":"((As, top (C~s) ) (rest (C~s), nil)) E P [i +dp, d -dp].","paragraphs":["635 Computational Linguistics Volume 19, Number 4 Since"]},{"title":"Ap (flp3p̀)","paragraphs":["is the m-terminator of"]},{"title":"A (tip31̀... 3m̀),","paragraphs":["we have to consider its length in order to know whether"]},{"title":"A (flp'Yl ... 3m̀)","paragraphs":["satisfies the TC-property, i.e., how it must be represented. Suppose"]},{"title":"len (flp3p̀) <","paragraphs":["TTC, then by inductive hypothesis we have the entry"]},{"title":"((Ap, 3p̀) (tip,nil)) E P[i, dp].","paragraphs":["Thus all antecedents of Rule 9.ps.C have been found. Since the terminator of"]},{"title":"A (tip31̀ ... 3m̀)","paragraphs":["has a stack of length less than TTC, the required entry,"]},{"title":"((A,3m̀)(flfYl...'~m-l,nil)),","paragraphs":["is added by the algorithm by the application of Rule 9.ps.L. Suppose"]},{"title":"len (flp3p̀) >__","paragraphs":["TTC, then"]},{"title":"Ap (flp3p̀)","paragraphs":["may or may not be represented as a terminator-type entry. Let us take the case where Ap (flp3p̀) does not satisfy the WC-property. Again by inductive hypothesis, we have the entry"]},{"title":"((Ap, 3'p)(tip,nil)) E P[i, dp].","paragraphs":["Since"]},{"title":"len (flp3p̀) >","paragraphs":["TTC and the antecedents entries of Rule 10.ps.L exist, the algorithm would add ((A, 3m̀)(3'1-.. 3m̀--1~"]},{"title":"(~ap~ 3p̀l ~ [i~dp])))","paragraphs":["to"]},{"title":"P[i~d]","paragraphs":["as desired. If we had assumed"]},{"title":"Ap (flp3p̀)","paragraphs":["satisfies the TC-property, then by applying the inductive hypothesis we can guarantee the existence of the entries corresponding to the antecedent of Rule 8.ps.L, and therefore the algorithm would have added"]},{"title":"((A~ 3m̀)(31̀ ..-3m̀-1, ( (Ap, \"YPI , [i, dp] ) ) )","paragraphs":["to P [i, d] as desired. 636"]}]}