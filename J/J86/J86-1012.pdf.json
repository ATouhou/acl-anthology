{"sections":[{"title":"ABSTRACTS OF CURRENT LITERATURE","paragraphs":["Copies of the technical reports abstracted below are available from Graeme Hirst Department of Computer Science University of Toronto Toronto, CANADA M5S 1A4 A Computational Model for the Analysis of Arguments Robin Cohen Ph.D. Thesis Technical Report CSR G-151, October 1983, 225 pages This thesis proposes a model for an argument understanding system - a natural language understanding system which processes arguments. The form of input considered is one-way communication in a conversational setting, where the speaker tries to convince the hearer of a particular point of view. The main contributions are: (i) a theory of expected coherent structure which limits analysis to the reconstruction of particular transmission forms; (ii) a theory of linguistic clues which assigns a functional interpretation to special words and phrases used by the speaker to indicate structure; (iii) a theory of evidence relationships which includes the demand for pragmatic analysis to accommodate beliefs not currently held. A system designed to incorporate these theories could be used to analyze the structure of arguments - the necessary first step for a hearer, before judging credibility and responding. Understanding Adjectives Yawar All M.Sc. Thesis Technical Report CSR[- 167, January 1985, 85 pages This thesis deals with the task of understanding noun phrases containing sequences of prenominal adjectives.","The first problem is to determine exactly what each adjective modifies. In general, this can only be done by taking account of the semantic properties of the adjective in question, as well as those of other adjectives to its right, and of the noun itself. \"Real-world\" knowledge and contextual factors also play a role in this process. This is addressed by developing a classification scheme for adjectives which allows us to substantially reduce the number of candidate interpretations, in some cases to a single one. A system is presented which takes account of the disparate semantic behaviour of different classes of adjectives, word order, punctuation in the noun phrase, and a frame-based store of real-world knowledge, in order to determine the scope of adjectives within a noun phrase.","The second problem is to construct a representation of the description embodied in such a noun phrase. Here, it is desirable that the structure of the representation correspond to the structure of modification within the phrase. Particular adjectives are taken to indicate restrictions on the values that objects may take on for associated properties. These properties may be featural, dimensional, or functional in nature. Frame-like structures are used to represent the generic concepts that are taken to be associated with noun phrases. Rule-Based Processing in a Connectionist System for Natural Language Understanding Bast Selman M.Sc. Thesis Technical Report CSRI-168, February 1985, 5 7 pages We present a connectionist model for natural language processing. In contrast with previously proposed schemes, this scheme handles traditionally sequential rule-based processing in a general manner in the network. Another difference is the use of a computational scheme similar to the one used in the Boltzmann machine. This allows us to formulate general rules for the setting of weights and thresholds.","We give a detailed description of a parsing system based on context-free grammar rules. Using simulated annealing, we show that at low temperatures the time average of the visited states at thermal equilibrium represents the correct parse of the input system.","The system is built from a small set of connectionist primitives that represent the grammar rules. These primitives are linked together using pairs of 58 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature Theory and Parsing of the Coordinate Conjunction \"and\" Victoria L. Snarr M.Sc. Thesis Technical Report CSRI-171, September 1985, 71 pages Toward a Computational Interpretation of Situation Semantics Yves Lesperance To appear, November 1985 The Representation of Ambiguity in Opaque Constructs Brenda Fawoett M.Sc. Thesis October 1985 Technical Report, December 1985 computing units that behave like discrete switches. These units are used as binders between concepts. They can be linked in such a way that individual rules can be selected from a collection of rules, and are very useful in the construction of connectionist schemes for any form of rule-based processing. Although the conjunction and appears to have a simple function in the English language, it has proved to be a stumbling block for both theoretical and computational linguists.","One of the theoretical problems of conjunction is to determine what governs the acceptability of a structure in which two elements are connected by and. The corresponding computational problem is, given this knowledge, to incorporate it into an efficient parser for English.","This thesis proposes a solution to the theoretical problem which is in the form of two general constraints - a syntactic constraint and a semantic one; and then incorporates these constraints into a \"strictly deterministic\" parser for English. Situation Semantics proposes novel and attractive treatments for several problem areas of natural language semantics, such as efficiency (context sensitivity) and propositional attitude reports. Its focus on the information carried by utterances makes the approach very promising for accounting for pragmatic phenomena. However, Situation Semantics seems to oppose several basic assumptions underlying current approaches to natural language processing and the design of intelligent systems in general. It claims that efficiency undermines the standard notions of logical form, entailment, and proof theory, and objects to the view that mental processes necessarily involve internal representations.","The paper attempts to clarify these issues and discusses the impact of Situation Semantics' criticisms for natural language processing, knowledge representation, and reasoning. I claim that the representational approach is the only currently practical one for the design of large intelligent systems, but argue that the representations used should be efficient in order to account for the system's embedding in its environment. The paper concludes by stating some constraints that a computational interpretation of Situation Semantics should obey and discussing remaining problems. A knowledge of intensions, which are used to designate concepts of objects, is important for natural language processing systems. Certain linguistic phrases can refer either to the concept of an entity or to the entity itself. To properly understand a phrase and to prevent invalid inferences from being drawn, the system must determine the type of reference being asserted. We identify a set of \"opaque\" constructions and suggest that a common mechanism be developed to handle them.","To account for the ambiguities of opaque contexts, noun phrases are translated into descriptors. It must be made explicit to whom the descriptor is ascribed and whether its referent is non-specific or specific. Similarly, sentential constituents should be treated as propositions and evaluated relative to conjectured states of affairs. As a test bed for these ideas we define a Montague-style meaning representation and implement the syntactic and semantic components of a moderate-size NLP system in a logic programming environment.","One must also consider how to disambiguate and interpret such a representation with respect to a knowledge base. Much contextual and world knowledge is required. We characterize what facilities are necessary for an accurate semantic interpretation, considering what is and is not available in current knowledge representation systems. Computational Linguistics, Volume 12, Number 1, January-March 1986 59 The FINITE STRING Abstracts of Current Literature The following abstracts are from Computational intelligence (a.k.a. Intelligence informatique), Volume 1, Number 2 (May 1985). What is a Heuristic? Marc H.J. Romanycia Information Services, Engineering and Planning Gulf Canada Calgary, Alberta, Canada 72P 2H7 Francis Jeffry Pelletier Departments of Philosophy and Computing Science University of Alberta Edmonton, Alberta, Canada T6G 2E5 1(2): 47-58 From the mid-1950s to the present the notion of a heuristic has played a crucial role in the AI researchers' descriptions of their work. What has not been generally noticed is that different researchers have often applied the term to rather different aspects of their programs. Things that would be called a heuristic by one researcher would not be so called by others. This is because many heuristics embody a variety of different features, and the various researchers have emphasized different ones of these features as being essential to being a heuristic. This paper steps back from any particular research program and investigates the question of what things, historically, have been thought to be central to the notion of a heuristic and which ones conflict with others. After analyzing the previous definitions and examining current usage of the term, a synthesizing definition is provided. The hope is that with this broader account of \"heuristic\" in hand, researchers can benefit more fully from the insights of others, even if those insights are couched in a somewhat alien vocabulary. Possible Events, Actual Events, and Robots Andrew Haas BBN Laboratories 10 Moulton Street Cambridge, MA 02238 1(2): 59-70 To plan means reasoning about possible actions, but a robot must also reason about actual events. This paper proposes a formal theory about actual and possible events. It presents a new modal logic as a notation for this theory and a technique for planning in the modal logic using a first-order theorem prover augmented with simple modal reasoning. This avoids the need for a general modal-logic theorem prover. Adding beliefs to this theory raises an interesting problem for which the paper offers a tentative solution. A Functional Approach to Non-Monotonic Logic Erik Sandewall Department of Computer and Information Science LinkOping University Link6ing, Sweden 1(2): 80-87 Axiom sets and their extensions are viewed as functions from the set of formulas in the language to a set of four truth values: t, f, u for undefined, and k for contradiction. Such functions form a lattice with \"contains less information\" as the partial order E_, and \"combination of several sources of knowledge\" as the least-upper-bound operation u. Inference rules are expressed as binary relations between such functions. We show that the usual criterion on fixpoints, namely, to be minimal, does not apply correctly in the case of non-monotonic inference rules. A stronger concept, approachable fixpoints, is introduced and proven to be sufficient for the existence of a derivation of the fixpoint. In addition, the usefulness of our approach is demonstrated by concise proofs for some previously known results about normal default rules. The following abstracts are from the Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics. See the form at the end of this issue for ordering information, or contact Donald E. Walker, ACL Bell Communications Research 445 South Street, MRE 2A379 Morristown, NJ 07960 USA Semantics of Temporal Queries and Temporal Data Carole D. Hafner College of Computer Science Northeastern University Boston, MA 02115 Proc. ACL, pp. 1-8 This paper analyzes the requirements for adding a temporal reasoning component to a natural language database query system, and proposes a computational model that satisfies those requirements. A preliminary implementation in Prolog is used to generate examples of the model's capabilities. Temporal Inferences in Medical Texts Klaus K. Obermeier The objectives of this paper are twofold, whereby the computer program is meant to be a particular implementation of a general natural language 60 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature BatteUe's Columbus Laboratories 505 King Avenue Columbus, OH 43201-2693 USA Proc. A CL, pp. 9-17 Tense, Aspect and the Cognitive Representation of Time Kenneth Man-kam Yip Artificial Intelligence Laboratory, M.I.T., 545 Technology Square Cambridge, MA 02139 Proc. ACL, pp. 18-26 Classification of Modality Function and its Application to Japanese Language Analysis Shozo Naito, Akira Shimazu, Hirasato Nomura Musashino Electrical Communication Laboratories, N.T.T. 3-9-11, Midori-cho, Musashino-shi Tokyo, 180, Japan Proc. ACL, pp. 27-34 Universality and Individuality: The Interaction of Noun Phrase Determiners in Copular Clauses John C. Mallery Political Science Department & Artificial Intelligence Laboratory, M.I.T. 545 Technology Square, NE43-797 Cambridge, MA 02139 Proc. ACL, pp. 35-42 processing system which could be used for different domains. The first objective is to provide a theory for processing temporal information contained in a well-structured, technical text. The second objectix~e is to argue for a knowledge-based approach to natural language processing in which the parsing procedure is driven by extralinguistic knowledge.","The resulting computer program incorporates enough domain-specific and general knowledge so that the parsing procedure can be driven by the knowledge base of the program, while at the same time employing a descriptively adequate theory of syntactic processing, i.e., X-bar syntax. My parsing algorithm not only supports the prevalent theories of knowledge-based parsing put forth in AI, but also uses a sound linguistic theory for the necessary syntactic information processing. This paper explores the relationships between a computational theory of temporal representation (as developed by James Allen) and a formal linguistic theory of tense (as developed by Norbert Hornstein) and aspect. It aims to provide explicit answers to four fundamental questions: (1) what is the computational justification for the primitives of a linguistic theory; (2) what is the computational explanation of the formal grammatical constraints; (3) what are the processing constraints imposed on the learnability and markedness of these theoretical constructs; and (4) what are the constraints that a linguistic theory imposes on representations. We show that one can effectively exploit the interface between the language faculty and the cognitive faculties by using linguistic constraints to determine restrictions on the cognitive representations and vice versa.","Three main results are obtained: (1) We derive an explanation of an observed grammatical constraint on tense - the Linear Order Constraint - from the information monotonicity property of the constraint propagation algorithm of Allen's temporal system; (2) We formulate a principle of markedness for the basic tense structures based on the computational efficiency of the temporal representations; and (3) We show Allen's interval-based temporal system is not arbitrary, but it can be used to explain independently motivated linguistic constraints on tense and aspect interpretations.","We also claim that the methodology of research developed in this study - \"cross-lever' investigation of independently motivated formal grammatical theory and computational models - is a powerful paradigm with which to attack representational problems in basic cognitive domains, e.g., space, time, causality, etc. This paper proposes an analysis method for Japanese modality. In this purpose, meaning of Japanese modality is classified into four semantic categories and the role of it is formalized into five modality functions. Based on these formalizations, information and constraints to be applied to the modality analysis procedure are specified. Then by combining these investigations with case analysis, the analysis method is proposed. This analysis method has been applied to Japanese analysis for machine translation. This paper presents an implemented theory for quantifying noun phrases in clauses containing copular verbs (e.g., 'be' and 'become'). Proceeding from recent theoretical work by Jackendoff (1983), this computational theory recognizes the dependence of the quantification decision on the definiteness, indefiniteness, or classness of both the subject and object of copular verbs in English. Jackendoff's intuition about the quantificational interdependence of subject and object has been imported from his broader cognitive theory and reformulated within a constraint propagation framework. Extensions reported here include the addition of more active deter-Computational Linguistics, Volume 12, Number 1, January-March 1986 61 The FINITE STRING Abstracts of Current Literature Meinongian Semantics for Propositional Semantic Networks William J. Rapaport Department of Computer Science, State University of New York Buffalo, NY 14260 Proc. ACL, pp. 43-48 Speech Acts and Rationality Philip R. Cohen Artificial Intelligence Center, SRI International & CSLI, Stanford University Hector J. Levesque Department of Computer Science University of Toronto Proc. ACL, pp. 49-60 Ontological Promiscuity Jerry R. Hobbs Artificial Intelligence Center, SRI International & CSLI, Stanford University Proc. ACL, pp. 61-69 Reversible Automata and Induction of the English Auxiliary System Samuel F. Pilate, Robert C. Berwick Artificial Intelligence Laboratory, M.I.T. 545 Technology Square Cambridge, MA 02139 Proc. A CL, pp. 70- 75 The Computational Difficulty of ID/LP Parsing G. Edward Barton, Jr. miners, the expansion of determiner categories, and the treatment of displaced objects. A further finding is that quantificational constraints may propagate across some clausal boundaries. The algorithm is used by the RELATUS Natural Language Understanding System during a phase of analysis that posts constraints to produce a 'constraint tree'. This phase comes after creation of syntactic deep structure and before sentential reference in a semantic-network model. Incorporation of the quantification algorithm in a larger system that parses sentences and builds semantic models from them makes RELATUS able to acquire taxonomic and identity information from text. This paper surveys several approaches to semantic-network semantics that have not previously been treated in the AI or computational linguistics literature, though there is a large philosophical literature investigating them in some detail. In particular, propositional semantic networks (exemplifed by SNePS) are discussed, it is argued that only a fully intensional (\"Meinongian\") semantics is appropriate for them, and several Meinongian systems are presented. This paper derives the basis of a theory of communication from a formal theory of rational interaction. The major result is a demonstration that illocutionary acts need not be primitive, and need not be recognized. As a test case, we derive Searle's conditions on requesting from principles of rationality coupled with a Gricean theory of imperatives. The theory is shown to distinguish insincere or nonserious imperatives from true requests. Extensions to indirect speech acts, and ramifications for natural language systems are also briefly discussed. To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple. In this paper I propose a logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional. The key move is to expand what kinds of entities one allows in one's ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process. Three classical problems - opaque adverbials, the distinction between de re and de dicto belief reports, and the problem of identity in intensional contexts - are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome. The paper closes with a statement about the view of semantics that is presupposed by this approach. In this paper we apply some recent work of Angluin (1982) to the induction of the English auxiliary verb system. In general, the induction of finite automata is computationally intractable. However, Angluin shows that restricted finite automata, the k-reversible automata, can be learned by efficient (polynomial time) algorithms. We present an explicit computer model demonstrating that the English auxiliary verb system can in fact be learned as a 1-reversible automaton, and hence in a computationally feasible amount of time. The entire system can be acquired by looking at only half the possible auxiliary verb sequences, and the pattern of generalization seems compatible with what is known about human acquisition of auxiliaries. We conclude that certain linguistic subsystems may well be feasible by inductive inference methods of this kind, and suggest an extension to context-free language. Modern linguistic theory attributes surface complexity to interacting subsystems of constraints. For instance, the ID/LP grammar formalism separates constraints on immediate dominance from those on linear order. 62 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature Artificial Intelligence Laboratory, M.I.T. 545 Technology Square Cambridge, MA 02139 Proc. ACL, pp. 76-81 Some Computational Properties of Tree Adjoining Grammars K. Vijay-Shankar, Aravind K. Joshi Department of Computer and Information Science Room 268, Moore School/D2 University of Pennsylvania Philadelphia, PA 19104 Proc. ACL, pp. 82-93 TAGs as a Grammatical Formalism for Generation David D. McDonald, James D. Pustejovsky Department of Computer and Information Science University of Massachusetts at Amherst Proc. ACL, pp. 94-103 Modular Logic Grammars Michael C. McCord IBM Thomas J. Watson Research Center P.O. Box 218 Yorktown Heights, NY 10598 Proc. ACL, pp. 104-117 Shieber's (1983) ID/LP parsing algorithm shows how to use ID and LP constraints directly in language processing, without expanding them into an intermediate \"object grammar\". However, Shieber's purported O( I G"]},{"title":"12\"n2)","paragraphs":["runtime bound underestimates the difficulty of ID/LP parsing. ID/LP parsing is actually NP-complete, and the worst-case runtime of Shieber's algorithm is actually exponential in grammar size. The growth of parser data structures causes the difficulty. Some computational and linguistic implications follow; in particular, it is important to note that despite its potential for combinatorial explosion, Shieber's algorithm remains better than the alternative of parsing an expanded object grammar. Tree Adjoining Grammar (TAG) is a formalism for natural language grammars. Some of the basic notions of TAGs were introduced in Joshi, Levy, and Takahashi (1975) and by Joshi (1983). A detailed investigation of the linguistic relevance of TAGs has been carried out in Kroch and Joshi (1985). In this paper, we will describe some new results for TAGs, especially in the following areas: (1) parsing complexity of TAGs, (2) some closure results for TAGs, and (3) the relationship to Head grammars. Tree Adjoining Grammars, or \"TAGs\", (Joshi, Levy & Takahashi 1975; Joshi 1983; Broch & Joshi 1985) were developed as an alternative to the standard syntactic formalisms that are used in theoretical analyses of language. They are attractive because they may provide just the aspects of context sensitive expressive power that actually appear in human languages while otherwise remaining context free.","This paper describes how we have applied the theory of Tree Adjoining Grammars to natural language generation. We have been attracted to TAGs because their central operation - the extension of an \"initial\" phrase structure tree through the inclusion, at very specifically constrained locations, of one or more \"auxiliary\" trees - corresponds directly to certain central operations of our own, performance-oriented theory.","We begin by briefly describing TAGs as a formalism for phrase structure in a competence theory, and summarize the points in the theory of TAGs that are germane to our own theory. We then consider generally the position of a grammar within the generation process, introducing our use of TAGs through a contrast with how others have used systemic grammars. This takes us to the core results of our paper: using examples from our research with well-written texts from newspapers, we walk through our TAG inspired treatments of raising and wh-movement, and show the correspondence of the TAG \"adjunction\" operation and our \"attachment\" process.","In the final section we discuss extensions to the theory, motivated by the way we use the operation corresponding to TAGs' adjunction in performance. This suggests that the competence theory of TAGs can be profitably projected to structures at the morphological level as well as the present syntactic level. This report describes a logic grammar formalism, Modular Logic Grammars, exhibiting a high degree of modularity between syntax and semantics. There is a syntax rule compiler (compiling into Prolog) which takes care of the building of analysis structures and the interface to a clearly separated semantic interpretation component dealing with scoping and the construction of logical forms. The whole system can work in either a one-pass mode or a two-pass mode. In the one-pass mode, logical forms are built directly during parsing through interleaved calls to semantics, added automatically by the rule compiler. In the two-pass mode, syntactic analy-Computational Linguistics, Volume 12, Number 1, January-March 1986 63 The FINITE STRING Abstracts of Current Literature New Approaches to Parsing Conjunctions using Prolog Sandiway Fong, Robert C. Berwick Artificial Intelligence Laboratory, M.I.T. 545 Technology Square Cambridge, MA 02139 Proc. ACL, pp. 118-126 Parsing with Discontinuous Constituents Mark Johnson CSLI & Department of Linguistics, Stanford University, Proc. ACL, pp. 127-132 Structure Sharing with Binary Trees Lauri Karttunen SRI International, CSLI Stanford Martin Kay Xerox PARC, CSLI Stanford Proc. ACL, pp. 133-136 A Structure-Sharing Representation for Unification-Based Grammar Formalisms Fernando C.N. Pereira Artificial Intelligence Center, SRI International & CSLI, Stanford University sis trees are built automatically in the first pass, and then given to the (one-pass) semantic component. The grammar formalism includes two devices which cause the automatically built syntactic structures to differ from derivation trees in two ways: (1) there is a shift operator, for dealing with left-embedding constructions such as English possessive noun phrases while using right-recursive rules (which are appropriate for Prolog parsing.) (2) There is a distinction in the syntactic formalism between strong non-terminals and weak non-terminals, which is important for distinguishing major levels of grammar. Conjunctions are particularly difficult to parse in traditional, phrase-based grammars. This paper shows how a different representation, not based on tree structures, markedly improves the parsing problem for conjunctions. It modifies the union of phrase marker model proposed by Goodal (1984), where conjunction is considered as the linearization of a three-dimensional union of a non-tree based phrased marker representation. A PROLOG grammar for conjunctions using this new approach is given. It is far simpler and more transparent than a recent phrase-based extraposition parser for conjunctions by Dahl and McCord (1984). Unlike the Dahl and McCord or ATN SYSCONJ approach, no special trail machinery is needed for conjunction, beyond that required for analyzing simple sentences. While of comparable efficiency, the new approach unifies under a single analysis a host of related constructions: respectively sentences, right node raising, or gapping. Another advantage is that it is also completely reversible (without cuts), and therefore can be used to generate sentences. By generalizing the notion of location of a constituent to allow discontinuous locations, one can describe the discontinuous constituents of non-configurational languages. These discontinuous constituents can be described by a variant of definite clause grammars, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages. Many current interfaces for natural language represent syntactic and semantic information in the form of directed graphs where attributes correspond to vectors and values to nodes. There is a simple correspondence between such graphs and the matrix notation linguists traditionally use for feature sets. The standard operation for working with such graphs is unification. The unification operation succeeds only on a pair of compatible graphs, and its result is a graph containing the information in both contributors. When a parser applies a syntactic rule, it unifies selected features of input constituents to check constraints and to build a representation for the output constituent."]},{"title":"b. I cat: np ql [-number: sg agr: Lperson: 3rd~J","paragraphs":["This paper describes a structure-sharing method for the representation of complex phrase types in a parser for PATR-II, a unification-based grammar formalism.","In parsers for unification-based grammar formalisms, complex phrase types are derived by incremental refinement of the phrase types defined in 64 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature Proc. ACL, pp. 137-144 Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Formalisms Stuart M. Shieber Artificial Intelligence Center, SRI International & CSLI, Stanford University Proc. ACL, pp. 145-152 Semantic Caseframe Parsing and Syntactic Generality Philip J. Hayes, Peggy M. Anderson, Scott Sailer Carnegie Group Incorporated Commerce Court at Station Square Pittsburgh, PA 15219 Proc. ACL, pp. 153-160 Movement in Active Production Networks Mark A. Jones, Alan S. Driscoll AT&T Bell Laboratories Murray Hill, NJ 07974 Proc. ACL, pp. 161-166 Parsing Head-driven Phrase Structure Grammar Derek Proudian, Carl Pollard Hewlett-Packard Laboratories 1501 Page Mill Road Palo Alto, CA 94303 Proc. ACL, pp. 167-171 grammar rules and lexical entries. In a naive implementation, a new phrase type is built by copying older ones and then combining the copies accord-ing to the constraints stated in the grammar rule. The structure-sharing method was designed to eliminate most such copying; indeed, practical tests suggest that the use of this technique reduces parsing time by as much as 60%.","The present work is inspired by the structure-sharing method for theorem proving introduced by Boyer and Moore and on the variant of it that is used in some Prolog implementations. Grammar formalisms based on the encoding of grammatical information in complex-valued feature systems enjoy some currency both in linguistics and natural-language-processing research. Such formalisms can be thought of by analogy to context-free grammars as generalizing the notion of non-terminal symbol from a finite domain of atomic elements to a possibly infinite domain of directed graph structures of a certain sort. Unfortunately, in moving to an infinite non-terminal domain, standard methods of parsing may no longer be applicable to the formalism. Typically, the problem manifests itself as gross inefficiency or even non-termination of the algorithms. In this paper, we discuss a solution to the problem of extending parsing algorithms to formalisms with possibly infinite non-terminal domains, a solution based on a general technique we call restriction. As a particular example of such an extension, we present a complete, correct, terminating extension of Earley's algorithm that uses restriction to perform top-down filtering. Our implementation of this algorithm demonstrates the drastic elimination of chart edges that can be achieved by this technique. Finally, we describe further uses for the technique - including parsing other grammar formalisms, including definite-clause grammars; extending other parsing algorithms, including LR methods and syntactic preference modeling algorithms; and efficient indexing. We have implemented a restricted domain parser called Plume (trademark of Carnegie Group Incorporated). Building on previous work at Carnegie-Mellon University, Plume's approach to parsing is based on semantic caseframe instantiation. This has the advantages of efficiency on grammatical input, and robustness in the face of ungrammatical input. While Plume is well adapted to simple declarative and imperative utterances, it handles passives and relative clauses and interrogatives in an ad hoc manner, leading to patchy semantic coverage. This paper outlines Plume as it currently exists and describes our detailed design for extending Plume to handle passives, relative clauses, and interrogatives in a general manner. We describe how movement is handled in a class of computational devices called active production networks (APNs). The APN model is a parallel, activation-based framework that has been applied to other aspects of natural language processing. The model is briefly defined, the notation and mechanism for movement is explained, and then several examples are given which illustrate how various conditions on movement can naturally be explained in terms of limitations of the APN device. The Head-driven Phrase Structure Grammar project (HPSG) is an English language database query system under development at Hewlett-Packard Laboratories. Unlike other product-oriented efforts in the natural language understanding field, the HPSG system was designed and implemented by linguists on the basis of recent theoretical developments. But, unlike other implementations of linguistic theories, this system is not a toy, as it deals with a variety of practical problems not covered in the theore-Computational Linguistics, Volume 12, Number 1, January-March 1986 65 The FINITE STRING Abstracts of Current Literature A Computational Semantics for Natural Language Lewis G. Creary, Carl J. Pollard Hewlctt-Packard Laboratories 1501 Page Mill Road Palo Alto, CA 94303 Proc. ACL, pp. 172-179 Analysis of Conjunctions in a Rule-Based Parser Leonard Lesmo, Pietro Torasso Dipartimento di Informatiea Universitd di Torino Via Valperga Caluso 37 10125 Torino, Italy Proc. ACL, pp. 180-187 A Pragmatics-Based Approach to Understanding Intersentential Ellipsis Sandra Carberry Department of Computer and Information Science, University of Delaware Newark, DE 19715 Proc. ACL, pp. 188-197 Some Pragmatic Issues in the Planning of Definite and Indefinite Noun Phrases Douglas E. Appelt Artificial Intelligence Center, SRI International & CSLI, Stanford University Proc. ACL, pp. 198-203 tical literature. We believe that this makes the HPSG system unique in its combination of linguistic theory and practical application.","The HPSG system differs from its predecessor GPSG, reported on at the 1982 ACL meeting (Gawron et al. 1982), in four significant respects: syntax, lexical representation, parsing, and semantics. The paper focuses on parsing issues, but also gives a synopsis of the underlying syntactic formalism. In the new Head-Driven Phrase Structure Grammar (HPSG) language processing system that is currently under development at Hewlett-Packard Laboratories, the Montagovian semantics of the earlier GPSG system (see Gawron et al. 1982) is replaced by a radically different approach with a number of distinct advantages. In place of the lambda calculus and standard first-order logic, our medium of conceptual representation is a new logical formalism called NFLT (Neo-Fregean Language of Thought); compositional semantics is effected, not by schematic lambda expressions, but by LISP procedures that operate on NFLT expressions to produce new expressions. NFLT has a number of features that make it well-suited for natural language translations, including predicates of variable arity in which explicitly marked situational roles supersede order-coded argument positions, sortally restricted quantification, a compositional (but nonextensional) semantics that handles causal contexts, and a principled conceptual raising mechanism that we expect to lead to a computationally tractable account of propositional attitudes. The use of semantically compositional LISP procedures in place of lambda-schemas allows us to produce fully reduced translations on the fly, with no need for post-processing. This approach should simplify the task of using semantic information (such as sortal incompatibilities) to eliminate bad parse paths. The aim of the present paper is to show how a rule-based parser for the Italian language has been extended to analyze sentences involving conjunctions. The most noticeable fact is the ease with which the required modifications fit in the previous parser structure. In particular, the rules written for analyzing simple sentences (without conjunctions) needed only small changes. On the contrary, more substantial changes were made to the exception-handling rules (called \"natural changes\") that are used to restructure the tree in case of failure of a syntactic hypothesis. The parser described in the present work constitutes the syntactic component of the FIDO system (a Flexible Interface for Database Operations), an interface allowing an end-user to access a relational database in natural language (Italian). Intersentential elliptical utterances occur frequently in information-seeking dialogues. This paper presents a pragmatics-based framework for interpreting such utterances, including identification of the speaker's discourse goal in employing the fragment. We claim that the advantage of this approach is its reliance upon pragmatic information, including discourse content and conversational goals, rather than upon precise representations of the preceding utterance alone. In this paper we examine the pragmatic knowledge an utterance-planning system must have in order to produce certain kinds of definite and indefinite noun phrases. An utterance-planning system, like other planning systems, plans actions to satisfy an agent's goals, but allows some of the actions to consist of the utterance of sentences. This approach to language generation emphasizes the view of language as action, and hence assigns a critical role to pragmatics. 66 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature Repairing Reference Identification Failures by Relaxation Bradley A. Goodman BBN Laboratories 10 Moulton Street Cambridge, MA 02238 Proc. ACL, pp. 204-217 Anaphora Resolution: Short-Term Memory and Focusing Raymonde Guindon Microelectronics and Computer Technology Corporation (MCC) 9430 Research Blvd. Austin, TX 78759 Proc. ACL, pp. 218-227 Explanation Structures in XSEL Karen Kukich Computer Science Department Carnegie-Mellon University Pittsburgh, PA 15213 Proc. ACL, pp. 228-237 The goal of this work is the enrichment of human-machine interactions in a natural language environment. We want to provide a framework less restrictive than earlier ones by allowing a speaker leeway in forming an utterance about a task and in determining the conversational vehicle to deliver it. A speaker and listener cannot be assumed to have the same beliefs, contexts, backgrounds, or goals at each point in a conversation. As a result, difficulties and mistakes arise when a listener interprets a speaker's utterance. These mistakes can lead to various kinds of misunderstandings between speaker and listener, including reference failures or failure to understand the speaker's intention. We call these misunderstandings miscommunication. Such mistakes constitute a kind of \"ill-formed\" input that can slow down and possibly break down communication. Our goal is to recognize and isolate such miscommunications and circumvent them. This paper will highlight a particular class of miscommunication - reference problems - by describing a case study, including techniques for avoiding failures of reference. Anaphora resolution is the process of determining the referent of anaphors, such as definite noun phrases and pronouns, in a discourse. Computational linguists, in modeling the process of anaphora resolution, have proposed the notion of focusing. Focusing is the process, engaged in by a reader, of selecting a subset of the discourse items and making them highly available for further computations. This paper provides a cognitive basis for anaphora resolution and focusing. Human memory is divided into a short-term, an operating, and a long-term memory. Short-term memory can only contain a small number of meaning units and its retrieval time is fast. Short-term memory is divided into a cache and a buffer. The cache contains a subset of meaning units expressed in the previous sentences and the buffer holds a representation of the incoming sentence. Focusing is realized in the cache that contains a subset of the most topical units and a subset of the most recent units in the text. The information stored in the cache is used to integrate the incoming sentence with the preceding discourse. Pronouns should be used to refer to units in focus. Operating memory contains a very large number of units but its retrieval time is slow. It contains the previous text units that are not in the cache. It comprises the text units not in focus. Definite noun phrases should be used to refer to units not in focus. Two empirical studies are described that demonstrate the cognitive basis for focusing, the use of definite noun phrases to refer to antecedents not in focus, and the use of pronouns to refer to antecedents in focus. Expert systems provide a rich testbed from which to develop and test techniques for natural language processing. These systems capture the knowledge needed to solve real-world problems in their respective domains, and that knowledge can and should be exploited for testing computational procedures for natural language processing. Parsing, semantic interpretation, dialog monitoring, discourse organization, and text generation are just a few of the language processing problems that might take advantage of the pre-structured semantic knowledge of an expert system. In particular, the need for explanation generation facilities for expert systems provides an opportunity to explore the relationships between the underlying knowledge structures needed for automated reasoning and those needed for natural language processing. One such exploration was the development of an explanation generator for XSEL, which is an expert system that helps a salesperson in producing a purchase order for a computer system. This paper describes a technique called \"link-dependent message generation\" that forms the basis for explanation generation in XSEL. Computational Linguistics, Volume 12, Number 1, January-March 1986 67 The FINITE STRING Abstracts of Current Literature Description Strategies for Naive and Expert Users C~cile L. Paris Department of Computer Science Columbia University New York, NY 10027 Proc. ACL, pp. 238-245 Stress Assignment in Letter to Sound Rules for Speech Synthesis Kenneth Church AT&T Bell Laboratories Proc. ACL, pp. 246-253 An Eclectic Approach to Building Natural Language Interfaces Brian Phillips, Michael J. Freiling, James H. Alexander, Steven L. Messlck, Steve Rehfuss, Sheldon Nicholl Tektronix, Inc. P.O. Box 500, M/S 50-662 Beaverton, OR 97077 Proc. ACL, pp. 254-261 Structure-Sharing in Lexical Representations Daniel Flickinger, Carl Pollard, Thomas Wasow Hewlett-Packard Laboratories 1501 Page Mill Road Palo Alto, CA 94303 Proc. ACL, pp. 262-267 A Tool Kit for Lexicon Building Thomas E. Ahlswede Computer Science Department Illinois Institute of Technology Chicago, IL 60616 Proc. ACL, pp. 268-276 It is widely recognized that a question-answering system should be able to tailor its answers to the user. One of the dimensions along which this tailoring can occur is with respect to the level of knowledge of a user about a domain. In particular, responses should be different depending on whether they are addressed to naive or expert users. To understand what these differences should be, we analyzed texts from adult and junior encyclopedias. We found that two different strategies were used in describing complex physical objects to juniors and adults. We show how these strategies have been implemented on a test database. This paper will discuss how to determine word stress from spelling. Stress assignment is a well-established weak point for many speech synthesizers because stress dependencies cannot be determined locally. It is impossible to determine the stress of a word by looking through a five or six character window, as many speech synthesizers do. Well-known examples such as degrdde / ddgradation and tdlegraph / teldgraphy demonstrate that stress dependencies can span over two and three syllables. This paper will present a principled framework for dealing with these long distance dependencies. Stress assignment will be formulated in terms of Waltz' style constraint propagation with four sources of constraints: (1) syllable weight, (2) part of speech, (3) morphology and (4) etymology. Syllable weight is perhaps the most interesting, and will be the main focus of this paper. Most of what follows'has been implemented. INKA is a natural language interface to facilitate knowledge acquisition during expert system development for electronic instrument trouble-shoot-ing. The expert system design methodology develops a domain definition, called GLIB, in the form of a semantic grammar. This grammar format enables GLIB to be used with the INGLISH interface, which constrains users to create statements within a subset of English. Incremental parsing in INGLISH allows immediate remedial information to be generated if a user deviates from the sublanguage. Sentences are translated into production rules using the methodology of lexical-functional grammar. The system is written in Smalltalk and, in INKA, produces rules for a 15rolog inference engine. The lexicon now plays a central role in our implementation of a Head-driven Phrase Structure Grammar (HPSG), given the massive relocation into the lexicon of linguistic information that was carried by the phrase structure rules in the old GPSG system. HPSG's grammar contains fewer than twenty (very general) rules; its predecessor required over 350 to achieve roughly the same coverage. This simplification of the grammar is made possible by an enrichment of the structure and content of lexical entries, using both inheritance mechanisms and lexical rules to represent the linguistic information in a general and efficient form. We will argue that our mechanisms for structure-sharing not only provide the ability to express important linguistic generalizations about the lexicon, but also make possible an efficient, readily modifiable implementation that we find quite adequate for continuing development of a large natural language system. This paper describes a set of interactive routines that can be used to create, maintain, and update a computer lexicon. The routines are available to the user as a set of commands resembling a simple operating system. The lexicon produced by this system is based on lexical-semantic relations, but is compatible with a variety of other models of lexicon structure. The lexicon builder is suitable for the generation of moderate-sized vocabularies and 68 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature Using an On-line Dictionary to Find Rhyming Words and Pronunciations for Unknown Words Roy ~. Byrd IBM Thomas J. Watson Research Center Yorktown Heights, NY 10598 Martin S. Chodorow Department of Psychology, Hunter College of CUNY & IBM Thomas J. Watson Research Center Proc. ACL, pp. 277-283 Towards a Self-Extending Lexicon Uri Zernick, Michael (7,. Dyer Artificial Intelligence Laboratory Computer Science Department 3531 Boelter Hall University of California Proc. ACL, pp. 284-292 Grammatical Analysis by Computer of the Lancaster.Oslo/Bergen (LOB) Corpus of British English Texts Andrew David Beale Unit for Computer Research on the English Language Bowland College, University of Lancaster Bailrigg, Lancaster, England LA1 4YT Proc. ACL, pp. 293-298 Extracting Semantic Hierarchies from a Large On-Line Dictionary Martin S. Chodorow Department of Psychology Hunter College of CUNY & IBM Thomas J. Watson Research Center has been used to construct a lexicon for a small medical expert system. A future version of the lexicon builder will create a much larger lexicon by parsing definitions from machine,readable dictionaries. Humans know a great deal about relationships among words. This paper discusses relationships among word pronunciations. We describe a computer system which models human judgement of rhyme by assigning specific roles to the location of primary stress, the similarity of phonetic segments, and other factors. By using the model as an experimental tool, we expect to improve our understanding of rhyme. A related computer model will attempt to generate pronunciations for unknown words by analogy with those for known words. The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words. As in the case of rhyme, the computer model will be an important tool for improving our understanding of these processes. Both models serve as the basis for functions in the WordSmith automated dictionary system. The problem of manually modifying the lexicon appears with any natural language processing program. Ideally, a program should be able to acquire new lexical entries from context, the way lbeople learn. We address the problem of acquiring entire phrases, specifically figurative phrases, through augmenting a phrasal lexicon. Facilitating such a self-extending lexicon involves (a) disambiguation - selection of the intended phrase from a set of matching phrases, (b) robust parsing - comprehension of partially-matching phrases, and (c) error analysis - use of errors in forming hypotheses about new phrases. We have designed and implemented a program called RINA which uses demons to implement functional-grammar principles. RINA receives new figurative phrases in context and through the application of a sequence of failure-driven rules, creates and refines both the patterns and the concepts which hold syntactic and semantic information about phrases. Research has been under way at the Unit for Computer Research on the English Language at the University of Lancaster, England, to develop a suite of computer programs which provide a detailed grammatical analysis of the LOB corpus, a collection of about 1 million words of British English texts available in machine readable form.","The first phase of the project, completed in September 1983, produced a grammatically annotated version of the corpus giving a tag showing the word class of each word token. Over 93 per cent of the word tags were correctly selected by using a matrix of tag pair probabilities and this figure was upgraded by a further 3 per cent by retagging problematic strings of words prior to disambiguation and by altering the probability weightings for sequences of three tags. The remaining 3 to 4 per cent were corrected by a human post-editor.","The system was originally designed to run in batch mode over the corpus but we have recently modified procedures to run interactively for sample sentences typed in by a user at a terminal. We are currently extending the word tag set and improving the word tagging procedures to further reduce manual intervention. A similar probabilistic system'is being developed for phrase and clause tagging. Dictionaries are rich sources of detailed semantic information, but in order to use the information for natural language processing, it must be organized systematically. This paper describes automatic and semi-automatic procedures for extracting and organizing semantic feature information implicit in dictionary definitions. Two head-finding heuristics are described for locating the genus terms in noun and verb definitions. The Computational Linguistics, Volume 12, Number 1, January-March 1986 69 The FINITE STRING Abstracts of Current Literature Yorktown Heights, NY 10598 Roy J. Byrd, George E. Heidorn IBM Thomas J. Watson Research Center Proc. ACL, pp. 299-304 assumption is that the genus term represents inherent features of the word it defines. The two heuristics have been used to process definitions of 40,000 nouns and 8,000 verbs, producing indexes in which each genus term is associated with the words it defined. The Sprout program interactively grows a taxonomic \"tree\" from any specified root feature by consulting the genus index. Its output is a tree in which all of the nodes have the root feature for at least one of their senses. The Filter program uses an inverted form of the genus index. Filtering begins with an initial filter file consisting of words that have a given feature (e.g., [+human]) in aU of their senses. The program then locates, in the index, words whose genus terms all appear in the filter file. The output is a list of new words that have the given feature in all of their senses. Dictionaries of the Mind George A. Miller Department of Psychology Princeton University Princeton, NJ 08544 Proc. ACL, pp. 305-314 How lexical information should be formulated, and how it is organized in computer memory for rapid retrieval, are central questions for computational linguists who want to create systems for language understanding. How lexical knowledge is acquired, and how it is organized in human memory for rapid retrieval during language use, are also central questions for cognitive psychologists. Some examples of psycholinguistic research on the lexical component of language are reviewed with special attention to their implications for the computational problem. The Use of Syntactic Clues in Discourse"]},{"title":"Processing Nan Decker","paragraphs":["1834 Chase Avenue Cincinnati, OH 45223 Proc. ACL, pp. 315-323 The desirability of a syntactic parsing component in natural language understanding systems has been the subject of debate for the past several years. This paper describes an approach to automatic text processing which is entirely based on syntactic form. A program is described which processes one genre of discourse, that of newspaper reports. The program creates summaries of reports by relying on an expanded concept of text grounding: certain syntactic structures and tense/aspect pairs indicate the most important events in a news story. Supportive, background material is also highly coded syntactically. Certain types of information are routinely expressed with distinct syntactic forms. Where more than one episode occurs in a single report, a change of episode will also be marked syntactically in a reliable way. Grammar Viewed as a Functioning Part of a Cognitive System Helen M. Gigley Department of Computer Science University of New Hampshire Durham, NH 03824 Proc. ACL, pp. 324-332 How can grammar be viewed as a functional part of a cognitive system? Given a neural basis for the processing control paradigm of language performance, what roles does \"grammar\" play? Is there evidence to suggest that grammatical processing can be independent from other aspects of language processing?","This paper will focus on these issues and suggest answers within the context of one computational solution. The example mode of sentence comprehension, HOPE, is intended to demonstrate both representational considerations for a grammar within such a system as well as to illustrate that by interpreting a grammar as a feedback control mechanism of a \"neural-like\" process, additional insights into language processing can be obtained.","Selected Dissertation Abstracts Compiled by Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20894 Bob Krovetz, University of Massachusetts, Amherst, MA 01002 The following are citations selected by title and abstract as being related to computational linguistics or knowledge representation, resulting from a computer search, using the BRS Information Technologies retrieval service, of the Dissertation Abstracts International DAD database produced by University Microfilms International. 70 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature Included are the title; author; university, degree, and, if available, number of pages; DAI subject category chosen by the author of the dissertation; and UM order number and year-month of DAI. References are sorted first, by DAI subject category and second by author. Unless otherwise specified, paper or microform copies of dissertations may be ordered from University Microfilms International Dissertation Copies Post Office Box 1764 Ann Arbor, MI 48106 telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042,","for Canada: 1-800-268-6090. Price lists and other ordering and shipping information are in the introduction to the published DAI. An alternate source for copies is sometimes provided at the end of the abstract.","NOTICE The dissertation titles and abstracts contained here are published with the permission of University Microfilms International, publishers of Dissertation Abstracts International (copyright 1985) by University Microfilms International), and may not be reproduced without their prior permission. Decision-Making in Manufacturing: An Information Processing Approach Snranjan De Purdue University Ph.D. 1984, 211 pages Business Administration, Management University Microfilms Order Number ADG85-O0360. 8504. The broad objective of this research is to employ concepts from optimiza-tion theory, artificial intelligence, and computer architecture to the analysis of organizational problem-solving. Abstract organizations can be characterized as consisting of information processors or resources that are interconnected through a network. A limited commonality of interest exists between the processors. The prirnary task is to use the resources to carry out some tasks or achieve some goals.","The purpose of this research is to model the manufacturing component of an organization and to explore various scheduling problems associated with this approach. The computer-based information system that models such a problem-solving system consists of three components: a knowledge base in which both general and domain-dependent knowledge will be represented, a natural language interface through which the user can communicate with the system, and a problem processor that carries out a combination of data retrieval, computation and deduction in order to respond to queries posed by the user.","An axiomatic framework based on equational logic is proposed to model the computer-based information system. The use of this framework to represent environmental and linguistic knowledge as well as perform the language processing and problem-solving functions in a uniform manner is demonstrated.","As an illustration of the functioning of the problem-solving component, a framework for the on-line scheduling of jobs as they enter the system is introduced. A heuristic solution based on artificial intelligence techniques is proposed for the off-line or static scheduling problem. The solution is then extended to a more dynamic environment where jobs continually arrive over time.","Finally, a model for the decentralized control of a general multilevel manufacturing system is proposed. Future research directions are suggested. Communication and Miscommunication Bradley Alan Goodman University of Illinois at Urbana-Champaign, Ph.D. 1984, 264 pages Computer Science University Microfilms Order Number ADG85-02154. 8505. This thesis discusses one aspect of enabling people to communicate in natural language with computers. The central focus of this work is a study on how one could build robust natural language processing systems that can detect and recover from miscommunication. The study of miscommunication is a necessary task within such a context since any computer capable of communicating with humans in natural language must be tolerant of the imprecise, ill-devised or complex utterances that people often use. This goal first requires an inquiry into how people communicate and how they recover from problems in communication. That investigation centers on the kinds of miscommunication that occur in human communication with a Computational Linguistics, Volume 12, Number 1, January-March 1986 71 The FINITE STRING Abstracts of Current Literature A Syntactic Approach to Three-Dimensional Object Representation and Recog-nition Wei-Chung Lin Purdue University Ph.D. 1984, 218 pages Computer Science University Microfilms Order Number ADG85-O0404. 8504. Information Modeling and Sharing in Highly Autonomous Database Systems Peter Lyngbaek University of Southern California Ph.D. 1984 Computer Science This item is not available from University Microfilms International ADG05-55439. 8505. special emphasis on reference problems, i.e., problems a listener has determining whom or what a speaker is talking about. A collection of protocols of a speaker explaining to a listener how to assemble a toy water pump was studied and the common errors seen in speakers' descriptions were categorized. This study led to the development of techniques for avoiding failures of reference that were employed in the reference identification component of a natural language understanding program.","The traditional approaches to reference identification in previous natural language systems were found to be less elaborate than people's real behavior. In particular, listener's often find the correct referent even when the speaker's description does not describe any object in the world. To model a listener's behavior, a new component was added to the traditional reference identification mechanism to resolve difficulties in a speaker's description. This new component uses knowledge about linguistic and physical context in a negotiation process that determines the most likely places for error in the speaker's utterance. The actual repair of the speaker's description is achieved by using the knowledge sources to apply relaxa-tion techniques that delete or replace portions of the description. The algorithm developed more closely approximates people's behavior. The syntactic approach to pattern representation and scene analysis has received increasing attention due to its unique capability in handling pattern structures and their relationships. However, it has been applied mostly on one and two-dimensional pattern recognition problems. The difficulties of syntactic approach in dealing with three-dimensional objects or scenes are caused by (1) model primitives are described in an observercentered coordinate system; (2) lack of the mechanisms for relating two-dimensional image to three-dimensional objects, and (3) projections of some primitives in the image are invisible or partially occluded.","In this thesis, we propose a syntactic approach to three-dimensional object recognition from a single view. The system consists of two major parts: analysis and recognition. The analysis part consists of primitive surface patches selection and modeling grammar construction. The recognition part consists of preprocessing, image segmentation, visible primitive surface identification, camera model estimation, and structural analysis. In the modeling phase, a three-dimensional object model is represented by using surface patches as primitives and 3D-plex grammar rules as structural relationship descriptors. Several algorithms to extract useful information for recognition from a given 3D-plex grammar are presented. The recognition task starts with preprocessing and image segmentation. Then, the transformation from three-dimensional object space to two-dimensional image space is determined by a camera model estimation procedure. The final phase of the recognition is carded out by a semantic-directed top-down backtrack recognizer. The past several years have seen a dramatic proliferation of personal computers, and anticipated future technological advances will continue to increase their power and reduce their cost. Perhaps one of the most excit-ing and far-reaching potential uses of a personal computer is as an information manager and a tool for information sharing and communication. This dissertation addresses the problems of information modeling and sharing in a collection of personal databases. Each database contains a number of information objects that may be related to other objects in other data-bases. Objects correspond to concepts or things with an associated meaning.","Two related aspects of object-oriented information modeling are described. First, a simple distributed database model is defined. The model provides a basic set of kernel operations for object definition, 72 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature A Resource Oriented Formalism for Plan Generation Paul Henry Morris University of California, Irvine, Ph.D. 1984, 165 pages Computer Science University Microfilms Order Number ADG85-02995. 8506. Conjunctive Conceptual Clustering: A Methodology and Experimentation Robert Earl Stepp III University of Illinois at Urbana-Champaign Ph.D. 1984, 208 pages Computer Science University Microfilms Order Number ADG85-02306. 8506. manipulation, and retrieval. Second, a high-level distributed database model is defined as a specific approach to personal information management in a network of personal workstations. Critical research issues include information modeling, object and database naming, object equivalence, object scoping, supporting database autonomy, and supporting the dynamics of the network structure, the information structure, and the information itself.","(Copies available exclusively from Micrographics Department, Doheny Library, USC, Los Angeles, CA 90089.) A formulation of planning based on multisets is introduced. Actions are represented by a simple well-behaved formalism which is related both to Petri nets and to predicate logic. It is inherently resource and quantity oriented and more accurately models constraints imposed by resource limitations.","The consequences of this for plan generation are explored. The formalism is simple enough to allow theoretical issues to be investigated. Parallel plans are represented by a concise graphical structure with the property that a syntactically complete plan is necessarily correct. Abstract algorithms for plan generation with provable completeness properties are exhibited. Techniques for recognizing and escaping from vicious circles are studied. These turn out to also have the effect of excluding certain correct but nonsensical plans, and allow the automatic derivation of planning-time checks on the appropriateness of specific operators, but cause incompleteness in some domains.","Novel methods for efficient planning that arise naturally from the formalism are described. Cut and splice operations on plan graphs permit the order of goal selection to be based on efficiency rather than correct-ness considerations. The multiset based formalism is particularly suited to developing the notion of an invariant, or conservation law. Invariants are exploited in a technique called goal targeting which provides an early outline of the final plan. This helps to eliminate misdirected search and facilitates certain intelligent goal selection and variable binding mechanisms. This thesis describes a machine learning methodology called conjunctive conceptual clustering. The methodology can find conceptual patterns in data as illustrated by three sample problems. In one problem, the method is used to rediscover categories of soybean disease when given a collection of 47 descriptions of diseased soybeans having one of four diseases. In a second problem, the method is used to find categories underlying a collection of blocks-world structures. In a third problem, categories of objects having a more complex structure are determined and contrasted with categories generated by people.","The described method of conjunctive conceptual clustering forms clusters of objects (or situations) not on the basis of a numerical similarity measure but on the basis of the \"conceptual cohesiveness\" of one object to another. The conceptual cohesiveness between two objects depends on the descriptions of the two objects as well as the descriptions of other nearby objects in the given collection and concepts which are available to describe object groups or object configurations as a whole. From a collection of objects, some background domain knowledge, and a goal or purpose for clustering, conceptual clustering generates a hierarchical classification composed of clusters of objects and corresponding conjunctive-form cluster descriptions (concepts). Conceptual clustering is one paradigm of \"learning from observation\" in which no teacher guides the learning process. Computational Linguistics, Volume 12, Number 1, January-March 1986 73 The FINITE STRING Abstracts of Current Literature Question Understanding: Effects on Children's Comprehension of Stories Lilli Kormos McGill University (Canada) Ph.D. 1984 Education, Psychology This item is not available from University Microfilms International. ADG05-55319. 8504. Speechkit: A Development Environment for Speech Systems Research Charles Jackson Cotton University of Delaware Ph.D. 1984, 126 pages Engineering, Electronics and Electrical University Microfilms Order Number ADG85-O0862. 8505. The Retrieval Expert Model of Information Retrieval Scott Craig Deerwester Purdue University Ph.D. 1984, 102 pages Information Science University Microfilms Order Number ADG85-O0361. 8504. Schema Theory in the Representation and Analysis of Text Sherrilynne Shirley Fuller University of Southern California Ph.D. 1984, 189 pages Information Science University Microfilms Order Number ADG85-O0206. 8504. This study investigated the effects of different types of questions on discourse comprehension. In addition, it examined performance on questions and its influence on comprehension. Within a theoretical framework of discourse processing, the research focused on question type, passage structure, and individual differences in comprehension. Comprehension was measured by analyzing propositions recalled and inferred during free recall. Performance on questions was measured by analyzing answers foc the presence of particular types of inferences.","Third grade children read selected fictional passages, answered questions about them, and recalled them. Results revealed that the effects of questions on comprehension are complex, interacting with passage structure, reading level, and response type. Findings also indicated that questions influence the selective processing of propositional information in text. Question-answering performance was found to reflect an interaction between question type and passage structure. Furthermore, ability to generate the appropriate inferences in responding to questions facilitated text comprehension. This dissertation describes the implementation and usage of a worksta-tion-like environment called Speechkit. Speechkit is a tool that eases the development and testing of speech and signal processing systems.","Speechkit is composed of three components: Nessie, Wed, and Spools. Nessie, a local area network based signal server, provides quick and convenient access to analog signals from a host computer. Wed, a screen-based signal editor, is a powerful, interactive tool for examining signal data files. Spools, a short name for speech tools, is a collection of utilities for manipulating and processing speech signals.","The application of Speechkit to the developments of several isolated word recognition systems is additionally described. The developed recognition systems form test beds for the evaluation of experimental speech recognition system components. The purpose of an information retrieval system is to meet information needs. People who are expert at meeting information needs go about satisfying them much differently and, in general, more successfully than automated systems. The model that forms the basis for this dissertation is a descriptive model of how these experts satisfy information needs. This model can be used prescriptively in the design of an information retrieval system whose performance is similar to that of a human expert. One of the most pressing issues in perfecting document retrieval methods is the establishment of rationale criteria for deciding how to index stored documents for purposes of later retrieval. Approaches to indexing have tended to focus on the word or sentences level rather than on the structure of the whole document. Linguists and cognitive psychologists suggest that texts have an inherent structure termed a schema which is used by individuals not only to understand texts but also to produce texts. Further, text schemata of different groups of individuals are thought to vary.","The purpose of this study is to test schema theory in the analysis and representation of text, namely published reports of clinical trials. The hypotheses are: (1) A clinical trial schema constructed from the expert opinions of what constitutes a clinical trial is replicated in the published reports of clinical trials, (2) The gross schema structure of trial reports is similar, however, representation of specific schematic elements varies significantly between diseases.","A trial schema consisting of essential trial elements was designed based upon a review of the literature. A random sample of trial reports was selected by a search of the Medline database and stratified by disease cate-74 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature gory. Trial reports in eleven disease categories were studied: Bacterial and Fungal, Virus, Parasitic, Oncologic, Musculoskeletal, Mouth and Tooth, Respiratory, Nervous System, Eye, Digestive System and Otorhinolaryngo-logic Diseases.","Results of the schema analysis revealed that trial reports, in general, are reflective of the clinical trial as envisioned by the experts. However, based on chi-square tests, major differences can be demonstrated between disease categories in the representation of specific trial elements. Further, tests of the inter-rater reliability of schema application to trial reports revealed a concurrence of between 74% and 95% for five indexers.","The findings suggest that schema theory may offer a paradigm for indexing which provides a means of capturing both the intra-document, i.e., the holistic structure of the document as well as the inter-document relationships, i.e., data on groups of documents. Further study is indicated.","(Copies available exclusively from Micrographics Department, Doheny Library, USC, Los Angeles, CA 90089.) A Functional Approach to English Sentence Stress Kathleen Bardovi-Harlig The University of Chicago Ph.D. 1983 Language, Linguistics This item is not available from University Microfilms International ADG03-69962. 8504. (No abstract.) Some Constraints Consideration on Conversational Interactions of Politeness and Relevance with Grice's Second Maxim of Quantity Susan Kay Donaldson University of Illinois at Urbana-Champaign Ph.D. 1984, 624 pages Language, Linguistics University Microfilms Order Number ADG85-02128. 8505. In his by now well-known paper \"Logic and conversation\", philosopher of language Paul Grice establishes four maxims speakers follow in conversing: maxims of quantity, quality, relation, and manner. The maxim of quantity he divides into two parts, saying that conversational participants must give enough information to each other, but must not give too much. However, after once establishing this maxim, Grice immediately casts doubt on its validity, saying that its second part is adequately covered by the maxim of relation, which states that what one says should be relevant--that is, that any remark that would be considered overinformative would be discounted by its being irrelevant, anyway, thus eliminating the need for the second half of the maxim of quantity.","This dissertation, employing examples from both tape-recorded 'real' conversations and conversations from short stories and novels, argues that Grice's first intuitions were correct, namely, that the second half of the maxim of quantity is both valid and necessary. Speakers refrain, at times, from conveying to one another information that could be highly relevant to the material at hand, the thesis maintains, for reasons that stem in large part from consideration for one another. A lengthy review of the literature is included, as well as a chapter distinguishing conversation from other sorts of verbal interaction, and one on the nature of consideration and precedents from the literature on human interaction for consideration as a valid form of motivation. Transcripts of four 'real' conversations follow the text. The Functional Role of the Closed Class Vocabulary in Children's Language Processing Carmen Egido Massachusetts Institute of Technology Ph.D. 1983 Language, Linguistics This item is not available from University Microfilms International (No abstract) Computational Linguistics, Volume 12, Number 1, January-March 1986 75 The FINITE STRING Abstracts of Current Literature ADG03-69666. 8504. Syntactic Affixation. Nigel Alexander Fabb Massachusetts Institute of Technology Ph.D. 1984 Language, Linguistics This item is not available from University Microfilms International ADG03.69503. 8504. Grammatical Configurations and Grammatical Relations Yehuda Nahum Falk Massachusetts Institute of Technology Ph.D. 1984 Language, Linguistics This item is not available from University Microfilms International. ADG03-69504. 8504. Generalized Phrase Structure Grammars, Head Grammars, and Natural Language Carl Jesse Pollard Stanford University Ph.D. 1984, 255 pages Language, Linguistics University Microfilms Order Number ADG84-29549. 8504. (No abstract.) (No abstract.) This thesis sets forth the elements of head grammar (HG), a linguistic framework which takes as one of its chief primitives the notion of a grammatical head. The purpose of the study is to shed light upon certain central questions of linguistic theory relating to computational complexity and grammatical organization.","HG's are a class of systems which slightly exceed the power of context-free grammars (CFGs) by manipulating strings containing a distinguished element called the head; in addition to concatenation, HG's permit headwrapping operations which insert one string into another at a point adja-cent to the latter's head. HGs share most of the formal and computational properties of CFGs; unlike CFGs, however, their expressive power suffices to provide a linguistically motivated account of discontinuous constituents.","Like GPSG and categorial grammar, HG posits only one level of grammatical structure; thus no recourse is made to the D-structures and Logical Form of Government-Binding Theory or the F-structures of LFG. Unlike GPSG, however, HG shares with LFG the lexical encoding of syntactic-semantic subcategorization information and the expression of paradigmatic regularities by lexical rules. The resulting system handles the kinds of dependencies that arise in natural language without high-powered mechanisms such as transformations, metarules, or semantic filtering.","In HG, as in GPSG, linguistic information is encoded in grammatical categories and propagated by a small set of local constraints (Head Feature Principle, Binding Inheritance Principle, Control Agreement Principle, etc.). Unlike GPSG, however, HG features may take category sequences as values; in addition, disjunctive specifications are permitted. Consequently both subcategorization and multiple unbounded dependencies can be handled by sequence-valued features; at the same time, feature \"instantiation\" is eliminated in favor of unification with concomitant reduction in the number and complexity of rules.","The theoretical discussions are illustrated with accounts of key English grammatical phenomena, including the following: agreement; subcategorization for subject, objects, and controlled complements; constituent order; subject-auxiliary \"inversion\"; equi and raising; natural quantifier scope; transparent/opaque ambiguity; existential there; passivization; topicaliza-tion; the tough-construction; multiple \"extractions\"; parasitic gaps; possessives; and reflexivization. In addition, a detailed account is provided of the cross-serial construction in Dutch subordinate clauses. 76 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature The Syntactic Forms of Predication. Susan Deborah Rothstein Massachusetts Institute of Technology Ph.D. 1983 Language, Linguistics This item is not available from University Microfilms International ADG03-69668. 8504. Syntax and Semantics of Resumptive Pronouns Peter Sells University of Massachusetts Ph.D. 1984, 508 pages Language, Linguistics University Microfilms Order Number ADG85-00135. 8504. Prosodic Constraints and Lexical Parsing Strategies Lori Alice Taft University of Massachusetts Ph.D. 1984, 309 pages Language, Linguistics University Microfilms Order Number ADG85-O0140. 8504. (No abstract) This work is intended as an attempt to bring the phenomenon of resumptive pronouns under the scrutiny of analytical techniques current in generative grammar, particularly the syntactic theories of Government and Binding and Generalized Phrase Structure Grammar. It is claimed that the notion 'resumptive pronoun' finds a definition within generative grammar that is both interesting typologically and theoretically. It is argued that not all apparent instances of resumptive pronouns are truly so, and that languages may differ typologically in whether their grammars countenance 'true' resumptive pronouns or not.","Data from Swedish, Hebrew, Irish and Welsh is considered in some detail and it is claimed that there is no universal uniformity in the grammatical devices a language may employ in its system of resumptive pronouns. The consequences of the proposed account of the data from the four mentioned languages for Government-Binding theory and Generalized Phrase Structure Grammar are discussed and integrated into the particular (sub-) theories that relate to them.","Semantically, it is claimed that resumptive pronouns show the characteristics expected of pronouns rather than gaps (empty categories); data from Hebrew are presented to show that there are systematic differences in interpretation for constructions depending on whether the construction contains an empty category or a resumptive pronoun. These semantic issues are presented within the framework of Discourse Representation Theory as developed by Hans Kamp.","Finally, data from English are presented to elaborate on the semantic nature of resumptive pronouns; it is claimed that English lacks resumptive pronouns and has instead what are dubbed 'intrusive' pronouns. The properties of the interpretation of intrusive pronouns are shown to follow from general and independent principles, providing support for the analysis in terms of Discourse Representation Structures. Among the concerns of current psycholinguistic research has been an attempt to specify the constraints available and used in the processing of linguistic material. Much of this work has focussed on the syntactic constraints which the processor uses in imposing structure on the incoming lexical material. In this thesis I argue that the case for phonological processing is not so different from the case for syntactic processing. I make the working assumption that grammatical information of various types is available to listeners to impose an interpretation on the incoming signal, not just at the level of syntactic parsing, but prior to that stage as well.","I hypothesize that the listener is using knowledge of language-specific prosodic constraints to impose structure on the incoming material and to identify likely places for word onsets, independent of lexical access itself. Two hypotheses are advanced concerning how constraints are used by the listener. First, I hypothesize a \"Salience-to-Onset Strategy\" which helps the listener locate word onsets on the basis of salient portions of the signal. Second, I hypothesize a \"Prosodic Domain Strategy\" with which the listener parses the incoming material into prosodic units for comparison with stored lexical representations. Computational Linguistics, Volume 12, Number 1, January-March 1986 77 The FINITE STRING Abstracts of Current Literature Particle Ellipses in Japanese Michio Tsutsui University of Illinois at Urbana-Champaign Ph.D. 1984, 180 pages Language, Linguistics University Microfilms Order Number ADG85-02325. 8505. Can Our Quantifiers Range over All Collections? Thomas Charles Antognini Massachusetts Institute of Technology Ph.D. 1984 Philosophy This item is not available from University Microfilms International ADG03-69725. 8504. Proper Names, Beliefs, and Definite Descriptions Thomas Charles Ryckman University of Massachusetts Ph.D. 1984, 197 pages Philosophy University Microfilms Order Number ADG85-O0131. 8504.","Supporting evidence is presented in a series of four psycholinguistic experiments testing the predictions of the SOS and PDS. Experiments 1-3 test the claims of the SOS and PDS for English. Experiment 4 is a pilot study testing the claims of the sos and PDS for Japanese.","I conclude by considering the implications of this work for psycholinguistic models of lexical access, and for explanations of certain phonological phenomena found in natural languages. In this study I consider two kinds of particle ellipsis, syntactic particle ellipsis and conversational particle ellipsis. Syntactic particle ellipsis (discussed in Chapter 1) is the type of particle ellipsis caused by certain syntactic conditions. Only case particles are elliptic under these conditions. When one of these conditions is satisfied, the naturalness of case particle ellipsis (or retention) varies from \"oto natural\" to \"oto unnatural\" depending on the particle and the function it performs. To describe this phenomenon, I hypothesize that there is a hierarchical relation among case particles which perform different functions regarding their ellipsis and retention. Statistical data supports this hypothesis.","Conversational particle ellipsis (discussed in Chapters 2, 3 and 4) is another type of particle ellipsis which takes place in conversation even if none of the conditions for syntactic particle ellipsis are satisfied.","Chapter 2 discusses the ~llipsis of the topic marker wa in conversation. Here, I show that the ellipsis of wa marking X is natural if the speaker and the hearer maintain close contact with the referent of X at the moment of speech.","Chapter 3 presents some general rules of case particle ellipsis in conversation. In one of these rules I claim that the ellipsis of the case particle (CP) of an NP-CP is unnatural if the NP-CP conveys the idea of exclusivity, i.e., the idea \"not others but X\" or \"X and only X\".","Chapter 4 discusses the ellipsis of the case particles ga and o in conversation. Here, I demonstrate that the ellipsis of ga in an utterance is natural if the speaker believes the utterance carries a certain kind of information. I also show that the position of an NP-ga or NP-o in a sentence affects the naturalness of the ellipsis of ga or o.","This study reveals that statements like \"recoverable particles can be elliptic\" cannot explain the ellipsis of particles, that. the ellipsis of particles is a matter of degree of naturalness rather than naturalness versus unnaturalness, and that various aspects of language, including phonology, syntax, information and situation, have bearing on the ellipsis of particles. (No abstract.) This dissertation investigates issues raised by these two questions: (i) what kinds of propositions are ordinarily expressed by uses of sentences that contain proper names; and (ii) what kinds of beliefs are ordinarily on the minds of speakers when they use sentences that contain proper names? It develops a new view about the connections between beliefs, linguistic behavior, and propositional content, one that explicitly denies that the kinds of propositions typically expressed by uses of such sentences are the objects of the beliefs typically on the minds of the speakers who use them. 78 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature","Chapter I presents both the Millian and the description theories of proper names, and reviews the advantages and disadvantages of each.","Chapter II critically evaluates Dummett's defense of the description theory against the Modal Objection.","Chapter III introduces Kripke's puzzle about beliefs and proper names. It shows that Kripke's puzzle is not solved by the theory of proper names recently presented by Devitt. It critically evaluates the \"consistency solutions\" proposed by Chisholm, Harrison, Noonan, and Over.","Chapter IV continues the discussion of Kripke's puzzle. It critically evaluates the \"inconsistency solution\" proposed by Marcus. It examines a commentary on the puzzle by Lewis. Finally, it presents an \"inconsistency solution\" based on views suggested by the Lewis commentary.","Chapter V compares my view about the connections between beliefs, linguistic behavior, and propositional content to the \"naive view\" and the \"RusseUean view\". It applies my view to solve two major problems for the Millian theory of proper names. Unearthing Grounds: Some Studies of Metaphor Comprehension Laurel J. End Kent State University Ph.D. 1984, 112 pages Psychology, Experimental University Microfilms Order Number ADG84-29789. 8504. Utterances which convey a meaning different from their literal meaning pose special problems for models of language comprehension. Primarily for this reason psychologists have recently begun to examine the processes involved in metaphor comprehension. Metaphors consist of a topic or subject, a vehicle or term used metaphorically, and a ground, the relationship between the topic and vehicle which gives the metaphor its meaning. For example, in Some Jobs are jails, the topic is job, the vehicle is jails, and the ground is the notion that some jobs may be restrictive, confining, or punishing.","Two experiments investigated the nature of the ground. Previous research indicated that metaphors based on a common ground can prime one another effectively. The strength of the priming effect was assessed in Experiment 1 by inserting either 0, 1, 3, or 7 literal sentences between pairs of metaphors related by a common ground. Subjects read each sentence presented individually on a CRT screen and the response times were recorded. Memory for the sentences was assessed by having subjects recall the topics associated with each vehicle. Priming was effective if related metaphors were presented consecutively, but the effect disappeared with 1, 3, or 7 fillers. Related metaphors, filler metaphors, and literal fillers were recalled equally well, but the second metaphor topic in each related metaphor pair was recalled more often than the first.","In Experiment 2, the nature of the ground was assessed with an interference task. The procedure was identical to Experiment 1 except that only one filler was presented between the prime and its target. That filler was either a literal or metaphorical sentence and was either high or low in imageability. No priming effect was found regardless of the imageability or type of filler sentence.","The results of Experiments 1 and 2 indicated that metaphors can share a common ground, but representation of the ground was quite fragile, but unavailable after only one intervening sentence. The results of Experiment 2 ruled out two possible explanations for the rapid dissipation of a priming effect in Experiment 1. The interference of priming could not be attributed to interference of an imaginal component of the ground nor was it due to differing processing strategies for literal and metaphorical sentences. However, since the interference paradigm may have been relatively insensitive to subtle alteration in the representation of the ground, it is not possible to conclude that the filler sentence type and imagery have no differential influence on the primed ground. The Nature of the Search for Referents in Kintsch and van Dijk assume that when a reader encounters a reference to Discourse Processing a concept no longer available in short-term memory that a search through Computational Linguistics, Volume 12, Number 1, January-March 1986 79 The FINITE STRING Abstracts of Current Literature Edward Joseph O 'Brien University of Massachusetts Ph.D. 1984, 150 pages Psychology, Experimental University Microfilms Order Number ADG85-O0111. 8504. A Neuropsychological Framework for the Assessment of Competing theories of Rhetoric as Epistemic Kathy Lynn Harbert The Pennsylvania State University Ph.D. 1984, 366 pages Speech Communication University Microfilms Order Number ADG84-29087. 8504. long-term memory for the original concept is necessary. A series of four experiments are reported that address the nature of this search process. In the first two experiments, subjects read passages that contained two possible referents; one referent appeared early in the passages and the other referent appeared relatively late. Read time differences for the first two experiments demonstrated that late referents are reinstated more quickly than early referents. Several viable search models within the Kintsch and van Dijk framework were considered. However, none of these models was capable of predicting faster access to the late referent. Following Experiment 2, it was proposed that text is represented as an integrated network and that a backward parallel search model provided the best account of the reinstatement time differences. Experiments 3 and 4 provided further support for these assumptions. The results of these experiments showed that concepts that appeared between a referent and the end of a passage are often considered during the search for a referent. Intervening concepts that are considered are tagged as \"not appropriate\". This tag produces response competition that slows verification times for statements containing these concepts. The results of all four experiments are discussed in terms of the Kintsch and van Dijk framework. This investigation examines the symbols of both thought and language for the purpose of evaluating competing theories of rhetoric as epistemic. Emerging from the literature on rhetoric and knowing are assumptions about the nature and function of the human brain that are no longer tena-ble in light of the evidence from neuropsychology since the advent of microtechnology. To update the literature and correct the assumptive base of the past, this investigation presents a meta-theoretical framework for understanding the symbols of both thought and language in terms that are compatible with the state of the art in cognitive theory.","From the history of epistemology, three perspectives toward the symbols of thought were isolated and traced to their correlates in the cognitive sciences. By isolating from this survey those characteristics that were found to be most neuropsychologically adequate, this investigation advanced a definition of \"knowing\", and a description of the cognitive symbol that were consonant with the most recent evidence from neuropsychology. Three perspectives toward the symbols of language were then examined and compared with the symbols of thought. By articulating the characteristics that were common to both thought and language, this investigation advanced a perspective from which the symbol itself could be viewed as inherently epistemic. With this perspective as a framework, the investigation assessed twelve competing theories of rhetoric as epistemic. Of the theories examined, most were found to advance claims about the nature of symbolic interaction that were at variance with the characteristics they imputed to the symbol itself. Only one theory developed a position that was based on an interactional perspective toward the symbol, and only that one made a viable claim to knowing.","This investigation concluded that neither a theory of language, nor a. theory of meaning has as yet been developed that can adequately address the interactional characteristics of the symbol itself. The metatheoretical framework advanced by this investigation presents guidelines for the development of a theory of meaning that would be compatible with the goals of George Herbert Mead, but not subject to the theoretical difficulties that were found to be inherent to his position. 80 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature Requests for the following papers should be addressed to ISSCO working papers 54 route des Acacias 1227 Geneva Switzerland The price per paper, including air mail postage, is SFr 10 (or equivalent). Dalle Molle\". Checks should be made payable to \"Institut Three Strategic Goals in Conversational Openings M. Rosner No. 46 (1981) This paper tries to explain a short transcript of a conversational opening as completely as possible within the framework which takes conversational behaviour as defined by the operation of a sophisticated planning mechanism. It is argued that a critical role is played by the satifaction, for each participant, of three strategic goals relating to attention, identification, and greeting. Additional tactics for gaining information are also described as necessary to account for this transcript. A Poor Man's Flavor System F. di Primio, Th. Christaller No. 4 7 (1983) A Government-Binding Parser for French Eric Wehrli No. 48 (1984) This paper is the result of an attempt to understand \"flavors\", the object oriented programming system in Lispmachine Lisp. The authors argue that the basic principles of such systems are not easily accessible to the programming public, because papers on the subject rarely discuss concrete details. Accordingly, the authors' approach is pedagogical, and takes the form of a description of the evolution of their own flavor system. An appendix contains programming examples that are sufficiently detailed to enable an average Lisp programmer to build a flavor system, and experiment with the essential concepts of object-oriented programming. This paper describes a parser for French based on an adaptation of Chomsky's Government and Binding theory. Reflecting the modular conception of GB-grammars, the parser consists of several modules corresponding to some of the subtheories of the grammar, such as X bar, binding, etc. Making an extensive use of lexical information and following strategies which attempt to take advantage of the basic properties of natural languages, this parser is powerful enough to produce all of the grammatical structures of sentences for a fairly substantial subset of French. At the same time, it is restricted enough to avoid a proliferation of alternative analyses, even with highly complex constructions. Particular attention has been paid to the problem of the grammatical interpretation of wh-phrases, to clitic constructions, as well as to the organisation and management of the lexicon. AI Approaches to Machine Translation Patrick Shann No. 49 (1985) This paper examines some experimental AI systems that were specifically developed for machine translation (Wilks' Preference Semantics, the Yale projects, Salat and CONTRA). It concentrates on the different types of meaning representation used, and the nature of the knowledge used for the solution of difficult problems in MT. To explore particular AI approaches, the resolution of several types of ambiguity is discussed from the point of view of different systems. Machine Translation: Pre-ALPAC History, Post-ALPAC Overview Beat Buchmann, Susan Warwick No. 50 (1985) This paper gives a historical overview of the field of Machine Translation (MT). The ALPAC report, the now well-known landmark in the history of MT, serves to delimit the two sections of this paper. The first section, Pre-ALPAC history, looks in some detail at the hopeful beginnings, the first euphoric developments, and the onsetting disillusionment in MT. The second section, Post-ALPAC overview, describes more recent developments on the basis of current prototype and commercial systems. It also reviews some of the basic theoretical and practical issues in the field. Computational Linguistics, Volume 12, Number 1, January-March 1986 81 The FINITE STRING Abstracts of Current Literature Software Engineering for Machine Translation Rod Johnson, Mike Rosner No. 51 (1985) In this paper we discuss the desirable properties of a software environment for MT development, starting from the position that succesful MT depends on a coherent theory of translation. We maintain that such an environment should not just provide for the construction of instances of MT systems within some preconceived (and probably weak) theoretical framework, but should also offer tools for rapid implementation and evaluation of a variety of experimental theories. A discussion of some potentially interesting properties of theories of language and translation is followed by a description of a prototype software system which is designed to facilitate practical experimentation with such theories. 82 Computational Linguistics, Volume 12, Number 1, January-March 1986"]}]}