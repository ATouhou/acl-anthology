{"sections":[{"title":"American Journal of Computational Linguistics PROCEEDINGS 13TH ANNUAL fl'EETING ASSOCIATION FOR COMPUTATIONAL LINGUI STI cs Timothy C.","paragraphs":["Diller, Editor Sperry-Univac"]},{"title":"St. Paul, Minnesata 55101 Microfiche 33 Copyright @ 1975 by the Association for Computational Linguistics PREFACE The papers comprising this microfiche (the second of five) present in expanded form (as submitted by their authors) the six talks given in Session 2: Language Generation Systems. Various aspects of generation are considered, among them: relationsHips between parsing and generation (Knaus), planning modules and data structures basic to story development (Meehan) , semantic networks and linguistic generatorq (Shapiro and Slocum), message structures and translation strategies","paragraphs":["(McDonald)"]},{"title":", and lexical processes in compound noun formation (Rhyne). Thanks to Martin Kay for chairing this session. Timothy C. Diller Program Committee Chairman TABLE OF CONTENTS A Rxtsbework for Writing Generation Grammars for","paragraphs":["Inter-active Computer Programs mvid Mcmnald"]},{"title":"......... 4 ...... Incremental Sentence Processing Rodger mus 18 A Lexical Process Model of Nominal Compounding in ................. English James R. Rhyne 33 Generation as Parsing from a Network","paragraphs":["into a Linear String Stuart C. shepiro"]},{"title":"................. 45 Speech Generation from Semantic Nets J~Adthdn S~OCW . . 63 Using Planning Structures to Generate Stories ~ams R. ......................... Heehan 78 American Jourdal of Computational tinguktics","paragraphs":["Microfiche 33 4 Artificial Intelligence Ldbora tory Massachusetts Ins ti tute of Technology Cambridge, Massachusetts 02139 ABSTRACT"]},{"title":"This paper outlines","paragraphs":["the"]},{"title":"stucture and operation of the 1 ingui stic component from a language generation system in an interactive program. The component receives messages","paragraphs":["describing what"]},{"title":"is to be said formulated in the representation of the main prograr and produces fluent English utterances appropriate","paragraphs":["to the"]},{"title":"current discourse situation.","paragraphs":["The"]},{"title":"component is data-directed and uses a procedural grammar, organized as a set of strategies. Interactive, speclalist prograas presently under developwent will heed to produce fluent, intentional English utterances in responce to particular, complex titutlons. This creates a requlroaont for language generating facilities that Is not faced in transformational grarapar, rochanical translation programs, or paraphrase generating programs. As a component of rn interactive, specialist program, the production of the English must be driven directlr by","paragraphs":["the"]},{"title":"communicative intentions of the program and by the discourse situation, We can imagine","paragraphs":["that the overall program consist,ยง of a number of cooperating modules"]},{"title":"- for parsing and interpreting what is said to it, ior solving ptoblens in its domain, for managing its renary, and, in particular,","paragraphs":["for generating utterances to"]},{"title":"com~unicate wl th Its users* This generation component","paragraphs":["can be profitably viewed as having three aspects or msub-corponentsw. 1)"]},{"title":"Situation/doaain specie1 ists","paragraphs":["that"]},{"title":"are activated","paragraphs":["when the program recognizes what situation it is in+ They then decide what message will be produced. They will decide what effect on the listener is desired, and exactly what objects and relations are to be nentioned. Fot example,"]},{"title":"an appoint~ent scheduling program might be told to *scm~ule a","paragraphs":["group meeting for Fridayw and then find that a critical aentber of the group is uncxvailable, The situation specialists in the scheduling prograr"]},{"title":"are","paragraphs":["the ones to decide whether it is"]},{"title":"more appropriate","paragraphs":["to simply say \"1 can'tR, OF whether to voluntcp information"]},{"title":"- wI can't; Mitch won't be back uWlt Mondayn. 2) Models of the audience and","paragraphs":["the discourse situation to use in construct2ng utterances. There must be a record of the past conversation to gulcfe in the selection af pronouns, Also, the program must have nodels of, and heuristics about what the audience"]},{"title":"already knows and therefore doesn't have","paragraphs":["to be told. This"]},{"title":"Informtion lay be very specific and domain dependent.","paragraphs":["Fot"]},{"title":"exarple, In chess, one can say \"the white queen","paragraphs":["could take e knightn. There is"]},{"title":"no need to say \"a black knlphtw, because this","paragraphs":["information is supplied by inferences from what one knows about chess - inferences that the spcarer assures the listener shares. 3)"]},{"title":"Llnpu!rtic know1 edge about how to construct understandable utterances in the English Isnpuagc.","paragraphs":["Obviously, this lnformatlon vill Include a"]},{"title":"lexicon assoclatlng objects and relations from","paragraphs":["the min program with rtrate&i@~ for realizing them in English (particular words, phrases, syntactic constructions, etc.). There is also a tremendous"]},{"title":"amount of informatian which describes the characteristics of the English language and the conditions of its use. It specifies","paragraphs":["rhe allowable"]},{"title":"arrangements of strategies and what niodlfications or alternatives to them nay be appropriate in particular circumstances. Of the three aspects just described, my work has concentrated on the third.","paragraphs":["What"]},{"title":"follows is drawn from sy thesis McDonald '75) and from ongoing research. The Lingufetio Component The 1 inguistic knowledge required for generating utterances is put into one component whose job is","paragraphs":["to take a message from the sltuatlon specialists and cpnstruct a translatlw of that message In English. The"]},{"title":"messages are in the representation used by the main program and the s1 tuation specialists. Tho translation is done by a","paragraphs":["data-directed process"]},{"title":"wherein","paragraphs":["the elenents and structure of the message itself provide the control. The design of the 1 ingui stics component was arrived at independent of"]},{"title":"any particular main program,","paragraphs":["for the simple reason that"]},{"title":"no programs of adequate complexity were available at the time. However, at the present time a grammar and fcxlcon is being developed to use with at least two prograRs being developed by","paragraphs":["other people at MIT. They"]},{"title":"are an appointment scheduling program (Goldstein","paragraphs":["'75) and an advisor to aid users of"]},{"title":"MACSYMA (Genesereth '75).","paragraphs":["The short dialog below is an example"]},{"title":"of","paragraphs":["the degree"]},{"title":"of","paragraphs":["fluency we arc hoping to eventually achieve. The dlalog"]},{"title":"ts between a scheduling prograa","paragraphs":["acting as"]},{"title":"an appolntaent secretary (P), and a student","paragraphs":["(5). (5)"]},{"title":"I want","paragraphs":["to see Professor"]},{"title":"Winston","paragraphs":["sometime"]},{"title":"in the next few days. (PI Hers","paragraphs":["pretty busy all week."]},{"title":"Can","paragraphs":["it wait? (S) No, it can't, All I need is his signature"]},{"title":"on a","paragraphs":["forb"]},{"title":"(PI Well, maybe","paragraphs":["he can squeeze you in tommorrow ~ornlng. Give me"]},{"title":"your name and check back in","paragraphs":["an hour."]},{"title":"Messages Using the current message","paragraphs":["foraat and Ignoring the details of the"]},{"title":"schedulerts representation,","paragraphs":["the phrase \"maybe he can squeeze you"]},{"title":"in","paragraphs":["to~morrow~ could have cow from a Pessage like this one, put together by"]},{"title":"one of","paragraphs":["the situation specialists."]},{"title":"Message- 1 features.","paragraphs":["( prediction ) event (event"]},{"title":"actor","paragraphs":["(Winston) action (fit person-into dull schedule> the (31-10-75, gar-12am)","hedge <fs possible)","aim-at-audience hedge Messages have features describing the program's communicative intentions"]},{"title":"- what sort of utterance is this","paragraphs":["to be; what effect is it to have."]},{"title":"Messages list","paragraphs":["the objects to be desert-bed (the right hand column) along with annotations for each object (left hand co1u.n) to show how they"]},{"title":"relate","paragraphs":["to the rest of the message. The phrases on the right"]},{"title":"in angle brackets represent","paragraphs":["actual structures from the scheduler wf th those"]},{"title":"The Lexicon Translatl~n ftor tho","paragraphs":["internal reprcscntai ton of"]},{"title":"s coaputcr program to natural language has","paragraphs":["the same sort of problew as translating betw~en two natural languages. Tk same concepts nay not be available as prim1 tivos"]},{"title":"in both rcpresents.tions, and","paragraphs":["the conventions of the target Isnguape my require additional information that was not"]},{"title":"in the source. Generally speaking translation cannot be one for one. What English phrase is best for","paragraphs":["a"]},{"title":"particular element in a program's message will depend on what is in","paragraphs":["the rest of"]},{"title":"the message and of what the external context is. In such circunstances, translation by table-lookup is inadequate. In this component, in order to allow all factors to be considered, the translation of","paragraphs":["each element ias done by individualized procedures called wcorposersH."]},{"title":"For each main program that the linguistic component becomes associated with, a lexicon must be created which will list","paragraphs":["the elements of"]},{"title":"the rain program's representation that could appear in a message","paragraphs":["(1. C. wprCdl~tl~nH, \"eventw, w<WinstonP, etc. ). With each element is"]},{"title":"recorded the composer","paragraphs":["that"]},{"title":"will be run when","paragraphs":["the"]},{"title":"time comes","paragraphs":["to produce"]},{"title":"en English description","paragraphs":["for it (examples will be given shortly). Some conposers"]},{"title":"nay be applicable","paragraphs":["for a whole class of elements, such as \"eventsw. They would know the structure that all events have"]},{"title":"in common (e.g. actor, actlon, tine) and would know how to interpret the idiosyncratic details of each event by using data in the lexicon associated with them. The Grammar - strategies The bulk of","paragraphs":["the grauar con&ists of \"strategiesw. Strategies arc associated with particular languages rather than with particular main prograas as composers are. A given strategy may be used for several different purposes. A typical case is the strategy use-s,?mp~resent-tense: a clause in the simple present (\"prices risew)"]},{"title":"my be understood as future, cond'itlonal,","paragraphs":["or timeless, according to what other phrases arc present, Each composer"]},{"title":"ray know of several strategies, or Coabinatlons of strategies which it could use in dcscrlbing an","paragraphs":["ele.cnt from the message,"]},{"title":"It wiT1 choose between","paragraphs":["the&"]},{"title":"according","paragraphs":["to the context"]},{"title":"- usually details of the element or syntactic constraints iaposed by","paragraphs":["previously selected strategies. The strategies themselves do no reasoning; they are implemented as functions which the"]},{"title":"corposers call to do all the actual corsst'ruction of","paragraphs":["the"]},{"title":"utterance. The Tr~nslation Prooesa At this","paragraphs":["point. the out1 ine of the data-dr"]},{"title":"Iven translation process can be su~rnriztd. A message is glven","paragraphs":["for translation. The ele~ents of the aessage are associated in a lexicon with procedures to describe the.. The procedures are"]},{"title":"run; they","paragraphs":["call grs~latica1 strategies; and the strategies"]},{"title":"construct","paragraphs":["the English utterance. Of"]},{"title":"course, if this were all","paragraphs":["there was to it, the process would"]},{"title":"never run, betause","paragraphs":["all of the subprocesses aust be throughly coordinated if they are not to \"trip over their own feet\", or, for that ~atter, if"]},{"title":"ordinary human beings are","paragraphs":["to bo able to design the.."]},{"title":"In a system where the knowledge of what to do is distributed over a large number of separate procedures, control structure assumes central. iaportance. Plans Before dt~cribing the control structure,","paragraphs":["I must lay out some additional aspects of the design of the ilngulstlcs coaponent. There is"]},{"title":"no interlingua or intermediate level of structure co~parablc to the dkep structures of Transformttlonal Gra~sar,","paragraphs":["or the sewntlc nets of Sl~aons (73) or Goldban (74)."]},{"title":"Detorminln~ the appropriate sutface structure, however, requires plannfnp, if for no other reason","paragraphs":["than that the Message can only be examined one piece ax a the. The entire utterance must be"]},{"title":"organized before a detailed analysis and translation can get underway. As this is done, the","paragraphs":["wproto-utterancew is represented"]},{"title":"in terns of a sort of scaffolding -","paragraphs":["a representatiqp"]},{"title":"of","paragraphs":["the ultimate surface structure"]},{"title":"tree insofar as its details","paragraphs":["are known with extensive annotation, explicit and implicit, to point out where elements that are not yet described may be positioned,"]},{"title":"and to implement","paragraphs":["the graamatical restrictions on possible"]},{"title":"future details as dictated by what has already been done. The scaffolding","paragraphs":["that is constructed"]},{"title":"in the translation","paragraphs":["of"]},{"title":"each message is called its wp4a0w. Plans are made up of syntactic nodes of the usual sort - clauses,","paragraphs":["noun groups, etc,"]},{"title":"- and nodes may have features in","paragraphs":["the"]},{"title":"Banner of tysteni; grammar Winograd '72~ Nodes have subplans consisting of a list of named slots marking the possible potftlons for sub-constituents, given in","paragraphs":["the order ~f tho"]},{"title":"eventual surface structure. Possible slots would be wsubJectw, *.%in verbw, \"noun headw, wpre-verb-adverbw, and so on. The syntactic node types will","paragraphs":["each have a nuaber of -possible plans, corresponding\" to the different possible arrangements or"]},{"title":"sub-consti tuents that may occur wl","paragraphs":["th tho different combinations of features that tho Mdc may have, Depending on tho rtage of the translation process, a slot may bc \"fl1lodw with a pointer to"]},{"title":"an lnternal object fro. the message, a syntactic node, a word or idior,","paragraphs":["ar nothing."]},{"title":"The translation proaaea Tho translation is done in two phases.","paragraphs":["The second phase does not begin until the first is coapletely finished. During the first phase, a plan is selected and the eleaents of the message are transferred, largely untouched; to the slots of the plan and features added to its"]},{"title":"nodes. During the second phase,","paragraphs":["the plan is wwalktdR topdown"]},{"title":"and from left","paragraphs":["to right."]},{"title":"Conpostrs for mssage","paragraphs":["ele~ehts"]},{"title":"in","paragraphs":["the"]},{"title":"plan's slots are activated","paragraphs":["to"]},{"title":"produce English descriptions for","paragraphs":["the"]},{"title":"elcments as","paragraphs":["they"]},{"title":"are reached in turn.","paragraphs":["Both"]},{"title":"processes are data-directed,","paragraphs":["the first by"]},{"title":"the particular","paragraphs":["contents of the message and the second by the structure of the plan"]},{"title":"and","paragraphs":["the"]},{"title":"contents","paragraphs":["of its slots."]},{"title":"There are sound linguistic reasons","paragraphs":["for this two stage"]},{"title":"processing. Most parts of","paragraphs":["a lessage Bay be translated"]},{"title":"in terms of very modular sysltactic and lexical units. But","paragraphs":["other parts"]},{"title":"are translated in terms of relations between such units, expressed usually by ordering","paragraphs":["or clause-"]},{"title":"level syntactic aechanislps. The","paragraphs":["exact for@ of the s~aller units cannot be deternlned"]},{"title":"until their larger scale relations have been fixed. Accordingly,","paragraphs":["the objective"]},{"title":"of","paragraphs":["the first phase is to"]},{"title":"determine what global relationships are required and","paragraphs":["to choose the plan,"]},{"title":"features, and positions of message elemnts within","paragraphs":["the"]},{"title":"plan's slots that will realfzt those relationships. Once this has","paragraphs":["been done,"]},{"title":"Engl tsh descriptions for the elements can be made Independent of","paragraphs":["each other"]},{"title":"and will","paragraphs":["not"]},{"title":"need to be changed after","paragraphs":["they arc initially created."]},{"title":"One of the lost","paragraphs":["iaportant features of natural language is the"]},{"title":"ability to omit, prono~inal ize,","paragraphs":["or otherwise abbreviate elements"]},{"title":"in certain contexts.","paragraphs":["The only known rules and hllristlcs for using this"]},{"title":"feature rro phrased in terms","paragraphs":["of Surface"]},{"title":"structure configurations and temporal ordering. Because the second ~hase works directly in thcse terms, stating and using","paragraphs":["the"]},{"title":"available heuristics becoaos a straightยฃ orwsrd, tractable problem. \"Maybe he can eg.ueeze you in tommoww morning\"","paragraphs":["The rest of this paper will try to put solae flesh on your picture of how this linguistics conponent works by following the translation of the message given in the beginning to the sentence above."]},{"title":"The message was this. Massage-","paragraphs":["1 features* ( prediction )","event (event actor <Winston>","action. tfit person into full schedule)","time <31-10-75,9aar-l2rm>)","h-@biie <is possible,","aim-at-audience hedge The intentional"]},{"title":"features of","paragraphs":["a"]},{"title":"message tend","paragraphs":["to require the nost global representation"]},{"title":"in the f Inal utterance, because that is where indicators for questions, special emphasis, speclal formats lee n. conpari son), and the like will be found. By convention then. the composers associated wlth the intentions are given the job of arranging for the disposition of all of the messake elements.","paragraphs":["The total aperatlon of phase one"]},{"title":"consists of executing","paragraphs":["the composer associated with each feature, one af"]},{"title":"$er the other. Thls aessage has only one","paragraphs":["feature, so its composer will assume al) tho work. Thc linguistics component is implemented"]},{"title":"in MACLISP, features (and annotations and slots and nodes) are atoms, and coar>Qsers ass functions on their property lists. Prediction composer-with (lambda ...","paragraphs":[") Making a prediction is a speech act, an& wo nay expect there to be particular forms"]},{"title":"in","paragraphs":["a language for expressing thee, for example, the use af the explicit \"willw for the future tense. Knowledge of these would De part of the composer. Inside the aain program, or the situation special lst, the concept of a prediction may always include certain parts: what"]},{"title":"is predicted, the time, any hedges, and so on. These part are directly reflected in thc makeup of the elements present in the message, and","paragraphs":["their annotations mark what internal roles the7 have."]},{"title":"There does","paragraphs":["not need to be a direct correspondence between these and the"]},{"title":"parts in the linguistic forms used,","paragraphs":["the actual correspondence is part of the knowledge"]},{"title":"of the prediction cosposer. Typically,","paragraphs":["for any feature, one particular annotated ele~ent will be of greatest l~portance"]},{"title":"in seting the","paragraphs":["character of the whole"]},{"title":"utterance. For predictions, this is","paragraphs":["the"]},{"title":"\"eventw.","paragraphs":["The prediction"]},{"title":"composer chooses a plan for","paragraphs":["the utterance to fit the"]},{"title":"requireaents of the event-element. The realization of any","paragraphs":["other elements will be restricted to be compatible with it. The prediction composer docs not need to know the element's linguistic correlates itself, it can delegate the work to the composer for the"]},{"title":"element itself. The","paragraphs":["element look like this, (event actor"]},{"title":"<Winston> action <fit person","paragraphs":["into full schedule> tine t31-10-75,9am-lZaa>)"]},{"title":"The first word points to the name of","paragraphs":["the composer, and the pairs give particular details. There is nothing special about the words used here"]},{"title":"(actor, action, tlme), Just a$ long as","paragraphs":["the composer"]},{"title":"is designed to expect the inforwatton in those places that the message-asseabler wants to put it. The event composerfs strategy is to use a clause, and","paragraphs":["the choice of plan is determined by the character of the event's \"actionw. The action is \"<fit person into full schedulerw, and it will have two"]},{"title":"relevant properties in","paragraphs":["the"]},{"title":"lexicon: \"plan*, and \".appingW. klan is either the nane of a standard plan to be used; or an actual plan, partially filled with words","paragraphs":["(1. e. it can be a phrase). \"Mappingw"]},{"title":"is an association list showing how the subelements of the message are to be transferred to the plan. <fit person into full schedule) PLAN node-i (clause trans1 particle) slots frontings","paragraphs":["nil subject"]},{"title":"nil vg node-j (verb-group particle) slots modal nil pre-vb-adv nil mvb \"squeezew prt \"inw object1 <person being talked about) post-modifiers nil MAPP I NG","paragraphs":["(("]},{"title":"actor subject","paragraphs":[") ( time post-modifiers)) The event composer proceeds to instanticte the nodes in the phrase and"]},{"title":"make the transfers; the prediction composer then takes the resulting plan, and makes it the plan of the whole utterance. Two message elements remain, but actually there is only one, because waim-at-audiancew is supplying additional informati~n about the hedge.","paragraphs":["The"]},{"title":"annotation means that the contents of the hedge (<is possible>) ere pore something","paragraphs":["that we"]},{"title":"want to tell the audience than a detail of the prediction. This will affect how the element is positioned in the plan. The prediction composer looks in the lexicon to see what grammatical unit will be used to realize <is possible,, and sees, let us say, two possibilities involving different configurations of the adverb wn&ybcn and the modal \"can be able ton, with the differences hinging on the placement of the adverb. Theoretically, adverbs can be positioned in a nunber","paragraphs":["of places in a clause, depending"]},{"title":"on their characteristics. In this instance, the choice is forced because of a heuristic written into the grammar of adverbs and accessible to the composer, that says","paragraphs":["that when the intent"]},{"title":"of an adverb is directed","paragraphs":["to the"]},{"title":"audience,","paragraphs":["it should be"]},{"title":"in the first position","paragraphs":["(the"]},{"title":"\"frontiogsW slot). This choice implies putting \"canw In","paragraphs":["the modal slot directly. The"]},{"title":"alternative with w~ybe* in the pre-vb-adv slot would have necessitated a different form of the .o8al, yielding \"ray be","paragraphs":["able ton,"]},{"title":"These details would have been taken care","paragraphs":["of by syntactic"]},{"title":"routines associated with","paragraphs":["the verb group"]},{"title":"node. A11 the message ererents have been placed and","paragraphs":["the first phase"]},{"title":"is over. The plan is now as below. n4e-l (clause trans1 particle) slats fmntfngs \"8aybeW subject twinston> vg node-2 (verb-group par","paragraphs":["tic1 el","slots modal \"canw pre-vb-adv nil mvb \"squeezen prt"]},{"title":"\" inw object1 cperson being talked about) post-modifiers","paragraphs":["nil The"]},{"title":"second phase controller is a simple dispaching function","paragraphs":["that mves"]},{"title":"from slot","paragraphs":["to slot."]},{"title":"\"Fronttngs* contains","paragraphs":["a word, so the word is printed .directly"]},{"title":"(there is a","paragraphs":["trap for"]},{"title":"morphological adjustnents when necessary). wSttbjectw contains an","paragraphs":["internal object, so the controller should go to the"]},{"title":"lexicon for its composer and then come back","paragraphs":["to handle whatever the composer replaced the clement with,"]},{"title":"However, there is always an","paragraphs":["lhtervening step to check for the possibility of pronominalizing. This check is made with the elelpent still"]},{"title":"in its internal foro.","paragraphs":["The record of the discourse is given directly"]},{"title":"in term of the","paragraphs":["internal representation and test for prior"]},{"title":"uccurence can be as simple as identity checks against a reference list, svoiding potentially intricate string matching operations with words. In the dialog","paragraphs":["that this message came from, there is clear reference to twin9ton>, so it can be prononinallzed and \"hew is printed."]},{"title":"Any slot, or any node","paragraphs":["type may have procedures associated with it that are executed when the slot or node is reached during the second phase. These procedures will handle syntactic processes like agreement, rearangelaent of slots to realize features, add function words, watch"]},{"title":"scope relationships, and in particular, position","paragraphs":["the particle"]},{"title":"in verb-particle pairs. Generally, particle position (\"squeeze John inn vs. n~q~me in Johnw) is","paragraphs":["not specifled by the grammar"]},{"title":"- except when the object is a pronoun and the particle - must be displaced. This, of course, will","paragraphs":["not be known untlll after the verb group has been passed. To deal with this, a subroutine"]},{"title":"in the \"when-ent$redn procedure of","paragraphs":["the verb group is activated by the \"particlen procedure. First, it records the particle and relaoves it from the VG plan so it will not be generated automatically. A \"hookw is available on any slot for a,procedure which"]},{"title":"can be run after prononinalization is checked and before the composer is called (if","paragraphs":["it is to be called). The subroutine incorporates the particle into a standard procedure and places it on that hook"]},{"title":"for the object1 slot. The procedure will check if","paragraphs":["the object has been prlnted as a pronoun, and if so, prints out the particle (which is now In the proper displaced pori tion). If the object wasn' t pronominal ized, then it does nothing, nothing has yet been printed beyond the verb group, and other heuristics will be free to apply to choose the proper position. Since (person being talked about, is here equal to the student, the person the prograa"]},{"title":"is talking with,","paragraphs":["it is realized"]},{"title":"as the pronoun \"youw and the particle is dlsplaccd. Going irom <31-10-75,9a~-12arn>","paragraphs":["to wtom~rr~~ ~orning* my be little more than table lookup by a wtfme\" coBposer that hat been designed to know the formats of the time expressions inside the scheduler. This presentation has had to be unfortunately short for the amount of new naterial involved. A large nu~ber of interesting detail s and questions about the processing have had to be oritted. At the moaent <September, 19751, the data and control structures ~cntioned have been fully iep1e~ented and tests are underway on gedanken data. Hopefully, by the end of 1975 the component will have a reasonable gramar and will be working with messages and lexicons form the two programs mentioned before. A MI7 A. I. lab technical report describing this work in depth should be ready"]},{"title":"in the spring of next","paragraphs":["year. David McDonald Cambridge, Mass. References cited in"]},{"title":"the text: Genesereth, M. (1975) A MACSYMA Advisor. Project MAC, MIT, Cambr ldge, Mass, Goldman, N. (1974) \"Computer Generation of Natural Language","paragraphs":["fro. a Deep Conceptual Basee. memo AIM-247, Stanford Artlf lcial Intel l igencc Lab., Stanford, Calif,","Goldstein, 1. (1975) \"Barganing Between Goalsw. ih the proceedings nf IJCAI-4, available from the MIT AI lab,","McDonald, D. (1975) The Design of a Program for Generating pat~.~~ Language. unpubl ished Master's Thesl s, MIT Dept. of El ectlcsl Engi neerf ng.","Simmons,"]},{"title":"R. (1973) wSeaantic Networks:","paragraphs":["Thclr Computation and Use for Understanding English Sentences\". In Schank and ~olby eds. Computer Models of Thought and Language.","Winograd, T. (1972) Understanding Natural Language. Academic Press, New York, NY."]},{"title":"American Journal of Computational Linguistics Microfiche 33","paragraphs":[": 28 RODGER KNAUS Systems of tware Division Social and Economic Statistics Administration Bureau of the Census Washington, D. C. 20233 A human who learns a language can both parse and generate sentences in the language. In contrast most artificial language processors operate in one direction only or requtre separate grammars for parsing and generation. This paper describes a model for human language processing which uses a single language description for parsing and generation. 1. Choice of Parsing Strategy A number of constraints limit the processors suitable as models of human language processing. Because short term memory is limited. the listener must absorb incoming words into larger chunks as the sentence is heard. Also because he is expected to reply within a couple seconds after the speaker finishes, regardless of length of the speaker's utterance, the listener must do much of the semantic processfng of a sentence as he hears it. 19"]},{"title":"Bever","paragraphs":["and Hatt point out that the difficulty in understanding a sentence S is not predicted by the number of transformations used to generate S. Furthermore the process of detransforrnation appears too time-consuming (petrick) for the approximately two seconds before a listeaer is expected","to reply. A depth first transition network parser (Woods, ~aplan), in which parsing difficulty is measured by the number of arcs traversed, correctly predicts the relative di fficul ty of active and passive sentences progressive and adjectival present participle sentences and the extreme difficulty of multiple center embeddings. However a syntactically directed depth first parser does not explain why syntactically similar sentences such as (5A) The horse sold at the fair escaped.","(5%) The horse raced past the barn fell. vary in difficulty, nor does it explain experiments on the completion and veriftcation of ambiguous sentences (MacKay, Olsen and MacKay) which suggest that a pruned breadth first strategy is used"]},{"title":"to","paragraphs":["par ce sentences. Sentences with two equal ly plausible a1 ternatives took longer to process than sentences with only one likely interpretation. This extra processing time may be attributed to the construction of two alternate interpretations over a 1onge~ portion of the sentence"]},{"title":"when more","paragraphs":["than one interpretation is plausible. In addition subjects somet~mes become confused by the two interpretations of an ambiguous sentence. Finally in experiments in which subjects hear an ambiguous sentence in ode ear and"]},{"title":"a","paragraphs":["disfrnbiguating sentence simultaneously in the other ear (Garrett) the interpretation af the ambiguity actually percelved by the subject may be switched between the possibilities by changing the disambiguating sentences. Step 3 (a): (S"]},{"title":"NP (N mail) (N Boxes)) [V like) (.NP) (PP*)) (b): (S (NP (NP (N mall) (N Boxes)) (PP (PREP like) NP') (PP*)) V(NP)","paragraphs":["fpp*)) (c): (S (NP (N mail)) (V"]},{"title":"Boxes) I PP (PREP like) NP) (PP*)) (d): (S V mail) (NP (N Boxes)) (PP (PREP like) NP) (PP*)) (e): (S (V mail) (NP (NP (N Boxes)) (PP (PREP like) NP) (PP*)) (pp* 1) After completing the sentence after Step","paragraphs":["4, the"]},{"title":"parser produces phrase markers from a, c, d and e by adding the last word and deleting unfilled optional nodes. The phrase marker obtained","paragraphs":["from 48 is rejected because it contains an unfilled obligatory V"]},{"title":"node. The incremental parser adds each successive sentence word to the partially completed phrase markers built from the earlier","paragraphs":["part of"]},{"title":"the sentence. The new word is added at the","paragraphs":["leftmost"]},{"title":"oblig unfilled node","paragraphs":["of each"]},{"title":"partial phrase marker and at all optional nodes to the left","paragraphs":["of this node. Three different"]},{"title":"operations are used to add a new word to","paragraphs":["a partial parse. The word may be directly added to an unexpanded node, as in Step 3a above."]},{"title":"A1 ternatively, a new word may be attached","paragraphs":["to"]},{"title":"an unfilled node with a","paragraphs":["left branching acyclic tree built from the grammar such as (PP PREP NP) or (S (NP N,"]},{"title":"IN*)) V (NP) (PP*)). Attaching occurs in steps","paragraphs":["1 and 3c. Finally a subtrbe of"]},{"title":"an existing","paragraphs":["partial phrase marker may be 4eft embedded"]},{"title":"in a larger structure","paragraphs":["of the same grammatjcal category, as in steps 3b and 3e above. The embedding operation uses"]},{"title":"at most two left branching trees bui1 t from the gr'ammar: a tree TI with a single cycle on the left branch is used to replace","paragraphs":["the existing subtree E being embedded."]},{"title":"In step 3e, for example, the structure (S (V mail) (NP NP","paragraphs":["(PP*)) (PP*))"]},{"title":"would be obtained. The E is used to expand","paragraphs":["the left-"]},{"title":"most unexpanded node of TI: for 3 b this results","paragraphs":["in: 3e."]},{"title":"(S (V mail) (NP (NP (N Boxes) (N*))","paragraphs":["PP*) (PP*))."]},{"title":"Finally","paragraphs":["to the resulting"]},{"title":"structure the new sentence word is added through direct node expansion or attaching with an acyclic left branching tree;","paragraphs":["in the"]},{"title":"example above","paragraphs":["this"]},{"title":"produces 3e","paragraphs":["from 3el' Using"]},{"title":"direct expanston attaching and embedding, the incremental parser finds a1","paragraphs":["1 the"]},{"title":"phrase markers of sentences fn context free","paragraphs":["or"]},{"title":"regular expression language; a","paragraphs":["formal definition of the"]},{"title":"parser and a","paragraphs":["proof of its"]},{"title":"correctness appear in [lo]. Sometlmes, as at steps 3b and 3e,","paragraphs":["the same structure (a prepositiobnal"]},{"title":"phrase in step 2) is used in more than one","paragraphs":["partial"]},{"title":"parse. Following Earley's Algo'rithm, the incremental parser builds a single copy","paragraphs":["of the shared"]},{"title":"substructure Sf!!","paragraphs":["and maintains pofnters lfnking"]},{"title":"Sb to nodes in larger structures which $9 expands. Far all its tree building operations the incremental parser uses a flnlte set","paragraphs":["of"]},{"title":"trees. e.,","paragraphs":["the"]},{"title":"trees","paragraphs":["with only left subnodes expanded and at"]},{"title":"most onelcycle on the","paragraphs":["leftmost branch. These"]},{"title":"trees may be computed djrectly from the grammar and","paragraphs":["ref-"]},{"title":"erenced by root and leftmost unexpanded node during the parse. Using these preconstructed trees, the incremental parser requires","paragraphs":["only"]},{"title":"a fixed number","paragraphs":["of operations"]},{"title":"to","paragraphs":["add"]},{"title":"a new word to a partial parse: a retrieval on a","paragraphs":["doubly"]},{"title":"indexed set, copying the left branching tree, and at most four structure changing operations +o paste words and trees together. Like Earley's Algorithm,","paragraphs":["IP"]},{"title":"processes each word proportion-","paragraphs":["ally"]},{"title":"to sentence length. However","paragraphs":["on"]},{"title":"sentences satisfying","paragraphs":["a"]},{"title":"depth difference bound, the parsing time per word is constant. Because humans can't remember large numbers of sentence words","paragraphs":["but"]},{"title":"must, process speech at an approximately constant rate, a constant parsing time per ward is a necessary","paragraphs":["property of any"]},{"title":"algorithm","paragraphs":["model i ng"]},{"title":"human language processing. Let the depth","paragraphs":["of"]},{"title":"constituent C","paragraphs":["in phrase"]},{"title":"marker","paragraphs":["P be"]},{"title":"defined as the length","paragraphs":["of the path"]},{"title":"from the root","paragraphs":["of C"]},{"title":"to","paragraphs":["the"]},{"title":"root of","paragraphs":["P. If"]},{"title":"TI and T2","paragraphs":["are tuo"]},{"title":"adjacent","paragraphs":["terminals with TI"]},{"title":"preceding T2. the depth difference","paragraphs":["from 11"]},{"title":"to","paragraphs":["T2 is"]},{"title":"defined","paragraphs":["as the dif-"]},{"title":"ference in depth between","paragraphs":["11 and the"]},{"title":"root","paragraphs":["~f"]},{"title":"the smallest tree containing TI and","paragraphs":["T2. For exarn~le"]},{"title":"in the phrase marker (9) (S (NP","paragraphs":["NP (DET"]},{"title":"the)","paragraphs":["(N"]},{"title":"telephone)) [PP (PREP IN) (F(P (DET the) (N room))) (V","paragraphs":["rang) (ADV"]},{"title":"loudly))","paragraphs":["the"]},{"title":"depth difference between \"the\" and \"telephone\"' is","paragraphs":["1 and between"]},{"title":"\"roam\" and \"rang\" is","paragraphs":["3. The depth difference between 11"]},{"title":"and","paragraphs":["T2 is the"]},{"title":"number","paragraphs":["of"]},{"title":"nodes from","paragraphs":["TI to the"]},{"title":"node expanded when adding","paragraphs":["T2"]},{"title":"on","paragraphs":["a"]},{"title":"postorder traversal from","paragraphs":["TI in"]},{"title":"the partial phrase marker containing","paragraphs":["TI but"]},{"title":"not","paragraphs":["T2."]},{"title":"The","paragraphs":["depth"]},{"title":"difference between","paragraphs":["TI"]},{"title":"and T2 also represents the number of constituents of which","paragraphs":["TI is the"]},{"title":"rightmost","paragraphs":["ward. A proof (requiring"]},{"title":"a","paragraphs":["forma.1 def ini tion of the i ncremental parse) that parsing time"]},{"title":"per word is constant in depth difference bounded sentences appears in [lo]. Informally the depth difference bound places a bound both on the number of next nodes","paragraphs":["to expand which may follow"]},{"title":"a given terminal and on the amount of tree traversal which the parser must perform to ffnd each next unexpanded node. Sfnce each modification requires only a fi'xed number of operati-ons,, each","paragraphs":["of which is bounded"]},{"title":"on the finite set","paragraphs":["of at"]},{"title":"most once cyclic left branching trees, the computation adding a new word","paragraphs":["to existing partial parses is bounded inde pendently of sentence length."]},{"title":"Natural language sentences tend","paragraphs":["to have"]},{"title":"small depth differences. Both right branching sentences and left branching sentences (found in Japanese","paragraphs":["for example) have an average depth difference over each three or four"]},{"title":"word segment","paragraphs":["of"]},{"title":"two or less. On the other hand sentences are difficult to understand when they have two consecutive large depth differences, such as the mu1","paragraphs":["ti"]},{"title":"ple center embedding","paragraphs":["(10) The"]},{"title":"rat the cat the dog bit chased diedb or the complex noun phrase","paragraphs":["in The pad on a clarfnet in the last row whicn 1 fixed earlier"]},{"title":"far Eb fe7l","paragraphs":["out."]},{"title":"Furthermore in amhlguous sentences such as","paragraphs":["(11)"]},{"title":"Joe figured","paragraphs":["that it was time to take the cat out. Kimball observes that subjects prifer the reading with the smaller depth difference. Flnally, Blumenthal found that subjects"]},{"title":"tended to understand a multfple center embedded sentence as a","paragraphs":["conjunct1 ve sentence. The conjunctive sentence ~ontains"]},{"title":"a","paragraphs":["rearrangement. with lower depth differences of the constituents of the center embedded sentence. 3. Sentence Generation The syntactic form given to a sentence depends"]},{"title":"on the","paragraphs":["information being communicated in a sentence and on the cultural context in which the sentence appears. Clark and Haviland show that"]},{"title":"a","paragraphs":["speaker"]},{"title":"uses","paragraphs":["various syntactic devices sentences to place the \"given\" informatian"]},{"title":"known to","paragraphs":["the listener before the information \"new\""]},{"title":"to","paragraphs":["the listener. Particular syntactic structures"]},{"title":"are","paragraphs":["also used"]},{"title":"to","paragraphs":["emphasize or suppress particular kinds of information; for example newspaper traffic accident reports usually begin with a passive sentence such as (12) An elderly Lakewood man was injured when.."]},{"title":". , presumably to emphasize the result of the accident. To capture","paragraphs":["the dependence of syntax on semantic content and bocjal context, the sentence generator"]},{"title":"uses","paragraphs":["function-like grammar rules of the form (Rul erame Cat Variables Predicate Forms )"]},{"title":". Rulename is the name","paragraphs":["of the rule and cat is the grammatieal category of the constituent generated by the rul em Variables is a list of formal parameters. Usually the variable list contains"]},{"title":"a","paragraphs":["varfable bound during rule execution to a node in a semantic network and another variable bound to"]},{"title":"a control association","paragraphs":["list containing information about the context in which the generated constituent will appear and possibly the syntactic"]},{"title":"form the constituent should","paragraphs":["have. Predicate is a Boolean-valued form on the parameters in Variables. A"]},{"title":"rule is used only when Predicate is true. Forms is a list of forms depending on Variables","paragraphs":["which generate"]},{"title":"terminals or calls","paragraphs":["to the grammar for subconstituerits of CAT. An example of a generation"]},{"title":"rule is (SPI SI(X Y) (Equal (Voice Y) (Quote Passive)) (NP (Object X) Y) t Beverb X) Pap (Action X)) (M* X Y)) which generates simple passive sentences. The variable X is bound to a node in a semantic network and Y to a","paragraphs":["control association list. The"]},{"title":"rule is applied only","paragraphs":["if"]},{"title":"the control alist contains a passfve flag and if the semantic node has an object and action; in general a rule is applied only","paragraphs":["if the semantic subnodes"]},{"title":"called in the rule body appear in the semantic net. The form (NP (Obj X) Y) generates a","paragraphs":["form (NP XI"]},{"title":"fl), where XB is the semantic node on the object indicator from X. and Yj3 fs","paragraphs":["the value of Y. Beverb and Pap are procedures"]},{"title":"which generate respectively a form","paragraphs":["~f the verb \"to be\" and"]},{"title":"a past particfple form","paragraphs":["of the verb Action(X). M* is a procedure which"]},{"title":"generates a list depending on X and Y such as (PP~Value of Tlme(X)> <Value of Y>) for generating optional prepositi~nal phrases or relatf vc clauses. AS each rule is applied, the list","paragraphs":["of terminals and calls to"]},{"title":"grammar rules generated by the rule is added to a phrase marker representing the structure of the sentence being generated. Grammar calls","paragraphs":["in the"]},{"title":"phrase marker are expanded","paragraphs":["top down-and"]},{"title":"left to","paragraphs":["right, in"]},{"title":"a preorder traversal of the","paragraphs":["growing"]},{"title":"phrase marker. As terminals are generated","paragraphs":["they"]},{"title":"are printed out. As an example, illustrating the effect of semantic and social contest on sentence generation, an initial sentence OF a traffic accident report, (13). A man was killed when","paragraphs":["a"]},{"title":"car","paragraphs":["hit him in"]},{"title":"Irvine. was generated from the semantic nodes Al: Agent Afl A2: Agent A@: Classman Object v0 Action","paragraphs":["hit"]},{"title":"Action Kill Object VJI)","paragraphs":["Place"]},{"title":"Irvine Instrument Car Cause AZ","paragraphs":["and"]},{"title":"the control alist. Purpose: 1ntroduction;cases: object, cause, place using a grammar built for generating traffic accident report sentences. To summarize a trace of the generation, a call to the sentence rule","paragraphs":["with purpose ="]},{"title":"introduction generates a sentence call","paragraphs":["with"]},{"title":"voice","paragraphs":["="]},{"title":"passive. The passive rule applies and a","paragraphs":["noun"]},{"title":"phrase on Afl is called for. Because Purpose Introduction a NP rule applies which calls for","paragraphs":["a NP to be"]},{"title":"generated on the semantic","paragraphs":["class"]},{"title":"to which","paragraphs":["A8"]},{"title":"belongs. Because","paragraphs":["CASES"]},{"title":"contains TIME","paragraphs":["and CAUSE, the"]},{"title":"passive rule generated","paragraphs":["calls far modifying structures"]},{"title":"of these CASES. Because the cause semantic node A2 has an action, the modifier rule M","paragraphs":["=>"]},{"title":"Relative conjunction","paragraphs":["S"]},{"title":"generates the cause while the time is described by a prepositlonal phrase. The pronoun \"him\" is generated","paragraphs":["by a"]},{"title":"noun phrase rule. NP-1 which generates","paragraphs":["a"]},{"title":"pronoun when","paragraphs":["the first"]},{"title":"semantic","paragraphs":["argument"]},{"title":"to the left","paragraphs":["of"]},{"title":"the","paragraphs":["NP-1"]},{"title":"call in the generation phrase marker which is described by the same pronoun","paragraphs":["as the semantic"]},{"title":"argument A","paragraphs":["of"]},{"title":"NF-1 is","paragraphs":["in"]},{"title":"fact","paragraphs":["equal"]},{"title":"to A.","paragraphs":["4. Finding"]},{"title":"Semantic Preimages While the generator described in section 3 produces sentences from semantic","paragraphs":["and"]},{"title":"contextual information, the incremental parser described in section","paragraphs":["2"]},{"title":"recovers merely","paragraphs":["the syntactic"]},{"title":"structure of a sentence. To obtain","paragraphs":["the"]},{"title":"semantic arguments from which a sentence","paragraphs":["might"]},{"title":"have been generated a procedure to Invert the generation rule forms must be added to the incremented parser. While the incremental parser begins the construction","paragraphs":["of"]},{"title":"constituents top down,","paragraphs":["it"]},{"title":"completes them syntactically","paragraphs":["in a"]},{"title":"bottom","paragraphs":["up"]},{"title":"direction.","paragraphs":["In fact IP"]},{"title":"executes postorder traversals on all the syntactic parse trees","paragraphs":["it builds; of"]},{"title":"course if a particular partial","paragraphs":["phrase marker"]},{"title":"can not be finished,","paragraphs":["the"]},{"title":"traversal is not completed. However each node not a tree terminal","paragraphs":["of"]},{"title":"a syntactic phrase marker visited by the incremental parser is a syntactically complete constituent. When","paragraphs":["the"]},{"title":"parser visits a syntactically complete constituent","paragraphs":["C, it"]},{"title":"applies a function","paragraphs":["INVERT"]},{"title":"to","paragraphs":["find"]},{"title":"the semantic preimages of","paragraphs":["C. In finding the semantic"]},{"title":"structure of C,","paragraphs":["INVERT has available not"]},{"title":"only the syntactic structure of","paragraphs":["C, but"]},{"title":"also","paragraphs":["the semantic"]},{"title":"preimages which it found for subconstituents","paragraphs":["of"]},{"title":"C. 'INVERT finds the set of generation rules which might proruce a constituent having the same syntactic form as","paragraphs":["C. For"]},{"title":"each such rule R., INVERT constructs","paragraphs":["all"]},{"title":"the possible parings between each","paragraphs":["output-generating"]},{"title":"form F of","paragraphs":["R and the"]},{"title":"constituents of","paragraphs":["C"]},{"title":"which F might","paragraphs":["produce. For example if C is (S (NP Man) (Beverb is) (PAP Injured)) the pairing established for the passive sentence rule would be (NP (Object X) Y) (NP the man) (Beverb X) (Beverb is) (pap (Action X)) (Pap Injured) (M* X Y) NIL The pair ((Equal (Voice Y) PASSIVE) 1) is also created, since the rule predicate is true whenever a rule applies. Each indicidual pair P in s-uch a pairing of a rule form and rule form outputs is processed by a function FIND which returns an association list containing possible values of the rule parameters (X and Y in the example above) which would produce the output appearing in P. For the example above FIND would produce ( X ((Object Man) Y NIL))"]},{"title":"(1 x","paragraphs":["((rime past) Y NIL)) (( X NIL) (Y (( Cases Nil)))). (( X NIL) (Y (( Voice Passive)))) Using an extension to association lists of the computational logic Unification Algorithm, these association lists are unified into a single association list, which for the example is (( X ((Agent man) (Time Past) (Action Injure)) (( Y ((Cases Nil) (Voice Passive)))) Finally INVERT creates a grammar rule call,","(S ((Agent man)(Time Past)(Action Injure)) ((Cases Nil)(Voice Passive)))) from the association list and stores the result in the inverse image of C. In finding a semantic preimaqe, the INVERT function must know which grammar rules might produce a parti cul ar grammati cal constf tuent. This information is computed by symbol ical ly evaluating the grammar rules to produce the strings of regular expression grammar ncnterminals (as opposed to grammar calls) representing the possible output of each rule. The resulting relation from rules to strings is inverted into a table giving possible rules generating each string. The heart of this symbolic evaluator is a function ETERM on the output generating forms of a rule which returns a list all lists of regular expression nonterminals representing the output of a form. ETERM takes advantage of the similar syntax of most grammar rule forms, and is defined in simplified form (with comments in angle brackets) as","Eterm (form) =","if atom (form) then NIL","<terminates recurs ion>","else if car (form) is a grammatical category then list (Ifst (car (form))) <these forms generate a single grammar call>","elsa if car (form) = FUNCTION ar LAMBDA then ETERM (cadr (form))","else if car (form) = LAMBDA then ETERM (caddr (form))","else if car (form) = LlST if form is not properly contained in a LIST expression then Mapcar((Function Concatenate)","(Cartesian","((Mapcar (Function ETERM)","cdr (form)) ) )","<outer LISTS are used to create lists of grammar calls>","else if farm Is inside e LIST expression ETERM (cadr (form)) <inner lists are used to create grammatically>","else if car (form) = MAPCONC then make optional","and repeatable all the nonterrninals returned","in ETERM ([function argument of MAPCONC])","else if car (form) = COND then MAPCONC((LAMBDA(X) ETERH ([last form in XI)","(cdr form) <returns alternatives from each branch of the COND>","else if car (form) is a user-defined function then ETERM ([definition of function])","else if there is a stored value for ETERM (form) then that value","else ask the the user for help The function FIND which returns possible bindings for rule variables when given a rule form and its output is defined below. The variable AL1,ST holds the value of the association list being hypothesized by FIND; this variable is NIL when FIND is called from INVERT. Like ETERM, the definition of FIND is based on the rules for evaluating recursive functions.","FIND (Alist form value)= if eval (form a1 fst)=val ue then l i st (A1 ist) else if recursion depth exceeded, then NIL else if atom (form) then list (Merge (list (cons","(form Value)) Alist)","else if car (form)= COND let L = clauses which might be entered by","evaluating form then Mapconc (FM 1) where FM (clause) = list (Merge Find (Alist Car (c1ause)T)","Find (Alist last (clause)))","else if car (form) = Quote then if cadr (form) = value","then Alist else NIL","else if car (form) is a defined function then FIND (Alist (Substitute cdr (form) for","formal parameters in definition","of car (form))","Value)","else if car (form) = MAPCONC (fn 1st) then Me'rge (Find (Alist 1st value)","For each X in 1st. Merge (Alist for X))","<this clause makes the assumption, which works in","practice, that fn generates either one-element","or empty lists,","else NIL With a definition of FIND similar to the one above, the parser found the preimage","(3 (((place ((class (park)))) (agent ([class (man)))) (action (walked] [the extra parentheses den3te lists of alternatives] for the sentence (13) The man walked in the park. generated by the grammar [Sp S (X) T (NP (Agent X)) (V(Action X))","(Optional (PP (Place X) ((Case Place] [NPfl NP (X) T (Det X)"]},{"title":"t","paragraphs":["N (Class XI [ppa PP (XY) T (Prep XY) (NPX]","and the preposition function","Prep (XY) = Selectq (Assoc CASE Y) (Place IN) (Instrument WITH) (Source FROM] 5. Imp1 ementation The processors described in this paper have been programmed in University of California, Irvine, LISP and run in about 45K on a PDP-10 computer. References Bever, Thomas G. 1970. In"]},{"title":"โฌ71","paragraphs":["and [5]. Clark, Herbert H. and Haviland, Susan E. 1975 Social Sciences Working Paper, 67. U.C. Irvine."]},{"title":"-","paragraphs":["Colby, en jam in W. 1973. American Anthropologist 75, 645-62. Florres d'Arcaio and Levalt, eds. 1970 Advances in Psycholinguistics, North Holland, Amsterdam. Garrett, Merrill, F. 1970. in [5]. Haynes, John R. 1970. Cognition"]},{"title":"--","paragraphs":["and the Development - of Lanquage. John Wfley. Kaplan, ~onald M. 1972. A.I. 3, 77-100 Kimball, John 1974. Cognftion 2,1,15-47. Knaus, Rodger. 1975. Ph.D Thesis. U.C. Irvine. MacKay, Donald G. 1966. Perception and Psychophysics. 426-36. Olson, James N. and MacKay, Donald G. JVLVB 13, 45770. Petrick, S. R. In [14]. Rustin, Randall. 1973. Natural Lanquage process in.ฬ Watt, Wm. 1970. In [7] woods,"]},{"title":"w","paragraphs":["1973. I\""]},{"title":"[iil. American Journal of Computational Linguistics Microfiche 33","paragraphs":[": 33 Department of Computer Science University of Houston Houston, Texas 77004 ABSTRACT A theoretical model for nominal compound formation in English is presented in which the rul-es are representations of lexical processes. It is argued that such rules can be generalized to account for many nominal compounds with similar structure and to enable new compounds to be produced and understood. It is shown that nominal compounding depends crucially on the existence of a llcharacteristic'' relationship between a nominal and the vexb which occurs in a relative clause paraphrase of a compound which contains the nominal."]},{"title":"A","paragraphs":["computer implementation of the model is presented and the problems of binding and rule selection are discussed."]},{"title":"Linguistic Issues. Nominal compounds are sequences of two or more nominals","paragraphs":["which"]},{"title":"have the semantic effect of noun phrases","paragraphs":["with attached"]},{"title":"relative clauses. The rightmost nominal is generally","paragraphs":["i"]},{"title":"he primary referent","paragraphs":["of the"]},{"title":"compound the other nominals restrict","paragraphs":["the"]},{"title":"reference of the rightmost nominal in much","paragraphs":["the same"]},{"title":"fashion that a relative clause does. Tbeae are, of course, exceptions","paragraphs":["in which the"]},{"title":"rightmost nominal is figurative or euphemistic (e.g.","paragraphs":["family"]},{"title":"jewels). Compounds occur frequently in English and Germanic","paragraphs":["languages, but"]},{"title":"infrequently","paragraphs":["in"]},{"title":"the Romance languages where their function is largely performed","paragraphs":["by"]},{"title":"nominal-preposition-nominal sequences (e.","paragraphs":["g. chemin de"]},{"title":"fer , agent de change) .","paragraphs":["__C"]},{"title":"- The syntact kc structure nominal compounds is quite simple --the three variants are NAN, N-participle-N, and N-gerund-N. In the N-N form, either 0% the two nominals may in fact be yet another nominal compound, giving","paragraphs":["a"]},{"title":"structure like (N-N)-N or N-(N-N); the first","paragraphs":["of"]},{"title":"these forms seems to occur","paragraphs":["much"]},{"title":"more often than the second (examples of each type are: typewriter mechanic,","paragraphs":["liquid"]},{"title":"roach poison). I assume that the process of nominal","paragraphs":["compounding"]},{"title":"is syntactically a process in which a relative clause is reduced by delet-","paragraphs":["ing"]},{"title":"all elements","paragraphs":["of the"]},{"title":"relative clause but one and preposing the single remaining element in front of the antecedent nominal. In addition, the clause verb may be nominalized 4nd preposed. Other","paragraphs":["linguists have"]},{"title":"proposed different derivations for nominal compounds; Lees [3], for example,","paragraphs":["derives"]},{"title":"nominal","paragraphs":["compounds from"]},{"title":"nominal-preposition-nominal sequences.","paragraphs":["There are"]},{"title":"two reasons","paragraphs":["why"]},{"title":"I feel that Lees approach is wrong: (1) there are English compounds for","paragraphs":["which no"]},{"title":"reasonable equivalent nominal-prepos","paragraphs":["it ion-"]},{"title":"nominal paraphrase can be given (e.g. windmill),","paragraphs":["and (2) there"]},{"title":"are subtle meaning differences between the nominal compounds and their nominal-preposition-nominal counterparts (county","paragraphs":["clerk"]},{"title":"vs. clerk for the county). If nominal compounds and nominal- -- preposition-nominal sequences are derived from forms like relative clauses, then","paragraphs":["the"]},{"title":"differences","paragraphs":["in"]},{"title":"meaning","paragraphs":["can be"]},{"title":"accounted","paragraphs":["for by deriving each form from"]},{"title":"a","paragraphs":["distinct relative clause; the relative"]},{"title":"clauses","paragraphs":["may, of course, be quite closely related to each"]},{"title":"other.","paragraphs":["I have spoken rather loosely about deriving nominal compounds"]},{"title":"from","paragraphs":["relative clauses;"]},{"title":"I","paragraphs":["am not proposing a derivation"]},{"title":"system","paragraphs":["which operates on surface forms of the language, and what I intend that the reader should understand is that an underlying form for a nominal compound is derived from an underlying form for a relative clause by a language process which I term a lexical"]},{"title":"rule","paragraphs":["because, as"]},{"title":"we","paragraphs":["shall see, the operation of such rules depends crucially on the specific lexical items which are present"]},{"title":"in the underlying","paragraphs":["structures. Linguists have identified a number of"]},{"title":"lexical processes in English; some examples","paragraphs":["of such processes may be found in"]},{"title":"[I] and [2].","paragraphs":["The underlying forms associated with relative clauves and nominal compounds"]},{"title":"in","paragraphs":["the model of nominal c om pounding being presented here are networks (trees for the most part) defined in terms of a case grammar which"]},{"title":"is","paragraphs":["closely related to that used by Simon8 [ 51. The cases which appear in this system fall into two general categories: (1) cases of the clause verb, which are the following"]},{"title":"--","paragraphs":["Performer, Object, Goal, Source, Location, Means, Cause, and Enabler"]},{"title":"--","paragraphs":["and (2) structural cases, which are REEL (relative clause) and COMP (compound). I will not explain these cases in detail, as that is the subject of a forthcoming paper. But the following observations will illuminate the case system for verb cases. The case system distinguishes the immediate performer of an act from a remote cause or agent of the act. The reason for this distinction lies in an intimate connection between verbs and the assumed or habitual performer of the act which is the reference of the verb. The case system also distinguishes an active causative agent of an act from an agent which merely permits thk act to occur; this distinction"]},{"title":"in the case system","paragraphs":["permits two classes of verbs to be distinguished according Po whether the surface subject commonly causes the act or permits the act to occur. The case system used in the present model of nominal campounding is not a deep case system; on the contrary, it Seems that nominal compounding is a lexical process which occurs rather near the surface in a derivatidnal grammar model. An example which can be given to support this is the compound ignition"]},{"title":"- key;","paragraphs":["this is a key Vhich turns a switch which enables"]},{"title":"a complex sequence of events to take place that ultimately result in the ignition of a fuel/air mixture","paragraphs":["in an engine,"]},{"title":"ar","paragraphs":["one may describe it equivaaently as a key which causes ignition. The first aescription corresponds to a deep case level of description while the second corresponds to the level at which the compound ignition key is formed."]},{"title":"I would argue tHat if one takes the deep case approach, then one is forced to include a great deal of structure in the rules for nominal compounding; in particular, the rule","paragraphs":["for ignition - key must remove all af the links in the causal chain leading to the ignition act. The deletion of this intermediate information mast be done to obtain the description given in the second case, and to include the deletion procedure in both a compounding rule and in the rule process which leads to the shorter description means unnecessarily duplicating the procedure. Moreover, if one derives compounds from paradigm relative clauses of the second sort, e.g. key which causes an action to occur, then it is possible to generalize compound forming rules so that a siqgle rule may produce several compounds. It will not be possible to do this if deep cases are used as the deep case structure of firing key will be quite"]},{"title":"- different from that of ignition key. In order tb understand the model of compounding whfch is being presented here, it is essential","paragraphs":["to consider the function of wmpounding in language. In my view, compounding is a process which allows a speaker to systematic8lly deLete information from an utterance just when the speaker has reason to expect that the hearer can reconstruct that information. In effect, I consider compounding (and a great many other linguistic procesbes) ro be examples of linguistic encoding which are used to speed up communication, and the grammar shared by the speaker and hearer must include the encodihg and decoding functions.","Consider the nominal compound steam distillation, which refers to the distillation of same substance with steam; the hearer of the compound steam distillation knows that distillation is the derived nominal form of distill. The hearep also knows what the common or characteristic cases of the verb distill are: the agent is invariably a person or machine (this would be the occupant of the Cause case slot in my system), the instrument (or Means) may be an apparatus or a heated medium such as steam and the Goal is a liquid which is missing some of the constituents that it entered the distillation process with. It happens that in English, whenever a, derived nominal of an act is the right element in a compound, then the left element is almbst always an occupant of one of the case slots of the verb. In order to recreate the underlying relative clause structure, it is only necessary for the hearer to properly choose the case for the nominal steam. A great deal of lexical information can be brought to bear on this question; for example, steam is not a liquid, is water vapor and thus cannot substance or the end product of a distillation process. Steam might be the Cause of the act of distillation except that there do not seem to be any compounds in English which have distillation as the right element and a Cause as the left element. Thus the hearer can assign steam to the Means case with some assurance."]},{"title":"-","paragraphs":["In another example, shrimp boat, the hearer can ascertain by lexical relations involving the word boat, that boats are characteristically used to catch marine life. One choice eor the main verb in a synonymous relative clause is catch, which will have boat as an element of the Means case. The Cause for catch is commonly a person or perhaps a sophisticated machine designed to oatch things (i.e. a trap). The Object-is characteristically an animal. There is a strong characteristic relation between the animal being caught and the means used to catch it, for example mink is trapped, calves are roped, birds are netted, and fish are caught with a boat. This relation exists as a rule in the lbxfcon of both the speaker and the hearer and it enables the speaker to produce the nominal compound and the hearer to understand it. Furthermore, shrimp"]},{"title":"-","paragraphs":["boat is one member of a class of closely related nominal compounds whioh includes lobster"]},{"title":"- boat, whale","paragraphs":["boat tuna boat and many others. It would be most"]},{"title":"- -' interesting if a single rule could be formulated which","paragraphs":["would generate all of these cqpounds. A lobster boat is a boat which is used to catch lobster, a tuna boat is a boat which is used to catch tuna, and so forth. All of these examples are identical except for the particular marine animal being caught. The logical next step"]},{"title":"is","paragraphs":["the creation of a rule which generalizes the individual marine anfmals to the cbmmon category of marine animal. This rule fill state that a marine animal boat is a boat which is used to catch marine animals, In making this generalization, I have given the rule the power to help interpret novel compounds and to generate them. With this power comes a difficulty, Which is constraining the rule so that it does not generate bad compounds or produce incorrect interpretations. The key to this constra'int lies in what I will term the characteristic or habitual aspect of nominal compomds. In the case of the boat compounds, a boat will only be a shrimp boat if it is characteristically, usually, habitually or invarfably used to catch shrimp. So the operation of a compounding rule is enabled only if a characteristic aspect is associated with the verb; in English, this is usually indicated by an adverb or an adverbial phrase. If the speaker is willing to assert that a boat is characteristically used to catch turtles, then the nominal compound turtle boat may be used. The hearer sill use the general rule to place turtle and boat in the proper case slots, and because a compound was used by the speaker, the hearer will infer Qhat the boat is one which is characteristically used to catch turtles, There are other problems which arise with the generalization of rules; for example, compounding never produces a compound in which the lert element"]},{"title":"is a proper noun, unless the","paragraphs":["proper noun ie the name of a process (e.g. Harkov process) or is a Source, Performer, or Goal of an act of giving. It also seems to be true that compounds"]},{"title":"are not","paragraphs":["generally formed when a lexical item is several levels below the general term which appears in the rule (e.g. repaimidget) or when a cross-classificatory term is used (e.g. automobile Indian as an Indian who repairs automobiles). With all of the preceding discussion"]},{"title":"in mind, I would","paragraphs":["now like to turn to the model of nominal compounding which I have presently implemented and running. The Computer Model The computer model of compounding accepts relative clause structures as input and produces nominal compound structures as output when the input is appropriate. It is written in a language with many parentheses the language was chosen for its program development facilities, i.e. built-in editor, rather than for its interpretive capabilities. The program which produces nominal compounds is a pattern matching interpreter; it applies a rule of compound formation by matching one side of the rule with the input structure, and if certain criteria are satisfied by the match, items from the input structure are bound into the rule, transferred to the other side of the rule, and a copy is then maae af the other side of the rule. The result is a nominal compound structure. The model has two components: a rule interpreter and a lexicon of rules for compounding. There is nothing tricky about rule application. Consider the nominal compound flower market and its associated relative clause paraphrase"]},{"title":"-","paragraphs":["market where flowers"]},{"title":"-","paragraphs":["are characteristically sold. These phrases have in my system the underlying structures shown in Figure 1. The notation in square braces means that the verb sell has the characteristic aspect in this instance. market"]},{"title":"I RELCL sell","paragraphs":["[+char]"]},{"title":"m / \\J-market flowers","paragraphs":["Figure 1. market flower These two structures can be made into a rule by linking them together. Whenever a relative clause structure identical to that in Figure 1 is received, the rule applies and a copy is created of the nominal compound flower"]},{"title":"-","paragraphs":["market. The matching procedure is"]},{"title":"a","paragraphs":["relatively straightforward, top down, recursive process which has backtracking capability in the event that a structure or case occurs more than once at any given level of the structure. There are two problems which arise; however: if e rule is generalized to account for compounds other than flower market, then the lexical items in the rule will behave as variables and some provisions must be made for binding of values to these variables; also, the rule interpreter must have some heuristics for selecting appropriate rules if the time required to produce a compound is not to increase exponentially with the size of the lexicon. The present version of the model only partly solves the binding problem. Consider the rule given in Figure 2 which is a generalization of that given in Figure 1. market I sell [+char] LOC market goods market"]},{"title":"I","paragraphs":["COW goods Figure 2."]},{"title":"If this rule","paragraphs":["is to apply to the relative clause structure glven in Figure 1 and generate the compound flower market, then the rule interpreter must recognize that the relative clause in Figure 1 is an instance of that given in Figure 2. The matching procedure does this by determining that the reference set of the nominal flowers is a subset of the reference set of the nominal goods. In addition, the nominal flowers must be carried across to the other side of the rule and substituted there for goods before the other side of the rule is copied. Thus market and goods must be bound across the rule so that whatever lexical item matches either of these nominals becomes the value associated with these nominals on the other side of the rule. In the initial version of the model, this binding was established explicitly when the rule was entered into the lexicon, but this seemed unsatisfactorily ad hoc. In a subsequent version,"]},{"title":"--","paragraphs":["the identity of the lexical items on both sides of the rule was the relation used to establish binding relationships. Consider, however, the structure shown in F:Lgure 3. person"]},{"title":"I RELCL","paragraphs":["steal [+char ] PERF/"]},{"title":"\\","paragraphs":["OBJ person valuables Figure 3 thief Here person should be bound to thief but the previous technique"]},{"title":"is","paragraphs":["not able to establish this binding. The reason that we know that person and thief should be bound is because we know that a thief is a person who steals characteristically. In the most recent version pf the model, this information is used to find the binding relationships when the rule of identity doe$ not work. The lexicon is searched"]},{"title":"for","paragraphs":["a rule which can be used to establish this bindiag. The rule which is used in the example shown in Figure 3 is displayed below in Figure 4. person"]},{"title":"I RELCL","paragraphs":["thief steal [+char ]"]},{"title":"I PERF person","paragraphs":["Figure 4 From the structures given in Figure 4, one can see that person shduld be bound to thief because the rule states that the reference set of thief is the same as the reference set of person as restricted by the relative clause. The technique of using lexical rules to establish bindings works in virtually every instance, but it has the defect of requiring that the information that a thief is a person who steals things be represented in the lexicon twice at least. A new model is under construction which attempts to reduce this redundancy by allowing the rules to have multiple left and right parts. The problem of selecting appr~priate rules is rather easier to solve. In most compounds in English, there is a characteristic association between the right element of the nominal compound and the main verb of the associated relative clause paraphrase. These two elements which occur on opposite sides of the compounding rule supply a great deal of information about the possibilities for application of the rule. So, in the model, each rule in the lexicon is indexed by the main verb of the relative clause and by the right element of the nominal compaund. This index actually contains some environmental information as well; for the clause verb, this environmental information is the case frame of the verb and the fact that it is the main verb of the relative clause"]},{"title":"-- for","paragraphs":["the compound nominal, the environmental information is just the fact that the nominal is the rightmost one in a nominal compound. The basic model has been tested with a set of several hundred nominal compounds and is very successful in coping with a wide vqriety of compound types. The productivity of the rules varies greatly; some rules may produce hundreds of compounds while other rules may only result in one or two compounds. Frozen forms such as keel boat are handled by a rule which generates only one compound; there is a rule for each frozen form. The rule structures contain exclusion lists associated with each lexical item in the rule, and these exclusion lists prevent the rule from operating whenever a lexical item matches one Of the items on an exclhsion list if the items occur at corresponding locations in the structures. The model is quite quick in operation; on a high speed dibplay console, it will generally produce compounds much faster, than a person sitting at the console can conveniently read them."]},{"title":"vi~","paragraphs":["is mainly due to the rule selection heuristic, but the match procedure has been carefully optimized as well. Conclusions The model program is an excellent demonstration of the appropriateness of the basic theory; moreover, the rules themselves"]},{"title":"can","paragraphs":["be generalized to deal with syntactic processes, so"]},{"title":"there is","paragraphs":["no discontinuity in the grammar model between the lexical processes and the syntactic processes. It seems clear that the rules could also be used to represent other lexical processes in language and this is currently being pursugd. There"]},{"title":"is","paragraphs":["no reason why the rules could not be used for recognition as"]},{"title":"well","paragraphs":["as for the production of nominal compounds."]},{"title":"The","paragraphs":["bindings are not one-way, and the matching procedure will"]},{"title":"work","paragraphs":["equally"]},{"title":"well for","paragraphs":["compound structures. The reasons why the computer model"]},{"title":"is","paragraphs":["a production model are: (1) that the computer model"]},{"title":"assumes","paragraphs":["the semantic correctness of the input relative clause structures, and (2) that compounds are often ambiguous and may be paraphrased by two or more relative clauses, while the converse of this is almost never true. A recognition model would have to generate underlying relative clause structures for each ambiguity and a semantic component would have to~screen the relative clauses for semantic errors. I hope that the reader has noticed the avoidance of rule procedures in this model."]},{"title":"When","paragraphs":["I began working on the design of the computer programs, I had in mind the creation of a model which once implemented"]},{"title":"in","paragraphs":["LISP could be extended merely by adding new ~ules without having to construct any additional LISP programs. I ultimately wanted to have a model which could lllearntl new rules by systematic generalization and restriction of existing rules. I feel that this would be relatively easy with rule structures and extremely difficult with rule procedures written in a programming language. Furthermore, I subscribe to Karl Popper's icfeas of scientific endeavour, and rule structures appealed because it would be more difficult to bury flaws or ill understood aspects of compounding and rule processes in structures than in procedures where the computational power of the programming language permits and"]},{"title":"even","paragraphs":["encourages"]},{"title":"--","paragraphs":["ad hoc solutions to be found to problems. Acknowledgements I would like to here acknowledge the suggestions made by Robert F. Simmons, Carlota Smith, Mary Boss T. Rhyne, Uurent Siklossy, and Stanley Peters which have helped tsprbve my understanding of nominal compounding. 1. Chomsky, N. \"Remarks on Naninalizatlon, '' in Readings"]},{"title":"- in EngXish Transformational G-r, Jacobs, R. and Rosenbaum, P. eds. Ginn, Waltham, Xassachusetts, 1970. 2. Gruber, J. vvStudies in Lexical Eklat ions.","paragraphs":["\" Ph. D"]},{"title":". thesis, HIT, 1965. 3, Lees, R, - The Grammar - of English NosninalTzations, Mouton, The Rague, 1968. 4. Rhyne, J. \"Lexical Rules and Structures in a Computer %lode1 of Nominal Compounding in English.\" Ph. D. thesis, The University of Texas at Austin, 1975. 5, Simmons, R. \"Semantic Networks","paragraphs":[": Their Camputat ion and Use for Understanding English Sentences, '' in Computer Models"]},{"title":"- of Thought and Language, Schank, R. and Colby, K. eds. W. H. - Freeman, San Francisco, 1973. American Journal of Computational Linguistics Microfiche 33","paragraphs":[": 45 Computer Science Department Indiana University Bloonlington 47401 ABSTRACT Generation of English surface strings from a semantic network is viewed as the creation of a linear surface string that describes a node of the semantic network. The form of the surface string is controlled by a recursive augmented transition network grammar, which is capable of examining the form and content of the semantic network connected to the semantic node being described. A single node of the grammar network may result in different forms of surface strings depending on the semantic node it is given, and a single semantic node may be described by different surface strings depending on the grammar node it is given to. Since generation from a semantPc network rather than from disconnected phrase markers,, the surface string may be generated directly, left to right. Introduction In this paper, we discuss the approach being taken in the Engllsh generation subsystem of a natural language understanding system presently under development at Indiana University. The core of the understander is a semantic network processing system, SNePS (Shapiro, 1975), which is a descendant of the MENTAL semantio subsystem (Shapiro, 1971a, 1971b) of the MIND system (Kay, 1973). The role of the generator 13 to describe, in English, any oP the nodes in the sernantjc network, all of which represent concepts of the understanding aystem. 46 and other computations are ~equired in the process of pasting these trees tog ther in appropriate places until a 'single phrase marker Is attained which will lead to the surface string. Since we are generating from a semantic network, aL1 the pasting together is already done. Grabbing the network by the node of interest and letting the network dangl-e from it gives a structure mich may be searched apppogriately in order to generate the surface strfng directly in left to right fashion. Our system bears a superficial resemblance to that described fn Simmons and Slocum, 1972 and in Simmons, 1973. That system, however, stores surface information such as tense and voice in its semantic rietwork and its ATN takes as input a linear list containing the semantic node and a generation pattern consisting of a \"series of constraints on the moclalltyfl (Simmons et al., 1973, p. 92 The generator d.escribed in Schank et al., 1973, translates from a \"conceptual structuref1 into a network of the form of Simmons"]},{"title":"' network","paragraphs":["which is then given to a version of Simmons generation program. The two stages use different mechanisms. Our system amounts to a unificatio of these two stages. The generator, as described in this pager, as well as SNePS, a parser and an inference mechanism have been written in LISP 1.6 and are running Interactively on a DEC system-10 on the Indiana University Computing Network. Representation in the Semantic_Network Conceptual information derived from parsed sentences or deduced from other information (or input directly via the SNePS user's language) is stored in a semantic network. The nodes in the network represent concepts which may be discussed and reasoned abaat. The edges represent semantic but non-conceptual binarx relations between nodes. There are also auxiliary nodes which SNePS can use or which the user can use as SNePS variables. (For a more complete diecussion of SNePS and the network nee Zhapiro, 1975.) The semantic network representation being used does not in- 47 olude information considered .t.6 be features of' the surface string such as tense, voice or main vs. relative clause. Ihstead of tense, temporal information is stored PeIative to a growing time line in a manner similar to that of Bruce, 1972. From this information a tense can be generated for an output sentence, but it may be a different tense than that of the original input sentence if time has progressed iri %he 5nterim. The voice of a generated sentence is usually determined by the top level call to the generator function. However, sometimes it is determined by the generator grammar. For example, when generating a relative clause, voice is determined by whether the nodn being modified-is the agent or object of the action described by the relative clause. The Main clawe of a generated sentence depends on which semantic node is given to the generator in the top level call. Other nodes connected to it may result in relative clauses being generated. These roles may be reversed in other top level calls to the generator. The generator is driven by two sets of data: the semantic network and a grammar in the form of a recursive augmented transition network (ATN) similar to that of Woods, 1973. The edges on our ATN are somewhat different from those of Woods since our view is that the generator is a tranducer from a network into a linear string, whereas a parser Is a transducer from a linear string into a tree or network. The changes this entails are discussed below. During any point in generation, the generator is working on some particular semantic node. Functions on the edges of the ATN can examine the network connecteb to this node and fail or succeed accordingly. In this way, nodes of the ATN can \"decide\" what surface form is most appropriate for describing a semantic node, while different ATN nodes may generate different surface foI?ms to describe the same semantic node. A common assumption among linguists is that generation begin3 with a set of disconnected deep phrase markers. Trnnuf o~-matlon~ LEX Fizure 1: Semantic Network RepresentatLon for \"Charlie believes that a dog kissed sweet young Lucy,\" \"Charlie is a person,\" and \"Lucy is a person. ,.Af=rmation considered to be features of surface strings are not stored in the semantic network, but are used by the parser in constructing the network rrom the input sentence and by the generator for generating a surface string from the network. For example, tense is mapped into and from temporal relations between a node representing that some action has, is, or will occur and a growing time line. Restrictive relative clauses are used by the parser to Identify a node being discussed, while non-restrictive relative clauses may result in new information being added to the network. The example used in this paper is designed to illustrate the generation issues being discussed. Although it also illustrates our general approach to representational issues, some details will *(SNEG MOOLb) (CHARLIE IS BELIEVING THAT A DOG KISSED SWEET YOUNG LUCY),"]},{"title":"* (SNEG M0023) (A DOG","paragraphs":["KISSED SWEET YOUNG LUCY) *(SNEG M0007) (CHARLIE WHO IS BELIEVING THAT A DOG KISSED SWEET YOUNG LUCY) \"(SNEG i4OOd;j (CHARLIE IS A PERSON WXO IS BELIEVING THAT A DOG KISSED SWEET YOUNG LUCY)"]},{"title":"* (SNEG M0006)","paragraphs":["(CHRRLIE~WHO IS BELIEVING THAT A DOG KISSED SWEET YOUNG LUCY IS A PERSON)","(SNEG M0008) (THE BELJEVING THAT A DOG KISSED SWEET YOUNG LUCY BY CHARLIE)"]},{"title":"* (SNEG M0011) (A DOG WHICH KISSED SWEET YOUNG LUCY) *(SNEG ~0OlOj (THAT WHICH","paragraphs":["KISSED SWEET YOYNG LUCY IS A DOG)"]},{"title":"* (SNEG M0012) (THE KISSING OF SWEET","paragraphs":["YOUNG LUCY BY k DOG) @(SNEG M0020) (SWEET YOUNG LUCY WHO WAS KISSED BY A DOG) *(SNEG M0014) (LUCY IS A SWEET YOUNG PERSON WHO NAS KISSED BY A DOG) *(SWG M0015) (SWEET YOUNG LUCY HH3 WAS KISSED BY A DOG IS A PERSON) *(SNEG M0017) (SWEET LUCY WHO WAS KLSSED BY A DOG IS YOUNG) *(SNEG M0019) (YOUNG LUCY WHO WAS KISSED BY A DOG IS SWEET)","Figure 2: Results of calls to the generator with nodes from Figure I* User input is on lines beginning with"]},{"title":"*.","paragraphs":["certainly change as work progresses. Figure 1 shows the semantic network representation for the information in the sefitencesb, \"Charlie believes that a dog kissed sweet young Lucy,\" \"Charlie is a person,\" and \"Lucy is a person.\" Converse edges are not shown, but in all cases the label of a converse edge is the label of the for-ward edge with"]},{"title":"'*'","paragraphs":["appended exoept for BEFORE, whose converse edge is labelled AFTER. LEX pointers point to nodes containing ;lexical entries. STIME points to the starting time of an action and ETIME to its ending time. Nodes representing instants of time are related to each other by the BEFORE/AFTER edges. The auxiliary node NOW has a :VAL pointer to the current instant of time. Figure 2 shows the generator's output for many of the nadcs of Figure 1. Figure 3 shows the Lcxicon uncd in the example."]},{"title":"(BELIEVE((CTGY.V)(I~BELIEVE) (PRES.BELIEVES)(PAST.BELIEVED)(PASTP.BELIEVED)(PRESP.BELIE~JING)\\)) (CHARLIE","paragraphs":["( (CTGY"]},{"title":". NPR) (PI . CHARLIE)","paragraphs":["1') (DO~'((CTQY.N)(SING.DOG)(PLUR.DOGS))) (K~SS((CTGY"]},{"title":".v) CINF.KISS) ~PRES.KISSES)~PP.ST.KISSED~~PASTP.KISSED)(PRESP.KISSING~~) (LUCY-((CTGY.NPR)(PI.LUCY))) (PERSON((CTGY.N!(sING.PERSON)(PLUR.PEOPLE)~~ (SWEET(~CTGY.ADJ)(PI.SUEET)","paragraphs":["1) (YO~NG((CTGY.ADJ)(PI.YOUNG))) Figure 3: The lexicon used in the example of Figures 1 and 2. Generation as Parsing Normal pa~sing involves taking input from a linear string and producing a tree or network structure as output. Viewing this in terns of an ATN grammar as described in Woods, 1973, there is a well-defined next input function which simply places successive word6 into the** register. The output function, however, is more complicated, uslng BUILDQ to build pieces of trees, or, as in our parser, a BUILD function to build pieces of network. If we now consider generating in these terms, we see that there is no simple next input function. The generator will focus on some semantic node for a while, recursively shifting its attention to adjacent nodes and back. Since there are several adjacent nodes, connected by variously labelled edges, the grammar author must specify which edge to follow when the generator is to move to another semantic node. For these reasons, the same focal semantic node is used when traversing edges of the grammar network and a new semantic node is- specified by gl,ving a path from the current semantic node when pushing to a new grammar node. The register SNODE is usedto hold the current semantic node. The output function of generation is straightforward, simply being concatenation onto a growing string. Since the output string is analogous to the parser's Input string, we store it in the reg-","garc : := (TEST test [action]* (TO gnode) ) (JUMP [action]*(TO gnode))"]},{"title":"(mM wform (wqrd*) test [action]*(TO","paragraphs":["gnode)) (NOTMEM wform (word#) test [action]*(T~ gnode)) (TRANSR ([regname] regname regname) test [action]* (TO gnode) ) (GEN gnode sform [action]*regnarne [action]*(TO gnode))","sform ::= wform SNODE","wform : := (CONCAT f~~m form*) (GETF sarc [aform]) ( GETR re gname ) (LEXLOOK lf e at [ sf orm] ) sexp","form -: := wform sform","act ion : := (SETR regname f om (ADDTO regname form* ) (ADDON regname f om* ) sexp","test :: (MEMS form form) (PAT23 sform sarc* sform) form sexp gnode : := <any LISP atom which represents a grammar node> word : := <any LISP atom> regname ::= <any non-numeric LISP atom used as a register name> sarc ::= <any LISP atom used as a semantic arc label> lfeat : := <any LISP atom used as a lexical feature> sexp : := <any LISP s-expression> Figure 4: Syntax of edged of generator ATN grammars ister"]},{"title":"*.","paragraphs":["When a pop occurs, it is always the current value of"]},{"title":"*","paragraphs":["that is retu~ned. Figure 4 shows the syntax of the generator ATN grammar. Object language symbols are )"]},{"title":",","paragraphs":["(, and elements in capital letters. Meta-language symbols are in lower case, Square brackets enclose optional elements. Elements followed by"]},{"title":"*","paragraphs":["may be repeated one or more tjmes. Angle brackets enclose informal English descr~ptions. Semantics of Eage Functions In this section, the semantics of the grammar arcs, forms and tests ape preaehted and compared to those of Woods1 ATNs.? The --- t All comparlsona arc with Woodo, 1973. NCL ') JUMP(SETR"]},{"title":"*","paragraphs":["@(//// NO GRAMMAR NODE FOUND)) MEND ) Figure 5: The default entry into the grammar netwo~k. essential differences are those required by the differences between generating and parsfng as discussed in the previous section. (TEST test [action]*(TO gnode)) If the test is successful (evaluates to non-NIL), the actions are performed and generation continues at gnode. If the test fails, this edge is not taken. TEST- is the same as WoodsT TST, while TEST(GETF sarc) is analogous to Woods' CAT. (JUMP action]*(^ฬ gnode)) Equivalent to (TEST T [actlon]*(TO gnode)). JUMP is simllar In use to Woods JUMP, but the difference from TEST T disappears since no edge r'consumesl' anything. (MEM wform (word*) test [action]* (TO gnode) ) If the value of wform has a non-null intersection with the list of words, the test is performed. If the test is also successful the actions are performed and generation continues at gnode. if either the Intersection is null or the test fails, the edge MEM(GETR VC)(PASS)T G-EN NCLNP(~ETF OBJECT)","(ADDTO DONE SNODEI* ( SREG PRED) OEN NCLNP(GETF AGENT)(ADDTO DONE SNODE)* Figure 6: Generation of subject of sub ject-verb-ob ject sentence. is not taken. This is similar in form to Woods1 MEM, but mainly used for testing registers. (NOTMEM wform (word*) test [action]*(TO gnode)) This is exactly like MEM except the intersection must be null. (TRANSR ([regnamel] regname2 regname3) test [action]* gnode) ) If regnamel is present, the contents of regname2 are added on the end of regnamelo If regname is empty, the edge is not 3 taken. Otherwise, the first element in regname is removed and 3 placed in regname2 arid the test is performed. If the test fails, the edge is not taken, but if it succeeds, the actions are performed and generation continues at gnode. TRANSR is used to iterate through several nodes all in the same semantic relation with the main semantic node. (GEN gnodel sform [action]*regname [action]*(TO gnode*)) The first set of actions are performed and the generation is called recursively with the semantic node that is the value of sform and at the grammar node gnodel. If this generation is successful (returns non-NIL), the result is placed in the register regname, the second set of actions are performed and generation continues at gnode*. If the generation fails, the edge is not taken. This Is the same as Woods1 PUSH but requires a semantic node to be npecified and allows any register to be used to hold the result. In-stead of having a POP edge, a return automatically occurs when (ADDON"]},{"title":"*","paragraphs":["@WILL @HAVE) (ADDON"]},{"title":"*","paragraphs":["QWQULD) UI@(ADDON"]},{"title":"*","paragraphs":["@(///CANNOT COMPUTE TENSE)) TEST( MENS (GETR REF) (* @NOW) ) (ADDON PIS') \\a MEM(GETR VC) (PASS)T(ADDON"]},{"title":"* @BEEN) CVPF MEM(GETR VC)(PASS)T(ADDON * @BE)","paragraphs":["(tp~ JUMP @FAST~ JUMP(ADD0N"]},{"title":"* (LExLOOK PASTP(GETF VERB))) JuMP(ADDON * (LEXLOOK INF(GETF","paragraphs":["VERB)))"]},{"title":"CTIINF","paragraphs":[") - Figure 7: Tense generation network. transfer is made to the node END. At that point, the contents of the register named"]},{"title":"*","paragraphs":["are returned. (CONCAT form form*) The forms are evaluated and concatenated in the order given. Performs a role analogous to that of Woodst BUILDQ. (GETF sarc [sform]) Returns a list of all semantic nodes at the end of the semantic arcs labelled sarc from the aemantic node whl ch is the value","Figure 8: The tenses of \"breakn which the network of Figure 7 can generate. of sfomn. If sform is missing, SNODE is assumed. Returns NIL if"]},{"title":"I","paragraphs":["there are no such semantic nodes. It is similar in the semantic Tense past futil~e present prog~essive past progressive future progr6ssive past In future future fq past Active Passive domain to Woods' GETF in the lexical domatn. broke will break is break-lng was breaking will be breaking'"]},{"title":"will","paragraphs":["have broken would break (GETR regname) Returns the contents of register regname. It is essentially the same as Woodst GETR. was broken will be broken is being broken was being broken will be being broken will have been broken"]},{"title":", wquld be","paragraphs":["broken"]},{"title":".","paragraphs":["(LEXLOOK lfeat [sform]) Returns the value of the lexical feature, lfeat, of the lexical entry associated with the semantic node which is the value of sform. If sform is missing, SNODE is assumed. If no lexical entry is associated with the semantic node, NIL is returned. LEXLOOK is sirnilar to Woods1 GETR and as also in the lexical domain. (SETR regnarne form) The value of form is placed in the register regname. It is the same as Woods1 SETR. (ADDTO Pegname form*) Equivalent to (SETR regname (CONCAT (GETR regname) form*)). Equivalent to (SETR regname (CONCAT form* (GETR regname))). (MEW form form) Returns T If the values of the two forms have a non-null intersection, NIL otherwise. TEST( GETF OBJECT ~PREDOBJ) UMP"]},{"title":"cpmDAm GEN","paragraphs":["KLNP (GETF AGENT) REG (ADDON"]},{"title":"*","paragraphs":["(GETR 'REG) )"]},{"title":"CpRGDOB3 GEN","paragraphs":["NCLNP (GETF OBJECT) REG (ADDON"]},{"title":"* (GETR REG","paragraphs":[") )"]},{"title":"I","paragraphs":["Figure"]},{"title":"9: Generating","paragraphs":["the surface object. (PATH sfoml sarc* sformp) Returns T if a path descr3bedl By the sequence of semantic arcs exists between the value of sfoml and sformp. If %he sequence is sarcl sarc2"]},{"title":"...","paragraphs":["sarc,, the path described is the same as that indicated by sarcl* sarc2*"]},{"title":"... sarcni. If no such","paragraphs":["path exists, NIL is returned. (Remember,"]},{"title":"* means","paragraphs":["repeat one or more times. ) Discussion of an Example Orammar Network, The top level generator function, SNEG, Ls given as arguments a semantic node and, optionally, a grammar node. If the grammar node is not given, generation begins at the node G1 which should be a small disccirnination net to choose the preferred description for the given semantic node, This part of the example grammar is shown in Figure"]},{"title":"5. Jn it","paragraphs":["we see that the preferred description for any semantic node is a sentence. If no sentence can be formed a noun phrase will be tried. Those are the only presently available options. Semadtic nodes with an outgoing VERB edge can be described by a normal SUBJECT-VERB-OBJECT sentence. (For this example, we have not used additional cases.) First the subject is generated, Figure 10: Generating the three \"non-regular\" sentences. which depends on whether the sentence is to be in active or passive voice. Alternatively, the choice could be expressed in terms of whether the agent or object is to be the topic as suggested by Kay, 1975. Figure 6 shows Lhe network that generates the subject. The register DONE halds semantic noaes for which sentences are being generated for later checking too prevent infinite recursion. WPthout it, node MOO23 of Figure I would be described as, \"A dog which kissed young sweet Lucy who was kissed by a dog which kissed."]},{"title":"..\"","paragraphs":["The initla1 part of the PRED network is concerned with generating the tense. This depends on the BEFORE/AFTER path between the starting and/or endinb time of the action and the current value of NOW, which,is given by the form (# @NOW). Figure 7 shows the tense generation network. Figure 8 shows the tenses this network is able to generate. After the verb group is generated, the surface object is generated by describing either the semmtic agent or object. Figure 9 skuws this part of the network The other three kinds of sentences are lor describing nodes representing: (1) that something has a particular adjective attribu-able to it, (2) that something has a name, (3) that something is a member of some class. The networks for these are shpwn in Figure 10. Again, the DONE register is used to prevent such sentences as \"Sweet young Lucy Is sweet,\" \"Charlie is Charlie.\" and \"A dog is a dog.\" GEN 3 (GETF OBJECT(GETF VERB*))REG(ADDON"]},{"title":"* @THAT(GETR","paragraphs":["REG)) cICL>GEN SREG SNODE *(ADDTO"]},{"title":"* @THAT) Figure 11:","paragraphs":["Generating norninalized verbs and sentences. Pugure 5 showed three basic kinds of noun phrases that can be generated: the noun clause or nominalized sentence, such as \"that a dof~ kissed sweet young Lucyt'; the nominalized verb, such as \"the kisslng of sweet young Lucy by a dogn; the regular noun phrase. The first two of these are generated by the network shown in Figure 11. Bere DONE is ased to prevent, for example, \"the kissing of sweet young Lucy who was kiesed by a dog by a dog.\" The regular noun phrase network begins wlth another descrimination net which has the following priorities: use a name of the object; use a class the obJect belongs to; use something else known about *he obJect. A lower priority description will be used if all higher priority descriptions are already in DONE. Figure 12 shows the be-glnnlrrg of the noun phrase network. Adjectives are added before the mame or before tkre class name and a relative clause is added after. GEN ADJS SNODE *(ADDTO"]},{"title":"* @A)","paragraphs":["Cm&uMp (sETR"]},{"title":"* BA) NEM~","paragraphs":["(LEXLOOK SING(GETE CLASS(GETF MEMBER*)))) Figure 12: The beginning of the noun phrase network. Figure 13 shows the adjective string generator and Figure 14 shows the relative clause generator. Notice the use of the TRANSR edges for iterating. At this time, we have no theory for determining the number or which adjectives and relative clauses to generate, so arbitrarily we generate all adjectives not already on DONE but only one relative clause. We have not yet implemented any ordering of adjectives. It is merely fortuitous that \"sweet young Lucyt is eenerated rather than \"young sweet Lucyft. The network is written so that a relative clause for which the noun is the deep agent is preferred over one in which the noun is the deep object. Notice that this choice determines the voice of the embedded clause. The fomn (STRIP(FIND MEMBER (1. SNODE) CLASS (FIND LEX PERSON))) is a call to a SNePS function that determines if the object is known to be a person, in ~hich case \"WHO\" is used rather than \"'WHICHft. This determination is made by referring to the semantic network rather than by including a HUMAN feature on the lexical entries for LUCY and CHARLIE. nDJs7JUMP(SETR ADJS(GETF WEICH*)) Figure 13,: The network for generating a string of adjectives. Notice that any information about the object being described by a noun phrase may be used to construct a relatfve clause even if that Lnfomnation derived from some main clause. Also, while the generator is examining a semantic node all the information about that node is reachable from it and may be used directly. There is no need to examine disjoint deep phrase markers to discover where they can be attached to each other so that a complex sentence can be derived. Future Work Additional work needs ts be done in developing the style of generation described in this paper. Experience with larger and richer networks will lead to the following issues: describing a node by a pronoun when that node has been described earlier in the string; regulating verbosity and complexity, possibly by the use of resource bound8 simulating the limitations of short term memory* keeping sub-"]},{"title":", ordinate clauses and descriptions to","paragraphs":["the point of the conversation posslbly by the use of a TO-DO register holding thenodes that are to be included in the string. In thla paper, only indefinite descriptions were generated. We are working on a routine that wlll Identify the proper subnet of the semantic network to justify a definite description. This must be such that it uniquely identiflee the node being described. (SETR VC @ACT) JUMP \\JUMP F JUMP TEST(STRI? !*I!JD :.lEMEEFl(f SI\\l; DE)CLASS (YT?ID EX PERSON) ) ) Figure 14: The relative clause generator. Acknowledgements The author is indebted to John Lowrance, who lfiplemnted the generator, Stan Kwasny, who implemented the parser, Bob Bechtel, who worked but the temporal representation, Nich Vitulli and Nick Eastridge, who implemented versions of SNePS, and Jim McKew for general software ?upport. Computer service was provided by the IUPUI Computing Facilities. Typing and graphics were d~ns by Christopher Charles.","Bruce, B.C. 2972, A model for temporal references and its application in a question answering program, Artificial Intelligence 3, 1, 1-25.","Ray, M. 1973. The MIND system. Natural hanguage, Probesstng, Re Rustin (EB. )"]},{"title":", AlgorithmScs Press, Mew Ysrk, 155-188.","paragraphs":["'","Kay, M. 1975. Syntactic processing and functional sentence perspective. Theoretical Issues in Natural Language Processfng R, Sch& and B,L, maah-Webber (Eds,), Bolt Beranek,& Newman, fnc., Ombrfdge, Massachusetts. Schan~, R.C.; @oldman, ; Rteger, C"]},{"title":", 111;","paragraphs":["and Riesbeck, C. 1973. NARGlE: memory, analysis, responae generation, and inference on English, Proc, Third xntematf onal, Joint Conference on Arti. ficiql, ,Intelligence, Stanford University, August 20-23, 255-26T.","Shapiro, S,C. 1971a. The WIN3 system: a data structure for semantic Infornation processlrig. R-837-PR. The Rand Corp"]},{"title":". ,","paragraphs":["Santa Monica, Cal%fornla.","Shapirs. S.C. lgrlb. A net structure for semantic information - storage, deduetlon and retrieval. 2nd International Joint- Con- - .- Shapiro, S. C. 1975 * bAn 2 ntroduction to SNePS"]},{"title":". Technical","paragraphs":["Report No. 31, CumpuLer Science Department, Indiana University, Bloom-f ngt on,","SS@@ons, R,F. 1973, Semantic networks: their cornputatSon and use","for understanding English sentences. computer Models of Thohght","R.C* Schank and K.M. Colby (Eds.), W.B. Freeman","Francisco, 63-113.","Sirnone, R.B., and Slocurn, 3. 1972. Generating English discourse from rralnantic net6 + Corn,. ACM 15, 10, 891-905.","Woods, W.A+ 1973. An exgerlmental parsing system for transition ne %work gramars"]},{"title":". ,","paragraphs":["R. Rustin (Ed.), Algsrf tmioa Press,"]},{"title":"American Journal of Computational Linguistics","paragraphs":["~icrofiche 33 : 63 Artificial Intelligence Center Stanford Research Institute Men10 Park, California 94025 ABSTRACT 8atur.l languag8 output"]},{"title":"can b* generatrd fram remantic nets by ptoc*rsing ternplats8 asroeiated with concept# in the netr A #qt at verb teaplater is belng","paragraphs":["derived"]},{"title":"from a Study of the surfrc9 syntax of ran@ 3000 Englirh Verb88 fhe actlve forms of the verb$ have been tZ~rriflsd rtcorb&n$ to subjectr objeetCm1~ and compl~aent(r1) there Syntactic Patterns, augmented with case nams, ara used as a grammat to Cantrol th4 generation Ot text, Thlr text in turn is pasrad through a speech syntheri8 program and output by 4 VOTRAX speech rynth4o&zax, ThLr analyrar rhould ultimately benefit systems attsaptlng to understand Erl~li~h input by praviding surface etructurs to deep cul itrueturd maps using the rare trmpirter rr emp1Qytd by the generator. Thfr reararch w4r rupportrd by the Detenra Advaneed Reuearch Proleeta Agancy af th* Dcprrtmmnt @t D~fenre and manit6rrd by the U, 8. &rmY Rs#~rrch Offica under Cantract No, DAHC04~75-C-0006. TNTRQDUCTXON It computer6 rtr to canmonicrtr eftretlvcly with people, they nust spark, or at irart write, the urcr*r nrtur61 Language, The bulk ot the work in co~pytatlonrl Llnguilticr has been drvotrd to computer undcrstandlnq of natutal language input# but relatively littie rtfo<rt ha8 been e%pend@d in developin$ natural Language output. Mort Enplirh output systems hrve been along the line of nfill In the blanku with Perhap& soma semantic cenrtrrint6 imporad! thera have bean few attempts at language generation from what one could crLl wsamrntic netR structures [8g@aon8 and 8locua, 1972; Sloeuar 1973; Goldman, 19741, PeFh&p@ generation is canrldered a much aasisr problem, The ruccrrr of understanding efforts ir generally bclisvtd to drprnd on rome warkrble theory of Rdlrcaurrr 6rganlzatlonc which would rccbunt","paragraphs":["gar affect8 of"]},{"title":"context and would ahow how iniphoric ~~pren8l~ns (pronoun# and noun phrases) are resolved and how #rnt*nCeS rro order84 tn the bUtPUtr As","paragraphs":["tt hrppdnot"]},{"title":"there m@chanlr@r are pr~L#*lY those that r wrr8ponre g~nerrtor~ must Incorporate it it 1s to appear Lntrlligent. Tha lrtudy of qrnrtrtion rill play an important rots in rolving the problem of undrrttandino if it ern draonrtrrte","paragraphs":["& mapping"]},{"title":"t~om derp ramantic rtrUctura6 to rutface rtrtngr, bet ua briefly outlina gome relevant preccrrrs in the #peach understanding system bring drYcL0~1d by 8RI and SDC (Walker at a1.t 1915~ and Rite., 1915). The urar inltlatrs rrrrton by rrtrblirhtng conmunLc~tion with the ryrtrmf all subarqurnt dialog (input and output) is manlter@d by a Wdircaurae madula8 (Dautacb, 11975) to maintain rn accurate ca~ntrerskition41 context, An exeeuttvc eserdinata# variaur Knarladge sauxcos 4cWrti~r prosodic, syntactic, r~mrntie, pzagmrtlc, and di~eeurrc to Bunderotrnbw cuccersivt utt@rane@rr The analyzed UttaFlnge iQ then Pas8rB","paragraphs":["la ZRa"]},{"title":"\"resPandarw PBItll another eamponrnt of the dileaurtc module. The responder may call the pucrtlonransrcrrr it the lnput ir a question!","paragraphs":["it may"]},{"title":"call 4 data bead update Program It ths","paragraphs":["input is"]},{"title":"r otatcmant","paragraphs":["of"]},{"title":"fact1 or it may drcldr an romc other rppropriatc reply. Tha content ab the r@DpDnD@ Is prrora to the gsnaratar~ perhapa #i$h Some tndicrtian of hsa","paragraphs":["it"]},{"title":"is to be tarmulat@ds TRa reply may bs a rtsrcatypsd rsrpsnss (wy@8Bp \"noM, \"1 @e@C)I noun phrars (nodel, r o@nt+nes (verb node)r Orr e~~ntu111y~","paragraphs":["a paragraph, The qen~PaQ~r outputs stettotyped raspanreg Pmmed$atslyt if! the"]},{"title":"rcrpanrr ir nor@ complicated","paragraphs":["[a"]},{"title":"\"nounB nab@, *verbR nads, ar rvcnturlly r nrtwork), r mbrr detallcd","paragraphs":["proptam ir ragulrcd. This program"]},{"title":"nbLl dctrrmina exactly how the responge is ts bs bormulrtad ma &g NBI 61 OH teqUIRCQ 8f St! it may bb","paragraphs":["~d~~lr6d to Chooag Verb@ and"]},{"title":"noun@ with which to @Xgs@@@","paragraphs":["the deep ClSc"]},{"title":"nae ctructurrr, rr wall r8","paragraphs":["a"]},{"title":"ryntaetlc from. for the genaration.","paragraphs":["The"]},{"title":"g~narator P~O~UCII tha rrrponrr in wfrxtw form1","paragraphs":["thir"]},{"title":"in turn ir p&gsad to a B~OIC~ tynthrllr program tar trrnrtarmatlon","paragraphs":["and cutput by"]},{"title":"r comn@rCial VOTRAX","paragraphs":["rprach"]},{"title":"rynthe~izar. Curr~ntly no IsntQner intenstion sr rtr~as con%sWing i~ being p@rt~rmed, 8tnea","paragraphs":["the"]},{"title":"major Bntmsost at this papsr","paragraphs":["it in \"textH ganaratlane further reference to the synfhcrlg rtrp rill be meden CQNSTRAXNTS Or? RESPONDING"]},{"title":"There are several censLdcr8tianr involved in responding appropriateby to an utterance. First, therc are *conver:ctlanal Po~tUl&te#\" (Gordon and Lakoff, 19751 8harsd by","paragraphs":["the u6ers aยฃ e"]},{"title":"Lanuurprt there pottu2ater serve","paragraphs":["to"]},{"title":"canstrain the content and form of ca~~unicationr from the speaker","paragraphs":["to the heerare For"]},{"title":"instance, the rperker should not tall the hearer re~sthing the hearer all@&dy knows, lcrt he be bore61","paragraphs":["yet the speaker cannot tell the"]},{"title":"hcarer samething the hearer knows ab~olutely nathlnq about, or the hearer rlll not comprehend. The ggenkee","paragraphs":["chould"]},{"title":"relate knt ncwa in his marsaga to the","paragraphs":["prtor knowledgs of the"]},{"title":"hearer; this requires the rperkcr","paragraphs":["te have ti model of! the haarcf There heuristic6 murt operate"]},{"title":"in canjunctbon w~th","paragraphs":["c w~~c~~nc~"]},{"title":"producerw to constrain what may be","paragraphs":["output by a"]},{"title":"q~entcncen generatore We are only beginning","paragraphs":["to understanG how to"]},{"title":"lncorparrtt there partuiater in a languagc groce6~bng kygtcm, Then there Is the matter of con8tructlng the batic sentence Normal English ryntax rcquirrr at ltaat ant verb In the santcncaf chooring a mrln verb constrains the rurfdct","paragraphs":["Rtructure, For inltrncer in thr rblcncr of campounds any verbs other than the main vwb"]},{"title":"will have to appear","paragraphs":["in another Fornr"]},{"title":"nominal, infinitive, gerundr p~rtLcIpL@~ or subordinate Cfauce, How daec the relevant fnfarmatlon centaintd In","paragraphs":["& gementic net Indicate tha rpproprlate"]},{"title":"farm? The traditlondl answer Is \"by meann of tht laxiC0nrU We","paragraphs":["will QXplorQ the relationship between net @r~d"]},{"title":"lexicon and rdvsnce a methodology far raprelcntlng a map tram deep case structura ta surface structure, Thlr prgrr ~OCULII an","paragraphs":["a philosophy ot"]},{"title":"sLnglam6cnfencr farmattinqr cmeaaing","paragraphs":["a"]},{"title":"main vatbr choosing tna grass structure st the","paragraphs":["output"]},{"title":"scntencer and deciding how to generate spgropriata noun phraras, Qur exarnpl~a will anB1oY limPlifia4 semantic","paragraphs":["net"]},{"title":"rtrueturrr, remcrhat like those in","paragraphs":["the"]},{"title":"actual 5RI ~aartitianad BlarntP~ neta 8ystam lHandrlxr 19751, MMes In","paragraphs":["the"]},{"title":"net may raprerant phylicrl objects, rcllttonshipr, evantr, sbtr, rulas, ar uttsrencesr a0 in the Qxempla balaw, Directad lab@Pled areg conncct nsdaa and rsprasant certain wprimi%iran tlmc-invariant ralat tsnrhtpa, In the n@t Lrrqm~nt above, the U.8. an4","paragraphs":["thr U,K. rra"]},{"title":"elcmrntl (a) of the gat of CQMW~~~@@, as EXP@tisnt@rr","paragraphs":["they baeh participate"]},{"title":"in OWNLng rlfurtionl lnvolvlnp as OBJlctr","paragraphs":["particular"]},{"title":"rubmariner) reeh rubmarlnt 11 an rlcmsnt of same class ot subm&r1ntsr rnd te~plafes fat Engllah rtntcner8, Bc choa~e a simple verb for dernmstrrtfin a= OWN, Wa not@ that our verb","paragraphs":["h $@v&t&1 VynanymrRr WAVE, POSSESS, and BELONG C10L Slmc clfh of thcsc verb8"]},{"title":"(including OWN1 ha8 other Ssnsr meanings, re Darlt","paragraphs":["a node"]},{"title":"tense they have in Comment this node will be &ha m~rototypZcrlE OWHI in thst It w112 incarpareta the umsrnin~R of the altuatlsn and Ln that all inaerncrs af owning rituatians will b4 rerat~d PQ POSSESSt HIVEI BELONG] and trmplatrr, Wsts that one template rill hat rufftea","paragraphs":["Ear all"]},{"title":"dour verbs; For inatanesr the subject 0% the subject ir the FXPeriencQsr EXP own& OBJ 1 0BJ is owned by EXP EXP poroass~a OBJ OBJ","paragraphs":["1s Pabl@lldd by EXB"]},{"title":"EXP hrr aBJ OBJ Gdronv to EXP tOWK (EXP V&ct OBJ) (OBJ Vprr BY EXD]) [PO3SES8 (EX$ Yact OBJ) (063 Ypar BY EXPI] fHAVE (EXP Vact OBJ)] IBEbON6 (084 Vact TO EXP)] New# in Order tb rpcilc about a particular Owning sltuatbonr nt t%dWr BELONG) and an ag~oe1ated template (OBJ VaeE TO EXPI, and But we have 4 problem! there is no indication of how tho EXP and OBd Qrgumcntt &re","paragraphs":["to be"]},{"title":"generated, NP will not always rofticrr note for inrtanec that the predicate argument of in %?shn hapad to go homen must ba an Infinltlva PhP48e (rather than the QetUfid phrree that NP might producal, Even","paragraphs":["a"]},{"title":"cursory 8tUdY of a few hundred verbs in th@ language shawl that they have very defintts Cand regular1 Consttaint& on the syntactic form 0% tnrrr canststucntr, Thaas constraint8 appear ts bc matters %or the lexicon rrthar than the grammar, we associate net) rather than imwemsnt them via QP4MmBF TUI~II and wg cxpllcltly incorporate ths conrtltutnt types in the tamplatcst COWM ((NP EXP)","paragraphs":["Va~t (NP OBJ))"]},{"title":"(CNP OBJ) Ypar BY (NP EXP))I (POSSES6 ((NP EXP) Vact (NP OBJ)) ((NP OBJ) Vpe8 BY (NP EXP))] [HAVE ((HP EXPI Vact (NP OBJ))] [BELONG ((NP OBJ] Yaet TO (NP EXP))I A act of patterns liLe thtra is associated","paragraphs":["*Ith rvsry \"PIO~~YP. verbn nods"]},{"title":"in the knswZ~dg@ bra@* $t would seam that mll ra need is an Pntnrgsebr tkab giv~n any w~e~b instanceR node fn the knowb@dg@ b~ger l0bko up the psttbrns PQT that type of! nodh C~OQGQ~ Q VQtbr 1 eOrraapdnding tsmglate for the verb@ and then Proeaedr ts *evrluatan fha patterno verb [OWN, S-38,DWNI sms bQl~n$ tamp [(MP OBJ) Vact TQ (NP EXpll (NP Odd) 913 the S~awoLf YIC~ -13 Bclonqs TO --3 to (NP GXP) the u,s, But us ttlil run into trouble with our rimplc cchema, Consldsr the rent@ncc, *John burned the taarf black,\" \\ ACT \\~BJ colorl By uging the simple pattern UNP AGT) Vact (HP OBJI) we could easily generate the *Lncorrrctw sentence, rJahn burned","paragraphs":["the black tmr;trW"]},{"title":"since (NP QBJ) might","paragraphs":["include the"]},{"title":"color of the te8ltt8 Me need a pattern more llke ((NP AG,T) Vact [NP OW) (nod R~SIII Zn which the RESult of the action wflk bc directly related to","paragraphs":["the"]},{"title":"verb. Ha*rvcrr thlr 1% not quite enough a= rt laart, not Wtthout a very c0mpfic.tad Lntcrpretcr","paragraphs":["-*"]},{"title":"because the interpreter must RnoW that (NF OBJ) cannot inelude ha vrabPs RE8 ergurnawl [black). Thus# by convention, nay indicate an extra argument to bs Passed to r eon6tttuant qansrdor (such as the Ounetisn NP) te denote the item(@) not te appear in the resultant canrtiturnta (INP ACT) Vact (HP OBJ RES) (Nod RES)) *he pattern (NP OBJ RES) mrrnr Vgrnaratr an NP using the OBJsct of the VQrbr but do n~t inelude tha RESult of ths verb in the pattarno (Leer a pattern copy far every porr/blr *mmlrrLngfl constituent), This level of detail would be unrdasonable id f@w other verbs could we this template; however, there are","paragraphs":["rnt?t@ khan"]},{"title":"rstrtlvaly few tern~lrtarr baCh shw~d by several tang BB hundred@ of verbs, the urt of templates proves to ba","paragraphs":["quit* hefpfull"]},{"title":"There? rrr other roureqr ot potantla1 pattern proltfarati~n~ an impattant one being the cambindtorial arrangamants of CQea arguments of time, manner, and athar such ~B~Wbi~llr &it well aa other (pot8iblY non*advrrbiall ease arguments such as sourer, $041, inttrumrnt, etc. Some at $ha@@ arguma~td are rath@r censtralnsd In their paaitiona in the rantencar but others may \"esterday the ship satled from the lighthouse to the dock,\" \"The #hip sailrd from ths lighthau8a to the dock yesterday.\" wYast@rday the ship railed to the dock from the llghthour~.~ gt is af cauara unrsrranabl@ ta try to maintain","paragraphs":["all the ~as8ibXe prftrrnr! Lnefcad rs"]},{"title":"leave Lnrsrtlon of fhrte rdvcrbiaf rrguncntr to r ring11 heuristic routine (dcreribcd below), There are rrvcral jurtitlcations for thjl, amanp them8 ti) the particular farm 09 the verb cannot be grneobatsd until","paragraphs":["the subject bbject(a5)"]},{"title":"@r~rt@L possible PL&CI~~ and (31 theta rta some heuristic one may question whether passive tenplatao ahould be atore41 c@tt@inlyr they could be derivedr On the other hand, neglecting t~ rtare thorn woWd farce ur to Ind%bat@ wAth each Verb (84n03)~ whether Lt can (or, ronrtimar, must) be parrivizcd, Indicating \"tr.n~ltlv@Vir naf enough since there are tranritlvc vrrbr (i.a,, vcrbr that taka an object) that cannot bc pssslvizad. 8incs we hava to rtora the information anywayp we Can rave dame cads and Coaput$np tiar by storing thr parslvr ternplat@. There arr rtvsrrl rraronr for gcnrrafinq the verb after the mafor @rgumentr. Ftr~t the lubjret auat be panerrfad ro that","paragraphs":["the"]},{"title":"verb can br mrdc","paragraphs":["to aprrr"]},{"title":"in nuabnr. Second, certain rare trnrrr 8tr true of vrrb-prrticlr conblnitionc rkiir","paragraphs":["not cL the"]},{"title":"icolatrd verb, Btncr, in addition, prrticl@c must appear rfttr object8 that F Short (like pranownrl but bsf~rs abjaete","paragraphs":["that are long"]},{"title":"(like noun phraras), the particle","paragraphs":["nurt be"]},{"title":"positionad after the object jS grncrrtcd. FfnaL1~1 inocrtian af soma advarbirbr tr.~, annot\" rrqulrrr an ruxllirry verb I-","paragraphs":["thug verb"]},{"title":"grnaratlon aurt fotlor rdvrrbirl grncratlon, VERB PATTERNS This #tudy started with the 25 Verb pattcrnr\" prcrcntcd by Hornby (1954). The~e Ln turn crwr from a dlctgonary by Hornby ct el., (1948). VctM in the dict$onary &re cZ&irltird &c~ording ta their gross syntactic prttrrn~ af lubjrctp abjrct(s1,","paragraphs":["end"]},{"title":"com~~raentCs)~ most ot the ~aftqrnr rra rub-dlvgdade Thc rutkorr cl@La that there patfrrnr","paragraphs":["WXXNnt for all"]},{"title":"conttruetlanr involving &I& the verbs In theLr 4ietianary dndr by Nttnefon, In the IangUagr, I tl&SrifiertLon is not Zmmcdi&telY useful to C~thpUtdhti~nal lingulrtr rlnca it doas not addrags underlying ~emaatics~ Havatkhele~~~ it $r clarr","paragraphs":["that it can ssrve ae ths"]},{"title":"brrlr for r &artvatIan ol Und@rlYlng carr rtructuhar prrtlculatly, &r a bar16 for Vplnrrrtion tcngl&tro,* There pattern8 &re b@lng canvartad inta tamplet@@","paragraphs":["much llkt tho#* derived"]},{"title":"arrlfetr tha tnrlyris is Baing perfarmed with ralD*Ct t4 300b V6tbl drawn fr~~ the dtctionary (S~OCUW~ to appear), There templatea serve ro tho major portion of a modular *gan@ratLon grammarr* with the r@aarlnder in thr farm of hsutia;tic tunctianr tor constructing syntactic constituentr, NOUN PHRASES What to IncluBb Ln a noun phrase should be another mrttar far the dircaurte module to judge, There are no w~1l~Earm~laCad fulrl eccountlng far anaphorl In EngliShj indead, there arc trw wsflee8trbliehad parameters ather than that %ha hearer must ba able to r~aalva tho (pto)nouns","paragraphs":["to their reforants, The &D@ak@a?p Should tmploy"]},{"title":"anaphoxa In order to avola t@petitlonr but only Pt his B0deX of! the hearer indicrto~ that the ~~QPCP can tdsolvd the ambiguity, Thcrc arc same lawcpowrr ptaneminalization ruler that could be directly incarp0ratsd in","paragraphs":["a generator rafg@x$v%zalPanr"]},{"title":"for s~11p10. NcV~P~~B~~SS~ ft IS","paragraphs":["impastant LO"]},{"title":"P~BZIZB that","paragraphs":["When"]},{"title":"r generator ir unaware of the eanvcrtafl~nal context, it lhauld not indepsndantLy decide haw to ganaratr noun phpa1481","paragraphs":["It CQR"]},{"title":"only decide when to d6 roe This rbtuatlon har not been univ+roalLy r@caqnfiedr but","paragraphs":["It 1s becoming hnercarlngZY"]},{"title":"clear thrt a QZ~csurte msduh must be canrubtsd during the ~snarrtlan phrrs. The direourre","paragraphs":["modal. will not know ahead of timr what NPI"]},{"title":"are %O be pr~due@d un~@sg $t p4rforrn\\@ many of ths","paragraphs":["gem@ ~Poratiana that the"]},{"title":"paneratar would do anyr4y. Yat the cantcxt-8cnritivo decirion strrt~g~ may h.ve to resorb ts rush maarutbr ar dlaewb$quating the proporrb output uoLwg khs model of &he hrsrer in order ta d~termbnr what rnaphdra if4 teseavabla, t%","paragraphs":["Ir"]},{"title":"unr@rsanrble ts ancarporrte thlg strrt~gy in ths gancratsr~ rinca for rany rrrsonr","paragraphs":["it"]},{"title":"nurt be part of","paragraphs":["the dlrcourrr modulc,"]},{"title":"Therefate thr genrrrtor Should pa6S any RncunUconstitucnt to","paragraphs":["th@"]},{"title":"dJscourre module (plrhspt rith","paragraphs":["its"]},{"title":"rrcommcndation","paragraphs":["about how to PIgdUce the"]},{"title":"constituent,;","paragraphs":["th~ nodule must dctcrminr if a"]},{"title":"pronoun or b&ga noun it ambiguour","paragraphs":["to the hearer, and, if so, what to add to the"]},{"title":"noun ln order to makt the desired","paragraphs":["referent elear,"]},{"title":"Ln the current SRI system, noun prttcrnr [Slocum, ta rpPcar) ara USI~ to control noun phrase grnrrrtian,","paragraphs":["Much like verb patterns,"]},{"title":"noun prttcrnr order the ~on~tlfu@nt8 in","paragraphs":["the phrarr and indicate how"]},{"title":"each eon8tLturnf Is to be generated by naming a functicn","paragraphs":["to be Called with the"]},{"title":"nQtwerK ~anstftuentf ((DET] [Ad3 QUA&) (Adj SIZE) (Adj SHRFE) (Ad1 COLOR) (N)] Batttrhr like thir era dirtributrd about the network","paragraphs":["hi@r@rehyj"]},{"title":"in the future@ th@ QL~c~urrc aadulc rill dccLde tor","paragraphs":["each pqttrrn"]},{"title":"constituent whather ft i8 to appear in","paragraphs":["the phrrcs,"]},{"title":"MEURZSTXC RULES R~rnby d+rcrlb@r three bsrlc porltlanr far advcrbr in the clrussr \"rentff pa#b$tionp R~tdn pssltlonr and @andu ~eritlon~ Front poritgen rdvrrbr occur batare the subjrctt wVerta~day h@ want hornat from these ha took a taxi,*","paragraphs":["The intorroqltiwa rdvarbr (a,g,"]},{"title":"now, when? rrr typically constrdkn~d to front Pooitionl other8 miy rpprrt thrrr tat purporrr of rmpharla cr eantralt. Mid perleion rdvarba occur virh the verb (atringlt Sf there ar* medal sr ru#ilLrrv varhrs the dbvrrls occurs aftor","paragraphs":["the first"]},{"title":"one, Oth@twi8r the rd~~rb w~LL appear btk~r@","paragraphs":["the Verbl cxcapt fbr"]},{"title":"*un11tre8#aQ~ fhlte~ of bar and 401 '~4 O~~QR $36 thararl @oh@ if8 typi~llly buw8] *he It 1t11I waiting,@ End position adverbs occur rttrr the verb and after any dbteet or indirect object pteacnt, While talativ~ly fen clrurss have mare thrh one adVQrb in tront parttian or mara","paragraphs":["thrn"]},{"title":"sns In @id porltianr","paragraphs":["it"]},{"title":"ir common tat rrvcrrl .dV@rbS to appear in and ~arltlon in tha rams clausal *thaV play the Piana PeerlY tagcthrrw, hdvarblrlr of time (rnrrrring the 4uartiont wrh@n?w) usually deeut in and poaltion, but may appear In grsnt position far aaphrrtr or contrast. kdverbialr of frequency (anrwsrlng","paragraphs":["tka quettbon, wRaw"]},{"title":"~ftan?~l can be split inQ two groups, The first group is comgo@@d ~f ringla-nerd adverbs that tyalcally seeui fn &id porition but aIro may br in ind patLtianj th@ sacand","paragraphs":["ir eosparsd a$ multipls=word phrrrrr that dgp&ar Pn end parition"]},{"title":"er~ &err Lrr9UrntlYr in trent Position. Adverbs of duration [Vlf~rl hew lonqtw) udu~lly have end parltlan, wbth %song p6sBtPon fbr empha8fs or csntxartr Adverb8 of place and diraetlen narrnall~ hivQ and porition. Advcrbr of 4rgrca and manner have mid or and ~OSI~LQRI depbnding on thr advarb, Along With ruah ruler con~~rnlng","paragraphs":["the BarLtionr OL Vsriout types of"]},{"title":"advrrbr, there murt br a mcchanirm to 0rd.f","paragraphs":["the rdvcrbr that arr La"]},{"title":"s~cur In","paragraphs":["tha \"maQn"]},{"title":"petitLon,","paragraphs":["Thrrr"]},{"title":"rra eome h@ualrticl~ among rdverbi.11 of time Car pla~11 the smallat unit 81 ururlly pL4ccb tlrlt, unlrrl","paragraphs":["it"]},{"title":"tr 44604 48 &R sftarthauphtl @the army attaeksd thr village in farce an","paragraphs":["a hot Augutt"]},{"title":"@ftrrnoon, just attar riertaH, Adv@rbialr of","paragraphs":["placa end diroetton"]},{"title":"usually precede thore of frequency, which in turn praeode thoo~ at time, There rule8 re iaplrmontsd in th@ raRr rautLn~ Mat produce@ the verb1 when","paragraphs":["@ template is firat fnearprcked =- NWh eo B 8equencc of"]},{"title":"function erllr the RVa~tw or \"Vpao kaya arc lonored. once the lUbjCCt, ~bjCCt(S1 and compltmcnt(a) lndicrtrd by the tcnplrtr rrr Oanrrrfrdr this *clean upm routine $6 called. Et employg the hauxittiek d~rcrfbed abava La add the adverbial C~rt&tftU@nt& and VhFbr then concatsnatss the canstbtutnts ta produce r complete @IruseI DXSCUSSTON fn theory,","paragraphs":["the set of porsiblc EnpLlrh rantcncrs i a inf"]},{"title":"fnZtsr Tha obvlaor qusatlbn man ~t@@@r","paragraphs":["Olf"]},{"title":"sne tries to account ror","paragraphs":["them with tarnplitrrr"]},{"title":"won t","paragraphs":["there bc an infinite"]},{"title":"number eL taaplatt~?~","paragraphs":["The"]},{"title":"simple antxcr irr \"Nor tor romc of the Sam@ rell~nl that allow a finite grammar to gcnrrata","paragraphs":["an"]},{"title":"intlnite number of itr2ngr.R One can preduca rantrncer","paragraphs":["of arbitrary Length by (1) ~T'bltt'at~ anb@ddlngr and (21 arbitrary"]},{"title":"conjuncttan@ Caw@ Boto not do so by includdng arbitrary numbers sf distinct car@ erpuacntl. Evrn ra the numbcr ot basic patterns could be tnl~amelY Iaf gc , Evidrnca, horevrrr il tc the contrary! the lVentUII nUnbqr of t@Bpl.t@# would appear","paragraphs":["to br reveral time8 the"]},{"title":"number OI pltternr~ awing to thr SUbltitUtion","paragraphs":["of parttcular"]},{"title":"pra~aritlone Ear npr~pw In the r~~tictlc attaro or, and tha arrlgnaent ot dltfetent crle nancr","paragraphs":["to a particular"]},{"title":"canstitusnt depending on the prrticular vrgb ur@d. Dcotsch, Barbara G, Establishing Context in Task-Oriented blaldg~. Prrrcnted at the Thirteenth Annual Meeting ot the Atsooiatian fat Computational Llngutrflcr, Barton, Ma~rachu~dtt~, SO October","paragraphs":["* 1"]},{"title":"Navarnber 1975, OoLdmrnr Neil He Computcr G~neration of Natural L@nguaga tram a DII~ Conceptual Irre. A1 Memo 247, Artiti~ial Intelligen~~ Laboratory, 8trnford Univcr8ityt Strrnf ord, Cqif ornib 1974, Gordon DIvidt and Lakaff, George. Convearettonal Portulater. Syntax snd Senanticst Volume 31 Speech Acts, Edited by Peter Cole and Jerry L. Morganr Acadrmia Prsa~, New Ygrkr 1975 rn H@ndtixt Gary G. Expanding thr Utility of Bemantie Nafworkr f hrough Pact it toning, Advance Pav@r r of the Fourth Internationcl Joht Contcrrncs on Artificial Zntblllgenee, Tbiiiolr Csor~ia, U6SRl 3.8 Geptambcr 1975, IISaIZlr Hornby, 8tr Gatrcby, Vetand W~lK@flb@Xdr HQ The Advmced Laatnet@@ Dictionary of Currant Engllrh. Oxford Pr@rr~ London@ 1948, Rita.# H, Barry, Autoastic Speech Underrfandin~ SYrtamr. Proessdlngr, Eleventh Annual IEEE Camputst Saelety Contcranea, Harhlngton, D. Cer 9-i 1 S~ptrmber 1975, Siamanrr Robtrt &I and S~OCU~~ Jonathan. Gaa@rartinq English Dlrcourcr from Senrntlc NIEWO~KC. CommUnicstlonr ~t the ACH, 1992, 1st 8911909n SJocul, Jonathan. Quartion Anarrring","paragraphs":["via"]},{"title":"Csnanlcrl Verb8 and Bsrnrnt lc Madcllat t C~nsrating %ngllrh trow the WO~Q~, Technical Report Ht=i3t Department a& Csapartrr bcbtawcar r Univeoritp at Ta~ar~ Austin@ Ttx.8, January 1913. $locum, Jonathan, Verb Patterns and Noun Pattarns In Engli~ht A Cl&r Anrlyrir. Artlfi~Lal Lntrlliprncs Center, SRX4 Man10 Parkt Californirr (in preparatlsn), American Journal of Computational Linguistic8 Microfiche 33","paragraphs":[": 78 Yale University New Haven, Connectiuct 06511 ABSTRACT TALE-SPIN"]},{"title":"is s program which makes up stories by using planning structures as part of its world knowledge. Planning structures represent goals and the methods of achieving those goals. Requirements","paragraphs":["for aparticular method depend"]},{"title":"on static and dynamic facts about the world. TALE-SPIN changes the state of the world by creating new characters and presenting obstacles to goals. The reader / listener makes certain","paragraphs":["plot"]},{"title":"decisions during the telling of the story. The story is generated using the notation of Conceptual Dependency and is fed to another program which translates it into English. INTRODUCTION TALE-SPIN is a computer program which makes up stories about characters who plan how to solve certain problems This work was supported in part by the Adudnced Research Projects Agency of the Department of Defense and monitored by the Office of Naval Research under contract N00014-75-C-1111. and","paragraphs":["then carry out their plans. The planning procedures interact with a data base of knowledge about other characters and objects in the world, memory, and the personal relationships which exist between characters. The stories are represented in Conceptual Dependency and are passed to a program which expresses them in English. The reader is asked to make certain decisions about the story during the process of generation. Here is an example.","JOE BEAR WAS FAMISHED, HE DECIDED HE WOULD BE FULL IF HE ATE SOME HONEY, HE WANTED TO FIND OUT WHERE THE HONEY WAS, HE THOUGHT TRAT IRVING BIRD WOULD TELL HIM WHERE THE HONEY WAS.","JOE BEAR WALKED TO THE TREE WHERE IRVING BIRD WAS. HE ASKED IRVING BIRD IF HE WOULD TELL HIM WHERE THE HONEY WAS."]},{"title":">> DECIDE:","paragraphs":["DOES *IRVINGBIRD* AGREE? *NO","IRVING BIRD REFUSED TO TELL JOE BEAR WHERE THE HONEY WAS. JOE BEAR DECIDED IRVING BIRD WOULD LIKE HIM TO GIVE THE WORM TO HIM. JOE BEAR ASKED IRVING BIRD IF HE WOULD TELL HIM WHERE THE HONEY WAS IF HE GAVE THE WORM TO HIM."]},{"title":">> DECIDE:","paragraphs":["DOES *IRVINGBIRD* AGREE? *YES HE THOUGHT THAT HE WOULD"]},{"title":"LIKE JOE BEAR","paragraphs":["TO GIVE IT TO HIM, HE WANTED TO FIND OUT WHERE THE WORM WAS, HE THOUGHT THAT IRVING BIRD WOULD TELL HIM WHERE THE WORM WAS, JOE BEAR ASKED IRVING BIRD IF HE WOULD TELL HIM WHERE THE WORM WAS."]},{"title":">.>","paragraphs":["DECIDE: DOES *IRVINGBIRD* AGREE? *YES","IRVING BIRD DECIDED HE WOULD TELL JOE BEAR WHERE THE WORM WAS- IRVING BIRD TOLD HIM IT WAS AT A PATCH OF GROUND,","HE WALKED TO THE PATCH OF GROUND WHERE WORM WAS, HE TOOK THE WORM HE WALKED TO THE TREE WHERE IRVING BIRD WAS. HE GAVE THE WORM TO IRVING BIRD."]},{"title":">> DECIDE: DOES","paragraphs":["*IRVING%IRDk KEEP HIS PROMISE? *NO","HE REFUSED TO TELL JOB BEAR WHERE -THE HONEY WAS. JOE BEAR TOLD IRVING BIRD HE XS GOING TO STRIKE HIM IF HE DOES NOT TELL HIM WHERE THE HONEY WAS."]},{"title":">>","paragraphs":["DECIDE: DOE$ *IRVINGBIRD* IGNORE THE THREAT? *NO","IRVING BIRD DECIDED HE WOULD TELL JOE BEAR WHERE THE HONEY WAS, IRVING BIRD TOLD HIM IT WAS AT THE BEEHIVE.","80","JOE BEAR THOUGHT THAT HENRY BEE WOULD GIVE THE HONEY TO","WIM. JOE BEAR WALKED TO THE BEEHIVE WHERE HENRY BEE WAS. HE","ASKED HENRY BEE IF HE WOULD GIVE THE HOMEY TO HIM."]},{"title":">>","paragraphs":["DECIDE: DOES \"HENRYBEE* AGREE? *YES","HENRY BEE DECIDED HE WOULD GIVE IT TO JOE BEAR. HENRY BEE GAVE IT TO JOE BEAR. BE ATE IT. HE WAS FULL. THE END. Here is a story which TALE-SPIN generates which the translator"]},{"title":"is not yet capable","paragraphs":["of producing"]},{"title":"in","paragraphs":["~nglish: JOE BEAR W4S HUNGRY. HE THOUGHT"]},{"title":"TEWJ!","paragraphs":["IRVING BIRD WUQLD TELL HIM WHERE SOME HONEY WAS, HE WALKED TO TlWE TREE WHERE"]},{"title":"IRVING BIRD WAS. HE ASKED IRVING","paragraphs":["BIRD TO TELL HIM"]},{"title":"WHERE","paragraphs":["THE HONEY WAS. IRVING BIRD TOLD HIM THE HONEY WAS IN A [certain] BEEHIVE","JOE BEAR WALKED TO THE BEEHIVE WHERE THE HONEY WAS. HE ASKED HENRY BEE TQ GIVE HIM THE HONEY. HENRY BEE REFUSED. JOE BEAR TOLD HIM WHERE SOME FLOWERS WERE. HENRY BEE FLEW FROM THE BEEHIVE TO THE FLOWERBED WHERE THE- FLOWERS WERE. JOE BEAR ATE THE HONEY,","HE WAS VERY TIRED. HE WALKED TO HIS CAVE. HE SLEPT. THE END. TALE-SPIN starts with a small set of characters and various facts about them, It also has"]},{"title":"a set of","paragraphs":["problem-solving procedures which generate the events in the story. Many"]},{"title":"decisions have to be made as the story","paragraphs":["is being told. Some are made at random"]},{"title":"(names","paragraphs":["of characters, for example) ; others deperld"]},{"title":"on the relationships between characters","paragraphs":["(whom"]},{"title":"one asks for information, fox example); others are made by the reader (whether a character keeps","paragraphs":["a promise, for example)"]},{"title":". TALE-SPIN generates sentences using the representation system of Conceptual Dependency (Schank 1975).","paragraphs":["Some of the Conceptual Dependency (CD) structures are passed on to a program which expresses them"]},{"title":"in English. (The original version of that program was written by Neil","paragraphs":["Goldman"]},{"title":"for the MARGIE system.","paragraphs":["The present version"]},{"title":"has been modified by Walter Stutzman and","paragraphs":["Gerald 8 1 De Jong .) The sentences which are"]},{"title":"not passed","paragraphs":["to the translator"]},{"title":"are","paragraphs":["those"]},{"title":"which","paragraphs":["represent easily inferred ideas. Neither program yet worries about"]},{"title":"khe","paragraphs":["style of expression; that is, we worry about"]},{"title":"whether to say a newly","paragraphs":["generated piece of the story, but not"]},{"title":"much","paragraphs":["about how to say it. A TALE-SPIN story"]},{"title":"involves","paragraphs":["a"]},{"title":"single","paragraphs":["main character who"]},{"title":"solves","paragraphs":["some problem. To make the process interesting, obstacles"]},{"title":"are","paragraphs":["introduced,"]},{"title":"some","paragraphs":["by the reader if he chooses, and some"]},{"title":"at","paragraphs":["random."]},{"title":"For","paragraphs":["instance, the reader's decision Chat Irving Bird is not going to tell Joe Bear what he"]},{"title":"wants","paragraphs":["to know produces an"]},{"title":"obstacle","paragraphs":["to Joe"]},{"title":"ear's","paragraphs":["plan to find something out. Some obstacles are c~aated when certain scenes are included in the story. For instance, the initial world state has no bees in it, but when it comes time in the story"]},{"title":"to conjure","paragraphs":["up some actual honey, we do so by creating a whole scene which includes some honey"]},{"title":"in a","paragraphs":["beehive in a tree and a bee who owns that honey. The bee may or may not be at home. If he"]},{"title":"is,","paragraphs":["Joe Bear is going to have another obstacle in his plan when he gets to the beehive. The story is the narration of some of the events which occur during the solution"]},{"title":"(or","paragraphs":["non-solution) of the problem. (That is, more things happen"]},{"title":"in","paragraphs":["the solution of a problem than a storyteller says or needs to say.) TALE-SPIN differs from other"]},{"title":"problem-solving systems","paragraphs":["in several ways : (1) the problems it solves are those requiring interaction with other, unpredictable characters rather than with a data base of theorems or blocks"]},{"title":"or","paragraphs":["circuits; (2) the world inside TALE-SPIN grows: new characters are created with unpredictable effects on the story; (3) obstacles are deliberately introduced; (4) an unsuccessful\" story, one in which the problem is not solved, can be just as interesting as a Usuccessful~ one. PLANNING STRUCTURES Planning structures are what we use to"]},{"title":"organize knowledge","paragraphs":["about planf ul activity, which is represented"]},{"title":"in CD","paragraphs":["by a chain of causes and effects. The planning structures include delta-acts, planboxes, packages, scripts, sigma-states, r ho-states, and pi-states. A delta-act is used to achieve a particular goalstate. Delta-prox (written here as dPROXl is the procedure for becoming proximate to some location. A delta-act is defined as a goal, a set of planboxes, and a decision algorithm for choosing between planboxes. A planbox is a particular method for achieving a goalstate. All the planboxes under a delta-act achieve the same goalstate. Each planbox has a set of preconditions (some of which may be delta-acts)"]},{"title":",","paragraphs":["and a set of actions to perform. \"UnconsciousN preconditions are attached to planboxes which would never occur to you to use. If you're trying to become proximate to X, you don't even think about persuading X to come to you when X is an inanimate object. \"~ncontrollab~e\" preconditions cannot be made true if they're not already true. (The assumption is that they are sometimes true.) \"~lantime' preconditions are the things you"]},{"title":"worry about when you're making up","paragraphs":["the plan. You don't worry about \"runtimen preconditions until you're executing the plan. (\"Planning\" is a mental activity. PLAN is, in fact, one of the primitive ACTS of CD. \"Executing a plan\" is performing a logically structured sequence of actions to achieve the goal of the plan.) If 1'm planning to get a Coke out of the machine upstairs, I worry about having enough money, but I don't worry about walking up the stairs until 1'm"]},{"title":"at the stairs. That the machine","paragraphs":["actually has some Coke is an uncontrollable runtime pr econd it ion : I don't worry about it until I get there, and there's nothing I can do if it is empty when I get there. A package is a set of planboxes which lead to a goal act I' ather than state. The PERSUADE package, for instance, contains planboxes for X to persuade Y to do some act 2. The planboxes include asking, giving legitimate reasons, offering favors in return, threatening, and so on. Goalstates come in various flavors. There are the goals which are associated with the delta-acts: the goal of dPROX is to be somewhere, the goal"]},{"title":"of dKNOW","paragraphs":["is to find out the answer to some question, the goal of dCONTROL is to possess something. But there"]},{"title":"are also","paragraphs":["goals of satiation, called sigma-states. For example, sHUNGER organizes the knowledge about satisfying hunger (invoking dCONTROL of some food, eating). TALE-SPIN also uses sigma-state knowledge in the bargaining process; offering someone some food in return for a favor is legitimate since"]},{"title":"it","paragraphs":["will satisfy a precondition for sHUNGER. There are also goals of preservation, called"]},{"title":"pi-states,","paragraphs":["which are most interesting when they are"]},{"title":"in","paragraphs":["danger of being violated. The logic of the THREATEN planbox in the PERSUADE package, for example, derives from the fact that physical violence conflicts with pHEALTN. A SAMPLE DELTA-ACT: dPROX"]},{"title":"-","paragraphs":["TALE-SPIN does not include all nine delta-acts described by Abelson (1975). It contains the three which closely correspond to pr"]},{"title":"im itive acts: dPROX (PTRANS) , dCONTROL (ATRANS) , dKNOW (MTRANS). Here is an outline of dPROXt ~PROX(X,Y) -- X wishes to be near Y Planbox 0: if x is already near Y, succeed. Planbox 1: X qoes to Y uncontrollable precondition: can X move himself? plantime precondition:","paragraphs":["P KNOW (location of Y) runtime precondition: dLINK (location of Y) action: PTRANS to location of Y"]},{"title":"runtime precondition: is Y really there? (We may have gotten false information dur ing the","paragraphs":["KNOW. ) Planbox 2: Y comes to X unconscidus precondition: is Y animate? uncontrollable precondition:"]},{"title":"is Y rno~ able? action: PERSUADE Y to PTRANS himself to X (PERSUADE package) Planbox 3: Agent A brings X to Y uncontrollable precondition: is X movable? action: X gets AGENT to bring X to Y (AGENCY package) Planbox 4: Agent A brings Y to X unconscious precondition: is Y animate? uncontrollable precondition: is Y movable? action: X gets AGENT to bring Y to X (AGENCY package","paragraphs":[") Planbox 5: X and Y meet at location Z"]},{"title":"unconscious precondition: is Y animate? uneontrollabls precondition: is Y movable?","paragraphs":["actions: PERSUADE Y to PTRANS himself to Z and dPROX(X,Z) THE DATA BASE Planning structures are essentially procedural."]},{"title":"-- - -","paragraphs":["The non-procedural data base used by the planning structures is divided into five classes. 1. Data about individual PPs (Picture Producers, nouns) where applicable : height; weight; where their home is; who their acquaintances are. 2. Data common"]},{"title":"to","paragraphs":["classes of PPs (e.g., data common to all birds) where applicable: what they eat; what their goals (sigma-states) are; whether they are animate (capable of MBUILDing)"]},{"title":",","paragraphs":["movable, self -movable ; how they move around. 3 Sigma-state knowledge indicating how to achieve a sigma-state and what the plantime preconditions are that someone other than the planner can achieve. This is used in the bargaining process. Joe Bear offers to bring Irving Bird a worm because dCONTROL (FOOD)"]},{"title":"is","paragraphs":["a plantime precond it ion for sHUNGER which Joe Bear can achieve for Irving Bird. There are no plantime preconditibns for sREST that he can achieve for Irving Bird (except maybe to leave him alone)"]},{"title":".","paragraphs":["4. Memory: what everybody knows (thinks, believes) ; what Joe Bear"]},{"title":"knows;","paragraphs":["what Joe Bear thinks Irving Bi~d knows; etc. Planbox 0 of BKNOW, for example, accesses Memory to test whether"]},{"title":"Joe","paragraphs":["Bear already knows the answer to the question being asked, or whether it is public knowledge. Since both the question and the facts in Memory are represented in CD, the pattern match is very simple, taking advantage of CD's canonical representation of meaning. 5. Personal relationships. The relationship of one character to another is descibed by a point on each of three scales: COMPETITION, DOMINANCE, and FAMILIARITY. Scale values range from -10 to"]},{"title":"+I@.","paragraphs":["The relation \"is a friend of\" is represented by a certain range on each of the three scales. The relation \"would act as an agent for\" is represented by a different range. The sentence \"Joe Bear thought that Irving Bird would tell him where the honey was\" comes from the \"Ask a Friend\" planbox of dRNOW. There is a procedure which goes through a list of Joe Bear's acquaintances and produces a list of those who qualify as \"friends\", i.e., those who fit somewhere within the \"friendn range. Relations are not symmetric: Joe Bear may think of Irving Bird as his friend, so he might ask him where the honey is, but Irving Bird may not think of Joe Bear as his friend at all, in which case he might refuse to answer Joe Bear. Relationships can change. If Gee Bear becomes sufficiently aggravated at his \"friend\" Irving Bird and has to threaten to bash him in the beak in order to get him to tell him where the honey is, then the relationship between them deter iorate s. We plan to extend this feature to describe a character's \"default\" relationship: how he relates to total strangers. This would not necessarily be the point (0,8,0) but rather some point which would be used to give a rough indication of the character's \"persoaalityn. Big bad Joe Bear might rate at (+6 ,+9,+4)"]},{"title":", where small meek Bill Worm might","paragraphs":["rate at (-6 ,-la ,-4)"]},{"title":". Changing a relationship is a type of","paragraphs":["goal we haven't yet considered in much detail, a1 though goals of relationships (rho-states) clearly exist. The procedure for getting someone to like you (rLIKE) might contain planboxes for ATRANSing gifts, MTRANSing sweet nothings, etc., in addition to changing your own feelings toward that person so that if he (she) asks you"]},{"title":"to","paragraphs":["do something, you don't refuse. Information gets into the data base in several ways. Memory data gets produced directly by the planning structures. Changes in"]},{"title":"relations are","paragraphs":["side-effects of the present set of planning structures. But things have fo start somewhere. There is a function CREATE (X) which invents a new item of type X (e.g., bear, flower, berry). Associated with each type of item is a"]},{"title":"small procedure called","paragraphs":["a"]},{"title":"picture","paragraphs":["which invents the desired item and others as required. For example, when we create some honey, we"]},{"title":"also create a","paragraphs":["beehive,"]},{"title":"a tree,","paragraphs":["and a"]},{"title":"bee. The honey is \"owned\"","paragraphs":["by the bee and"]},{"title":"is","paragraphs":["inside the beehive which is in the tree. The bee may or not be at home. Randomly chosen names, heights, weights, etc., are attached. All this data is then added to Memory. The CREATE function is called when needed; remembet that TALE-SPIN models the process of making"]},{"title":"- ue","paragraphs":["a story as you go along. We will now follow, in detail, the production of the second sample story. CREATE"]},{"title":"a","paragraphs":["bear, which invokes a picture procedure which invents a beat. Assume the bear is named Joe; although since the name"]},{"title":"is chosen","paragraphs":["at random from a list of first names, it is just as oiten Irving. A cave is also invented, and has Joe in if. ~oe's location becomes public knowledge. CREATE a bird, named Irving, and a tree which is his home. ~rving's location is also now public knowledge. Assert that Joe is hungry. This fact enters ~oe's Memory. We also \"say\" this: that"]},{"title":"is, we pass it to the English translator which then produces the sentence \"JOE BEAR WAS HUNGRY N. Invoke sHUNGER. Choose at random a food that bears eat: honey. Assert that Joe is now planning to achieve the goal (sigma-state) of satisfying his hunger. Assert that he has decided that eating the food can lead to the achievement of his goal. sHUNGER calls dCONTROL (honey) . This forms a new goal, namely, that Joe have some honey.","paragraphs":["CONTROL'S \"Planbox 0\" asks Memory if the goal is alseady true: does Joe already have some honey? The answer comes back: no. A plantime precondition is to"]},{"title":"know the location of some honey, so dCONTROL calls dKNOW(where is honey?). (The question is represented in CD, not English.) dKNOW forms the new goal.","paragraphs":["KNOW'S \"Planbox 0\" asks Memory whether Joe knows the location of any honey. Memory says no. Planbox 1 tests whether the question can be answered by consulting a standard reference (e.g., \"What time is it?\")"]},{"title":".","paragraphs":["That fails. Planbox 2 tests whether the question requires expertise: no. Planbox 3 tests whether this is a \"general information\" question. It is, so we assert that Joe is planning to answer this question using Planbox 3 (\"Ask a Friend\"). Planbox 3 starts. Choose a friend: Irving. dKNOW calls the PERSUADE package to try to get Irving to answer ~oe's question. PERSUADE asks Memory whether Joe thinks- that Irving cannot answer the question. ~nswer: no. Irving is a \"friend\", so we try the ASK planbox. Assert that Joe thinks that Irviag will tell him where the honey is. PERSUADE calls dPROX(,Irving), since Joe needs to speak to Irving. dPROX asks Memory whether Joe is already near Irving. Memory says no. Planbox 1: is Joe self-movable? Yes. Assert that Joe is planning to be near Irving by going there himself. dPROX calls dKNOW (where is Irving?)"]},{"title":". ~KNOW'S \"Planbox 0\" asks","paragraphs":["Memory whether Joe already knows where Irving is. The answer comes back: yes, Irving"]},{"title":"is in","paragraphs":["a certain tree. dKNOW returns this to ~PROX. (We will omit future references to \"Planbox On. ) dPROX asserts that Joe walks to the tree where Irving is. We ask Memory whether Irving is actually there. He is, so dPROX has achieved its desired goal; his change in location is added to Memory. dPROX returns to PERSUADE. 30e asks Irving where some honey is. The reader now gets to decide whether Irving agrees to do 39. Assume the reader says yes. We ask Memory whether Irving actually knows where any honey is. If he did, we would have Irving tell him, but he doesn't, so we CREATE some honey: a storyteller can create s6lutions to problems as well as obstacles! Some honey is invented, along with a beehive, a tree, and a bee (Henry) who is at home. Irving tells Joe that the honey is in the beehive. ASK succeeds, so 90 PERSUADE succeeds, so dKNOW succeeds: Joe knows where some honey is. Back in dCONTROL, we ask Memory whether [Joe thinks that] anyone owns the honey. Memory says that Henry does, so CONTROL'S Planbox 1 (\"Free for the taking\") fails. Planbox 2 is to PERSUADE Henry to give the honey to Joe. Given no relation between Joe and Henry (they don't know each other)"]},{"title":", the","paragraphs":["only planboxes in PERSUADE which can be used are ASK and INFORM REASON. We try ASK first. This calls dPROX<Henry) which succeeds sirfce Joe knows where Henry is; we omit the details here. Joe asks Henry to give him the honey, and the reader decides that Henry refuses. We try INFORM"]},{"title":"REASON next. We choose a goal of ~enry's and","paragraphs":["build a causal chain backwards from the goal,. For example, one of Henry s goals is to \"eat\" flowers. (TALE-SP'IN thinks that what beesa do to flowers is equivalent to eating.) In order to eat a flower, you have to \"control\" a flower, which results from someone (possibly you yourself) ATRANSing the flower to you. We test whether what Joe is trying to PERSUADE Henry to do matches ATRANSing a flower. It doesn't, (Joe is trying to PERSUADE Henry to A-TFWNS the honey to him.) We then consider that in order to ATRANS a flower, you. have to be near the flowes, which results from someone PTRANSing you to the flower. Does this match? No"]},{"title":". We repeat this process a few times,","paragraphs":["trying to construct a short inference chain which -connects= what Joe is trying to persuade Henry to do"]},{"title":"with","paragraphs":["one of Henry's goals. INFORM REASON fails, and we return to dCONTROL. The next Planbox is called \"Steal\". We ask Memory ~hether Henry is home: if he weren't, Joe would simply take the honey. But Memory tells us that Henry is home, so STEAL calls PERSUADE to get Henry to leave home; that is,"]},{"title":"Joe","paragraphs":["is now going to try to persuade Henry to PTRANS himself from the hive. In the context of STEAL, the ASK planbox is not used. Joe tries INFORM REASON again and succeeds in producing the following"]},{"title":"chain: we","paragraphs":["get"]},{"title":"to","paragraphs":["the idea"]},{"title":"of","paragraphs":["someone PTRANSing himself to a flower again as we did before, but we notice that this does match what we are trying to persuade Henry to do: the connection is 4 that Henry will PTRANS himself from the beehive to the flowtx. Joe now considers the precondition for ~enry 's PTRANSing himself to the flower, namely, that Henry has to know where the flower"]},{"title":"is. Memory does","paragraphs":["not indicate that Joe thinks that Henry knows where a flower is, nor does Joe know where a flower is, but rather than invoke dKNOW(where is a flower?), we CREATE a flower: this is legitimate in a plan to steal something. Joe now tells Henry that there is a flower in a certain flowerbed, and then asks Hengy if he would like to fly to that flower. Henry aqrees and flies away. PERSUADE succeeds, and returns to dCONTROL. Joe now takes the honey from the hive, so dCONTROL succeeds and returns to sHUNGER. Memory is modified to indicate that Joe knows that he has the honey, but that Henry does not. Joe now oats the honey, and has achieved the sigma-state of not being hungry. But, when bears eat, they become tired, so sREST is invoked. sREST is very short. It requires a dPROX (cave)"]},{"title":",","paragraphs":["which is easily achieved, and then Joe goes to sleep. Since the main goal has been achieved, and the goal produced as a consequence of that goal has also been achieved, the story ends. What distinguishes stories from simple sequences of events? Coherency is important: there has to be a logical flow from one event to the next. This is represented in CD as a chain of acts which re,sult in states which enable further acts and so on. Interest is important: something interesting or unusual has to happen or else the reader will begin to wonder what the point of the story is. TALE-SPIN creates impediments to goals, on the assumption that the overcoming of obstacles can make an interesting skory. \"One day Joe Bear was hungry. There was a jar of honey right next to him. He ate it. The end\" is not a story. It shouldn't be that easy. On the other hand, it shouldn't be too hard either. In theory at least, there is"]},{"title":"a","paragraphs":["cost-effectiveness calculus which people employ when deciding how much energy to expend on a subgoal, based an how much the goal is worth to them. This process prevents the plans from being too complicated. As the story is generated, various plot decisions have to be made. Some decisions are made at random, others are made by the reader. When Joe Bear threatens Irving Bird because Irving Bird won't tell him where the honey is, the reader gets to decide whether Irving Bird is going to ignore the threat. We use planning structures because any program which reads or writes a story, whether of the folktale variety or the New York Times variety, must have a model of the logic of human activity. It might be easier to simulate the generation of a highly stylized form of story, as Klein (1974) has done using ~ropp's analysis of a class of Russian fairy tales, but there is little generality there. One could use any of the wel.1-known problem-solving systems like MICRO-PLANNER, but the story is the proof procedure, and the procedure used there does not correspond to my conception of how people solve problems. hat's not a criticism of MICRO-PLANNER as a problem-solver, but only as a model human problem-solving"]},{"title":".","paragraphs":["user interaction was included for two reasons. First, the interactive feature now serves as a heuristic for placing bounds on the complexity of the story. Beyond, some number of obstacles to the goal, a story becomes a kind of joke. Second and more important, ex tensions to TALE-SPIN will include more sophisticated responses than the present yes/no variety. THE FUTURE OF TALE-SPIN."]},{"title":"-- -","paragraphs":["There are a lot of things that TALE-SPIN doesn't do yet that would improve it as a storyteller. Here are some of the theoretical problems we will be working on in the immediate future. (1) Bargaining, as it exists now in TALE-SPIN, is a pretty one-sided affair, with the main character making all the proposals. Irving Bird is just as likely to suggest that Joe Bear go get him a worm as Joe is to offer to do SO Counter-proposals are certainly common enough. (2) Future stories should include planning on the part of more than one character. The present stories are all \"about\" the bear, and"]},{"title":"only incidentally involve the","paragraphs":["bird and"]},{"title":"other characters.","paragraphs":["The"]},{"title":"stories are more concerned with reaction","paragraphs":["than"]},{"title":"interaction.","paragraphs":["(3)"]},{"title":"For every plan, there may","paragraphs":["be"]},{"title":"a counter-plan,","paragraphs":["a"]},{"title":"plan to block","paragraphs":["the"]},{"title":"achievement of a goal: a","paragraphs":["plan for"]},{"title":"keeping","paragraphs":["away"]},{"title":"from somethinq or someone; a plan not to find out something,","paragraphs":["or to be c~nvinced that"]},{"title":"it isn't true; a","paragraphs":["plan"]},{"title":"to","paragraphs":["get rid"]},{"title":"of something you own. (4) How much","paragraphs":["of a plan do people"]},{"title":"consider in advance?","paragraphs":["We have made"]},{"title":"some efforts in","paragraphs":["this area by making the"]},{"title":"distinctions between kinds","paragraphs":["of"]},{"title":"preconditions. Certainly","paragraphs":["the"]},{"title":"most","paragraphs":["important improvement"]},{"title":"here will be","paragraphs":["the"]},{"title":"cost-effectiveness reasoning.","paragraphs":["(5) The theory of"]},{"title":"telling stories","paragraphs":["(what"]},{"title":"to say) now implemented in","paragraphs":["TALE-SPIN is to express"]},{"title":"violations of sigma-states (\"Joe Bear","paragraphs":["was hungryn)"]},{"title":", physical acts, and those mental acts which","paragraphs":["provide"]},{"title":"motivation","paragraphs":["or"]},{"title":"justification for later events. The reader","paragraphs":["is assumed to be able"]},{"title":"to infer the rest.","paragraphs":["This seems"]},{"title":"to work reasonably we11 for","paragraphs":["the"]},{"title":"present","paragraphs":["simple"]},{"title":"stories, but may have to","paragraphs":["be modified"]},{"title":"to suit longer, more complicated storie s. REFERENCES Abelson, Re","paragraphs":["P. (1975)."]},{"title":"Concepts","paragraphs":["for"]},{"title":"representing mundane reality in plans. In D, Bobrow","paragraphs":["and A."]},{"title":"Collins, eds. Representation and understanding: Studies in cognitive science. Academic","paragraphs":["'"]},{"title":"- 7 Press, New York. Rlein,","paragraphs":["S. et a1 (1974)."]},{"title":"Modelling","paragraphs":["Propp and"]},{"title":"Levi-Strauss","paragraphs":["in a"]},{"title":"rneta-symbolic simulation system. Technical Report","paragraphs":["226,"]},{"title":"University","paragraphs":["of."]},{"title":"Wisconsin at Madison. Schank, Ro","paragraphs":["C. (1975)."]},{"title":"Conceptual Information Processin . American Elsevier, New YZ *~bis includes -___r% contri utions","paragraphs":["by Neil M."]},{"title":"Goldman, Charles J. ~iegec","paragraphs":["111, and Christopher K."]},{"title":"Riesbec k, Schank,","paragraphs":["R. C."]},{"title":"and Abelson,","paragraphs":["R. P."]},{"title":"(1975). Scripts, plans","paragraphs":["and knowledge,"]},{"title":"In Proceedings of","paragraphs":["the 4th"]},{"title":"International Joint Conference","paragraphs":["on"]},{"title":"Artificial Intelligence.","paragraphs":[]}]}