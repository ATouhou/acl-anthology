{"sections":[{"title":"Book Reviews Head-driven Phrase Structure Grammar Carl Pollard and Ivan A. Sag","paragraphs":["(The Ohio State University and Stanford University) Stanford, CA: Center for the Study of Learning and Information and Chicago: The University of Chicago Press (Studies in contemporary linguistics, edited by John Goldsmith, James D. McCawley, and Jerrold M. Sadock), 1994, xi+440 pp; hardbound, ISBN 0-226-67446-0, $80.00, £63.95; paperbound, ISBN 0-226-67447-9, $34.95, £27.95"]},{"title":"German in Head-driven Phrase-structure Grammar John Nerbonne, Klaus Netter, and Carl Pollard (editors)","paragraphs":["(Rijksuniversiteit Gronigen, DFKI Saarbr~icken, and The Ohio State University) Stanford, CA: Center for the Study of Learning and Information (CSLI lecture notes 46), 1994, xi+404 pp; distributed by the University of Chicago Press; hardbound, ISBN 1-881526-30-5, $49.95, £39.95; paperbound, ISBN 1-881526-29-1, $21.95, £17.50 Reviewed by Jean-Pierre Koenig State University of New York at Buffalo It has been almost ten years since the classic Generalized Phrase Structure Grammar (Gazdar, Klein, Pullum, and Sag 1985) appeared. And like its predecessor, Head-driven Phrase Structure Grammar will probably become a classic too. Head-driven phrase structure grammar (HPSG) is the state of the art in what Pullum and Zwicky (1991) have called category-based syntax, and this book makes available to a wide audience recent developments in a grammatical framework used extensively in syntactically oriented research in natural language processing. Moreover, of the grammatical theories using inheritance-based grammars, a widespread tradition in the NLP community, HPSG achieves the widest coverage (vide the special issues of Computational Linguistics devoted to this topic in 1992). The book thus gives the computational linguist a good idea of how to apply the basic knowledge-representation ideas of inheritance and typing to state-of-the-art linguistic analyses. It also complements the more theoretically oriented works of Carpenter (1992) and Keller (1993) on typed-feature structures and their logic. So, although its intended audience is clearly primarily linguists, this book is essential Computational Linguistics Volume 22, Number 1 reading for anybody interested in building an NLP system with a nontrivial syntactic component. All the more so, since Pollard and Sag, in order to challenge the dominant Principles and Parameters syntactic framework of Chomsky and his associates (Chomsky 1981, 1986), are meticulous in comparing their theory to the alternatives of Principles and Parameters: the book provides a welcome cross-theoretical discussion of all major syntactic issues.","For readers interested in either HPSG or German syntax, German in Head-driven Phrase-structure Grammar is also highly recommended: it presents more current, cuttingedge research and gives an idea of the kinds of questions an HPSG approach raises. It also includes several technical and theoretical innovations that have become increasingly popular within the HPSG community--function composition or generalized raising and domain union, for example. For reasons of space, this review will focus only on Pollard and Sag's book. It should be noted, though, that, unlike Pollard and Sag's book, German in Head-driven Phrase-structure Grammar is editorially poor: typos and incorrect referencing of examples abound.","The topics covered by Polland and Sag (henceforth P&S) include all the usual major syntactic phenomena that grammatical theories must account for: agreement, subcategorization, unbounded dependencies (including parasitic gaps and relative clauses), control, binding theory, and, to a small extent, quantification. ,As in the tradition of Gazdar et al. (1985), the analyses are detailed and made precise, so that a fair evalua-tion is possible. Moreover, the formalism used is similar enough to by-now traditional NLP grammar formalisms (in particular feature-based grammars) that readers without prior knowledge of HPSG implementations can have a reasonable idea of how to implement the analyses. As mentioned, the book carefully compares P&S's proposals to standard analyses within Principles and Parameters. Unfortunately, there is much less explicit comparison with work done within frameworks intellectually closer to HPSG, such as lexical-functional grammar (LFG) (Bresnan 1982). Finally, although the book focuses mainly on English syntax, P&S attempt to provide a cross-linguistic perspective in several chapters, in particular when dealing with agreement and relative clauses.","To the novice reader, HPSG is a constraint-based grammatical formalism that be-longs to the growing family of frameworks using feature structures as their basic data structure. In contrast to other feature-based frameworks, such as PATR-II (Shieber 1986, Shieber et al. 1983), LFG, or GPSG, HPSG does not rely on a context-free backbone; constituency is only one of the attributes of the linguistic object par excellence, the linguistic sign. It is on a par with other syntactic and semantic attributes. Moreover, HPSG is characterized by a systematic use of typing of feature structures (not unlike the use of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&S de facto use a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). Typing in HPSG is used to factor out shared properties of linguistic objects, be they words, phrases, or anything else, into appropriate classes. Typing is also used to restrict the application of general principles to the right class of linguistic structures. The Head-Feature Principle, for example, which identifies the relevant syntactic properties of a phrasal head with that of its mother node, is restricted to headed structures (i.e., objects of the type headed structure). Nonheaded structures are not subject to the principle. 130 Book Reviews","As in most modern syntactic frameworks, in particular lexically oriented theories, HPSG uses a minimal number of stipulated syntactic rules: . 2. . a few phrase-structural schemata (six in the book); 1 a few principles governing feature percolation (head-feature principle, non-local feature principle); principles governing the major classes of syntactic phenomena (binding principles, various principles relating to unbounded dependencies, control theory). Most of the information is located--and most of the action takes place in a richly structured lexicon.","Having described the basic framework used by P&S, let me give a brief summary of some of their analyses. Chapter 2 presents a general theory of agreement as token-identity between indices rather than copying of features. To anybody versed in the unification-grammars literature, this is hardly news. A detailed discussion of the variety of kinds of agreement found cross-linguistically makes this chapter more interesting. Two important linguistic claims are made in the chapter. First, agreement is partly semantic. It consists in identifying indices at a level of discourse representa-tion (in fact, in the theory they propose, number, gender, and person are properties of these indices). Obvious correlations between properties of indices (such as gender or number) and properties of their denotata (male/female or singularity/aggregation) are a matter of pragmatic constraints on the anchoring of indices, on which languages differ. Second, because of the nature of indices and the inclusion of background or pragmatic information in the description of linguistic signs, HPSG can account for honorifics agreement as well as partially semantic agreement.","Overall, the theory that P&S propose is compelling and illustrates perfectly the theoretical value of integrating various kinds of information in each constituent or sign, a hallmark of sign-based grammatical theories. Similarities and differences between various kinds of agreements attested cross-linguistically can be easily modeled. Despite its success, one important issue is not addressed. Agreement, P&S claim, in-volves unification (more precisely token-identity of indices). There are instances of agreement in several languages, though, which do not seem to involve token-identity and unification, but rather feature-checking, as shown by Ingria (1990) or Pullum and Zwicky (1986). A discussion of these challenges would have been welcome.","Chapter 6 proposes a theory of anaphoric binding that does not rely on configurational notions such as c-command, but rather on the notion of relative obliqueness of the antecedent and anaphor. Relative obliqueness is defined (roughly) in terms of the relative order of the antecedent and anaphor in the subcategorization lists of predicators. Furthermore, many classes of sentences involving anaphors generally assumed to be subject to grammatical constraints are in fact subject, P&S claim, to pragmatic constraints on anaphors (be it NPs such as"]},{"title":"pictures of each other","paragraphs":["in English or long-distance anaphors such as Japanese"]},{"title":"zibun).","paragraphs":["As in the case of their theory of agreement, one interesting aspect of P&S's binding theory is its willingness to appeal to both syntactic and pragmatic constraints on grammatical phenomena. Part of binding, according to P&S, is syntactic in nature, but part of it has to do with notions such as point of 1 Again, these are simply a partial description of feature-structures that are subtypes of the general type phrase, rather than separate kinds of objects. 131 Computational Linguistics Volume 22, Number 1 view or processing considerations (e.g., the relative distance of potential antecedents to anaphors).","The same interaction between syntactic and semantic constraints is at play, according to P&S, in control phenomena, discussed in chapter 7. As is traditional in recent linguistic work, they consider the unexpressed subject of"]},{"title":"eat","paragraphs":["in"]},{"title":"John tries to eat","paragraphs":["to be an anaphor. What's more interesting is their claim that the choice of controller (i.e., the NP whose index is identified with that of the unexpressed subject) is determined by purely (lexical) semantic considerations.","To be fair, though, several aspects of P&S's theory of control are unclear. First, is the principle determining the controller of an unexpressed (reflexive) subject on a par with the binding theory (i.e., a general constraint on feature structures)? If yes, is it a universal principle? Or is it, rather, a lexical constraint, like the Raising Principle (i.e., a constraint on lexical entries rather than on entry-tokens instantiating this entry)? P&S mention several phenomena that militate against a purely lexical treatment of control. But there is one aspect of control that still assimilates the phenomenon to constraints on lexical types. There are several exceptions to the \"principle\", such as"]},{"title":"deserve, claim, afford, defy,","paragraphs":["none of which denote a semantic relation of type"]},{"title":"commitment, orientation,","paragraphs":["or"]},{"title":"influence.","paragraphs":["Moreover, although control verbs tend cross-linguistically to denote relations that belong to these three semantic classes, some languages contain control verbs that denote other kinds of semantic relations. French, for example, contains at least two other classes of control verbs: verbs of saying and verbs of mental representation, as illustrated in examples 1 and 2:","(1) Marc Marc","(2) Marc Marc {dit ] pr6tend} ~tre heureux. {say.PRES ] pretend.PRES} be.INFIN happy croit avoir r6solu le probl6me. believe.PRES have.INFIN resolve.PPT the problem Such cross-linguistic lexical variation is the hallmark of lexical constraints, not of general grammatical principles such as the Head-Feature Principle, or even the binding principle. How to reconcile both aspects of control needs further study.","Second, the treatment of control as involving a reflexive anaphor forces the addi-tion of a separate clause in the definition of local o-command that, to my knowledge, is not independently motivated. Although such disjunctive definition of the relation crucial to anaphoric binding is not unique to HPSG, it still requires an explanation. A final point worth mentioning about P&S's binding theory: despite some mention of cross-linguistic data, the theory presented covers mostly English anaphors. Recent work by Xue, Pollard, and Sag (1994) has extended HPSG binding theory, parameterizing it somewhat to cover the Chinese long-distance anaphor"]},{"title":"ziji.","paragraphs":["But the theory does not as yet cover the full range of attested grammatically-governed anaphors, discussed for example in Manzini and Wexler (1987) or Dalrymple (1993). Comparison with Dalrymple's theory in particular would have been particularly useful, given the overall formal similarities between LFG and HPSG.","Chapters 4 and 5 and half of chapter 9 are devoted to unbounded dependencies. Technically, the modeling of unbounded dependencies in HPSG (leaving aside the reformulation in chapter 9, for now) is similar to the classic SLASH percolation method used by Gazdar et al. (1985). There are three major differences between the two treatments. 1. The SLASH feature is set-valued, thus allowing for multiple gaps. 132 Book Reviews . . Coordinate structures are not headed, thus allowing a crucial separation of the Coordinate Structure Constraint from the theory of parasitic gaps (a dissociation necessary to account for languages, such as Swedish, that obey the Coordinate Structure Constraint, but allow extraction out of subjects without the presence of other gaps in the sentence). Traces are no longer a necessary part of an adequate treatment of unbounded dependencies (see below for more on this). The detail ~ind empirical coverage of the analyses make these chapters stand out. An added bonus is the treatment of relative clauses of a kind different from those present in English, namely correlative relative clauses and internally headed relative clauses.","An important revision in the theory of unbounded dependencies is suggested in the final chapter of the book, where a theory of unbounded dependencies that does not rely on traces is proposed (see Sag and Fodor (1994) for an extension of the same idea). The gist of the modification is simple: rather than introduce the bottom of unbounded dependencies by means of a particular empty sign with a non-null SLASH value, the bottom of unbounded dependencies is introduced by a lexical rule that passes the (nonlocally satisfied) valence requirement from the SUBCAT list to the SLASH set of the relevant predicator. The newly created predicator entry then serves as the bottom of the dependency; the rest of the theory remains unchanged. Such a move illustrates the increasingly important role of lexical rules in HPSG (see below for more on this topic): most, if not all, of the variation in the environments in which lexical entries occur (actives vs. passives, of course, but also here local vs. nonlocal realiza-tion of subcategorized complements) is accounted for by assuming that noncanonical environments are the result of the presence of a variant of the \"base\" entry derived by lexical rules.","The treatment of English relative clauses proposed in chapter 5 is particularly unintuitive (despite its similarities to widely accepted accounts within the Principles and Parameters approach). Since other accounts are possible within a typed-feature-structure approach, I dwell on it some more. P&S's basic idea is simple enough: they posit a null relativizer (eR in the example below) which subcategorizes for both an S-slashed complement and a specifier containing the relative marker. Example (3a) is thus given the structure in (3b) (irrelevant information is omitted; RP stands for Relative Phrase):","(3) a. person to whom Kim gave a book b. IN' person [RP"]},{"title":"[PP","paragraphs":["to whom] [R' eR [s Kim gave a book]]]] Although positing the empty relativizer"]},{"title":"ea","paragraphs":["in (3b) allows for a relatively simple account of the English facts, there are several drawbacks to P&S's analysis. First, there is no independent motivation for this specific kind of empty category. Note that this is the only empty category with independent subcategorization requirements; that is, its subcategorization properties do not follow from that of a corresponding non-empty category somewhere else in the string. Moreover, its semantic content is entirely parasitic. Its index is identified with that of both the modified noun and the relative marker"]},{"title":"(whom","paragraphs":["in (3b)). The restrictions on its index are the union of the semantic content of its sentential (or VP) complement and of the modified noun. The absence of any independent semantic contribution suggests that the role of this empty relativizer is merely theory-internal.","Finally, there is a technical difficulty with the solution proposed by P&S. To understand the nature of this difficulty, consider example (3b) again. To terminate the relative 133 Computational Linguistics Volume 22, Number 1 (unbounded) dependency introduced by the relative marker (whom), the noun modified by the relative phrase must bear a specification \"[TO-BINDIREL {# 1}]\" where # 1 is the index of the relative marker. So, the N ~ person in (3b) must bear this specification. The question is: where does this specification come from? Certainly, it does not result from a lexical specification on the lexical entry person: it is not a lexical property of person to be (sometimes) modified by a relative clause. Nor can it originate on other nodes and percolate down (or up) to person. The Non-Local Feature Principle regulates only the upward percolation of the values of inherited long-distance features, not of the TO-BIND features. Finally, it does not come from any constraint on the head-modifier-structure schema. The (necessary) presence of \"[TO-BINDIREL {#1}]\" on person as used in (3a) therefore remains mysterious. One possible solution is to introduce a special phrase-structural schema (of the form N' --* N' RelC1) that would directly introduce this feature specification.","Introducing a schema specific to relative clauses suggests another kind of solution within HPSG that would not rely on empty relativizers. Indeed, within a typed grammar using multiple inheritance, another kind of analysis is possible, using a crossclassification of phrases (see unpublished work by Fillmore and Kay (1993) and Sag (1994)). The basic idea is to distinguish between the kind of phrase and the kind of clause instantiated by a given constituent structure (say head-complement/head-filler structures vs. interrogative/declarative/relative structures). One can then define a particular kind of relative clause as inheriting from both the head-filler and relative-clause types. The relevant properties of relative clauses are ascribed directly to this particular subtype of phrase, rather than being projected from an empty relativizer.","The reason that such a solution was not adopted by P&S, I suspect, comes from linguists' bias against multiplying phrase-structural schemata in favor of multiplying lexical entries. The linguist's intuition is that phrase-structural schemata represent the universal structure of sentences whereas lexical entries always contain a certain amount of idiosyncrasy. When in doubt, then, one ought to add to the idiosyncrasy of lexical entries rather than make phrase-structural schemata less universal. In a non-inheritance-based theory, this reasoning is sound: one can indeed expect a phrase-structural solution to increase the number of stipulations that must be made. But in an inheritance-based grammar, the phrase-structural solution is not necessarily less economical. The similarities between relative clauses and other unbounded dependencies can be preserved by making relative clauses (or a subclass of relative clauses) a subtype of the head-filler schema, for example. In fact, the phrase-structural solution has the added advantage of capturing the obvious similarity between relative clauses and nonsubject questions by making them both subcases of the head-filler schema.","Aside from these quibbles on the detail of the analyses, the book has two small, general shortcomings. First, the explanation of the formal underpinnings of the theory is too short. In particular, the notions of typed-feature structures, sorts, or type hierarchy are only briefly described. True, some of these ideas were already broached in P&S's 1987 book, but to novice readers, this will not provide much help. This omission is all the more unfortunate, as most books and articles on the subject are very technical (for example, Carpenter's otherwise excellent 1992 book) and unlikely to help linguistically oriented readers not familiar with typed feature structures.","A more significant omission is the absence of any substantive discussion of the theory of lexical rules assumed by P&S, although, again, their 1987 book contains more details. Given that more and more of the \"grammatical action\" in a lexically oriented theory such as HPSG takes place in the lexicon, this omission is more serious. A discussion of the theory of lexical organization assumed in the theory would certainly have been welcome. In fact, some of the few statements about this theory made in the 134 Book Reviews book are misleading: P&S refer to Bresnan's (1982) notion of lexical rule (p. 37) (itself borrowed from Jackendoff (1975)). But Bresnan's original view of lexical rules is that they function as redundancy rules over separately listed lexical entries. This view is implausible, given the use of lexical rules in HPSG to model productive inflectional morphology (already suggested in P&S (1987)) and the postulation of two entries for eat in What did you want to eat? and Joe wanted to eat pasta (see Krieger and Nerbonne (1993) or Koenig and Jurafsky (1994) for some of the difficulties associated with lexical rules in HPSG, and Godard and Sag (1995) for a response).","This somewhat critical summary of P&S's major analyses does not give a good idea of the book's richness and scholarship. The empirical coverage and the savvy of the analyses is truly remarkable. Moreover, the structure of the type hierarchy being assumed, the relevant type declarations, and the principles being proposed are all laid out and summarized in the appendix, so that the reader can easily assess the proposals: a welcome relief to anybody accustomed to recent syntactic work! Overall, this is a linguistic book that ought to be on every computational linguist's shelf and is likely to have a profound impact on computational linguistics. Acknowledgment I am grateful to Carl Pollard and Ivan Sag for comments and clarifications on a previous version of this review.","References","Bresnan, Joan, ed. (1982). The Mental Representation of Grammatical Relations. Cambridge, MA: The MIT Press.","Briscoe, Ted; de Paiva, Valeria; and Copestake, Ann, eds. (1993). Inheritance, Defaults, and the Lexicon. Cambridge University Press.","Carpenter, Bob. (1992). The Logic of Typed Feature Structures. Cambridge University Press.","Chomsky, Noam. (1981). Lectures on Government and Binding. Dordrecht: Foris.","Chomsky, Noam. (1986). Barriers. Cambridge, MA: The MIT Press.","Dalrymple, Mary. (1993). The Syntax of Anaphoric Binding. Stanford, CA: Center for the Study of Language and Information.","Evans, Roger and Gazdar, Gerald. (1989a). Inference in DATR. Proceedings, 4th Conference of the European Chapter of the Association for Computational Linguistics, Manchester, 66-71.","Evans, Roger and Gazdar, Gerald. (1989b). The semantics of DATR. Proceedings, 7th Conference of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour, 79--87.","Fillmore, Charles and Kay, Paul. (1993). On construction grammar. Unpublished ms., University of California, Berkeley.","Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan. (1985). Generalized Phrase Structure Grammar. Oxford: Basil Blackwell.","Godard, Daniele and Sag, Ivan. (1995). \"Reflexivization and intransitivity: The case of French.\" Paper presented at the annual meeting of the Linguistic Society of America.","Ingria, Robert. (1990). The limits of unification. Proceedings, 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, 194-204.","Jackendoff, Ray. (1975). \"Morphological and Semantic Regularities in the Lexicon.\" Language, 51(3): 639-671.","Kasper, Robert and Rounds, William. (1986). A Logical Semantics for Feature Structures. Proceedings, 24th Annual Meeting of the Association for Computational Linguistics, New York, 257-266.","Keller, Bill. (1993). Feature Logics, Infinitary Descriptions, and Grammars. Stanford, CA: Center for the Study of Language and Information.","King, Paul. (1989). A Logical Formalism for Head-driven Phrase Structure Grammar. Ph.D. dissertation, University of Manchester. Koenig, Jean-Pierre and Jurafsky, Daniel. (1994). Type Underspecification and On-line Type Construction in the Lexicon. Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information, 270-285. 135 Computational Linguistics Volume 22, Number 1","Krieger, Hans-Ulrich and Nerbonne, John. (1993). \"Feature-based inheritance networks for computational lexicons.\" In Inheritance, Defaults, and the Lexicon, edited by Ted Briscoe, Valeria de Paiva, and Ann Copestake. Cambridge Univeristy Press, 90-136.","Manzini, Maria Rita and Wexler, Kenneth. (1987). Parameters, Binding, and Learnability. Linguistic Inquiry, 18(3): 413-444.","Pollard, Carl and Sag, Ivan. (1987). Information-based Syntax and Semantics, Volume 1: Fundamentals. Stanford, CA: Center for the Study of Language and Information.","Pullum, Geoffrey K. and Zwicky, Arnold. (1986). Phonological Resolution of Syntactic Feature Conflict. Language, 62(4): 751-774.","Pullum, Geoffrey K. and Zwicky, Arnold. (1991). A Misconceived Approach to Morphology. Proceedings, lOth West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information, 387-398.","Sag, Ivan. (1994). \"Relative Clauses--A Multiple Inheritance Analysis.\" Paper presented at the HPSG 1994 Conference, Copenhagen.","Sag, Ivan and Fodor, Janet D. (1994). Extractions Without Traces. Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information, 365-384.","Shieber, Stuart. (1986). An Introduction to Unification-based Approaches to Grammar. Stanford, CA: Center for the Study of Language and Information.","Shieber, Stuart; Uszkoreit, Hans; Robinson, Jane; and Tyson, Mabry. (1983). \"The Formalism and Implementation of PATR-II.\" In Research on Interactive Acquisition and Use of Knowledge, Menlo Park, CA: Artificial Intelligence Center, SRI International.","Xue, Ping; Pollard, Carl; and Sag, Ivan. (1994). A New Perspective on Chinese 'ziji'. Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information, 432-447. Jean-Pierre Koenig is Assistant Professor in the Linguistics Department at the State University of New York at Buffalo. His main research interests are in lexical knowledge representation. He has worked recently on developing a notion of lexical relatedness that does not rely on lexical rules, but rather on underspecified types. Koenig's address is 684 Baldy Hall, SUNY at Buffalo, Buffalo, NY 14052; e-mail: jpkoenig@acsu.buffalo.edu. 136"]}]}