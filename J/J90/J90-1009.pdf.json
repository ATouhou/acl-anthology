{"sections":[{"title":"","paragraphs":["Book Reviews From Syntax to Semantics: Insights from Machine Translation generally very thorough, although the chapter on pitch is unusually skimpy. For some reason, Waibel chooses to limit his discussion to two sentence tunes--yes-or-no questions (with a rising pitch contour) and declarative statements (falling pitch contour)--that he feels are relevant to connected speech recognition. Waibel does not investigate this in any depth, however; nor does he consider the possible contribution of pitch to the recognition of polysyllabic words, which themselves can make up a single intonation phrase.","The discussion of other prosodic features is much more satisfying, and I was especially intrigued by the chapter on stress. Previous researchers on speech recognition and prosody (e.g. Lea 1980) have held that stressed syllables provide \"islands of phonetic reliability\" because stress in itself makes a speech segment more identifiable to human and machine listeners. Waibel's results contradict this claim. His experiments show no significant effect of stress on recognition accuracies. What makes stressed syllables special is their closeness to lexical representation; stressed syllables have closer agreement between acoustic reality and abstract representation than unstressed syllables. A recognizer will therefore have better success at using stress to find syllables and words than particular phonetic segments; in Waibel's implementation, stress (specifically, stress probabilities) is used for locating word boundaries and for distinguishing between function and content words. Such results have important implications for the psychological reality of lexical representations and for the place of cognitive modeling in speech recognition systems. Unfortunately, the effects of Waibel's experiments are limited by his vocabulary sample, which contained a large number of monosyllabic words. I hope Waibel and others will replicate this study with a different and larger set of materials.","I recommend this book as a text and reference. Readers will benefit from a knowledge of speech basics, but no special knowledge of synthesis or recognition technology is necessary. Finally, I wish to recommend this volume especially to those who are currently looking at ways of improv-ing a recognizer's performance through using better-known tools of computational linguistics, such as morphological analysis and parsing. REFERENCES Lea, W. A. 1980 Trends in Speech Recognition. Prentice-Hall, Englewood Cliffs, NJ. Joan Bachenko holds a Ph.D. in linguistics from New York University. Her research interests include prosody and speech synthesis, telegraphic sublanguages, and Deaf English. Bachenko's address is: AT&T Bell Laboratories, 3D462, Murray Hill, NJ 07974. E-mail:joan-b@allegra.att.com. FROM SYNTAX TO SEMANTICS: INSIGHTS FROM MACHINE TRANSLATION Erich Steiner, Paul Schmidt, and Cornelia Zelinsky-Wibbelt (eds.) (Institut fiir Angewandte Informationsforschung,","Saarbriicken) London, U.K.: Pinter Publishers, 1988, vii + 262 pp. (Communication in artificial intelligence series) Hardbound, ISBN 0-86187-960-0, Â£29.50 and Norwood, N J: Ablex Publishing Corp, 1988, vii + 262 pp. Hardbound, ISBN 0-89391-526-2, $48.50 (institutional), $29.50 (personal) Reviewed by Harold Somers UMIST Keen observers of the world of machine translation have long awaited the first book-length publication on the Commission of the European Communities' MT project EUROTRA from the Saarbriicken-based German group; other readers may have been attracted by this book's title. Both, regrettably, risk disappointment.","The former will find this book nothing like the hoped-for detailed description of the world's biggest and best-funded MT project. Not only is the book explicitly \"not some official report on EUROTRA work\" (p. 1), but in several places it contradicts or argues against EUROTRA doctrine (\"... the concept of transfer developed in this chapter is the opinion of the author, not necessarily the view underly-ing the project as a whole\") (p. 161). However, it assumes in the readership either some prior knowledge of the fine details of the project and, especially, its formalisms and jargon, or else access to numerous Commission documents to which it makes frequent reference (particularly the \"EUROTRA Reference Manual\"), even though they are not in the public domain. A good (or bad) example of this occurs when we are asked (p. 13) to consult Arnold et al. 1985 (an internal report) for an explanation of the use in EUROTRA of the term \"unification.\" What is the reader to make of this? All we get is a hint that this term might be being used in a nonstandard way, with no hint as to what it might mean here, and no reasonable chance of following up the reference. Similarly, the dual use of the term \"translation\" both in its everyday meaning (i.e. between natural languages) and to describe a relationship between representations (e.g.p. 21) is very misleading. Other examples include \"euroversal\" (e.g. pp. 5, 187), and reference to \"the corpus\" (p. 6), though we are not told which corpus, or of what it is a corpus.","This brings us to the second set of potentially disappointed readers: those expecting to read about syntax and semantics in an MT system. The recurring theme, inas-much as there is one, is that a purely syntactic representation is not suitable for a 72-language-pair transfer MT system, and so some sort of \"semantics\" must be used. This Computational Linguistics Volume 16, Number 1, March 1990 47 Book Reviews From Syntax to Semantics: Insights from Machine Translation turns out to be a combination of Case Grammar, a classification of verbs borrowed from Systemic Grammar, and the use of syntactic valency frames plus semantic features for lexical disambiguation. Nothing especially innovative here then.","Apart from jargon and unobtainable references, other problems with the book include some disastrous typesetting; notably, for example, the use of slash, bold slash, backslash, and vertical bar in one of the formalisms (pp. 120if), and the nonuse of superscripts and subscripts from time to time, as in this example: Xnbar\\---~ (C1... Cm) Xn\\- lbar (Cm + 1 ... Cn) (where each Ci is a maximal projection or a lexical formative) I also found the use of lowercase for acronyms irritating (e.g. \"ult,\" \"scomp\"). All of these are problems a good copyeditor should have ironed out. The book reads for the most part like a collection of interim progress reports written for a local audience, and one of its main failings is the only partial sense of a developing coherent theme or thread as the book progresses.","The book is divided into an introductory chapter plus four parts, each of two or three chapters.","Chapter 1 (Hailer, Schmidt, Steiner, Teich, and Zelinsky-Wibbelt) promises \"an outline of the EUROTRA project,\" but in fact not enough detail is given. Distracting use of EUROTRA jargon and reference to articles not in the public domain, as mentioned above, add to the alienation of the reader.","Chapter 2 (Schmidt) is the first chapter proper, and is a description of the treatment of German syntax within the EUROTRA framework. Given that we are not granted a sufficient explanation of the formalism, which is not especially transparent, it is difficult to make much of the subsequent discussion of how it needs to be improved. The chapter is patchy: a reasonable discussion of how to treat German uniformly as a subject-object-verb language using movement is followed by an incomplete sentence (p. 20): \"As the EUROTRA framework does not allow for transformations (and for empty elements on ECS the possible occurrence of constituents has to be regulated by optionality and a-rules (see (23)).\" [sic] and then some more unexplained formalism. Then there is a long discussion of valency theory, though how this fits in with everything else is not clarified; and then there is some more formalism (p. 36), which is comprehensible if you know some Prolog and you assume that the formalism has the same conventions.","Chapter 3 (Steiner, Eckert, Roth, and Winter-Thielen) discusses the use of semantic relations (SRs) in EUROTRA. The background explanation is too brief and vague to be helpful. The authors should have started by clarifying the role of SRs and process types and also clarifying the difference between analyzing SRs as a part of dictionary coding (done by humans) and the analysis done by the system. For example, the use of paraphrase tests (pp. 45, 57) is clearly part of the former task. In fact, nowhere in this chapter are we told how the system correctly assigns SPs, nor is there any indication of what they will be used for (cf p. 41). We are given a few examples of problems, but we are not told anything about general principles (of p. 78); for example, is it bad to introduce a new SR? (p. 57) A long section on tests for distinguishing between complements and modifiers (pp. 64-70) seems to cover much the same ground as a previous section (pp. 27-30). In conclusion, it is difficult to see what they are talking about in this chapter unless the reader already knows some of the background. There: is no explanation of the different process types, and a comparison with other similar models (Chafe, Longacre) would have been appropriate. Also, there is no discussion of how easy it is to get consistent coding, especially across languages, nor about how the MT system uses the classification (one can guess that it is like the use of SUBCAT features in GPSG, but one would like reassurance).","The first part of the book closes with a chapter by Zelinsky-Wibbelt on semantic feature representations of lexical units. The reader may be alarmed at the thought that this will be a thinly disguised rehash of Generative Semantics, but two pages into the chapter we are reassured that we are talking about the use of features to distinguish readings, not define meanings. Apart from that, however, the rest of the chapter is not very clear at all. We discover (p. 110) that lexical categories can be described by inventories of semantic features, the definitions (or characterizations) of which are given in terms of syntactic categories, despite the claim that \"there exists no regular relationship between the semantic principles and the surface structure of language (p. 107).\" The discussion changes abruptly (p. 113) from general principles to definitions of the feature system. The only indication of how these features were arrived at is that they \"empirically have proved necessary\" (p. 116) and the subsequent mixture of an appalling choice of formalism (mentioned above), inconsistent use of terms (eg. ENT or ENTITY?), garbled explanations, and often no explanation at all (e.g. what are ind, toll, part, sort, num, and priv on p. 1187) make the rest of the chapter fairly hard going.","Part II of the book contains two chapters discussing semantic relations. The first is a comparison by Steiner of SRs in EUROTRA and LFG. It is not very clear what the aim of this chapter is, in fact, or what motivated its inclusion. To start with, the choice of comparands is not very good. In LFG (at least in the reference used; namely, Bresnan 1982--if we can assume that this is what is meant by \"Bresnan 1982b\" referred to in the text but absent from the bibliography), SRs are not properly worked out; for example, as the author says \"it is not obvious what Bresnan's overall set of SRs is\" and there are no criteria for their definition (p. 135). One wonders why some of the o[her theories of SRs (case theories, theta-role theories) mentioned on page 133 were not chosen for comparison instead.","The second chapter on SRs is a report by Heid, R/Ssner, and Roth on a joint experiment in collaboration with the Stuttgart group using their SEMSYN generator to pro-48 Computational Linguistics Volume 16, Number 1, March 1990 Book Reviews From Syntax to Semantics: Insights from Machine Translation duce German text from representations as described in the foregoing chapter. In fact, the chapter amounts to a clear description of the possible syntactic realizations of the SRs, but it is not really a description of an experiment in text generation.","Part III of the book has three chapters on transfer. In the first, Schmidt discusses transfer strategies in general. The chapter begins with a discussion of why semantic disambiguation is necessary in lexical transfer, and makes the obvious point that there are typically not one-to-one lexical correspondences in languages. The example given is"]},{"title":"schleiflen","paragraphs":["with eight different readings, corresponding variously to"]},{"title":"close, shut, close down, lock, lock in, conclude,","paragraphs":["and"]},{"title":"infer.","paragraphs":["The author then goes on to discuss where lexical disambiguation might take place: wholly in analysis, wholly in generation, or partly in both.","Disambiguation wholly in analysis is swiftly dismissed, though the argument that it \"contradicts any idea of multilinguality\" (p. 163) is quite wrong. In fact the opposite is true, since the ideal truly multilingual system would presumably be an interlingual system, in which disambiguation wholly in analysis would be unavoidable.","Likewise, the dismissal of lexical disambiguation totally in generation (p. 164ff) is based on the false assumption that it would imply generating all possible combinations of lexical alternatives (illustrated by giving 24 \"possible\" translations of the sentence"]},{"title":"Der Rat faflte den BeschluJ3","paragraphs":["based on the three-way ambiguity of"]},{"title":"Rat,","paragraphs":["four possible translations of"]},{"title":"fassen,","paragraphs":["and two of"]},{"title":"Beschlufl)","paragraphs":["and then reducing the choice by hoping \"that twenty-three of [them] will be killed off by rules holding for English\" (p. 165). That may be the way the EUROTRA formalism would force one to do lexical disambiguation totally in generation, but the other obvious way to do it would be to enter generation with unique quasi-English lexical items for each of the German source words, and then make sure that the generation grammars had some means of choosing between the alternatives. This is the obvious strategy in cases in which there is a genuine generation ambiguity, as where a single concept in the source language corresponds to multiple concepts in the target language; the well-known examples of this are"]},{"title":"know","paragraphs":["as"]},{"title":"savoir","paragraphs":["or"]},{"title":"conna~tre, wall","paragraphs":["as"]},{"title":"Wand","paragraphs":["or"]},{"title":"Mauer","paragraphs":["and so on. I am not advocating disambiguation wholly in generation, but trying to suggest that Schmidt's refutation of it is flawed. For example, paragraph (ii) on page 166 (which is in any case extremely difficult to understand) seems to say first that for the person coding the grammars (or dictionaries?) it does not make any difference, and that it is better strategically to deliver the information in analysis. So is this now an argument for disambiguation in analysis after all?","The only alternative, claims Schmidt, is lexical disambiguation partly in analysis and partly in generation. That is a surprise indeed, because there is at least one more alternative not even considered, which is that disambiguation might be the job of transfer! As we read on, we find, however, that this is what Schmidt has in mind after all, since he next contrasts two versions of the shared disambiguation scenario. In one, called \"decorated lexical transfer\" or \"dlt,\" there is a general sematic typing of lexical units of the source language, while in \"undecorated lexical transfer\" or \"ult,\" semantic information is only invoked as it is needed. Schmidt attempts to support dlt by arguing against ult. Ult works like disambiguation totally in generation by killing off impossible structures; only the generator has to know about the conditions for choosing between alternatives, so the source language coder does not have to know or worry about it. So far it looks like an argument for ult, not against it. But, says Schmidt, how can the source language coders know that they do not need to worry? Only by knowing that generation will take care of the problem, which, we all agree, is unreasonable (analysis and generation should be modular and mutually independent). Confused? I still am.","I have always assumed that the logical thing to do would be to disambiguate sometimes in analysis, sometimes in transfer, and sometimes in generation, depending on the type of lexical ambiguity (roughly, homograph, translation ambiguity, or stylistic choice), but in any case there is a point, not completely brought out by Schmidt, that, in general, lexical information does not become available as the translation proceeds, since neither transfer nor generation can add to what is presumably available from the beginning. So it is really a question of organizing where information gets brought into play.","The remainder of Schmidt's paper reiterates the old idea that SRs are better for matching frames in transfer than arbitrarily numbered arguments, and then there is a presentation of how to write transfer rules for various cases of \"structural transfer,\" that is, when there is a basic structural mismatch between two languages. Again, because of the opacity of the formalism and lack of explanation, this section is difficult to judge.","In Chapter 8, Eckert and Heid describe an experiment in which some German verbs are classified on a monolingual basis according to the system of semantic predicate types and associated case frames described in Chapter 3. The same is done independently for the corresponding verbs in French, and the experiment is to see to what extent correct translation pairs can be established automatically by matching up the codings. The conclusion the authors draw is that it works rather well, though to this reader at least, the ratio of 70 incorrect matches for every 100 correct seems to suggest that it is not very reliable. The last chapter in this section appears to be a fairly solid discussion by Zelinsky-Wibbelt of the treatment of determiners and quantifiers. The chapter is rather impenetrable, mainly because of the proliferation of terminology, though perhaps not to the reader who is more familiar with this particular field. For once there are ample references to the literature, though again a barely available local working-paper seems to be a key reference, variously cited for explanation, fuller details, or exemplification. Also, the chapter must have been a typesetter's nightmare with its bewildering variety of typefaces. Computational Linguistics Volume 16, Number 1, March 1990 49 Book Reviews Studies in Computer-Aided Lexicology","The last two chapters of the book form Part IV, subtitled \"Explorations.\" The first of these comes as something of an agreeable surprise, and is certainly the best chapter in the book. The authors, Hauenschild and Busemann, investigate the possibility of adapting GPSG to MT. In particular, they address the problem of developing a \"constructive\" (or constructional) version of the \"purely axiomatic\" version found in Gazdar et al. (1985). Here at last is something of an answer to the very apposite question posed by Kimmo Kettunen (1986) in this journal.","The second chapter might have been a similar explora-tion of LFG by Schmidt. Instead, it \"tries to give an idea of how to overcome some weaknesses of [EUROTRA's] CAT-formalism.., by relating it to LFG\" (p. 239). The last two sentences of the book seem to confirm the impression that perhaps they might have done better just to start with LFG in the first place: This chapter was a glimpse at a theory from which a great deal has been imported into CAT, namely LFG. However, some of the virtues of the original have been ignored, above all the most clever LFG mechanism, the functional uncertainty device. (p. 250) REFERENCES Bresnan, J. (ed.) 1982 The Mental Representation of Grammatical Relations. Cambridge, MIT Press, Cambridge, MA. Gazdar, G.; Klein, E.; Pullum, G. K.; and Sag, I. 1985 Generalized Phrase Structure Grammar. Basil Blackwcll, Oxford. Kcttuncn, K. 1986 \"Is MT Linguistics?\" Computational Linguistics, 12(1), 37-38. Harold Somers, a lecturer at UMIST, is a member of the machine translation research group thcrc with a special interest in the relationship between syntax and semantics. Although for-merly involved in the EUROTRA project, hc has not been associated with that project for three years. Somers's address is: Centre for Computational Linguistics, UMIST, P.O. Box 88, Manchester M60 1QD, U.K. E-mail: hls@ccl.umist.ac.uk STUDIES IN COMPUTER-AIDED LEXICOLOGY Martin Gellerstam (ed.) (University of GSteborg) Stockholm: Almqvist & Wiksell International, 1988, 375pp.(Data Linguistica 18) Hardbound, ISSN 0347-948X, ISBN 91-22-01258-3, SEK287.00 Reviewed by Martha Evens Illinois Institute of Technology This volume is a collection of papers in honor of the sixtieth birthday of Sture All6n, Chairman of the Department of Computational Linguistics at the University of Gothenburg and Permanent Secretary of the Swedish Academy, written by his students and colleagues. These papers appear in excellent English but they describe work on Swedish (although many of the authors represented here have also worked on problems in English lexicology). The organiza-tion of the book, in alphabetical order by last name of author, has the effect of emphasizing the amazing breadth of All6n's work, as we move from a paper on applications of lexical databases in linguistics to a paper on the creation of such a database, or from a paper on the structure of the lexicon to a paper on how a computer program can access a lexic, al database.","The problem of creating a lexical database from dictionary data is discussed in three papers. Rudolf Rydstedt gives a preliminary report on the construction of the lexical database for the Dictionary of the Swedish Academy, a project headed by All6n himself (comparable to the construction of the new Oxford English Dictionary by the University of Waterloo and IBM in cooperation with the Oxfi)rd University Press). The paper includes a description of the languages used for defining the data, the problems of structuring fixed- and free-format text, and the question of creating a database that can be distributed in CD-ROM form. Sture Berg and Kaisa Samuelson exhibit the cooperation between linguist and computer scientist that is such a happy feature of this book in a description of the production of a. glossary for use in spelling correction; the major problems are the generation of all inflected forms and the analysis of the novel compounds that are a common feature of modern Swedish. Rolf Gavare talks about the problem of sorting in alphabetical order any collection of \"words\" including uppercase letters, numerals, Greek and other non..Roman characters, and logograms like percent signs and ampersands.","A number of the papers discuss the structure of the lexical database and desirable features of the lexical entry. Staffan Hellberg proposes a novel classification of adjectives. Martin Gellerstam and Maria Toporowska-Gronostaj both discuss methods of representing the way verbs and their arguments combine. Bo Ralph looks at semantic rather than syntactic structures for verbs, concentrating mainly on taxonomy.","Some lexical databases are created primarily for use by other computer software; others are intended to be used by human beings. Anna S~igvall Hein describes the lexical database used by her parser for Swedish, a unification-based chart parser designed for such diverse sublanguages as news agency telegrams, medical texts, and dictionary definitions. Christian Sj6green describes a project, also designed by All6n, to create a commercial dictionary from a lexical database, complete to the writing of the printer's tape. Gu5rfn Magndsd6ttir discusses problems of access-ing the lexicon in the machine translation process. The advantages of a lexical database as a source for linguistic research are illustrated by studies of synaesthesia (Asa Abelin), semantic change (Birgitta Ernby), and loan words (Kerstin Nor6n).","Two papers of extraordinary interest span all these categories. Jerker J/irborg presents a formal structure for the description of both syntactic and semantic features. Sven-50 Computational Linguistics Volume 16, Number 1, March 1990"]}]}