{"sections":[{"title":"Natural Language Generation in Artificial Intelligence and Computational Linguistics C6cile Paris, William R. Swartout, and William C. Mann (editors)","paragraphs":["(Information Sciences Institute, University of Southern California) Boston: Kluwer Academic Publishers (The Kluwer International Series in Engineering and Computer Science: Natural Language Processing and Machine Translation), 1991, xxii + 403 pp. Hardbound, ISBN 0-7923-9098-9, $85.00, Â£50.75, Dfl 180.00"]},{"title":"Reviewed by Robert Dale University of Edinburgh","paragraphs":["A number of collections of papers from the field of natural language generation"]},{"title":"(NLG)","paragraphs":["have been published over the last few years: Kempen (1987), Zock and Sabah (1988), Dale, Mellish, and Zock (1990), and now the present volume. All have in common that they are derived in one way or another from workshops on the subject, and should therefore make available new and often exploratory research in a timely fashion. If such a book is to be more than a conference proceedings, it has to do a little more too, of course; it should present the research in more detail than a conference proceedings would, there should be greater cohesion amongst the papers, and it should be produced to an appropriate standard. The present book, like its predecessors, succeeds on some counts but fails on others. The papers in the book are organized into three strands, described in turn below:"]},{"title":"text planning, lexical choice,","paragraphs":["and"]},{"title":"grammatical resources.","paragraphs":["The balance between these is rather skewed, however: the first section contains eight papers, and the second and third contain only three papers each."]},{"title":"1. Text Planning","paragraphs":["It is the first section of the book that exhibits the most coherence. Papers by Moore and Swartout, Paris, and Hovy all describe experiments in text planning based on Rhetorical Structure Theory"]},{"title":"(RST).","paragraphs":["The first two of these papers describe work in the Explainable Expert Systems framework, whose basic idea is that expert systems have to be designed from the outset with explanation in mind, rather than having a separate generation module tacked on the end. Paris provides some good background on the framework, describing the different kinds of knowledge that this requires such systems to have, along with the concomitant problems this poses for generation; she describes an earlier experiment using McKeown-like schemata (McKeown 1985) for generation in this context, and uses this to motivate the switch to a top-down text planner based on Rhetorical Structure Theory. Moore and Swartout describe in some detail the Program Enhancement Advisor, a system that participates in explanatory dialog, and emphasize the need for such systems to explicitly represent and reason about the design of the explanations they construct, so as to permit elaboration of previous explanations, provision of clarifications, and the answering of follow-up questions. 448 Book Reviews","Although also based on RST, Hovy's system is fundamentally different from the two just described, primarily because it uses RST only to organize preselected content, whereas Moore and Swartout's and Paris's systems also use RST to determine content. Each of these three papers devotes some discussion to the others, which is good, and provides the coherence alluded to earlier; however, there is a little redundancy of background information across the papers, which could have been cut. Together, these three papers give the reader a good idea of some of the issues in text planning; Hovy's exposition in particular could serve as the basis for implementation of a simple text structuring program.","Of the other papers in this section, my favorites are those by Bateman and Meteer. Bateman describes what he calls the \"grammar-as-filter\" methodology, and provides a good introduction to the basic tenets of systemic functional linguistics (SFL). The basic points here will not be new to most people in the generation field, but, like other papers in this volume, the background provided makes the paper potentially more useful to newcomers to the field. Bateman elaborates on how, in SFL, the three metafunctions of language -- the ideational, the interpersonal, and the textual -- are treated equally, in contrast to other approaches where the ideational (roughly, the notion of propositional content) usually takes precedence, with the other categories all too often being assigned to the wastebasket of pragmatics. In SFL, the core idea is that each variation in language carries a functional load, unless there are good reasons to think otherwise. Bateman begins by demonstrating the relatively uncontroversial idea that interpersonal meaning (the expression of social relationships and speaker attitudes) is grammaticized in Japanese; then, with some fairly good linguistic argument, he focuses on the textual metafunction (roughly, those aspects of language use concerned with connectivity and cohesion), and unfolds his story by examining the use of particle assignment in Japanese. He shows how the topic particle wa carries two types of textual meaning (topicality and contrast) and goes on to present a detailed argument for the view that Japanese case particles play a role in information structuring. The result of these analyses is a number of textual distinctions that a text planner needs to be able to talk in terms of. Although this is really only a very small exploration of the space of things that have to be considered, the paper is a valuable exposition of the approach.","The basis of Meteer's paper is an analysis of texts revised by professional editors, comparing the changes with the linguistic structures used in the originals. After presenting a fascinating collection of examples of each of the categories of changes she posits, she looks at how these revisions might be explained, and in so doing in-troduces a notion of text structure that abstracts away from specific realizations, and talks in terms of more general notions of heads and arguments, and matrices and adjuncts; revisions in texts can then be characterized as transformations at this level. This level of text structure has a fundamental role to play in that old chestnut for generation researchers of how you draw the line between what to say and how to say it: the problem is that part of the motivation for the distinction is to keep conceptual and linguistic knowledge separate, and yet, without any understanding of linguistic resources, there is the danger that the what-to-say component may construct some message specification that has no linguistic realization. The notion of text structure provides a level of abstraction that can be used as the interface language between the two components: in my view, the significance of Meteer's work here is very great indeed.","Reithinger describes the POPEL system, which takes a different stance on the problem of integrating what to say and how to say it. The system he describes in some detail consists of two components, POPEL-WHAT and POPEL-HOW, which together em-449 Computational Linguistics Volume 17, Number 4 body an incremental generation process. An important aspect of the system is the bidirectional interaction between the two components, which allows each component to post queries to the other. The paper also includes a couple of tantalizing tasters that I would liked to have seen taken further: a brief pointer' to the multi-modal capabilities of the system, and some consolidatory work on integrating the discourse theories of Reichman, Grosz and Sidner, and RST that is, alas, described in too little detail to assess.","The other papers in this section are by McCoy and Cheng, and Sparck Jones. McCoy and Cheng describe their notion of focus trees, an attempt at providing a unified approach to focus phenomena that integrates a number of observations that people have made over the years: there is definitely an intuition of importance here, but it's clear that there's a lot more working-out to be done. Sparck Jones's paper questions the general presumption that the output of a natural language generation system should be tailored to the user. After a quick summary of some of the relevant distinctions in user modeling -- useful for those who are new to that field -- she uses a number of examples to press her point that systems should not be too quick to make assumptions about their users. The limited sophistication of our systems means that any hypotheses we may derive are generally poorly supported, so it's better to play safe; that way, you are less likely to make mistakes or offend people. Furthermore, she argues, input data and application information is enough to do the job, so, while in principle you could get heavily into user modeling, why bother? This is an interesting paper, if a little overlong. 2. Lexical Choice I found this section of the book the most thought-provoking, and at the same time the most frustrating. It is here that integration across the papers is lacking most, and yet would have been, at least for me, most interesting and useful.","McDonald takes the view that lexical variation at the surface level has its origins very deep in the conceptual system, and that the selection of key lexical items is the first step in the generation process; this is not the standard approach, and so, like Sparck Jones's paper in the previous section, the general argument here is one of questioning widely held assumptions. Much of the paper is driven by a detailed analysis of the differences that motivate the choice between You can only stay until 4 and You have to leave by 4, leading to a closer scrutiny of the conceptual models that must underlie generation.","Matthiessen's paper focuses on a different aspect of lexical choice: how should lexical resources be organized? His view is that lexical choice should be viewed as part of the unified problem of lexicogrammatical choice. Like Bateman's paper earlier in the volume, he provides some background on systemic functional linguistics for those who are new to the area; just as in the descriptions of RST in the first three papers on text planning, this leads to some redundancy across the book as a whole. Matthiessen describes an interesting model of lexis (not just the lexicon, which, as he points out, makes us think too much of lexical data divorced of its related processing) that is closer to a thesaurus than a dictionary.","The final paper in the section describes the use of Mel'~uk's meaning-text theory (MTT) in the context of a generation system that provides reports about operating system usage. The focus here is on how the multiple levels of representation in MTT allow for the introduction of paraphrase in the process of mapping between the levels. The number of paraphrases thus available is quite large, and has then to be further constrained by notions of information structure; it would be interesting to see what 450 Book Reviews Meteer, McDonald, and Bateman's approaches would say about the variation here.","All three papers in this section are interesting, but the interconnections are weak: there are lots of links the reader is tempted to construct for herself, but can only ponder because the authors don't generally situate themselves with respect to each other. 3. Grammatical Resources This is the least satisfying section of the book, since it has the feel of a grab-bag section for papers that wouldn't fit in the other two. De Smedt and Kempen provide a very good and detailed exposition of segment grammar as an incremental tactical generation process; particularly valuable here is the comparison with other grammatical formalisms. Comparison of this kind is also a virtue of the paper by McKeown and Elhadad, who argue that unification-based formalisms such as their Functional Unification Grammar (FUG) allow for a much more flexible ordering of decisions than other formalisms; this makes it easier to integrate multiple constraints in the generation process, a point that they demonstrate with respect to the choice of connectives between clauses. The comparison with other approaches is quite detailed, although the inclusion of ATNs as one of these alternatives seems a little dated.","Most striking in this section is the lack of any papers that focus specifically on reversible grammar (although, to be fair, the McKeown and Elhadad paper can be seen in this light). In fact, that part of the generation community who started their research lives in parsing seems quite under-represented overall. It's clear that there is a fundamental divide between those who take the view that generation is simply parsing in reverse, and those who feel that the deeper issues in text organization and planning are where the real action is, but this divide will only be overcome if opportunities are provided for members of each community to read the work of the other side, and one way of doing this is to put together books that represent each more equally. 4. Conclusions I have a general ambivalence toward books of this kind. On the one hand, they are valuable because they make available research that is otherwise hard to find out about, typically only described in technical reports; on the other hand, I feel rather concerned about the prevalent expectancy that, if you have a workshop, you must have a book derived from the workshop. On balance, I think this volume is a Good Thing; as a whole, it provides a decent snapshot of a reasonably recent span of work in NLG, warts and all.","I don't know why it is that so little of this work is reported in, for example, the ACL conferences: is it because the program committees are biased against generation, because the research just isn't up to standard, or because we NLG people have decided to stay with our own at our cozy workshops? There's probably an element of truth in each explanation. Inevitably, the quality of papers in this volume is variable -- only a few approach the standard one would expect of a journal article -- but there is some genuinely thought-provoking work here, some of which has relevance for people working in natural language understanding too. This is particularly true of those that address more fundamental issues and matters of methodology, such as the papers by Bateman, Meteer, McDonald, and Sparck Jones: if the issues they raise are important for NLG, then they are surely also important for NLU.","The book does have its bad points too, most of which have already been alluded to. Some of these are par for the course: the preliminary and exploratory nature of much 451 Computational Linguistics Volume 17, Number 4 of the work described, and the lack of integration and cohesion across the papers, are also faults of the other workshop-based books mentioned earlier. A major deficiency is the lack of any papers that address directly either grammar reversibility or multi-modal generation, other than in passing (Reithinger's paper contains a page on this aspect of his system); these lacunae are particularly serious in a book that claims to present \"the most current research\" in the field (this claim is also rather odd given the rather long time lag between the 1988 workshop on which the book is based and the appearance of the book itself).","By far the most annoying thing about the book, however, is the quality of produc-tion. The text has not been properly proofread or copyedited, resulting in mistakes at all levels, from spelling errors through formatting mistakes to at least one reference to another paper in the volume that turns out not to be present. Such a state of affairs is, I suppose, acceptable for fast-turnaround conference proceedings, but this book took three years to appear. The overall impression is of a publisher not doing the job it is there to do; this alone would be bad enough, but it is quite unacceptable when combined with the outrageous price charged for this book.","Who should buy this book? At its extortionate price, I imagine that it will be mostly libraries. As workers in the field, we have an obligation to ensure that books are made available more cheaply than this. If it were cheaper, then I would say it should be on every generation researcher's bookshelf. Its audience outside this community is, I think, limited; as a subarea of computational linguistics, we have not yet gotten around to describing our research in such a way as to make it more generally useful. The papers that address more fundamental issues will be of interest outside the NLG community, but I cannot see anyone other than generation researchers being interested in those papers that are essentially system descriptions.","References","Dale, Robert; Mellish, Chris; and Zock, Michael (1990). Current Research in Natural Language Generation. London: Academic Press.","Kempen, Gerard (1987). Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics. Dordrecht: Martinus Nijhoff Publishers.","McKeown, Kathleen R. (1985). Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge, England: Cambridge University Press.","Zock, Michael, and Sabah, G6rard (1988). Advances in Natural Language Generation: An Interdisciplinary Perspective. London: Pinter Publishers. Robert Dale lectures in Computational Linguistics at the Centre for Cognitive Science and the Department of Artificial Intelligence at the University of Edinburgh. He did his Ph.D. work on the generation of referring expressions; a book derived from this work will be published in the near future in the ACL-MIT Press series in natural language processing. He co-edited a book of papers derived from another workshop on generation. 452"]}]}