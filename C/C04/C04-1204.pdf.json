{"sections":[{"title":"Deep Linguistic Analysis for the Accurate Identificationof Predicate-Argument Relations Yusuke Miyao Department of Computer Science University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science University of Tokyo CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract","paragraphs":["This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations. We could directly compare the output of HPSG parsing with PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations."]},{"title":"1 Introduction","paragraphs":["Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-the-art PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicate-argument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other.","In this paper, we employ PropBank (Kingsbury and Palmer, 2002) for the evaluation of the accuracy of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles are manually annotated. Since the PropBank has been developed independently of any grammar formalisms, the results are comparable with other published results using the same test data.","Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003). They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. These results imply the superiority of deep linguistic analysis for this task.","Although the statistical approach seems a reasonable way for developing an accurate identifier of PropBank annotations, this study aims at establish-ing a method of directly comparing the outputs of HPSG parsing with the PropBank annotation in order to explicitly demonstrate the availability of deep parsers. That is, we do not apply statistical model nor machine learning to the post-processing of the output of HPSG parsing. By eliminating the effect of post-processing, we can directly evaluate the accuracy of deep linguistic analysis.","Section 2 introduces recent advances in deep linguistic analysis and the development of semantically annotated corpora. Section 3 describes the details of the implementation of an HPSG parser evaluated in this study. Section 4 discusses a problem in adopting PropBank for the performance evaluation of deep linguistic parsers and proposes its solution. Section 5 reports empirical evaluation of the accuracy of the HPSG parser."]},{"title":"2 Deep linguistic analysis and semantically annotated corpora","paragraphs":["Riezler et al. (2002) reported the successful applica-tion of a hand-crafted LFG (Bresnan, 1982) grammar to the parsing of the Penn Treebank (Marcus et al., 1994) by exploiting various techniques for robust parsing. The study was impressive because most researchers had believed that deep linguistic analysis of real-world text was impossible. Their success owed much to a consistent effort to maintain a wide-coverage LFG grammar, as well as var-S VP have to choose this particular moment S NP VP VP NP they NP-1 did n’t *-1 VP VP ARG0-choose ARG1-chooseARG0-choose REL-choose Figure 1: Annotation of the PropBank ious techniques for robust parsing.","However, the manual development of wide-coverage linguistic grammars is still a difficulttask. Recent progress in deep linguistic analysis has mainly depended on the acquisition of lexicalized grammars from annotated corpora (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000; Hockenmaier and Steedman, 2002a; Cahill et al., 2002; Frank et al., 2003; Miyao et al., 2004). This approach not only allows for the low-cost development of wide-coverage grammars, but also provides the training data for statistical modeling as a by-product. Thus, we now have a basis for integrating statistical language modeling with deep linguistic analysis. To date, accurate parsers have been developed for LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and LFG (Cahill et al., 2002; Burke et al., 2004). Those studies have opened up the application of deep linguistic analysis to practical use.","However, the accuracy of those parsers was still below PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score, i.e., labeled bracketing accuracy of CFG-style parse trees. Since one advantage of deep parsers is that they can output a sort of semantic representation, e.g. predicate-argument structures, several studies have reported the accuracy of predicate-argument relations (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003; Miyao et al., 2003). However, their evaluation employed a treebank developed for a specificgrammar formalism. Hence, those results cannot be compared fairly with parsers based on other formalisms including PCFG parsers.","At the same time, following the great success of machine learning approaches in NLP, many research efforts are being devoted to developing various annotated corpora. Notably, several projects are underway to annotate large corpora with semantic information such as semantic relations of words and coreferences.","PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998) are large English corpora annotated with the semantic relations of words in a sentence. Figure 1 shows an example of the annotation of the PropBank. As the target text of the PropBank is the same as the Penn Treebank, a syntactic structure is given by the Penn Treebank. The PropBank includes additional annotations representing a predicate and its semantic arguments in a syntactic tree. For example, in Figure 1, REL denotes a predicate, “c hoose” , and ARG ","represents its semantic arguments: “the y” for the 0th argument (i.e., subject) and “this particular moment” for the 1st argument (i.e., object).","Existing studies applied statistical classifiers to the identification of the PropBank or FrameNet annotations. Similar to many methods of applying machine learning to NLP tasks, they first formulated the task as identifying in a sentence each argument of a given predicate. Then, parameters of the identifierwere learned from the annotated corpus. Features of a statistical model were defined as a pattern on a partial structure of the syntactic tree output by an automatic parser (Gildea and Palmer, 2002; Gildea and Jurafsky, 2002).","Several studies proposed the use of deep linguistic features, such as predicate-argument relations output by a CCG parser (Gildea and Hockenmaier, 2003) and derivation trees output by an LTAG parser (Chen and Rambow, 2003). Both studies reported that the identification accuracy improved by in-troducing such deep linguistic features. Although deep analysis has not outperformed PCFG parsers in terms of the accuracy of surface structure, these results are implicitly supporting the necessity of deep linguistic analysis for the recognition of semantic relations.","However, these results do not directly reflect the performance of deep parsers. Since these corpora provide deeper structure of a sentence than surface parse trees, they would be suitable for the evaluation of deep parsers. In Section 4, we explore the possibility of using the PropBank for the evaluation of an HPSG parser."]},{"title":"3 Implementation of an HPSG parser","paragraphs":["This study evaluates the accuracy of a generalpurpose HPSG parser that outputs predicate argument structures. While details have been explained in other papers (Miyao et al., 2003; Miyao et al., 2004), in the remainder of this section, we briefly review the grammar and the disambiguation model of our HPSG parser. S VP have to choose this particular moment S NP VP VP NP they NP-1 did n’t *-1 VP VParg head head head head head head head arg arg arg arg mod  have to choose this particular moment they did n’t HEAD verb SUBJ < > COMPS < > HEAD noun SUBJ < > COMPS < > HEAD verb SUBJ < >2 HEAD verb SUBJ < _ > HEAD verb SUBJ < >2","HEAD verb","SUBJ < >1 HEAD verb SUBJ < >1","HEAD noun","SUBJ < >","COMPS < > head-comp head-comp head-comp head-comp subject-head 1  have to they did n’t HEAD verb SUBJ < > COMPS < > HEAD noun SUBJ < > COMPS < > HEAD verb SUBJ < > COMPS < >1 HEAD verb SUBJ < > COMPS < > HEAD verb SUBJ < > COMPS < >1 HEAD verb SUBJ < > COMPS < >1 1 2 2 HEAD verb SUBJ < > COMPS < >1 3 HEAD verb SUBJ < > COMPS < >13 1 choose this particular moment HEAD noun SUBJ < > COMPS < > 4 HEAD verb SUBJ < > COMPS < >1 4 Figure 2: Extracting HPSG lexical entries from the Penn Treebank-style parse tree 3.1 Grammar The grammar used in this paper follows the theory of HPSG (Pollard and Sag, 1994), and is extracted from the Penn Treebank (Miyao et al., 2004). In this approach, a treebank is annotated with partially specified HPSG derivations using heuristic rules. By inversely applying schemata to the derivations, partially specifiedconstraints are percolated and in-tegrated into lexical entries, and a large HPSG-style lexicon is extracted from the treebank.","Figure 2 shows an example of extracting HPSG lexical entries from a Penn Treebank-style parse tree. Firstly, given a parse tree (the top of the figure), we annotate partial specificationson an HPSG derivation (the middle). Then, HPSG schemata are applied to each branching in the derivation. Finally, COMPS < > SUBJ < > PHON “choose” HEAD verb REL choose ARG0 ARG1 HEAD noun SEM 1 HEAD noun SEM 2","SEM 1 2 Figure 3: Mapping from syntactic arguments to semantic arguments we get lexical entries for all of the words in the tree (the bottom).","As shown in the figure, we can also obtain complete HPSG derivation trees, i.e., an HPSG treebank. It is available for the machine learning of disambiguation models, and can also be used for the evaluation of HPSG parsing.","In an HPSG grammar, syntax-to-semantics mappings are implemented in lexical entries. For example, when we have a lexical entries for “c hoose” as shown in Figure 3, the lexical entry includes mappings from syntactic arguments (SUBJ and COMPS features) into a predicate-argument structure (ARG0 and ARG1 features). Argument labels in a predicate-argument structure are basically definedin a left-to-right order of syntactic realizations, while if we had a cue for a movement in the Penn Treebank, arguments are put in its canonical position in a predicate-argument structure. 3.2 Disambiguation model By grammar extraction, we are able to obtain a large lexicon together with complete derivation trees of HPSG, i.e, an HPSG treebank. The HPSG treebank can then be used as training data for the machine learning of the disambiguation model.","Following recent research about disambiguation models on linguistic grammars (Abney, 1997; John-son et al., 1999; Riezler et al., 2002; Clark and Curran, 2003; Miyao et al., 2003; Malouf and van Noord, 2004), we apply a log-linear model or maximum entropy model (Berger et al., 1996) on HPSG derivations. We represent an HPSG sign as a tuple",", where","is a lexical sign of the head word,","is a part-of-speech, and","is a symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the Penn Treebank). Given an HPSG schema","and the distance","between the head words of the head/non-head daughter constituents, each (binary) branching of an HPSG derivation is represented as a tuple"," ",", where","","","","","","","","","","","","","","","","  –","","","","","","","  –","  –","   – –","  –"," ","","",""," –"," ","– –","  –"," ","–"," –  – – ","– – –  – –  –","","","  ","","","","","","  –"," "," ","","– –  – Table 1: Feature function templates used in the disambiguation model of HPSG parsing: for binary schema applications (top) and for unary ones (bottom)","denote head/non-head daughters.1","Since an HPSG","derivation","is represented by a set of B, a prob-","ability of","assigned to sentence","is defined as","follows:","","","","","","","","","","","            ","is a probability of a sequence of lexical entries, and is definedas the product of unigram probabilities","","      , where  is a lexical entry assigned to word  . We divided the probability into","","and   ","in order to accelerate the estimation","of the probability model by using","","as a ref-","erence distribution (Miyao et al., 2003), because the","direct estimation of","was computationally","expensive.","Feature function   returns 1 when a certain part of tuple ","is observed. Table 1","lists templates of feature functions used in the","disambiguation model, where a check means","that the corresponding element in the tuple is","seen. For example, when we have a branching","","head comp","  trans","VB","VP","noun","NNS","NP",", 2","the following feature functions return 1, while all 1 A unary branching is represented by",". 2 In this example, head comp and trans stand for the Head","Complement Schema and a transitive verb. In our probabilistic","model, lexical entry templates are more fine-grained(as shown","in Section 5, a grammar has more than 1,000 templates), while","we used a simple example here. S the window HeNP NP VP ARG0-broke ARG1-broke","broke REL-broke S the windowNP VP ARG1-broke","broke REL-broke Figure 4: Annotation of an ergative verb in the PropBank S the windowNP VP ARG1-broke broke into PP","NP a million pieces ARG3-broke REL-broke Figure 5: Annotation of another usage of “brok e”","the other features are 0: ","head comp","trans","VB VP","noun","NNS","NP","","head comp","","trans","VB VP","noun","NNS","NP","","head comp","","VB VP","","NNS","NP","","head comp","","","VB VP","","NNS","NP","","head comp","trans VB","","noun","NNS","","","head comp","","trans VB","","noun","NNS","","","head comp  VB","","","NNS","","","head comp","  VB","","","NNS","","Given the HPSG treebank as training data, the model parameters ","are efficientlyestimated using a dynamic programming algorithm for maximum entropy estimation (Miyao and Tsujii, 2002; Geman and Johnson, 2002)."]},{"title":"4 Evaluating HPSG parsing with semantically annotated corpora","paragraphs":["Our study aims toward the fair evaluation of deep linguistic parsers, thus we want to directly compare the output of HPSG parsing with hand-annotated test data. However, disagreements between the output of HPSG parser and the PropBank prevents us from a direct comparison.","In the PropBank annotation, semantic arguments can occur in multiple syntactic realizations, as in the following example (Figure 4). 1. He broke the window. 2. The window broke. In the first example, a semantic object appears in a syntactic object position, while in the second sentence it becomes the syntactic subject. This alterna-tion is caused by two reasons: syntactic alternations such as passive constructions and long-distance dependencies, and lexical alternations such as ergative verbs. It should also be noted that the assign-ment of argument labels have some arbitrariness. For example, Figure 5 shows the PropBank annotation for “The window broke into a million pieces.” , where a phrase “a million pieces” is annotated with ARG3, not with ARG2. This is because ARG2 is reserved for an instrument argument (e.g. “with a rock”). However, the choice of selecting ARG2 or ARG3 for “a million pieces” is arbitrary. Existing studies exploited statistical methods to mend these alternations and arbitrariness.","Basically, deep linguistic parsers derived from the Penn Treebank can handle syntactic alternations owing to trace annotation in the treebank. However, lexical alternations and arbitrariness of assignments of argument labels will be a problem when we directly compare the output of an HPSG parser with the PropBank.","However, we can see that the remaining disagreements are about the labels of argument labels. In general, we can assume that argument labels can be uniquely determined if a syntactic class of the predicate is given.3","In the example given in Section 2, “the window” always occurs in the object position when “br oke” is transitive, while it appears in the subject position when it is intransitive. Since syntactic classes are expressed by lexical entries in HPSG, this indicates that we can establish a unique mapping from an HPSG lexical entry into PropBank semantic roles.","Following this idea, we developed a mapping from HPSG argument labels into PropBank argument labels. This mapping was developed with a very simple algorithm as follows. We first computed predicate-argument structures from an HPSG treebank. We then compared the obtained predicate-argument structures with the PropBank annotations, and for each pair of a surface form of a word and its syntactic class, the mapping from argument labels of a predicate-argument structure into those of PropBank was registered. When we found a conflict, that is, multiple mappings were found for a pair, a mapping found later was simply discarded.","Our method is much simpler than existing studies, and it should be noted that PropBank was not used for training the probabilistic model or statistical identifier. This might be a handicap for our evaluation, but this method can clearly show the lower bound of the accuracy that has been attained by HPSG parsing. 3 There exist some exceptions as follows:  “He opened the bottles.”  “The can opener opens the bottles.” In the PropBank, “he” is assigned ARG0, while “the can opener” is assigned ARG2 (instrument).  penn ","prop # words 8,539 8,496 # lexical entry template 1,106 1,178 # template per word 3.00 3.16 # features 50,158 52,151 Size of the training data 124 MB 131 MB Estimation time 68 min 51 min Table 2: Specifications of the HPSG grammar and the disambiguation model"]},{"title":"5 Experimental results","paragraphs":["In this section, we evaluate the accuracy of HPSG parsing using the November 2002 release of PropBank (Kingsbury and Palmer, 2002). An HPSG grammar was extracted from Section 02-21 and a disambiguation model was trained using the same data. Table 2 shows specifications of the grammar and the disambiguation model, where the size of the training data shows the file size of a compressed training data and the estimation time represents a user time required for estimating","","","","",". We prepared two grammars for the evaluation: ","penn was extracted from the Penn Treebank with the original algorithm (Miyao et al., 2004), and ","prop was extracted using the PropBank annotations for argument/modifier distinction by a method similar to Chen and Rambow (2003). That is, constituents annotated with ARG ","were treated as an argument in the grammar extraction. In ","penn, prepositional phrases are basically treated as modifiers since we have no cue to detect argument/modifier distinction in the original Penn Treebank. Section 02-21 was also used for developing HPSG-to-PropBank mapping. Note that the PropBank annotation was used only for this purpose, and was not used for training a statistical disambiguation model. This is very different from existing methods of identifying PropBank-style annotations where they trained the identificationmodel using the PropBank. In the following, Section 22 of the PropBank was used for the development of the parser, while Section 23 was used for the finalevaluation.","The accuracy of HPSG parsing was measured against the core-argument annotations (i.e., ARG0, ..., ARG5) of the PropBank. Each predicate-argument relation output by the parser was represented as a tuple","","","","","","","","","",", where  ","was a predicate,","","","","was the label of an argument position (i.e., one of ARG0, ..., ARG5), and","was the head word of the argument of","",". Each tuple was compared to the annotations in the PropBank. We used a mapping table described in LP LR UP UR  penn 70.3 56.0 86.7 69.2 ","prop 68.3 59.0 85.6 73.9 Gold parses 79.5 67.1 97.2 82.0 Table 3: Accuracy of PropBank annotations (head words of core arguments, without HPSG-to-PropBank mapping) LP LR UP UR  penn 80.3 64.1 86.7 69.2 ","prop 79.6 68.7 85.6 73.9 Gold parses 91.2 76.9 97.2 82.0 Table 4: Accuracy of PropBank annotations (head words of core arguments, wit