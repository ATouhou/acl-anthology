{"sections":[{"title":"Identificationof Confusable Drug Names: A New Approach and Evaluation Methodology Grzegorz Kondrak Department of Computing Science University of Alberta Edmonton, Alberta, Canada, T6G 2E8 kondrak@cs.ualberta.ca Bonnie Dorr Institute for Advanced Computer Studies & Department of Computer Science University of Maryland College Park, 20742, USA bonnie@umiacs.umd.edu Abstract","paragraphs":["This paper addresses the mitigation of medical errors due to the confusion of sound-alike and look-alike drug names. Our approach involves application of two new methods— one based on orthographic similarity (“look-alike”) and the other based on phonetic similarity (“sound-alik e”). We present a new recall-based evaluation methodology for determining the effectiveness of different similarity measures on drug names. We show that the new orthographic measure (BI-SIM) outperforms other commonly used measures of similarity on a set containing both look-alike and sound-alike pairs, and that the feature-based phonetic approach (ALINE) outperforms orthographic approaches on a test set containing solely sound-alike confusion pairs. However, an approach that combines several different measures achieves the best results on both test sets."]},{"title":"1 Introduction","paragraphs":["Many hundreds of drugs have names that either look or sound so much alike that doctors, nurses and pharmacists can get them confused, dispens-ing the wrong one in errors that can injure or even kill patients. In the United States alone, an estimated 1.3 million people are injured each year from medication errors, such as administering the wrong dose or the wrong drug (Lazarou et al., 1998).1 The U.S. Food and Drug Administration has sought to mitigate this threat by ensuring that proposed drug names that are too similar to pre-existing drug names are not approved (Meadows, 2003).","A number of different lexical similarity measures have been applied to the problem of identifying confusable drug names. Lambert et al. (1999) tested twenty two distinct methods on a set of drug names extracted from published reports of medication errors. The methods included well-known universal measures, such as edit distance and longest common 1 For example, a patient needed an injection of Narcan but","instead got the drug Norcuron and went into cardiac arrest. subsequence, several variations of measures based on counting common letter","-grams, and measures designed specifically for associating phonetically similar names, such as Soundex and Editex. They identified the normalized edit distance, Editex, and a trigram-based measure as the most accurate.","The evaluation methodology of Lambert et al. (1999) involves repeated selection of cut-off thresholds in order to compute precision and recall on a test set that contains equal number of positive and negative examples of confusable drug name pairs. However, our own experience with systems for automatic detection of potential drug-name confusions suggests that the usual approach is to examine a fixed number of most similar candidates rather than all candidates with similarity above certain threshold. Moreover, the number of non-confusable pairs can be expected to greatly exceed the number of confusable pairs.","We present a different method of evaluating the accuracy of a measure. Starting from a set of confusable drug name pairs, we combinatorially induce a much larger set of negative examples. The recall is calculated against an on-line gold standard for each potentially confusable drug name considering only the top ","candidate names returned by a similarity measure. The recall values are then aggregated using the technique of macro-averaging (Salton, 1971).","We formulate a general framework for represent-ing word similarity measures based on","-grams, and propose a new measure of orthographic similarity called BI-SIM that combines the advantages of several known measures. Using our recall-based evaluation methodology, we show that this new measure performs better on a U.S. pharmacopeial gold standard than the measures identified as the most accurate by Lambert et al. (1999).","Some potential drug-name confusions can be attributed solely to high phonetic similarity. Consider the example of Xanax vs. Zantac—tw o brand names that the Physicians’ Desk Reference (PDR) warns may be “mistak en for each other ... lead[ing] Distance Similarity","Orthographic EDIT","-GRAM NED LCSR","Phonetic SOUNDEX ALINE EDITEX Table 1: Classification of word distance and similarity measures. to serious medication errors” (24th Ed., 2003). The phonetic transcription of the two names, [zænæks] and [zæntæk], reveals their sound-alike similarity that is not apparent in their orthographic form. For the detection of sound-alike confusion pairs, we apply the ALINE phonetic aligner (Kondrak, 2000), which estimates the similarity between two phonetically-transcribed words. We demonstrate that ALINE outperforms orthographic approaches on a test set containing sound-alike confusion pairs.","The next section describes several commonly-used measures of word similarity. After this, we present two new methods for identifying look-alike and sound-alike drug names. We then compare the effectiveness of various measures using our recall-based evaluation methodology on a U.S. pharmacopeial gold standard and on another test set containing sound-alike confusion pairs. We conclude with a discussion of our experimental results."]},{"title":"2 Background","paragraphs":["Drug-name matching refers to the process of string matching to rank similarity between drug names. There are two classes of string matching: orthographic and phonetic. For each of these, there are two methods of matching: distance and similarity. If two drug names are confusable, their distance should be small and their similarity should be large. Some examples of orthographic and phonetic algorithms for both distance- and similarity-based approaches are shown in Table 1.","In the remainder of this section, we describe a number of measures that have been applied to the problem of identifying confusable drug names. Specific examples of values obtained by the measures are provided in Table 2.","String-edit distance (Wagner and Fischer, 1974) (EDIT) (also known as Levenshtein distance) counts up the number of steps it takes to transform one string into another, where the cost of substitution is the same as the cost of insertion or dele-tion. A normalized edit distance (NED) is calculated by dividing the total edit cost by the length of the longer string.","The longest common subsequence ratio (Melamed, 1999) (LCSR) is computed by dividing Measure Zantac/ Zantac/ Xanax/","Xanax Contac Contac EDIT 3 2 4 NED 0.500 0.333 0.667 LCSR 0.500 0.667 0.333 BIGRAM 0.222 0.600 0.000 TRIGRAM-2B 0.000 0.333 0.000 SOUNDEX 3 1 3 EDITEX 5 2 7 ALINE 9.542 9.333 8.958 BI-SIM 0.417 0.583 0.250 TRI-SIM 0.333 0.500 0.167 PREFIX 0.000 0.000 0.000 Table 2: Examples of values returned by various measures. the length of the longest common subsequence by the length of the longer string. LCSR is closely related to normalized edit distance. If the cost of substitution is at least twice the cost of insertion/deletion and the strings are of equal length, LCSR is equivalent to the normalized edit distance.","In","-gram measures, the number of","-grams that are shared by two strings is doubled and then divided by the total number of","-grams in each string:","","","-grams  -grams   -grams","  -grams  where","-grams(x) is a multi-set of letter","-grams in",". This formula is often referred to as the Dice coefficient. A slight variation of this measure is obtained by adding extra symbols, such as spaces, before and/or after each string (Lambert et al., 1999). The modification is designed to increase sensitivity to the beginnings and endings of words. For example, TRIGRAM-2B is calculated by applying the Dice formula with","","after adding two spaces before each string. In this paper, we consider two specific variants: BIGRAM, which is the most basic formulation, and TRIGRAM-2B.2","SOUNDEX (Hall and Dowling, 1980) is an approximation to phonetic name matching. SOUNDEX transforms all but the first letter to numeric codes (see Table 3) and after removing zeroes truncates the resulting string to 4 characters. For the purposes of comparison, we implemented a SOUNDEX-based similarity measure that returns the edit distance between the corresponding codes.","EDITEX (Zobel and Dart, 1996) is another quasi-phonetic measure that combines edit distance with a letter-grouping scheme similar to SOUNDEX (Table 3). As in SOUNDEX, the codes are designed 2","TRIGRAM-2B was identified by Lambert et al. (1999) as particularly effective for identifying confusable drug name pairs. Code SOUNDEX EDITEX 0 a e h i o u w y a e i o u y 1 b f p v b p 2 c g j k q s x z c k q 3 d t d t 4 l l r 5 m n m n 6 r g j 7 f p v 8 s x z 9 c s z Table 3: Character conversion codes in SOUNDEX and EDITEX. to identify letters that have similar pronunciations, but the corresponding sets of letters are not disjoint. The edit distance between letters that belong to the same group is smaller than the edit distance between other letters. Additional rules are aimed at eliminat-ing silent and reduplicated letters."]},{"title":"3 Phonetic Similarity: ALINE","paragraphs":["The ALINE cognate matching algorithm (Kondrak, 2000) assigns a similarity score to pairs of phonetically-transcribed words on the basis of the decomposition of phonemes into elementary phonetic features. The algorithm was initially designed to identify and align cognates in vocabularies of related languages (e.g. colour and couleur). Nevertheless, thanks to its grounding in universal phonetic principles, the algorithm can be used for estimating the similarity of any pair of words, including drug names. Furthermore, unlike SOUNDEX and EDITEX, ALINE is completely language-independent.","The principal component of ALINE is a func-tion that calculates the similarity of two phonemes that are expressed in terms of about a dozen binary or multi-valued phonetic features (Place, Manner, Voice, etc.). Feature values are encoded as floatingpoint numbers in the range","",". For example, the feature Manner can take any of the following seven values: stop = 1.0, affricate = 0.9, fricative = 0.8, approximant = 0.6, high vowel = 0.4, mid vowel = 0.2, and low vowel = 0.0. The numerical values reflect the distances between vocal organs during speech production. The phonetic features are assigned salience weights that express their relative importance.","The overall similarity score and optimal alignment of two words—computed by a dynamic programming algorithm (Wagner and Fischer, 1974)— is the sum of individual similarity scores between pairs of phonemes. A constant insertion/deletion penalty is applied for each unaligned phoneme. An-other constant penalty is set to reduce relative importance of the vowel—as opposed to consonant— phoneme matches. The similarity value is normalized by the length of the longer word.","ALINE’s behavior is controlled by a number of parameters: the maximum phonemic score, the insertion/deletion penalty, the vowel penalty, and the feature salience weights. The parameters have default settings for the cognate matching task, but these settings may not be appropriate for drug-name matching. The settings can be optimized (tuned) on a training set that includes positive and negative examples of confusable name pairs."]},{"title":"4 Orthographic Similarity: BI-SIM","paragraphs":["An analysis of the reasons behind the unsatisfactory performance of commonly used measures led us to propose a new measure of orthographic similarity: BI-SIM.3","Below, we describe the inherent strengths and weaknesses of","-gram and subsequence-based approaches. Next, we present a new, generalized framework that characterizes a number of commonly used similarity measures. Following this, we describe the parametric settings for BI-SIM—a specificinstantiation of this generalized framework. 4.1 Problems with Commonly Used Measures The Dice coefficient computed for bigrams (BIGRAM) is an example of a measure that is demonstrably inappropriate for estimating word similarity. Because it is based exclusively on complete bigrams, it often fails to discover any similarity between words that look very much alike. For example, it returns zero on the pair Verelan/Virilon. In addition, it violates a desirable requirement of any similarity measure that the maximum similarity of 1 should only result when comparing identical words. In particular, non-identical pairs4","like Xanex/Nexan—where all bigrams are shared—are assigned a similarity value of 1. Moreover, it sometimes associates bigrams that occur in radically different word positions, as in the pair Voltaren/Tramadol. Finally, the initial segment, which is arguably the most important in determining drug-name confusability,5","is actually given a lower weight than other segments because it participates in only one bigram. It is therefore surprising that BIGRAM has been such a popular choice of measure for computing word similarity.","LCSR is more appropriate for identifying potential drug-name confusability because it does not rely 3 BI-SIM was developed before we conducted the experi-","ments described in Section 6. 4 This observation is due to Ukkonen (1992). 5 74.2% of the confusable pairs in the pharmacopeial gold","standard (Section 6) have identical initial segments. on (frequently imprecise) bigram matching. However, LCSR is weak in its tendency to posit nonintuitive links, such as the ones between segments in Benadryl/Cardura. The fact that it returns the same value for both Amaryl/Amikin and Amaryl/Altoce can be attributed to lack of context sensitivity.","4.2 A Generalized -gram Measure Although it may not be immediately apparent, LCSR can be viewed as a variant of the","-gram approach. If","is set to 1, the Dice coefficient formula returns the number of shared letters divided by the average length of two strings. Let us call this measure UNIGRAM. The main difference between LCSR and UNIGRAM is that the former obeys the no-crossing-links constraint, which stipulates that the matched unigrams must form a subsequence of both of the compared strings, whereas the latter disregards the order of unigrams. E.g., for pat/tap, LCSR returns 0.33 because the length of the longest common subsequence is 1, while UNIGRAM returns 1.0 because all letters are shared. The other, minor difference is that the denominator of LCSR is the length of the longer string, as opposed to the average length of two strings in UNIGRAM. (In fact, LCSR is sometimes definedwith the average length in the denominator.)","We define a generalized measure based on","- grams with the following parameters:","1. The value of .","2. The presence or absence of the no-crossing-links constraint.","3. The number of segments appended to the beginning and the end of the strings.","4. The length normalization factor: either the maximum or the average length of the strings.","A number of commonly used similarity measures","can be expressed in the above framework. The com-","bination of ","","with the no-crossing-links con-","straint produces LCSR. By selecting","","","and the average normalization factor, we obtain the BIGRAM measure. Thirteen out of twenty two measures tested by Lambert et al. (1999) are variants that combine either","","","or","","","with various","lengths of appended segments.","So far, we have assumed that there are only two","possible values of","-gram similarity: identical or","non-identical. This need not be the case. Obviously,","some non-identical","-grams are more similar than","others. We can define a similarity scale for two","-","grams as the number of identical segments in the","corresponding positions divided by :","","","","","   ","","","","","","              where ","","","","","","returns 1 if","and","are identical, and 0 otherwise. The scale distinguishes","levels of similarity, including 1 for identical bigrams, and 0 for completely distinct bigrams.6","The notion of similarity scale between","-grams requires clarificationin the case of","-grams partially composed of segments appended to the beginning or end of strings. Normally, extra affixes are composed of one or more copies of a unique special symbol, such as space, that does not belong to the string alphabet. We define an alphabet of special symbols that contains a unique symbol for each of the symbols in the original string alphabet. The extra affixes are assumed to contain copies of special symbols that correspond to the initial symbol of the string. In this way, the similarity between pairs of","-grams in which one or both of the","-grams overlap with an extra affixis guaranteed to be either 0 or 1. 4.3 BI-SIM We propose a new measure of orthographic similarity, called BI-SIM, that aims at combining the advantages of the context inherent in bigrams, the precision of unigrams, and the strength of the nocrossing-links constraint. BI-SIM belongs to the class of","-gram measures definedabove. Its parameters are:","","",", the no-crossing-links constraint","enforced, a single segment appended to the begin-","ning of the string, normalization by the length of the","longer string, and multi-valued","-gram similarity. The rationale behind the specificsettings is as fol-","lows.","","","is a minimum value that provides context for matching segments within a string. The nocrossing-links constraint guarantees the sequentiality of segment matches. The segment added to the beginning increases the importance of the match of initial segment. The normalization method favors associations between words of similar length. Finally, the refined","-gram similarity scale increases the resolution of the measure.","BI-SIM is definedby the following recurrence:   ",""," ","","max     ","","                  ","       ","","","","   ","","","","6","The scale could be further refinedto include more levels of similarity. For example, bigrams that are frequently confused because of their typographic or cursive shape, such as en/im, could be assigned a similarity value that corresponds to the frequency of their confusions.","where","refers to the","-gram similarity scale defined","in Section 4.2, and ","and","","are the appended seg-","ments. Furthermore,   ",""," ","is defined to be","if"," ","","or","","",". The recurrence relation exhibits strong similarity to the relation for computing the longest common subsequence except that the subsequence is composed of bigrams rather than unigrams, and the bigrams are weighted according to their similarity. Assuming that the segments appended to the beginning of each string are chosen according to the rule specified in Section 4.2, the returned value of BI-SIM always falls in the interval","",". In particular, it returns 1 if and only if the strings are identical, and 0 if and only if the strings have no segments in common.","BI-SIM can be seen as a generalization of LCSR: the setting of","","","reduces BI-SIM to LCSR (which could also be called UNI-SIM). On the other hand, the setting of","","","yields TRI-SIM. TRI-SIM requires two extra symbols at the beginning of the string."]},{"title":"5 Evaluation Methodology","paragraphs":["We designed a new method for evaluating the accuracy of a measure. For each drug name, we sort all the other drug names in the test set in order of decreasing value of similarity. We calculate the recall by dividing the number of true positives among the top ","names by the total number of true positives for this particular drug name, i.e., the frac-tion of the confusable names that are discovered by taking the top ","similar names. At the end we apply an information-retrieval technique called macro-averaging (Salton, 1971) which averages the recall values across all drug names in the test set.7","Because there is a trade-off between recall and the ","threshold, it is important to measure the recall at different values of ",". Table 4 shows the top 8 names that are most similar to Toradol according to the BI-SIM similarity measure. A ‘+’/‘–’ mark indicates whether the pair is a true confusion pair. The pairs are listed in rank order, according to the score assigned by the indicated algorithm. Names that return the same similarity value are listed in the reverse lexicographic order. Since the test set contains four drug names that have been identifiedas confusable with Toradol (Tramadol, Torecan, Tegretol, and Inderal), the recall values are"," "," for"," "," , and","for    for","  ",".","7","We could have also chosen to micro-average the recall values by dividing the total number of true positives discovered among the top","candidates by the total number of true positives in the test set. The choice of macro-averaging over micro-averaging does not affect the relative ordering of similarity measures implied by our results.","Name Score +/– Recall 1. Tramadol 0.6875 + 0.25 2. Tobradex 0.6250 – 0.25 3. Torecan 0.5714 + 0.50 4. Stadol 0.5714 – 0.50 5. Torsemide 0.5000 – 0.50 6. Theraflu 0.5000 – 0.50 7. Tegretol 0.5000 + 0.75 8. Taxol 0.5000 – 0.75 Table 4: Top 8 names that are most similar to Toradol according to the BI-SIM similarity measure, and the corresponding recall values."]},{"title":"6 Experiments and Results","paragraphs":["We conducted two experim