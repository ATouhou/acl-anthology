{"sections":[{"title":"","paragraphs":["Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 345–352 Manchester, August 2008"]},{"title":"Modeling Chinese Documents with Topical Word-Character Models Wei Hu","paragraphs":["1"]},{"title":"Nobuyuki Shimizu","paragraphs":["2  1"]},{"title":"Department of Computer Science Shanghai Jiao Tong University Shanghai, China 200240 {no_bit,hysheng} @sjtu.edu.cn Hiroshi Nakagawa","paragraphs":["2 "]},{"title":"Huanye Sheng","paragraphs":["1  2"]},{"title":"Information Technology Center The University of Tokyo Tokyo, Japan 113-0033 {shimizu, nakagawa} @r.dl.itc.u-tokyo.ac.jp Abstract","paragraphs":["As Chinese text is written without word boundaries, effectively recognizing Chinese words is like recognizing collocations in English, substituting characters for words and words for collocations. However, existing topical models that in-volve collocations have a common limitation. Instead of directly assigning a topic to a collocation, they take the topic of a word within the collocation as the topic of the whole collocation. This is unsatisfactory for topical modeling of Chinese documents. Thus, we propose a topical word-character model (TWC), which allows two distinct types of topics: word topic and character topic. We evaluated TWC both qualitatively and quantitatively to show that it is a powerful and a promising topic model."]},{"title":"1 Introduction","paragraphs":["Topic models (Blei et al., 2003; Griffiths & Steyvers 2004, 2007) are a class of statistical models in which documents are expressed as mixtures of topics, where a topic is a probability distribution over words. A topic model is a generative model for documents: it specifies a probabilistic procedure for generating documents. To make a new document, we choose a distribution over topics. Then, for each word in this document, we randomly select a topic from the distribution, and draw a word from the topic. Once we have a topic model, we can invert the generating process, inferring the set of topics that was responsible for generating a collection of docu-  © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. ments.","Although most topic models treat a document as a bag-of-words, the assumption has obvious shortcomings. Suppose there are many documents about art and musicals in New York. Then, we find a topic represented by words such as “art”, “musical”, and “New”, instead of getting “New York”. The bag-of-words assumption makes the topic model split a collocation—a phrase with meaning beyond the individual words—into individual words that have a different meaning. One example of a collocation is the phrase “white house”. In politics, it carries a special meaning beyond a house that is white, whereas “yellow house” does not.","While it is reasonable for English, the equivalent bag-of-characters assumption is especially troublesome for modeling Chinese documents, where almost all basic vocabularies are the equivalents of English collocations. In Chinese, some of the most commonly used couple-of-thousand characters are combined to make up a word, and no word boundary is given in the text. Effectively, Chinese words are like collocations in English. The difficulty is that there are over-whelmingly more of them, enough to render a bag-of-character assumption unreasonable for Chinese. Therefore, a topical model for Chinese should be capable of detecting the boundary between two words, as well as assigning a topic to each word.","While topic models for Chinese documents bear some similarity to collocation models in English, existing topical collocation discovery models, such as the LDA (latent Dirichelt allocation) Collocation model (LDACOL) (Griffiths et al., 2007) and the topical N-gram model (TNG) (Wang et al., 2007), do not directly assign a topic to a collocation. These models find the boundaries of phrases and assign a topic to each word. The problem is in the next step—the topic of the collocation is exactly the same as one of the words. This is like saying that the topic of “white 345 house” is the same as either that of “white” or “house”. We propose a new topical model, the topical word-character model (TWC), which aims to overcome these limitations.","We evaluated the model both quantitatively and qualitatively. For the quantitative analysis, we compared the performance of TWC and TNG using a standard measure perplexity. For the qualitative analysis, we evaluated TWC’s ability to discover Chinese words and assign topics in comparison with TNG.","The rest of the paper is organized as follows. Section 2 reviews topic models that aim to in-clude collocations explicitly in the model and analyzes their limitations. Section 3 presents our new model TWC. Section 4 gives details of our consideration on inference for TWC. Section 5 presents our qualitative and quantitative experiments. Section 6 concludes with a summary and briefly mentions future work."]},{"title":"2 Topic Models for Collocation Discovery","paragraphs":["Since Chinese word discovery is similar to English collocation discovery, we first review some related topic models for collocation discovery.","Although collocation discovery has long been studied, most methods are based on frequency or variance. LDACOL is an attempt to model collocations in a topical scheme. Starting from the LDA topic model, LDACOL introduces special random variables xG",". Variable xi = 1 implies that the corresponding word wi and previous word wi-1 belong to the same phrase, while xi = 0 implies otherwise. Thus, LDACOL can decide the length of a phrase dynamically.","TNG is a powerful generalization of LDACOL. Its graphical model is shown in Figure 1.  Figure 1: Topical n-gram model. The model is defined in terms of three sets of variables: a sequence of words wG , a sequence of topics zG , and a sequence of indicators xG",". TNG","assumes the following generative process for","documents. 1. For each document d, draw θd ~","Dirichlet(α). 2. For each topic z, draw φz ~ Dirichlet(β). 3. For each topic z and each word w, draw σzw","~ Dirichlet(δ). 4. For each topic z and each word w, draw ψzw","~ Beta(γ). 5. For each word wd,,i in document d:","(a) draw xd,,i ~ Bernoulli( d,i 1 d,i 1z,wψ −−), (b) draw zd,,i ~ Discrete(θd), (c) draw wd,,i ~ Discrete( d,izφ ) if xd,,i=0, draw wd,,i ~ Discrete( d,i d,i 1z,wσ − ) if xd,,i=1, where α, β, δ are Dirichlet priors and γ is a Beta prior, zd,i denotes the ith","topic assignment in document d, wd,i denotes the ith","word in document d, and xd,i denotes the indicator between wd,i-1 and wd,i. Note that the variable xd,i = 1 implies that word wd,i-1 and its neighbor wd,i belong to the same phrase, while xd,i = 0 implies otherwise. However, the topics assigned to them (zd,i-1 and zd,i) are not required to be identical to each other. To decide the topic of a phrase, we can simply take the first (or last) word’s topic or the most common topic in the phrase. The authors of TNG prefer to choose the last word’s topic as the phrase topic because the last noun in a collocation is usually the “head noun” in English.","However, this simple strategy may be ineffective when we apply TNG to Chinese documents. The topics of “ 比赛” (game) and “ 锦标赛” (tournament) should be represented by their last characters while those of “农民” (farmer) and “ 农业” (agriculture) should be represented by their first characters. And occasionally, the topic of a Chinese word is not identical to any topic of its component characters. For example “蓝牙” (Bluetooth) is neither a color nor a tooth.","To overcome the limitation of TNG, we must discard its underlying assumption: that the topic of a whole word is the same as the topic of at least one of its components."]},{"title":"3 Modeling Word Topic and Character Topic","paragraphs":["This section describes our topical word-character model (TWC), which models two distinct types of topics: word topic and character topic.  σ δψ γ  φ β θα  ••• ••• ••• ••• ••• ••• zi+1 zi+2 zi zi-1 xi+2 xi+3xi-1xi xi-1 wi+1 wi+2 wi wi-1 346 3.1 Word topic and character topic","To solve the problem associated with the “蓝 牙” (Bluetooth) example, we need to distinguish between the topics of characters and words. Therefore, we introduce a new type of topic for words in addition to the topics assigned to characters. When generating a Chinese character, we first draw a word topic and then choose a character topic. A schematic description of this model is shown in Figure 2."," Figure 2: Schematic description of modeling Chinese documents with character and word topics. Here, we use random variables zG and tG","to denote word and character topics, respectively. Note that the word topic and character topics have a hierarchical tree-like structure (upper layer in Figure 2), whereas character topics and characters form a hidden Markov model (HMM) (lower layer in Figure 2). 3.2 Topical word-character model (TWC)","There are some indicators in the upper-right corner of each character topic in Figure 2. They help us to tell whether the current character be-longs to the same word as the previous one. Now the question left is how to probabilistically draw these indicators, i.e., how to determine the length of the Markov chain.","There are two ways to set the values of the indicators. One is similar to that applied in the hidden semi-Markov model (HSMM), which generates the duration of a segment from the state. Accordingly, we could first choose the length of a word from the distribution associated with the word topic and then assign 0 or 1 to each indicator. The other method is to directly draw indicators from the distribution associated with the previous character and topic, just as LDACOL and TNG do. The difference between these two methods is that the former determines the length of a word in advance while the latter increases the length dynamically.","We prefer the second choice because it takes into consideration a lot of context information. In fact, our experimental results indicate that it has better performance.","The formal definition of our model with word and character topics is as follows.  Figure 3: Topical word-character model.","TWC has four sets of variables: a sequence of characters cG , a sequence of character topics tG , a sequence of word topics zG , and a sequence of indicators xG",". A document is generated via the","following procedure. 1. For each document d, draw θd ~ Dirichlet(α); 2. For each word topic z, draw φz ~ Dirichlet(β); 3. For each word topic z and each character topic t, draw σzt ~ Dirichlet(δ); 4. For each word topic z, each character topic t and each character c, draw ψztc ~ Beta(γ); 5. For each character topic t, draw ηt ~ Dirichlet(ξ); 6. For each character cd,,i in document d: (a) draw xd,,i ~ Bernoulli( −− −d,i 1 d,i 1 d,i 1z,t,c"]},{"title":"ψ","paragraphs":["); (b) draw zd,,i ~ Discrete (θd) if xd,,i=0;","zd,,i= zd,,i-1 if xd,,i=1; (c) draw td,,i ~ Discrete( d,iz"]},{"title":"φ","paragraphs":[") if xd,,i=0; draw td,,i ~ Discrete( −d,i d,i 1z,t"]},{"title":"σ","paragraphs":[") if xd,,i=1; (d) draw cd,,i ~ Discrete( d,it"]},{"title":"η","paragraphs":["). Here, α, β, δ, η are Dirichlet priors and γ is a Beta prior, zd,i denotes the ith","word topic assignment in document d, td,i denotes the ith","character topic assignment in document d, cd,i denotes the ith","character in document d, and xd,i denotes the indicator between cd,i-1 and cd,i.","Note that compared with the schematic model in Figure 2, each character has its corresponding","Character topic Word topic  θ   z2 z1 t13 t21 t12 t11 c13 c21 c12 c11   t22 c22 1 1 0 1 0 σ δη ξ   φβ θα ψ γ  ••• ••• ••• ••• ••• ••• ••• •••  zi+1 zi+2 zizi-1 ti+1 ti+2 titi-1 ci+1 ci+2 cici-1 xi+2 xi+3 xi-1xixi-1 347 word topic in the TWC model. This is because we cannot decide how many words there will be in a document and how many characters there will be in a certain word in advance. In other words, the structure of the ideal model is not fixed. Therefore, we duplicate word topic variables for each character."]},{"title":"4 Inference with TWC","paragraphs":["Many approximate inference techniques such as variational methods, expectation propagation, and Gibbs sampling can be applied to graphical models. We use Gibbs sampling to perform our Bayesian inference in TWC.","Gibbs sampling is a simple and widely applicable Markov chain Monte Carlo (MCMC) algorithm. In a traditional procedure, variables are sequentially sampled from their distributions conditioned on all other variables in the model.","An extension of the basic approach is to choose blocks of variables first and then sample jointly from the variables in each block in turn, conditioned on the remaining variables; this is called blocking Gibbs sampling.","When sampling for TWC, we separate variables into three types of blocks in the following manner (as shown in Figure 4).","1. character variables ti","2. indicators xi, whose value is 1 after n iterations","3. word topics zi, zi+1, ..., zi+l-1 and indicator xi, satisfying xi=xi+l=1 and xj=0 (j from i to i+l-1) after n iterations"," Figure 4: Illustration of partition in a certain it-eration.","Note that variables ,,,θφσψG GGG and ηG","are not sampled. This is because we can integrate them out according to their conjugate priors. We only need to sample variables zG , xG , and tG",".","Before discussing the inference of conditional probabilities, let us analyze our partition strategy in detail. We will explain the reasons for (1) sampling zi, zi+1, ..., zi+l-1 together and (2) sampling zG and xi together 1. Why do we sample zi, zi+1, ..., zi+l-1 together? Assume that we draw zi, zi+1, ..., zi+l-1 one by one, and it is now time to sample zi+1 according to the conditional probability","()(|,,,,,,,,)i1 i1Pz j z xt cαβδγξ+−+= GG GG , where ()i1z− +G denotes a word topic except i1z + . Recall step 6-b in the generative TWC model: it says “If xd,,i=1, then zd,,i= zd,,i-1”, which implies","(|, )( )i1 i i1 i1 iPz z x 1 I z z++ += ==. As this probability is a factorial of the target probability, it follows that","()(|,,,,,,,,)i1 i1Pz j z xt cαβδγξ 0+−+= =GG GG  for all ijz≠ . In other words, zi+1 should be equal to zi and not change during sampling.","It seems that step 6-b in the generative model causes the problem. But supposing that we do not set zi+1 to zi; it is still more reasonable to sample zG","together. According to our partition principle, xi, xi+1, ..., xi+l-1, xi+l is a continuous indicator sequence whose head and tail are both 0 and the rest are 1, which implies that character string ci, ci+1, ..., ci+l-1 forms a word and has the same word topic. Recall the schematic model in Figure 2: the word topic and character topics have a tree-like structure and each word has only one word topic node. We add some auxiliary duplicates just because the ideal model is not fixed. Therefore, it is natural to sample the word topic together with its duplicates. 2. Why do we sample zG and xi together? Let us consider the probability of converting xi from 0 to 1 in the current sampling iteration. Assume that the number of word topics is 3, zi-1=2, and","(... | )/( ),","( | ... ) / ( ), i il1 i i i il1 Pz z j x 0 1 3 1 j 3 Px k z z 2 1 2 0 k 1 +− +−","= ==== ≤≤","=== == ≤≤","where other variables and priors are omitted. If","we first sample zG","and next sample xi, then the probability of drawing 1 for xi is 1/6, according to the multiplication principle. If we sample zG"," and xi together, the probability of drawing 1 for xi is ( ... , )i il1 iPz z 2x 1+−= ===. Since ( ... , ) ( ... , ) ( ... , ) ( ) 13 i il1 i k0j1 3 i il1 i j1 i il1 i i1 1Pzzjxk Pz z jx 0 zz2x1z2 +− == +− = +− − = == ===== ===== + == = = ="]},{"title":"∑∑ ∑","paragraphs":["and ( ... , )i il1 iPz z 2x 1+−= ===  1 0 1 1 0  ••• ••• •••ti ti+2 ti-1 ti-2 ••• zi zi+1 zi-1 zi-2 zi+2 ti+2 ••• ••• 0  348 ( ... , ) ( ... , ) ( ) i il1 i i il1 i Pz z 2x 0 Pz z jx 0 1 j 3 +− +− === == === == ≤≤, we get","( ... , ) / /i il1 iPz z 2x 1 1 4 1 6+−== = = = > . In conclusion, the model is more likely to form long words, if we sample zG","and xi together. This is preferred because both TNG and TWC tend to produce shorter words than we would like.","For each type of block, we need to work out the corresponding conditional probability. ,, ,, ,, , , ,(: ) , (|,,,,,,,,) (|,,,,,,,,) (, |,,,,,,,,) di d i di d i di di 1 di l 1 di diil1 di Pt s z xt cαβδγξ Px k zx t cαβδγξ Pz z z jx k zxtcαβδγξP − − ++− −+− − = = === = =","GGGG GGGG","\" GGGG  where ,dit −G","denotes the character topic assignments except td,i, ,dix −G denotes the indicators except xd,i, and ,(: )diil1z −+−G denotes the word topic assignments except ,djzG (j from i to i+l-1). Details of the derivation of these conditional probabilities are provided in Appendix A.1."]},{"title":"5 Experiments","paragraphs":["In this section, we discuss our evaluation of TWC in Chinese document modeling and Chinese word and topic discovery. 5.1 Modeling documents","To evaluate the generalization performance of our model, we trained both TWC and TNG on a Chinese corpus and computed the perplexity of the held-out test set. Perplexity, which indicates the uncertainty in predicting a single character, is a standard measure of performance for statistical models of natural language. A lower perplexity score indicates better generalization performance.","Formally, the perplexities for TWC and TNG are defined as follows.","()","̂ˆ̂ˆ̂log ( | , , , , ) exp{ } TWC test D d d1 D d d1 perplexity D pc θφσψη N = = =−"]},{"title":"∑ ∑","paragraphs":["GGGGGG , where Dtest is the testing data, D is the number of documents in Dtest, Nd is the number of characters in document d, ,d̂zθ is simply set to 1/Z (Z is number of word topics), and ˆ̂ˆ̂,,,φσψηGGGG","are posterior estimates provided by applying TWC to training data. Details of the parameter estimation","for TWC are provided in Appendix A.2. ()TNG testperplexity D","̂ˆ̂l̂og ( | , , , ) exp{ } D d d1 D d d1 pc θφσψ N = = =−"]},{"title":"∑ ∑","paragraphs":["GG GGG , where Dtest, D, and Nd are the same as defined for the TWC perplexity, ,d̂zθ is simply set to 1/T (T is number of topics), and ˆ̂,̂,φσψG GG are posterior estimates provided by applying TNG to training data.","Now, the remaining question is how to work out the likelihood function in the definition of perplexity. The likelihood function can be obtained by marginalizing latent variables, but the time complexity is exponential. Therefore, we propose an efficient method of computing the likelihood that is similar to the forward algorithm for an HMM. Details of the forward approach to computing likelihood for TWC and TNG are provided in Appendix B.","In our experiments, we used a subset of Chinese corpus LDC2005T14. The dataset contains 6000 documents with 4476 unique characters and 2,454,616 characters. We evaluated both TWC and TNG using 10-fold cross validation. In each experiment, both models ran for 500 iterations on 90% of the data and computed the complexity for the remaining 10% of the data.","TWC used fixed Dirichlet (Beta) priors α=1, β=1, δ=1, γ=0.1 and ξ=0.01 while TNG used α=1, β=0.01, δ=0.01, and γ=0.1. 0 50 100 150 200 250 0 20406080100","No. of topics (TNG) No. of charcter topics * no. of word topics Pe r p l e x i t y TNG TWC Figure 5: Perplexity results with LDC2005T14 corpora for TNG and TWC.","The results of these computations are shown in Figure 5. Note that the abscissa for TWC is the number of word topics (Z) multiplied by the (TWC) 349 number of character topics (T), while the abscissa for TNG is the number of topics (T). They both represent the number of partitions into which the model classifies characters.","Chance performance results in a perplexity equal to the number of unique characters, which was 4476 in our experiments. Therefore, both TWC and TNG are competitive models. And the lower curve shows that TWC is much better than TNG.","We also found that both perplexity curves increased with the number of partitions. In other words, both models suffer from overfitting issues. This is because the documents in a test set are very likely to contain words that do not appear in any of the documents in the training set. Such words will have a very low probability, which is inversely proportional to the number of partitions. Therefore, the perplexity of TWC increased from 7.3513 (Z*T=2*2) to 8.9953 (Z*T=10*10), while that of TNG increased from 20.3789 (T=5) to 193.6065 (T=100). 5.2 Chinese word and topic discovery","As shown in the previous subsection, TWC is a competitive method for topically modeling Chinese documents. Next, we show its ability to extract Chinese words and topics in comparison with TNG.","In our qualitative experiments, the task of Chinese word and topic discovery was addressed as a supervised learning problem, where a set of words with their topical assignments was given as a seed set. Each seed can be viewed as some constraints imposed on the TWC and TNG models. For example, suppose that “教师” (teacher) together with its assignment “education” is a seed. This assumption implies that the indicator between characters “教” and “师” is 1 and that the (word) topic for each character is “education”.","We make use of such constraints in a simple but effective way. In each sampling iteration, we first sample all variables as usual and then reset observed variables according to the constraints.","We used 8000 Chinese documents in the Chinese Gigaword corpus (LDC2005T14) provided by the Linguistic Data Consortium for our experiments. The dataset contains 4651 unique characters and 3,295,810 characters.","The number of word topics in TWC, the number of character topics in TWC, and the number of topics in TNG were all set to 15. Furthermore, 16 seeds scattered in 4 distinct topics were given, as listed in Table 1 column “seed”. Dirichlet (Beta) priors were set to the same values as described in the previous subsection.","Word and topic assignments were extracted after running 300 Gibbs sampling iterations on the training corpus together with the seed set. For the TNG model, we took the first character’s topic as the topic of the word. We omitted one-character words and ranked the extracted words using the following formula","() () i 15 i i1 occ W occ W ="]},{"title":"∑","paragraphs":[", where occi(W) represents how many words were assigned to (word) topic i. The top-50 extracted words are presented in Table 1.","We find found that both TWC and TNG could assemble many common words used in corresponding topics. And the TWC model had advantages over the TNG model in the following three respects.","First, TNG drew more words related to the seeds. In Table 1, highly related words are marked in pink (underline) and partly related words are marked in blue (italic). It is clear that the TWC column is more colorful than the TNG column.","Secondly, we found that many words extracted by TNG had the same prefix. For example, consider the topic “agriculture”: there are 14 words marked with superscript 1 in Table 1. They all have the prefix “农”. This is because we took the first character’s topic as the topic of the word. Although this strategy is beneficial in some cases, such as for words with prefix “农”, it is detrimental in other cases. For example, “ 甘蔗” (sugar cane) and “甘肃” (Gansu) have the same prefix and topic assignment, but the latter is a name of a province in China and is not related to agriculture. Similarly, even though the character string “伊外” does not form a Chinese word, this string “伊外” and “伊朗” (Iran) are classified in the same cluster. Compared with TNG, TWC can also extract words whose topics are identical to the topic of any character. For example, the topics “自由泳” (freestyle swimming), “混合泳” (medley swimming), and “ 蝶泳” (butterfly stroke) depend on their suffixes.","Thirdly, although TNG stands for “topical n-gram model”, it infrequently draws words containing more than two characters. On the other hand, the TWC model extracts many n-character words, such as “美国总统布什” (president of United States, George Bush), “ 个人混合泳” (individual medley), and “百分之四” (four per-350 Seeds TNG TWC 足球(football) 球员(player) 比赛(match)","冠军 (championship) 撑竿,乒乓,比赛,球员,足球,冠军,选手,摔跤,世 乒,淘汰,训练,纪录,选拔,世界,宇宙,选择,分钟, 朝鲜,今晚,威廉,分别,邀请,分之,世纪,分裂,辽 宁,分歧,姑娘,纪念,香港,今天,结果,公斤,结束, 今年,北京,公里,分析,威胁,毛腿,分配,开幕,沈 阳,分子,开始,记者,负责,蒙古,今后,保持 蝶泳,标赛 2",",上届,自由泳,混合泳,比分,总分,冠军,接 力,单打,蛙泳,比赛,赵剑,大师,女子,第一名,速滑,本 届,选手,苏联队,冰球,亚军,游泳,国队 2",",决赛,第三名, 两枚,第七届,国选手 2",",个人混合泳,全能,金牌,男子,战 胜,谢军,夺得,银牌,两项,名列,公斤级,冬运会,训练,苏 联,美分,领先,获得,成绩,目的,运动员,第三 粮食 (foodstuff) 农村 (country) 农民 (farmer) 水稻 (paddies)","粮食,农贸 1 ,水稻,农林 1",",橄榄,农副 1",",农药 1",",农户","1",",蘑菇,农膜 1 ,农村 1",",灌溉,农牧 1",",农闲 1",",蔬菜,潍","坊,农业 1",",农机,农奴 1",",玫瑰,农民 1",",农垦 1",",柑桔,农","田 1",",挂钩,鞠躬,甘蔗,甘薯,覆盖,甘肃,笼罩,帐篷,","土壤,偏僻,灿烂,左右,拉玛,拉萨,公斤,帮助,丝","绸,公路,群众,公顷,公里,百姓,沙漠,蒙古,积累,","培育","农村,推广,农民,粮食,水稻,苜蓿,秸秆,流通,农户,农","产,乡镇,社会化,土地,丰收,灌溉,妇女,农业,增产,责任","制,基层,万只 2 ,全省,亿公斤 2",",试验,百分之八,多万 2",",","反映,科技,各地,当地,多公斤 2",",集体,服务,承包,公斤,","负担,配套,万公斤 2",",生产,投入,全县,亿亩 2",",万户 2",",生","活,百分之四,生育,销售,总产,人均,年产 学校 (school) 教师 (teacher) 学生 (student) 教育 (education) 叮嘱,学生,教训,教徒,遐迩,孤寡,学校,胳膊,腼 腆,教界,喇叭,教材,学雷锋,荟萃,教授,学儒,慷 慨,教师,教练,蜘蛛,钥匙,凛冽,骄傲,教育,歹徒, 巾帼,瞻仰,残疾,雷锋,学谦,雷洁,裕禄,烹饪,姑 娘,残废,舞蹈,兄弟,旗帜,儿童,精彩,学习,遗址, 兢兢,呼唤,纪念,李瑞,群岛,记载,精锐,精神 学校,学生,教育,教师,国防,民族,宣传,毕业,学习,经 费,培养,职工,纪律,知识,家庭,重视,实施,教材,事业, 培训,社会主义,文化,进一步,工作,只有,保证,继续,活 动,任务,推动,小标题,帮助,通过,通讯,思想,加强,研 究,记者,考试,开始,要求,先后,期间,开展,方面,问题, 精神,技术,地区,成绩 战争 (war) 士兵 (solider) 将军 (general) 武器 (weapon) 伊外,伊朗,朝圣,伊斯,朝柱,巴勒,士兵,拉维,巴 基,威尔,拉曼,摧毁,伊始,伊利,朝觐,巴嫩,摧残, 伊奇,轰炸,巴塞,伊格,海盗,海夫,巴格,巴黎,巴 乔,轰鸣,巴西,轰击,武器,伊沙,拉彻,避难,拉法, 葡萄,将军,战壕,伊拉,威者,战舰,战双,拉脱,伊 军,战飞,拉姆,伊拉克,拉瓜,战争,拉底,伊拉姆 战争,将军,武器,士兵,钱其琛,轰炸,导弹,撤军,美军,美 国总统布什,伊军,讨论海湾,今天进入第 2",",对海湾 2",",科 威特,综合报道,海湾,危机,多国部队,爆发,新闻分析,死 亡,地面,结束后,之一,摧毁,综述,结束,爆发以来,战舰, 伊拉克,号决议 2",",表示,万桶,提供,造成,宣布,美国,包 括,问题,目前,同时,今年,公里,去年,其中,美元,亿元, 万元,万吨 Table 1: Top-50 words extracted by TWC and TNG. cent). This is partly due to our sampling strategy, discussed in subsection 4.1, which increases the probability of forming long words.","We also found that some extracted character strings were very close to real Chinese words. For instance, “标赛” is a substring of “锦标赛” (tournament); “国选手” is a suffix of “中国选 手 ” (Chinese player), “ 美国选手” (American player), and “法国选手” (French player); and “万公斤” is a substring of “十万公斤” (100 thousand kilograms) and “五万公斤” (50 thousand kilograms). (Such substrings are marked with superscript 2 in Table 1.) We believe that this result occurred because the training corpus was not large enough and that TWC will achieve better performance with a large dataset."]},{"title":"6 Conclusion and Future Work","paragraphs":["In this paper, we presented a topical word-character (TWC) model, which models two distinct types of topics: word topic and character topic. The experimental results show that TWC is a powerful approach to modeling Chinese documents according to the standard evaluation measure of perplexity. We also demonstrated TWC’s ability to detect words and assign topics.","Since TWC is a straightforward improvement that removes the limitations of existing topical collocation models, we expect that its application to English collocation will also result in higher performance."]},{"title":"Appendix A.1 Gibbs Sampling Derivation for TWC","paragraphs":["Symbols used here are defined as follows.","C is the number of unique characters, T is the number of character topics, and Z is the number of word topics.","Nd denotes the number of characters in a document d.","()I ⋅ is an indicator function, taking the value 1 when its argument is true, and 0 otherwise.","qd,z,0 represents how words are assigned to topic z in document d; pz,t,c,k represents how many times an indicator is k given the previous character c, the previous character topic t, and the previous word topic z; nz,t,0 represents how many times a character topic is t given a word topic z and the corresponding indicator 0; mz,v,t,1 represents how many times a character topic is t given a word topic z, the previous character topic v, and the corresponding indicator 1; and rt,c represents how many times character c is assigned to character topic t. ' '","',","' ' , '","',","' ,,","'','','','' '''","', ', ' ', ' , , ''' '' ' (|,,,,,,,,) (|) ( | , ,) (|) (| ) ( d d","di","1 d i 1","d","i","1","di d i NDD ddididi1d","d1 d1i1","NZTC D","ztc di z t c z1t1c1 d1i1 z Pt s zxt cαβδγξ P θα Pz x z θ dθ P ψγ Px ψ dψ P φ","−− − − − === === == = ∝⋅ ⋅× ⋅⋅ ×"]},{"title":"∏∏∏∫ ∏∏∏ ∏∏∫","paragraphs":["GG GG GGG G GG","G '","',","'', ' ', ' ', '","''' ''|) ( |) ( | , , d di NZZT D","zt di di z z1 z1t1 d1i1β P σδ Pt x φ === ==⋅⋅"]},{"title":"∏∏∏ ∏∏∫∫","paragraphs":["GG  351 '","', ' ' , '","',","' ', ', ' , '","',","',","' ' , ' , ' , ,, ,,",",'',' ''' '' ''' ' ' )(|)(|) Γ() () ()","Γ() Γ() Γ() d","di di 1","di zt c x","zt","c z t c di 1","zs c","di d i NTD","zt t di t","t1 d1i1","ZTC 2 2","px γ 1x","2 z1t1c1 x1 x1 x C t σ dφ dσ P ηξ Pc η dη 2γ ψψ γ Cξ ψ dψ ξ − + === − === = = ⋅⋅× ⋅ ⋅ ∝⋅⋅⋅ ⋅×"]},{"title":"∏∏∏∫ ∏∏∏ ∏ ∏∫","paragraphs":["GG GGGG","G ',","'",",","'' ', ' ,",", ' , ', ', ,, '' ''' '' ''","'' ' '' '' ', ' ', '","''",", () () Γ() Γ()","() () Γ() Γ() () ()","tc d i","tt s zt0","di","zt v 1","di di 1","TCC rcc ξ 1c 1c1c1","ZT T ZT","nt β 1t zzTT","z1 t1 t1 z1t1","s","TT","z","mv δ 1v","zt zt","v1 v1","zt ηηηdη Tβ Tδ","φφ βδ φ","σσ σ − − === − == = == − == ⋅⋅× ⋅⋅ ⋅ ⋅ ⋅⋅"]},{"title":"∏∏∏∫ ∏∏ ∏ ∏∏∫∫ ∏∏","paragraphs":["G ,",",",",,",",",",",",, ,, ,, , , ,, ,",",*, ,, , , ,,,, , ,* ,* , ,,*, di","di","di","di di","di di","di","1","di","di","di di 1 di s di zs0 di","z0 zscx sc zt s1zsc s di zt 1 x0","dφ dσ x1 n β x 0 nTβp γ r ξ m δp2γ rCξ","x 1 mTδ − −","⎧ =⎪","⋅⋅⎨ =⎪⎩ +⎧","=⎪ +++⎪","∝××⎨","+++⎪ =⎪ +⎩ G G  Similarly, ,",",, ,",",, , ,, ,",",, , ,, ,, ,, ,,, ,*, ,,,* ,, ,, ,*, ,,, ,,*, (|,,,,,,,,) () di","di","1 d i 1","di","1","di","1 d i 1","di","1","di d i di","di d i 1 d","i","di di 1 di d i dz 0 ztck d0 ztc di di 1 zt0 z0 zt t1 zt Px k zx tcαβδγξ q α p γk0 qZα p2γ Iz z k 1 n β k0 nTβ m δ m","−− −","−− − − − − − = +⎧","+=⎪","+∝××⎨","+⎪ ==⎩ + + ∝ = + GGGG 1 k1 Tδ ⎧ ⎪ ⎪ ⎨ ⎪ =⎪ +⎩ ",",, ,",",, ,",",, , ,, , , ,(:), ,, ,,, ,*, ,,,* , ,,, (,|,,, ,,,,,) ()","di","1 d i 1","d","i","1","di","1 d i 1","di","1","di","u","2","di u 2","d","i","u","1 di di 1 di l 1 di d ii l 1 d i dj0 ztck d0 ztc di 1 jt c x j P zz z jxkz xt","c αβδγξ q α","k0 p γ qZα p2γ Iz j k P 1 p γ p","−− −","−− −","+− + −","+− ++− −+−− − === = =","+⎧ = +⎪ +∝×⋅⎨ ∝","+⎪ ==⎩ + GGG \" G ,,",",, , , ,, , ,,, ,,,* ,,* ,, ,*, ,,, ,,*,","()di u 2","di","u","1","d","i","u2","d","i","u2 d i u","2 di","di 1 d i di 1","l1 l jt t 1 u2 u2tc jt 1 jt 0 j0 jt t 1 jt 1","m δ 2γ mTδ n β k0 nTβ m δ","k1 mTδ","+− +−","+−","+−","+","− − − + == ∝","+ ×⋅","++ +⎧","=⎪ +⎪","⎨ +⎪ =⎪ +⎩"]},{"title":"∏∏  Appendix A.2 Parameter estimation for TWC","paragraphs":["After each Gibbs sampling iteration, we obtain posterior estimates","̂ˆ̂,̂,,θφσψG GGG and r by ,, ,,,",",,,, ,*, , , ,* ,,, ,,",",, , ,,*, ,*, , , ,* ̂ˆ ˆ̂ ˆ dz0 ztck","dz ztck d0 ztc zvt1 zt0","zvt zt zv 1 z 0 tc tc t q α p γ","θψ qZα p2γ m δ n β","σφ mTδ nTβ r ξ η rCξ ++","== ++ ++","== ++ + = + , where the symbols are the same as those defined in Appendix A.1. These values correspond to the predictive distribution over new word topics, new indicators, new character topics, and new characters."]},{"title":"Appendix B. Likelihood Function Derivation for TWC and TNG","paragraphs":["To compute the likelihood function for TWC, a quaternion function gi is defined as follows: (formula has a broken character) ,, ,, , ,,","( , , , ) ( , ,..., , , , ̂ˆ̂ˆ̂,|,,,,) id1d2dididi di 1 di g rsuv Pc c c z rx s tutvθφσψη−===== == =======  G GGGG .","Then, it is clear that ̂ˆ̂ˆ̂(|,,,,) (,,,)dZ1TT","dN r1s0u1v1Pc θφσψη g rsuv ====="]},{"title":"∑∑∑∑","paragraphs":["GG GGGG , where Z is the number of word topics and T is the number of character topics. The function gi can be rewritten in a recursive manner. , , ,,, , (,, ,) (̂,, ,) ˆ̂(,,,) (,,,)","̂ˆ (̂) d1 di 1 di 1","crv 1drv","Z1T","cs","i1 i juc v","j1k0l1","v","r","r","d","v","ru gr1uv 0 1","gr0uv θφη T grsuv gjkluψη s0φθ s1σIr j + + === = =××× =×× ⎧ =⎪ ××⎨","==⎪⎩ ==========="]},{"title":"∑∑∑ ","paragraphs":["Similarly we can define function hi to help compute the likelihood for TNG. (formula has a broken character) , ,",", , , ,, , , ̂ˆ̂(̂,) ( ,..., , , | , , , ) ̂ˆ̂(̂|,,,) (,)","(,) ̂(̂, ) ˆ ˆ̂(,) (, ) ˆ d d1 di 1","di di 1 di","id1diii Z1","dN r1s0 1 cr","1dr","c","Z1","r","sr","i1 i jc d c","j1k0","rc hrs Pc c z rx sθφσψ Pc θφσψ hrs hr1 0 hr0 θφ φ s 0","hrs hjkψθ","s 1σ + + == + == == = = =× ⎧ =⎪","=×××⎨","=⎪⎩"]},{"title":"∑∑ ∑∑","paragraphs":["G GGG  GGGGG "]},{"title":"References","paragraphs":["Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.","Griffiths, T. L. and Steyvers, M. 2004. Finding Scientific Topics. Proceedings of the National Academy of Sciences, 101 (suppl. 1), 5228-5235.","Steyvers, M. and Griffiths, T. L. 2007. Probabilistic topic models. Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum.","Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B. T. 2007. Topics in Semantic Representation Psychological Review, 114(2), 211-244.","Wang, X., McCallum, A., and Wei, X. 2007. Topical N-grams: Phrase and Topic Discovery, with an Application to Information Retrieval. Proceedings of the 7th IEEE International Conference on Data Mining (ICDM 2007). 352"]}]}