{"sections":[{"title":"","paragraphs":["Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 497–504 Manchester, August 2008 "]},{"title":"Understanding and Summarizing Answers in Community-Based Question Answering Services Yuanjie Liu1 , Shasha Li 2 , Yunbo Cao 1,3 , Chin-Yew Lin3 , Dingyi Han1 , Yong Yu1","paragraphs":["1"]},{"title":"Shanghai Jiao Tong University, Shanghai, China, 200240 {lyjgeorge,handy,yyu} @apex.sjtu.edu.cn","paragraphs":["2"]},{"title":"National University of Defense Technology, Changsha, China, 410074 Shashali @nudt.edu.cn","paragraphs":["3"]},{"title":"Microsoft Research Asia, Beijing, China, 100080 {yunbo.cao,cyl} @microsoft.com  Abstract","paragraphs":["Community-based question answering (cQA) services have accumulated millions of questions and their answers over time. In the process of accumulation, cQA services assume that questions always have unique best answers. However, with an in-depth analysis of questions and answers on cQA services, we find that the assumption cannot be true. According to the analysis, at least 78% of the cQA best answers are reusable when similar questions are asked again, but no more than 48% of them are indeed the unique best answers. We conduct the analysis by proposing taxonomies for cQA questions and answers. To better reuse the cQA content, we also propose applying automatic summarization techniques to summarize answers. Our results show that question-type oriented summarization techniques can improve cQA answer quality significantly."]},{"title":"1 Introduction","paragraphs":["Community-based question and answering (cQA) service is becoming a popular type of search related activity. Major search engines around the world have rolled out their own versions of cQA service. Yahoo! Answers, Baidu Zhidao, and Naver Ji-Sik-In1","are some examples.","In general, a cQA service has the following workflow. First, a question is posted by the asker in a cQA service and then people in the community can answer the question. After enough number of answers are collected, a best answer can  © 2008. Licensed under the Creative Commons Attribution- Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. be chosen by the asker or voted by the community. The resulting question and answer archives are large knowledge repositories and can be used to complement online search. For example, Naver’s Ji-Sik-In (Knowledge iN) has accumulated about 70 million entries2",".","In an ideal scenario, a search engine can serve similar questions or use best answers as search result snippets when similar queries are submitted. To support such applications, we have to assume the best answers from cQA services are good and relevant answers for their pairing questions. However, the assumption might not be true as exemplified by the following examples. Question Title Which actress has the most seductive voice?..could range from a giggly goldie hawn..to a sultry anne bancroft? Question Description or any other type of voice that you find allur-ing. .. Best Answer (Polls & Surveys) Fenella Fielding, wow!!!! Best Answer (Movies) i think joanna lumlley has a really sexy voice Table 1. Same Question / Different Best Answers","Question Title Does anyone know of any birthdays coming up soon? Question Description Celerities, people you know, you? Anyway I need the name and the date. If you want to know it is for my site, http://www.jessicaparke2.piczo.com... and that is not site advertising. Answer Novembers Are: Paul Dickov nov 1st Nelly (not furtado) nov 2nd ...","Best Answer Check imdb.com, they have this celebrity birthdays listed. Table 2. Question with Alternative Answers","Table 1 presents a question asking community opinions about “who is the actress has the most seductive voice”. The asker posted the same question twice at different Yahoo! Answers categories: one in Polls & Surveys and one in Movies.  1 Yahoo! Answers: answers.yahoo.com; Baidu Zhidao: zhidao.baidu.com; Naver Ji-Sik-In: kin.naver.com 2 www.iht.com/articles/2007/07/04/technology/naver.php 497  Two different best answers were chosen by the same asker due to non-overlapping of answers. Table 2 shows another example, it asks about “the coming birthdays of stars”. The best answer chosen by the asker is very good because it provides useful URL information where the asker can find her answers. However, other answers listed a variety of birthdays of stars that also answered the question. These two examples indicate that the conventional cQA policy of allow-ing askers or voters to choose best answers might be working fine with the purpose of cQA but it might not be a good one if we want to reuse these best answers without any post-processing.","To find out what might be the alternatives to the best answers, we first carried out an in-depth analysis of cQA data by developing taxonomies for questions and answers. Then we propose summarizing answers in a consideration of question type, as the alternative to the best answers. For example, for the ‘actress voice’ question, a summary of different people’s opinions ranked by popularity might be a better way for express-ing the question’s answers. Similar to the ‘actress voice’ question, the ‘celebrity birthday’ question does not have a fix set of answers but is different from the ‘actress voice’ question that its answers are facts not opinions. For fact-based open ended questions, combining different answers will be useful for reuse of those answers.","The rest of this paper is arranged as follows. We review related work in Section 2. We develop a framework for answer type taxonomy in Section 3 and a cQA question taxonomy in Section 4. Section 5 presents methods to summarize cQA answers. Finally, we conclude this paper and discuss future work in Section 6."]},{"title":"2 Related Work","paragraphs":["Previous research on cQA (community-based Question and Answering) domain focused on three major areas: (1) how to find similar questions given a new question (Jeon et al. 2005a; Jeon et al., 2005b), (2) how to find experts given a community network(Liu et al., 2005; Jurczyk & Agichtein, 2007), and (3) how to measure answer quality and its effect on question retrieval. The third area of focus is the most relevant to our research. Jeon et al. (2006)’s work on assessing cQA answer quality is one typical example. They found that about 1/3 of the answers among the 1,700 Q&A pairs from Naver.com cQA data have quality problems and approximately 1/10 of them have bad answers 3",". They used 13 nontextual features and trained a maximum entropy model to predict answer quality. They showed that retrieval relevance was significantly improved when answer quality measure was integrated in a log likelihood retrieval model.","As mentioned in Section 1, cQA services provide an alternative way for users to find information online. Questions posted on cQA sites should reflect users’ needs as queries submitted to search engines do. Broder (2002) proposed that search queries can be classified into three categories, i.e. navigational, informational, and transactional. Ross and Levinson (2004) suggested a more elaborated taxonomy with five more subcategories for informational queries and four more subcategories for resource (transactional) queries. In open-domain question answering research that automatic systems are required to extract exact answers from a text database given a set of factoid questions (Voorhees and M. Ellen, 2003), all top performing systems had in-corporated question taxonomies (Hovy et al., 2001; Moldovan et al., 2000; Lytinen et al., 2002; Jijkoun et al., 2005). Based on the past expe-riences from the annual NIST TREC Question and Answering Track 4","(TREC QA Track), an international forum dedicating to evaluate and compare different open-domain question answering systems, we conjecture that a cQA question taxonomy would help us determine what type of best answer is expected given a question type.","Automatic summarization of cQA answers is one of the main focuses of this paper. We propose that summarization techniques (Hovy and Lin, 1999; Lin and Hovy, 2002) can be used to create cQA answer summaries for different question types. Creating an answer summary given a question and its answers can be seen as a multi-document summarization task. We simply re-place documents with answers and apply these techniques to generate the answer summary. The task has been one of the main tasks the Docu-ment Understanding Conference5","since 2004."]},{"title":"3 A Framework for Answer Type","paragraphs":["To study how to exploit the best answers of cQA, we need to first analyze cQA answers. We would like to know whether the existing best answer of a specific question is good for reuse. If not, we  3 Answers in Jeon el al.’s work were rated in three levels: good, medium, and bad. 4 http://trec.nist.gov/data/qamain.html 5 http://duc.nist.gov 498 want to are. We by cQA ferentiat tomatica","We m swers fo for answ al categ examini the 4 mo ries (100 tainmen (S&C), H we dev based on termines can be r ilar to th","One o ercise an The tax cussions notators category annotato on a sin made t taxonom discuss answer t Figur","Figur onomy. Reusabl means th similar while a reused. Factual that can jective B as the be F Unique understand w will refer to askers or vo te it with be ally generate made use of or developin wer type. The gories in Yah ng 400 rando ost popular 0 questions f t & Music Health, and eloped a cQ n the princip s a BA’s ans reused or not he BA’s ques of the author nd developed xonomy was s among the to do the a y label that ors. If none o ngle categor the final d my is describ the question taxonomy in e 1. cQA Ser e 1 shows th It first divid le and Not hat it can be question to Not Reusab The Reusabl and Subject be used as t BA is one of est answer. Reusable Factual Not Unique Direct Indire Su why and wha o the ‘best a oters as BA h est answers d in our expe f questions fr g and testing ere are over hoo! Answe omly selecte top Yahoo! from each ca (E&M), So Computers & QA answer ple of BA reu swer type bas t when a que stion is asked rs carried ou d the initial a then modif authors. We annotation. W was agreed of the three a ry label, one decision. Th bed in this n type and t next section rvices BA Ty he resulting des BA into Reusable. A reused as the its question ble BA mea le BA is fur tive. A Fact the best answ the opinions Best Answer ect bjective Re at the alterna answers’ sele henceforth to annotated or eriments. from Yahoo! g our framew 1,000 hierar ers. By manu ed questions Answers cat ategory) – E ociety & Cu & Internet (C type taxon usability tha sed on “if th estion that is d again”. t this manua answer taxon fied through e asked three We assigned by at least annotators ag e of the aut he answer section and the relation n. ype Taxonom answer type o two catego A Reusable e best answe n is asked a ans it canno rther divided tual BA is a wer; while a s that can be","Not Reusable elevant Irre  atives ected o dif-r au-An- work chicually from tego-Enter- ulture C&I), nomy at de-e BA sim-al ex-nomy. h dis-e and the t two greed thors type d we with  my. e tax-ories:","BA er if a gain; ot be d into a fact Sub-used","T Uniq a un answ Uniq The type its q swer ple, Indi whil birth","A for o ques Each","T vant used leva aske Nick “I'm ly So answ er’s ques best its q tion give answ ema To b Answ Uniqu Direc Indire Factu Subje Reusa Relev Irrelev Not R","T type mor ries two","A mos (50% the o or v answ levant he Factual que and Not nique best an wer add m que BA has Not Unique es: Direct an question dire rs its questio the question rect BA wh le there is al hday lists. A Subjective opinions or r stion asked “ h answerer w he Not Reus t and Irrelev d as a best an ant to its qu ed “Why was k Lachey so m not sure wh outh Jersey, wer is releva location wh stion; an Irre t answer to i question. Th period has en that meets wer’.” of th ail without sh box” is in thi wer Type ue t ect ual Total ective able Total vant vant Reusable Total","Table 3. D able 3 show es on four ca re than 48%. tend to hav categories. Among the fo stly not uniq %) of subjec one BA per c oters is not g wer. Howeve","BA type t Unique. A nswer to its q more informa s other alter BA type is d nd Indirect. A ectly; while on through i n mentioned hich gives a lso a Direct a BA answers recommenda “Which is th would have h sable BA has ant. A Relev nswer to its uestion, for s \"I Can't Ha shortlived?” here you live that song wa ant but witho hich does n elevant BA c ts question a e BA “It ap expired. If s your needs e question “ howing the em s case.","C&I","47%","28%","9%","84%","4%","88%","3%","9%","12% Distribution o ws the dist ategories. Un The C&I an ve more fac our categorie que and hav ctive answers cQA questio good enough er, we might has two s Unique BA question and ation; while rnative best divided into A Direct BA an Indirect inference. Fo d in section 1 a website re answer just g s questions t ations. For ex he best sci-fi his own idea. s two subtyp vant BA cou question bu example, a ate You Anym ” A Relevant e, but in NJ, as played ou out knowing ot really an could not be u and it is irre ppears that t an answer h s, please pic “how to for mail address E&M Heal 28% 48 7% 30 3% 5 38% 83% 40% 7 78% 90% 1% 1 21% 9 22% 10%","of Answer Ty tribution of nique answer nd the Health ctual BAs th es, S&C ans ve a high pe s. This indic on chosen by h for reuse as t be able to a subtypes: has only no other e a Not answers. two sub-A answers t BA an-or exam-1 has the eference, gives the that look xample, a movie?”  pes: Releld not be ut it is re-question more\" by t BA said especial-t...”, this the ask-nswer the used as a elevant to the ques-has been k a ‘best rward an ses in the lth S&C 8% 13% 0% 18% 5% 2% % 33% 7% 50% % 83% % 0% 9% 17% % 17% ype Answer rs are no h catego-han other swers are ercentage cates that its asker s the best apply au-499 tomatic summar (but not ible solu E T","Table over wh on a sin the ques stable (o"]},{"title":"4 A C","paragraphs":["As we w my, we themselv well. As question best answ Rose an engine q their tax engine q we follo onomy a modate t Fi","Figur my. We and prop Informa ilar as in ry consi an answ with peo","Navig seeking would li know the","Trans tend to g compute Navigat summariza rized answers unique) answ utions in Sect","Category Computer & In Entertainment &","Health Society & Cul able 4. Disag e 4 shows t hich none of ngle category stion taxonom over at least 7"]},{"title":"CQA Quest","paragraphs":["were develop often could ves and had s we discuss n would help wer types. nd Levinson’ queries has xonomy was queries. Inste owed the ba and made so the particula igure 2. Que e 2 shows th e retain Brod pose a new S tional and Tr n Broder’s ta ists of questi wer but just w ople in cQA gational ca URLs of sp ike to visit, e fan sites of sactional ca get resources er program th ional Inform Constant Opinion ation techni s for at least wers. We pro tion 5. y nternet & Music lture greement on the percenta f the three a y label. The r my develope 79% question"]},{"title":"tion Taxon","paragraphs":["ping our answ d not solely to consider t sed in Sectio p us determ s (2004) tax similar goal developed t ead of starti asic hierarchy ome modific ar of cQA ser estion Type T he resulting der’s taxono Social catego Transactional axonomy whi ions that do were used to services. ategory con ecific websit for example f Hannah Mo ategory con s. A typical o hat lets you c","cQA Question mational Dynamic Context‐ Dependent Trans iques to c t half of reus ovide some p","Percenta 18% 17% 21% 20%","Answer Typ age of ques annotators ag results show ed above is p ns)."]},{"title":"nomy","paragraphs":["wer type tax rely on ans their question on 2, the typ ine the expe xonomy of se l to ours th to classify se ing from scr y of R&L’s cations to acc rvices. Taxonomy question tax omy at top le ory. Navigati l are defined ile Social cat not intend to elicit intera ntains ques tes that the a e, “Does any ontana?” ntains ques one is “Is the create a plan Open sactional  create sable possage pe tions greed w that pretty xono-swers ns as pe of ected earch ough earch ratch, tax-com-  xonoevels ional, simtego- o get ction tions asker ybody tions ere a net?”","F to t Con answ dich port betw taxo R&L ques latio wou","F tego Opin Que peop think jects ple. tion diffe “Wh diffe Ope som have selv tion com follo clud cont","T serv to g joke tially or o lazy toge beco goog will they a ne who","T ques cate only ques occu sinc sear Social or Informati two subcateg nstant questio wers while d hotomy of in t our intentio ween the que onomy. Cons L’s closed q stion is “Whi on?” but “W uld be a dyna or Dynamic ories: Opinio nion questio estions in th ple in cQA k of some p s. “Is Micros Context-dep s having dif erent contex hat is the pop erent answer en category me facts or m e a variety o es may have “Can you li ming week?” ows R&L’s des what is n text-depende The new Soc vices. Questio et an answer es and expre y, askers trea nline forums people com ether with th ome a hacker gle search... continue to y can give up egative sentim o asked how t Table 5 show stion types gories. We y occupy 1 stions are ev ur in the sam e people ve rch engines on category, gories: Con ons have a f dynamic que nformational on to establi estion taxon stant questio query type. A ich country h What is the po amic question category, w on, Context-D ns are those his category communiti eople, some soft Vista wo pendent ques fferent answ xt. For exa pulation of C rs according contains q methods. Th of answers o e unconstrain ist some birt is an exam open query not covered ent categories cial category ons in this ca r. These que essing askers at cQA servi s. The questi me on here si he question r? It really is hopefully so ask, will cli p faster... ” a ment toward to become a ws the distr on 4 differe observe tha 1% ~ 20% ven fewer su mple question ery likely w to discover , we first div nstant and D fixed or stab estions do n l category is sh intuitive omy and the on type is s An example has the large opulation of n. e define thre Dependent an asking for o seek opinio es about w events, or s orth it?” is a stions are tho wers accordin ample, the China?” sho to the differ questions ask he questions or their answ ned depth. T thdays of sta mple. This es category. It by the opin s. y is specific ategory do n estions includ s’ own ideas ice as chattin ion “Why do imply just to description sn't that har me of the pe ick the link b actually is ex s a number o hacker. ribution of ent Yahoo! at constant q % while nav uch that they ns. This is re would be abl answers of vide it in-Dynamic. ble set of not. This s to supmapping e answer imilar to constant est popu-f China?” ee subca-nd Open. opinions. ons from what they some ob-an examose ques-ng to the question ould have rent date. king for s usually wer them-The ques-ars in the ssentially t also in-nion and to cQA not intend de telling s. Essen-ng rooms so many o ask...?” “how to d to do a eople that below so xpressing of people different Answers questions vigational y do not easonable le to use f naviga-500  tional and constant questions. They do not have to ask these types of question on community-based question answering services. On the contrary, open and opinion questions are frequently asked, it ranges from 56%~83%. Question Type C&I E&M Health S&C Navigational Total 0% 0% 0% 0% Constant 15% 20% 15% 11% Opinion 8% 37% 16% 60% Context Dependent 0% 1% 1% 0% Open 59% 19% 67% 18% Dynamic Total 67% 57% 84% 78% Informational Total 82% 77% 99% 89% Transactional Total 14% 8% 0% 1% Social Total 4% 15% 1% 10%","Table 5 Distribution of Question Type Intersection Number UNI DIR IND SUB REL IRR Navigational 0 0 0 0 0 0 Constant 48 9 3 0 1 0 Open 51 62 13 15 5 17 Context-dep 0 0 1 0 0 1 Opinion 15 13 1 84 0 8 Transactional 10 7 4 1 0 1 Social 0 0 0 1 0 29 Table 6. Question Answer Correlation","Table 6 (UNI: unique, DIR: direct, IND: indirect, SUB: subjective, REL: relevant, IRR: irrelevant) gives the correlation statistics of question type vs. answer type. There exists a strong correlation between question type and answer type. Every question type tends to be associated with only one or two answer types (bold numbers in Table 6)."]},{"title":"5 Question-Type Oriented Answer Summarization","paragraphs":["Since the BAs for at least half of questions do not cover all useful information of other answers, it is better to adopt post-processing techniques such as answer summarization for better reuse of the BAs. As observed in the previous sections, answer types can be basically predicted by question type. Thus, in this section, we propose to use multi-document summarization (MDS) techniques for summarizing answers according to question type. Here we assume that question type can be determined automatically. In the following sub-sections, we will focus on the summarization of answers to open or opinion questions as they occupy more than half of the cQA questions. 5.1 Open Questions Algorithm: For open questions, we follow typical MDS procedure: topic identification, interpretation & fusion, and then summary generation (Hovy and Lin, 1999; Lin and Hovy, 2002). Table 7 describes the algorithm. 1. Employ the clustering algorithm on answers 2. Extract the noun phrases in each cluster, using a shallow parser.6"," 3. For each cluster and each label (or noun phrase), calculate the score by using the Relevance Scoring Function: pw|θPMIw, l|C D θ|C   Where θ is the cluster, w is the word, l is the label or noun phrase, C is the background context which is composed of 5,000 questions in the same category, p(·) is conditional probability, PMI(·) is pointwise mutual information, and D(·) is KL-divergence 4. Extract the key answer which contains the noun phrase that has the highest score in each cluster 5. Rank these key answers by cluster size and present the results. Table 7. Summarization Algorithm(Open-Type)","In the first step, we use a bottom-up approach for clustering answers to do topic identification. Initially, each answer forms a cluster. Then we combine the most similar two clusters as a new cluster if their similarity is higher than a threshold. This process is repeated until no new clusters can be formed. For computing similarities, we regard the highest cosine similarity of two sentences from two different clusters as the similarity of the two clusters. Then we extract salient noun phrases, i.e. cluster labels, from each cluster using the first-order relevance scoring func-tion proposed by Mei et al. (2007), (step 2,3 in Table 7). In the fusion phase (step 4), these phrases are then used to rank answers within their cluster. Finally in the generation phase (step 5), we present the summarized answer by ex-tracting the most important answer in every cluster and sort them according to the cluster size where they come from.","Case Example: Table 8 presents an example of summarization results of open-type questions. The question asks how to change Windows XP desktop to Mac style. There are many softwares providing such functionalities. The BA only lists one choice – the StarDock products, while other answers suggest Flyakite and LiteStep. The automatic summarized answer (ASA) contains a variety of for turning Windows XP desktop into Mac style with their names highlighted as cluster labels. Compared with manually-summarized answer (MSA), ASA contains most information of MSA while retains similar length with BA and MSA. 5.2 Opinion Questions Algorithm: For opinion questions, a comprehensive investigation of this topic would be beyond the scope of this paper since this is still a field  6 http://opennlp.sourceforge.net 501  under active development (Wiebe et al., 2003; Kim and Hovy, 2004). We build a simple yet novel opinion-focused answer summarizer which provides a global view of all answers. We divide opinion questions into two subcategories. One is sentiment-oriented question that asks the sentiment about something, for example, “what do you think of ...”. The other is list-oriented question that intends to get a list of answers and see what item is the most popular.","For sentiment-oriented questions, askers care about how many people support or against some-thing. We use an opinion word dictionary7",", a cue phrase list, a simple voting strategy, and some heuristic rules to classify the sentences into Support, Neutral, or Against category and use the overall attitude with key sentences to build summarization. For list-oriented questions, a simple counting algorithm that tallies different answers of questions together with their supporting votes would be good answer summaries. Details of the algorithm are shown in Table 9, 10.","Case Example: Table 11 presents the summarization result of an sentiment-oriented question, it asks “whether it is strange for a 16-year child to talk to a teddy bear?”, the BA is a negative response. However, if we consider all answers,  7 Inquirer dictionary http://www.wjh.harvard.edu/~inquirer. we find that half of the answers agree but another half of them disagree. The distribution of different sentiments is similar as MSA. Table 12 shows the summarization result of a list-oriented question, the question asks “what is the best sci-fi movie?” The BA just gives one choice “Independence day” while the summarized answer gives a list of best sci-fi movies with the number of supporting vote. Though it is not complete compared with MSA, it contains most of the options which has highest votes among all answers. 1. Employ the same cluster procedure of Open-Type question. 2. If an answer begins with negative cue phrase (e.g. “No, it isn’t” etc.), it is annotated as Against. If a response begins with positive cue phrase (e.g. “Yes, it is” etc.), it is annotated as Support. 3. For a clause, if number of positive sentiment word is larger than negative sentiment word, the sentiment of the clause is Positive. Otherwise, the sentiment of the clause is Negative. 4. If there are negative indicators such as “don’t/never/...” in front of the clause, the sentiment should be reversed. 5. If number of negative clauses is larger than number of positive clauses, the sentiment of the answer is Negative. Otherwise, the sentiment of the answer is Positive. 6. Denote the sentiment value of question as s(q), the sentiment value of an answer as s(a), and then the final sentiment of the answer is logical AND of s(q) and s(a) 7. Present key sentiments with attitude label","Table 9. Summarization Algorithm (Senti-","ment-Opinion) 1. Segment the answers into sentences 2. Cluster sentences by using similar process in open-type 3. For each cluster, choose the key sentence based on mutual information between itself and other sentences within the cluster 4. Rank the key sentences by the cluster size and present them ogether with votes","Table 10. Summarization Algorithm (List-","Opinion) Question (http://answers.yahoo.com/question/?qid=1006050125145) I am 16 and i stil talk to my erm..teddy bear..am i wierd??? Best Answer Chosen not at all i'm 14 and i too do that Auto-summarized Answer Support A: It's might be a little uncommon for a 16 year old to talk to a teddy bear but there would be a serious problem if you told me that your teddy bear answered back as you talked to him!!:) A: I slept with my teddy bear until I graduated. Can't say that I ever had a conversation with him, but if I had I'm sure he would've been a very good listener. Against A: i talk to a seed im growing .. its not weird .... :) A: No, you're not weird.....you're Pratheek! :D A: no, i like to hold on to my old memories too. i do it sometimes too. A: It will get weird when he starts to answer back! A: not really. it depends how you talk i mean not if you talk to it like its a little kid like my brother does. Overall Attitude: Support 5 / Neutral 1 / Against 5 Manually-summarized Answer support (vote 4) neutral (vote 2) against (vote 5) reasons: i like to hold on to my old memories too. (vote 1) I slept with my teddy bear until I graduated. (vote 1) i'm 14 and i too do that (vote 1)","Table 11. Summary of Sentiment-Opinion","Question   Question (http://answers.yahoo.com/question/?qid=1005120801427) What is the best way to make XP look like Mac osX? Best Answer Chosen I found the best way to do this is to use WindowsBlinds. A program that, if you use the total StarDock, package will allow you to add the ObjectBar in addition to changed the toolbars to be OS X stylized. If you want added functionality you can download programs off the internet that will mimic the Expose feature which will show you a tiled set of all open windows. Programs that will do this include: WinPlosion, Windows Exposer, and Top Desk Auto-summarized Answer LiteStep:An additional option is LiteStep - a \"Shell Replacement\" for Windows that has a variety of themes you can install. Undoubtedly there are various Mac OSX themes avaialable for LiteStep. I have included a source to a max osx theme for Litestep at customize.org. Flyakite:Flyakite is a transformation pack and the most comprehensive in terms of converting an XP system's look to that of an OS X system, google it up and you should find it, v3 seems to be in development and should be out soon. Window Blinds:http://www.stardock.com/products/windowb... Manually-summarized Answer One way is to use WindowsBlinds. The package will allow you to add the ObjectBar for changing to the OSX theme. You can also make added functionality of Expose feature by downloading the programs like WinPlosion, Windows Exposer and Top Desk. The URL of it is http://www.stardock.com/products/windowblinds/. Another option is to use Flyakite which is a transformation pack. The third Option is the LiteStep, it is a \"Shell Replacement\" for windows that has a variety of Mac OSX tehmes you can install. The url is http://litestep.net and I have included a source of Mac OS theme for Litestep at http://www.customize.org/details/33409.","Table 8. Summary of Open-Question 502   Question (http://answers.yahoo.com/question/?qid= 20060718083151AACYQJn) What is the best sci-fi movie u ever saw? Best Answer Chosen Independance Day Auto-summarized Answer star wars (5) Blade Runner (3) fi movie has to be Night of the Lepus (2) But the best \"B\" sci (2) I liked Stargate it didn't scare me and I thought they did a great job recreating Egypt (3) Independance Day (3) Manually-summarized Answer Star Wars (vote 6); The Matrix (vote 3); Independence Day (vote 2); Blade Runner (vote 2); Starship Troopers (vote 2); Alien (vote 2); Alien v.s Predator (vote 1); MST3K (vote 1);","Table 12. Summary of List-Opinion Question 5.3 Experiments Information Content: To evaluate the effectiveness of automatic summarization, we use the information content criterion for comparing ASA with BA. It focuses on whether ASA or BA contains more useful information to the question. Information point is used in the evaluation. Usually, one kind of solution for open questions or one kind of reason for opinion questions can contribute one information point. By summing all information points in both ASA and BA, we then can compare which one contains more information. Intuitively, longer texts would contain more information. Thus, when comparing the information content, we limit the length of ASA with several levels to do the evaluation. Take question in Table 8 as an example, the BA just gives one software, which contributes one information point while the ASA lists three kinds of software which contributes three information points. Thus, ASA is considered better than BA.","For each question, we generate 100%, 150%, and 200% BA word-length ASAs. Three annotators are asked to determine whether an ASA is better than, equal to, or worse than its corresponding BA in terms of information content. Voting strategy is used to determine the final label. If three labels are all different, it is labeled as Unknown. We extract 163 open questions and 121 opinion questions from all four categories by using final question category labels mentioned in Section 4. To make meaningful comparison, questions having unique answers or having only one answer are excluded. After the removal, there are 104 open questions and 99 opinion questions left for comparison. The results are shown in Table 13. We are encouraged by the evaluation results that our automatic summarization methods generate better coverage of contents in most of the cases at every answer summary length. We observe a big difference between 100% and 150% answer summaries. It should not be a surprise since a 150% answer summary contains 50% more content than its corresponding BA. While at the 100% length, we still have about 30% ASAs better than BA. Questions which have better ASA than BA usually have a long BA but with little information. Table 14 provides the example. By using summarization, answers that are compact and direct to the question can be included. The results indicate that summary compression technique might be helpful to pack more information in short answers. Open ASA Better BA Better Equal Unknown 100% 30% 12% 45% 13% 150% 55% 7% 28% 10% 200% 63% 4% 24% 9% Opinion ASA Better BA Better Equal Unknown 100% 37% 20% 32% 11% 150% 44% 16% 30% 10% 200% 54% 16% 23% 7% Table 13. Evaluation by Information Content Q Why wont japanese characters burn onto the DVD? BA man, the answers here are too stupid for hteir own.You are","creating a DVD on Western Platform. I take it, you are","using an OS that is in English?In order to \"view\" japanese","as part of your filenames, you need your operating system","to accept Japanese coding (characters).If you are using","Windows, then you will need ot isntall the Japanese cha-","racter Set for your operating system","If you are using MacOS . i have no idea. 100% ASA"," The dvd writer Probably because your burner, the DVD writer, doesn't support double bytes code, such as Japanese, Korean, and Chinese. Check the supporting language of your software. Or change all the file name in single byte code, like alphabets. man, the answers here are too stupid for hteir own. You are creating a DVD on Western Platform. I take it, you are using an OS that is in English?","Table 14. Examples of 100% ASA","Readability: Besides the information content, we would also like to study the readability of automatic summarized answers. 10 questions (each from open and opinion category) are ex-tracted and we make both manual summarized answer (MSA) and automatic summarized answer (ASA) for comparison with BA. We used the information content (INFO) and readability (READ) criteria for evaluation. The readability is judged basically by the time for understanding. We make two kinds of comparison: ASA vs. BA and MSA vs. BA. The first one is used to judge whether the current summarization method is better than current cQA scenario. The second one is used as an expectation for how much the summarization methods can be better than BA. 503  For ASA vs. BA, the results in Table 15 show that all the annotators agree ASAs providing more information content but not being with satisfying readability. For MSA vs. BA, better results in readability can be achieved as Table 16. This suggests that the proposed approach can succeed as more sophisticated summarization techniques are developed. Open Annotator 1 Annotator 2 Annotator 3 ASA INFO READ INFO READ INFO READ Better 40% 10% 90% 10% 80% 0% Equal 60% 60% 10% 80% 20% 60% Worse 0% 30% 0% 10% 0% 40% Opinion Annotator 1 Annotator 2 Annotator 3 ASA INFO READ INFO READ INFO READ Better 90% 10% 90% 10% 70% 40% Equal 10% 60% 10% 60% 10% 20% Worse 0% 30% 0% 30% 20% 40%","Table 15. ASA vs. BA Evaluation Open Annotator 1 Annotator 2 Annotator 3 MSA INFO READ INFO READ INFO READ Better 100% 30% 100% 90% 100% 90% Equal 0% 50% 0% 0% 0% 0% Worse 0% 20% 0% 10% 0% 10% Opinion Annotator 1 Annotator 2 Annotator 3 MSA INFO READ INFO READ INFO READ Better 90% 20% 60% 70% 100% 100% Equal 10% 80% 40% 30% 0% 0% Worse 0% 0% 0% 0% 0% 0%","Table 16. MSA vs. BA Evaluation"]},{"title":"6 Conclusion and Future Work","paragraphs":["In this paper, we have carried out a comprehensive analysis of the question types in community-based question answering (cQA) services and have developed taxonomies for questions and answers. We find that questions do not always have unique best answers. Open and opinion questions usually have multiple good answers. They occupied about 56%~83% and most of their best answers can be improved. By using question type as a guide, we propose applying automatic summarization techniques to summarization answers or improving cQA best answers through answer editing. Our results show that customized question-type focused summarization techniques can improve cQA answer quality significantly.","Looking into the future, we are to develop automatic question type identification methods to fully automate answer summarization. Further-more, we would also like to utilize more sophisticated summarization techniques to improve content compaction and readability."]},{"title":"Acknowledgements","paragraphs":["We thank the anonymous reviewers for their valuable suggestions and comments to this paper."]},{"title":"References","paragraphs":["Broder A. A taxonomy of web search. 2002. SIGIR Forum Vol.36, No. 2, 3-10.","Hovy Edward, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran. 2001. Toward Semantics-Based Answer Pinpointing. In Proc. of HLT’01.","Hovy E., C. Lin. 1999. Automated Text Summarization and the SUMMARIST System. In Advances in Automated Text Summarization","Jeon J., W. B. Croft, and J. Lee. 2005a. Finding se-mantically similar questions based on their answers. In Proc. of SIGIR’05.","Jeon J., W. B. Croft, and J. Lee. 2005b. Finding similar questions in large question and answer archives. In Proc. of CIKM’05.","Jurczyk P., E. Agichtein. 2007. Hits on question answer portals: exploration of link analysis for author ranking. In Proc. of SIGIR '07.","Jeon J. , W.B. Croft, J. Lee, S. Park. 2006. A Frame-work to predict the quality of answers with nontextual features. In Proc. of SIGIR ’06.","Jijkoun V., M. R. 2005. Retrieving Answers from Frequently Asked Questions Pages on the Web. In Proc. of CIKM’05. Kleinberg J. 1999. Authoritative sources in a hyper-","linked environment. Journal of the ACM, vol. 46, Kim S., E. Hovy. 2004. Determining the Sentiment of Opinions. In Proc. of COLING’04.","Liu X., W.B. Croft, M. Koll. 2005. Finding experts in community-based question-answering services. In Proc. of CIKM '05.","Lin C.Y., E. Hovy. 2002. From single to multi-document summarization: a prototype system and its evaluation. In Proc. of ACL'02.","Lytinen S., N. Tomuro. 2002. The Use of Question Types to Match Questions in FAQFinder. In Proc. of AAAI’02.","Moldovan D., S. Harabagiu, et al. 2000. The Structure and an Open-Domain Question Answering System. In Proc. of ACL’00.","Mei Q., X. Shen, C. Zhai. 2007. Automatic labeling of multinomial topic models. In Proc. of KDD'07.","Rose D. E., D. Levinson. 2004. Understanding user goals in web search. In Proc. of WWW '04.","Voorhees, M. Ellen. 2003. Overview of the TREC 2003 Question Answering Track. In Proc. of TREC’03.","Wiebe J., E. Breck, et al. 2003. Recognizing and Organizing Opinions Expressed in the World Press 504"]}]}