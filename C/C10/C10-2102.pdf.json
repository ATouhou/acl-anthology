{"sections":[{"title":"","paragraphs":["Coling 2010: Poster Volume, pages 885–893, Beijing, August 2010"]},{"title":"A Learnable Constraint-based Grammar Formalism Smaranda Muresan School of Communication and Information Rutgers University smuresan@rci.rutgers.edu Abstract","paragraphs":["Lexicalized Well-Founded Grammar (LWFG) is a recently developed syntactic-semantic grammar formalism for deep language understanding, which balances expressiveness with provable learnability results. The learnability result for LWFGs assumes that the semantic composition constraints are learnable. In this paper, we show what are the properties and principles the semantic representation and grammar formalism require, in order to be able to learn these constraints from examples, and give a learning algorithm. We also introduce a LWFG parser as a deductive system, used as an inference engine during LWFG induction. An example for learning a grammar for noun compounds is given."]},{"title":"1 Introduction","paragraphs":["Recently, several machine learning approaches have been proposed for mapping sentences to their formal meaning representations (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Muresan, 2008; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009). However, only few of them in-tegrate the semantic representation with a grammar formalism: λ-expressions and Combinatory Categorial Grammars (CCGs) (Steedman, 1996) are used by Zettlemoyer and Collins (2005;2009), and ontology-based representations and Lexicalized Well-Founded Grammars (LWFGs) (Muresan and Rambow, 2007) are used by Muresan (2008).","An advantage of the LWFG formalism, compared to most constraint-based grammar for-malisms developed for deep language understanding, is that it is accompanied by a learnability guarantee, the search space for LWFG induction being a complete grammar lattice (Muresan and Rambow, 2007). Like other constraint-based grammar formalisms, the semantic structures in LWFG are composed by constraint solving, semantic composition being realized through constraints at the grammar rule level. Moreover, semantic interpretation is also realized through constraints at the grammar rule level, providing access to meaning during parsing.","However, the learnability result given by Muresan and Rambow (2007) assumed that the grammar constraints were learnable. In this paper we present the properties and principles of the semantic representation and grammar formalism that allow us to learn the semantic composition constraints. These constraints are a simplified version of ”path equations” (Shieber et al., 1983), and we present an algorithm for learning these constraints from examples (Section 5). We also present a LWFG parser as a deductive system (Shieber et al., 1995) (Section 3). The LWFG parser is used as an innate inference engine during LWFG learning, and we present an algorithm for learning LWFGs from examples (Section 4). A discussion and an example of learning a grammar for noun compounds are given is Section 6."]},{"title":"2 Lexicalized Well-Founded Grammars","paragraphs":["Lexicalized Well-Founded Grammar (LWFG) is a recently developed formalism that balances expressiveness with provable learnability results (Muresan and Rambow, 2007). LWFGs are a type of Definite Clause Grammars (Pereira and Warren, 1980) in which (1) the context-free backbone is extended by introducing a partial ordering relation among nonterminals, 2) grammar nonterminals are augmented with strings and their syntactic-semantic representations, called semantic molecules, and (3) grammar rules can have 885 1. Syntagmas containing elementary semantic molecules a. (w1, ` h 1 b 1 ́ )= (laser, 0 B B B B B @ 2 6 4cat noun head X1 mod X2 3 7 5 D X1.isa = laser, X2.P1=X1 E 1 C C C C C A ) b. (w2, ` h 2 b 2 ́ )=(printer, 0 B B B B B @ 2 6 4 cat noun nr sg head X3 3 7 5 D X3.isa = printer E 1 C C C C C A ) 2. Syntagmas containing a derived semantic molecule (w, ` h b ́ )=(laser printer, 0 B B B B B @ 2 6 4cat nc nr sg head X 3 7 5 D X1.isa = laser, X.P1=X1, X.isa=printerE 1 C C C C C A ) 3. Constraint Grammar Rule NC(w, ` h b ́ ) → Noun(w1, ` h 1 b 1 ́ ), Noun(w2, ` h 2 b 2 ́ ): Φc(h, h1, h2), Φonto(b) Φc(h, h1, h2) = {h.cat = nc, h1.cat = noun, h2.cat = noun, h.head = h1.mod, h.head = h2.head, h.nr = h2.nr} Φonto(b) returns ⟨X1.isa = laser, X.instr = X1, X.isa = printer⟩ Figure 1: Syntagmas containing elementary semantic molecules (1) and a derived semantic molecule (2); A constraint grammar rule together with the semantic composition and ontology-based interpretation constraints, Φc and Φonto (3) two types of constraints, one for semantic composition and one for semantic interpretation. The first property allows LWFG learning from a small set of examples. The last two properties make LWFGs a type of syntactic-semantic grammars. Definition 1. A semantic molecule associated with a natural language string w, is a syntactic-semantic representation, w′","= (h","b",")",", where h (head) encodes compositional information, while b (body) is the actual semantic representation of the string w. Grammar nonterminals are augmented with pairs of strings and their semantic molecules. These pairs are called syntagmas, and are denoted by σ = (w, w′",") = (w, (h","b",")",").","Examples of semantic molecules for the nouns laser and printer and the noun-noun compound laser printer are given in Figure 1. When associated with lexical items, semantic molecules are called elementary semantic molecules. When semantic molecules are built by the combina-tion of others, they are called derived semantic molecules. Formally, the semantic molecule head, h, is a one-level feature structure (i.e., values are atomic), while the semantic molecule body, b, is a logical form built as a conjunction of atomic predicates ⟨concept⟩.⟨attr⟩ = ⟨concept⟩, where variables are either concept or slot identifiers in an ontology.1 1 The body of a semantic molecule is called OntoSeR and","Muresan and Rambow (2007) formally defined LWFGs, and we present here a slight modification of their definition. Definition 2. A Lexicalized Well-Founded Grammar (LWFG) is a 7-tuple, G = ⟨Σ, Σ′",", NG, ⪰ , PG, PΣ, S⟩, where:","1. Σ is a finite set of terminal symbols.","2. Σ′","is a finite set of elementary semantic molecules corresponding to the terminal symbols.","3. NG is a finite set of nonterminal symbols. NG ∩ Σ = ∅. We denote pre(NG) ⊆ NG, the set of pre-terminals (a.k.a, parts of speech)","4. ⪰ is a partial ordering relation among nonterminals.","5. PG is the set of constraint grammar rules. A constraint grammar rule is written A(σ) → B1(σ1), . . . , Bn(σn) : Φ(σ̄), where A, Bi ∈ NG, σ̄ = (σ, σ1, ..., σn) such that σ = (w, w′","), σi = (wi, wi′","), 1 ≤ i ≤ n, w = w1 · · · wn, w′","= w′","1 ◦ · · · ◦ w′","n, and ◦ is the composition operator for semantic molecules (more details about the composition operator are given in Section 5). For brevity, we denote a rule by A → β : Φ, where A ∈ NG, β ∈ N +","G . PΣ is the set of constraint grammar rules whose left-hand side are pre-terminals, A(σ) →, A ∈ pre(NG). is a flat ontology-based semantic representation. 886 We use the notation A → σ for this grammar rules. In LWFG due to partial ordering among nonterminals we can have ordered constraint grammar rules and non-ordered constraint grammar rules (both types can be recursive or non-recursive). A grammar rule A(σ) → B1(σ1), . . . , Bn(σn) : Φ(σ̄), is an ordered rule, if for all Bi, we have A ⪰ Bi. In LWFGs, each nonterminal symbol is a left-hand side in at least one ordered non-recursive rule and the empty string cannot be derived from any nonterminal symbol.","6. S ∈ NG is the start nonterminal symbol, and ∀A ∈ NG, S ⪰ A (we use the same notation for the reflexive, transitive closure of ⪰).","The partial ordering relation ⪰ makes the set of nonterminals well-founded2",", which allows the ordering of the grammar rules, as well as the ordering of the syntagmas generated by LWFGs. This ordering allow LWFG learning from a small set of representative examples (Muresan and Rambow, 2007) (PΣ is not learned).","An example of a LWFG rule is given in Figure 1(3). Nonterminals are augmented with syntagmas. Moreover, in LWFG the semantic composition and interpretation are realized via constraints at the grammar rule level (Φ(σ̄) in Definition 2). More precisely, syntagma composition means string concatenation (w = w1w2) and semantic molecule composition ( (h b",") = (h1 b1 ) ◦ (h2 b2",")",") —- where the bodies of semantic molecules are concatenated through logical conjunction (b = (b1, b2)ν, where ν is a variable substitution ν = {X2/X, X3/X}), while the semantic molecules heads are composed through compositional constraints Φc(h, h1, h2), which are a simplified version of “path equations” (Shieber et al., 1983) (see Figure 1(3)). During LWFG learning, compositional constraints Φc are learned together with the grammar rules. Semantic interpretation, which is ontology-based in LWFG, is also encoded as constraints at the grammar rule level — Φonto — providing access to meaning during parsing. Φonto(b) constraints are applied to the body of the semantic molecule corresponding to the syn-2 ⪰ should not be confused with information ordering de-","rived from flat feature structures tagma associated with the left-hand side nonterminal. The ontology-based constraints are not learned; rather, Φonto is a general predicate that succeed or fail as a result of querying an ontology — when it succeeds, it instantiates the variables of the semantic representation with concepts/slots in the ontology (see the example in Figure 1(3)). 2.1 Derivation in LWFG The derivation in LWFG is called ground syntagma derivation, and it can be seen as the bottom up counterpart of the usual derivation. Given a LWFG, G, the ground syntagma derivation relation,","∗G","⇒, is defined as: A→σ","A∗G","⇒σ (if σ =","(w, w′","), w ∈ Σ, w′","∈ Σ′",", i.e., A ∈ pre(NG, ), and Bi∗G","⇒σi, i=1,...,n, A(σ)→B1(σ1),...,Bn(σn): Φ(σ̄)","A∗G","⇒σ .","The set of all syntagmas generated by a gram-","mar G is Lσ(G) = {σ|σ = (w, w′","), w ∈","Σ+ , ∃A ∈ NG, A","∗G","⇒ σ}. Given a LWFG G, Eσ ⊆ Lσ(G) is called a sublanguage of G. Extending the notation, given a LWFG G, the set of syntagmas generated by a rule (A → β : Φ) ∈ PG is Lσ(A → β : Φ) = {σ|σ = (w, w′","), w ∈ Σ+ , (A → β : Φ) ∗G","⇒ σ}, where (A → β : Φ) ∗G ⇒ σ denotes the ground derivation A","∗G","⇒ σ obtained","using the rule A → β : Φ in the last derivation","step."]},{"title":"3 LWFG Parsing as Deduction","paragraphs":["Following Shieber (1995), we present the Lexicalized Well-Founded Grammar parser as a deductive proof system in Table 1. The items of the logic are of the form [i, j, σij, A → α • βΦA","], where A → αβ : ΦA","is a grammar rule, ΦA","— the constraints corresponding to the grammar rule whose left-hand side nonterminal is A— can be true, • shows how much of the right-hand side of the rule has been recognized so far, i points to the parent node where the rule was invoked, and j points to the position in the input that the recogni-tion has reached. We use the following notations: σR ij = (wR","ij, (hR ij bR ij ) ) are syntagmas corresponding to the partially parsed right-hand side of a rule; σL ij = (wL","ij, (hL ij bL ij ) ) are ground-derived syntagmas (i.e., they are augmenting the left-hand side non-887","Item form [i, j, σij, A → α • βΦA ] 1 ≤ i, j ≤ n + 1, A ∈ NG, αβ ∈ N∗","G","the ΦA constraint can be true","Axioms [i, i + 1, σL ii+1, Bi → •] 1 ≤ i ≤ n, Bi ∈ pre(NG), Bi → σL","ii+1 ∈ PΣ","Goals [i, j, σL","ij, A → αΦA","•] 1 ≤ i, j ≤ n + 1, A ∈ NG, α ∈ N+","G Inference Rules Prediction","[i,j,σL","ij,B→βΦB","•]","[i,i,σR","ii,A→•BγΦA","] ⟨A → Bγ : ΦA ⟩ (A → Bγ : ΦA",") ∈ PG","σR","ii = σ∅ (i.e., wR","ii = ε, bR","ii = true and hR","ii = ∅) Completion","[i,j,σR","ij,A→α • B γ ΦA","] [j,k,σL jk,B→β ΦB","•]","[i,k,σR","ik,A→α B • γ ΦA","] σR","ik = σR","ij ◦ σL","jk, where","wR","ik = wR","ijwL","jk, bR","ik = bR","ijbL","jk, hR","ik = hR","ij ∪ hL","jk Constraint","[i,j,σR","ij,A→α•ΦA ]","[i,j,σL","ij,A→αΦA •] ⟨ΦA","is satisfiable ⟩ σL","ij = φ(σR","ij) Table 1: LWFG parsing as deductive system","terminal of a LWFG rule). The goal items are","of the form [i, j, σL ij, A → αΦA","•], where σL","ij is ground-derived from the rule A → α : ΦA",".","Compared to the deductive system in (Shieber et al., 1995), the LWFG parser has the following characteristics: each item is augmented with a syntagma; the Constraint rule is a new inference rule, and the goal items are associated to every nonterminal in the grammar, not only to the start symbol (i.e., LWFG parser is a robust parser). The Constraint inference rule is the only one that obtains an inactive edge3",", from an active edge by executing the grammar constraint ΦA","(the • is shifted across the constraint). By applying the Constraint rule as the last inference rule we obtain the ground-derived syntagmas σL","ij. Thus, the goal items are obtained only after the Constraint rule is applied. During this inference rule we have that σL ij = φ(σR","ij ), where φ is defined by: wL","ij = wR","ij,","bL","ij = bR","ijνij, and hL","ij = φ(hR","ij). The substitution","νij and the function φ are implicitly contained in","the grammar constraint ΦA","c (hL","ij, hR","ij) (see Section 5 for details)","Definition 3 (Robust parsing provability). Robust","parsing provability corresponds to reaching the","goal item: ⊢rp A(σL","ij) iff [i, j, σL","ij, A → αΦA","•].","Thus, we can notice that the ground syntagma derivation is equivalent to robust parsing provability, i.e., A ∗G ⇒ σ iff G ⊢rp A(σ). 3","We use Kay’s terminology: items are edges, where the axioms and goals are inactive edges having • at the end, while the rest are active edges (Kay, 1986)."]},{"title":"4 Learning LWFGs","paragraphs":["The theoretical learning model for LWFG induction, Grammar Approximation by Representative Sublanguage (GARS), together with a learnability theorem was introduced in (Muresan and Rambow, 2007). LWFG’s learning framework characterizes the “importance” of substructures in the model not simply by frequency, but rather linguistically, by defining a notion of “representative examples” that drives the acquisition process. Informally, representative examples are “building blocks” from which larger structures can be in-ferred via reference to a larger generalization corpus referred to as representative sublanguage in (Muresan and Rambow, 2007). The GARS model uses a polynomial algorithm for LWFG learning that take advantage of the building blocks nature of representative examples.","The LWFG induction algorithm belongs to the class of Inductive Logic Programming methods (ILP), based on entailment (Muggleton, 1995; Dzeroski, 2007). At each step a new constraint grammar rule is learned from the current representative example, σ. Then this rule is added to the grammar rule set. The process continues until all the representative examples are covered. We describe below the process of learning a grammar rule from the current representative example:","1. Most Specific Grammar Rule Generation. In the first step, the most specific grammar rule is generated from the current representative example σ. The category annotated 888 STEP 1 (Most Specific Grammar Rule Generation) STEP 2 (Grammar Rule Generalization) (laser printer, CANDIDATE GRAMMAR RULES laser printer Performance CriteriaBEST RULE ((laser printer) manual) (desktop (laser printer)) K ! Background Knowledge Lexicon (laser, ) Previously learned grammar rules cat nc ) (printer, )","σ =(w,( h b ) ) - Current representative example","a",")","c","h","u","n","k","s","= { [NA(las e r),","Noun(las","e","r)],","[NC","(prin","t","er),Noun(prin","ter)]","}","r","g","1","NC","→","N o u n N o u n",":","Φ","c","4","(s","core=1)","r","g","2","NC","→","N A N o u n : Φ","c","5","(s","core=2)","b)","r:","NC","(","w,(","h","b)",") → N o u n ( w","1",",(","h","1","b","1",")",")","N o u n ( w 2 ,(","h","2","b","2 ) ) :","Φ","c","4","(","h",",","h","1",",h","2",")","Φ","c","4","(","h",",","h","1",",h","2",")=","{","h.cat","=","nc,","h","1",".cat","= nou n , h 2 .cat","=","nou","n",", E σ - Representative Sublanguage","NC","→","N A N C : Φ","c","7","r","g","4","NC","→ N A N C : Φ","c","7","(s","core=3)r","g","3","NC","→ N o u n N C",":","Φ","c","6","(s","core=2) N o u n → cat noun head X","1 mod X","2 ⟨X 1 .isa = laser,X","2",".Y = X","1","⟩ cat noun ⟨X 3 .isa = printer⟩","NA","→ N o u n : Φ c","1","⟨B.isa = laser,A.P 1 = B,A.isa = printer⟩","NA","→ N A N A : Φ","c","2","NC","→ N o u n : Φ c 3 nr sg head A","head X 3nr sg N o u n →","h.head","=","h","1",".mod,","h",".head = h 2 .head,","h",".nr","=","h","2",".nr","} Figure 2: Example of Grammar Rule Learning in the representative example gives the left-hand-side nonterminal, while a robust parser returns the minimum number of chunks covering the representative example. The categories of the chunks give the nonterminals of the right-hand side of the most specific rule. For example, in Figure 2, given the representative example laser printer annotated with its semantic molecule, and the background knowledge containing the already learned rules N A → N oun : Φc1, N A → N A N A : Φc2, N C → N oun : Φc3 the robust parser generates the chunks corresponding to the noun laser and the noun printer: [NA(laser),Noun(laser)] and [NC(printer),Noun(printer)], respectively. The most specific rule is N C → N oun N oun : Φc4, where the left-hand side nonterminal is given by the category of the representative example, in this case nc. Compositional constraints Φc4 are learned as well. In section 5 we give the algorithm for learning these constraints, and several properties and principles that are needed in order for these constraints to be learnable.","2. Grammar Rule Generalization. In the second step, this most specific rule is generalized, obtaining a set of candidate grammar rules (the generalization step is the in-verse of the derivation step used to define the complete grammar lattice search space in (Muresan and Rambow, 2007)). The performance criterion in choosing the best grammar rule among these candidate hypotheses is the number of examples in the representative sublanguage Eσ (generalization corpus) that can be parsed using the candidate grammar rule, rgi in the last ground derivation step, together with the previous learned rules, i.e., |Eσ ∩Lσ(rgi)|. In Figure 2 given the representative sublanguage Eσ={ laser printer, laser printer manual, desktop laser printer} the learner will generalize to the recursive rule N C → N A N C : Φ7, since only this rule can parse all the examples in Eσ."]},{"title":"5 Learnable Composition Constraints","paragraphs":["In LWFG, the semantic structures are composed by constraint solving, rather than functional application (with lambda expressions and lambda reduction). This section presents the properties and principles that guarantee the learnability of the compositional constraints,Φc, and presents an algorithm to generate these constraints from examples, which is a key result for LWFG learnability.","The information for semantic composition is encoded in the head of semantic molecules. There are three types of attributes that belong to the semantic molecule head h: category attributes Ac","h,","variable attributes Av h, and feature attributes A f h.","Thus, Ah = Ac h ∪ Av","h ∪ Af","h and Ac","h, Av","h, Af","h are","pairwise disjoint. For example, in Figure 1 for the","noun-noun compound laser printer, we have that","Ac","h = {cat}, A f h = {nr}, and Av","h = {head},","while for the noun laser we have that Ac","h1 = {cat}, A","f","h1 = ∅, and Av","h1 = {head, mod} (nouns can be modifiers of other nouns, so their representation is similar to that of an adjective).","We describe in turn each of these types of attributes and their corresponding principles. All principles, except the first and the last mirror principles in other constraint-based linguistic for-malisms, such as HPSG (Pollard and Sag, 1994).","The category attributes Ac","h are state attributes, and their value set gives the category of the semantic molecule. There is one attribute, cat ∈ Ac","h, which is mandatory and whose value is the name of the category (e.g., h.cat = nc in Figure 889 1). The category of a semantic molecule can be given by: 1) the cat attribute alone, or 2) the cat attribute together with other state attributes in Ac","h which are syntactic-semantic markers. Principle 1 (Category Name Principle). The category name h.cat of a syntagma σ = (w, (h b",")",") is","the same as the grammar nonterminal augmented","with syntagma σ.","When learning a LWFG rule from an example σ, the above principle allows us to determine the nonterminal in the left-hand side of the grammar rule. For example, when learning the LWFG rule from the syntagma corresponding to laser printer in Figure 2, the nonterminal in the left-hand side of the LWFG rule is N C since h.cat = nc.","The variable attributes Av","h are attributes whose values are logical variables and represent the semantic valence of the molecule, which allows the binding of the semantic representations. These logical variables appear in the semantic molecule body as well. For example, in Figure 1(2) for the noun-noun compound laser printer, the value of the variable attribute head ∈ Av","h is a variable X, which appears also in the body of the semantic molecule ⟨X1.isa = laser, X.P1 = X1, X.isa = printer⟩. It can be noticed that the semantic molecule body contains other variables as well (X1, P1). However, only the variables present in the semantic molecule head as well (X) will participate in further composition. Principle 2 (Semantic Representation Binding Principle). All the logical variables that the body b of a semantic molecule corresponding to a syntagma σ = (w, (h b",")","), share with other syntagmas,","are at the same time values of the variable at-","tributes (Av","h) of the semantic molecule head. There is one variable attribute, head ∈ Av","h that","represents the head of a syntagma, giving the fol-","lowing principle: Principle 3 (Semantic Head Principle). Given a syntagma σ = (w, (h b",")",") ground derived from a","grammar rule, r, there exists one and only one","syntagma σi = (wi, (hi bi",")",") corresponding to a nonterminal Bi in rule r’s right-hand side, which has the same value of the attribute head, i.e., h.head = hi.head. The feature attributes Af","h are the attributes whose values express the specific properties of the semantic molecules (e.g., number, person). Principle 4 (Feature Inheritance Principle). If σi = (wi, (hi bi ) ) is the semantic head of a ground-derived syntagma σ = (w, (h b",")","), then all feature attributes of σ inherit the values of the corresponding attributes that belong to the semantic head σi. That is, if h.head = hi.head , then h.f = hi.f , ∀f ∈ Af","h ∩ A f hi.","Besides this principle, the feature attributes are used for category agreement. The categories that enter in agreement are maximum projection categories. This linguistic knowledge about agreement is used in the form of the following principle: Principle 5 (Feature Agreement Principle). The agreeing categories and the agreement features are a-priori given based on linguistic knowledge, and are applied only at the semantic head level.","Given all the above principles, we can now for-mulate the general Composition Principle: Principle 6 (Composition Principle). A syntagma σ = (w, w′",") corresponding to the left-hand side nonterminal of a grammar rule is obtained by string concatenation (w = w1 . . . wn) and the composition of semantic molecules corresponding to the nonterminals from the rule right-hand side:","w′ =","( h b )","= (w1 · · · wn)′","= w′","1 ◦ · · · ◦ w′","n =","( h1 b1 ) ◦ · · · ◦ ( hn bn ) = ( h1 ◦ · · · ◦ hn ⟨b1, . . . , bn⟩ν )","The composition of the semantic molecule bodies is realized through conjunction after the application of a variable substitution ν. The body variable specialization substitution ν is the most general unifier (mgu) of b and b1, . . . , bn, s.t b = (b1, . . . , bn)ν. It is a particular form of the commonly used substitution (Lloyd, 2003), i.e., a finite set of the form {X1/Y1, . . . , Xm/Ym}, where X1, . . . , Xm, Y1, . . . , Ym are variables, and X1, . . . , Xm are distinct.","The composition of the semantic molecule heads is realized by a set of constraints Φc(h, h1..., hn), which is a system of equations 890 similar to “path equations” (Shieber et al., 1983; van Noord, 1993), but applied to flat feature structures:","","  hi.c = ct hi.vi = hj.vj hi.f = ct or hi.f = hj.f    where","0 ≤ i, j ≤ n, i ̸= j","c ∈ Ac h i","vi ∈ Av h i, vj ∈ Av","h","j","f ∈ Af h i, f ∈ Af","h","j","When learning a LWFG rule from a representative example σ as in Figure 2, the robust parser returns the minimum number of chunks, n, covering σ. The body variable substitution ν is fully determined by the representative example as mgu of b and b1, . . . , bn, and the compositional constraints Φc(h, h1, . . . , hn) are learned using Alg 1. For example, in Figure 2, when learning from the representative example corresponding to the string laser printer, we have that ν = {X1/B, X2/A, X3/A, Y /P1}.","In Alg 1 we use the notation σ0 = (w0, (h0 b0 ) ) to denote the representative example σ. Alg 1: Learn Constraints(σ0, σ1, . . . , σn) σi = (wi, h̀ i b i ́ ), 0 ≤ i ≤ n","Φc ← ∅","ν ← mgu(b0, (b1, . . . , bn))","foreach 0 ≤ i ≤ n ∧ c ∈ Ac","h","i do1 if hi.c = c1 then","Φc ← Φc ∪ {hi.c = c1}","foreach 0 ≤ i, j ≤ n ∧ i ̸= j ∧ X/Y ∈ ν∧2","vi ∈ Av h i ∧ vj ∈ Av","h","j do","if hi.vi = X ∧ hj.vj = Y then Φc ← Φc ∪ {hi.vi = hj.vj}","if hs.head = h0.head, 1 ≤ s ≤ n then3","foreach f ∈ Af","h","0 ∩ Af","hs do","if h0.f = c1 ∧ hs.f = c1 then Φc ← Φc ∪ {h0.f = hs.f}","if hs.cat = cs ∧ hi.cat = ci ∧ agr(cs, ci),","1 ≤ i ≤ n then foreach f ∈ agrF eatures(cs, ci) do","if hs.f = c1 ∧ hi.f = c1 then","Φc ← Φc ∪ {hs.f = hi.f}","for all other f ∈ Af","h","i, 0 ≤ i ≤ n do4","/*i.e., if we are not in case 3 */","if hi.f = c1 then","Φc ← Φc ∪ {hi.f = c1} return Φc /*i.e., Φc(h0, h1, . . . , hn) */","In the first step, the constraints corresponding to category attributes are fully determined by the values of these attributes that appear in the semantic molecule heads of σ0, . . . σn. In Figure 2, when learning the most specific rule r from the representative example laser printer, the set of constraints {h.cat = nc, h1.cat = noun, h2 = noun} ⊂ Φc4 are the constraints corresponding to category attributes. In the second step, the constraints corresponding to variable attributes are fully determined by the variables in the substitution ν that also appear as values of variable attributes hi.vi, hj.vj, where 0 ≤ i, j ≤ n and i ̸= j. In Figure 2, only {X2/A, X3/A} ⊂ ν will be used, generating the set of constraints {h.head = h1.mod, h.head = h2.head} ⊂ Φc4. In the third step, the values of the feature attributes which obey Principles 4 and 5 are generalized — agr(cs, ci) is the predicate which gives us the agreement between the categories cs and ci (e.g., the subject agrees with the verb), and agrFeatures(cs, ci) gives us the set of feature attributes that participate in agreement (e.g., nr, pers, case). In Figure 2, the set of constraints {h.nr = h2.nr} ⊂ Φc4 represents the generalization of the feature attribute values for nr, using Principle 4 . For all features attributes besides the ones that obey the above two principles, the generated constraints keep the particular values of these attributes (step 4 of Alg 1)."]},{"title":"6 Examples","paragraphs":["The LWFG formalism allows us to learn grammars for deep language understanding from examples. Instead of writing syntactic-semantic grammar by hand (both rules and constraints), we need to provide only a small set of representative examples — strings and their semantic molecules. Qualitative experiments on learning LWFGs showed that complex linguistic constructions can be learned and covered, such as complex noun phrases, relative clauses and reduced relative clauses, finite and non-finite verbal constructions (including, tense, aspect, negation, and subject-verb agreement), and raising and control constructions (Muresan and Rambow, 2007). In Figure 3 we show an example of learning a LWFG grammar for noun-noun compounds. The first four examples (1-4) are representative examples, while the last four examples are used for gener-891 A. Learning Examples: 1. (laser, 0 B B B B B @ 2 6 4cat na head A mod B 3 7 5 D","A.isa = laser, B.P 1=A E 1 C C C C C A ) 5. (laser printer manual, 0 B B B B B @ 2 6 4cat na head A mod B 3 7 5 D","C.isa = laser, D.P","1=C, D.isa=printer,A.P","2=D, A.isa=manual, B.P","3=AE 1 C C C C C A ) 2. (laser printer, 0 B B B B B @ 2 6 4cat na head A mod B 3 7 5 D","C.isa = laser, A.P 1=C, A.isa=printer, B.P","2=AE 1 C C C C C A ) 6. (desktop laser printer, 0 B B B B B @ 2 6 4cat na head A mod B 3 7 5 D","C.isa = desktop, A.P","1=C, D.isa=laser,A.P","2=D, A.isa=printer, B.P","3=AE 1 C C C C C A ) 3. (printer, 0 B B B B B @ 2 6 4cat nc nr sg head A 3 7 5 D A.isa = printerE 1 C C C C C A ) 7. (laser printer manual, 0 B B B B B @ 2 6 4cat nc nr sg head A 3 7 5 D","B.isa = laser, C.P 1=B, C.isa=printer, A.P","2=C, A.isa=manualE 1 C C C C C A ) 4. (laser printer, 0 B B B B B @ 2 6 4cat nc nr sg head A 3 7 5 D","B.isa = laser, A.P 1=B, A.isa=printerE 1 C C C C C A ) 8. (desktop laser printer, 0 B B B B B @ 2 6 4cat nc nr sg head A 3 7 5 D","B.isa = desktop, A.P 1=B, C.isa=laser, A.P","2=C, A.isa=printerE 1 C C C C C A ) B. Learned LWFG Rules: NA(w, “ h b ” ) → Noun(w","1, “ h 1 b 1","”",") : Φc","1(h, h","1) , where Φc","1(h, h","1) = 8 > > < > > : h.cat = na h 1.cat = noun h.head = h","1.head h.mod = h","1.mod 9 > > = > > ; NA(w, “ h b ” ) → NA(w","1, “ h 1 b 1 ” ), NA(w","2, “ h 2 b 2","”",") : Φc","2(h, h 1, h 2) where Φc","2(h, h","1, h","2) = 8 > > > > > < > > > > > : h.cat = na h 1.cat = na h 2.cat = na h.head = h","1.mod h.head = h","2.head h.mod = h","2.mod 9 > > > > > = > > > > > ; NC(w, “ h b ”",") → Noun(w 1, “ h 1 b 1","”",") : Φc","3(h, h","1) , where Φc","3(h, h","1) = 8 > > < > > : h.cat = nc h 1.cat = noun h.head = h","1.head h.nr = h","1.nr 9 > > = > > ; NC(w, “ h b ”",") → NA(w 1, “ h 1 b 1 ” ), NC(w","2, “ h 2 b 2","”",") : Φc","4(h, h 1, h 2) where Φc","4(h, h","1, h","2) = 8 > > > > > < > > > > > : h.cat = nc h 1.cat = na h 2.cat = nc h.head = h","1.mod h.head = h","2.head h.nr = h","2.nr 9 > > > > > = > > > > > ; Figure 3: Learning LWFG Rules for Noun-Noun Compounds alization (5-8). The learned grammar rules, in-cluding the learned composition constraints are also shown. The first two LWFG rules ground derive syntagmas for noun adjuncts, while the last two rules ground derive syntagmas for noun compounds. For example, ”desktop laser printer” can be either a fully-formed noun compound (category nc), or it can be further combined with the noun ”invoice” to obtain ”desktop laser printer in-voice”, case in which it is a noun adjunct (category na). The learned rule for noun adjuncts is both left and right recursive, accounting for both left and right-branching noun compounds. Even though we can obtain overgeneralization in syntax, the ontology-based interpretation constraint at the rule level will prune some erroneous parses. Preliminary results in the medical domain show that Φonto can help remove erroneous parses even when using just a weak ontological model (semantic roles of verbs, prepositions, attributes of adjectives and adverbs, but no synonymy, or hierarchy of concepts or roles). However, more experiments need to be run for reporting quantitative results."]},{"title":"7 Conclusions","paragraphs":["We have presented the properties and principles that the semantic representation integrated in LWFG requires so that the semantic compositional constraints are learnable from examples. These properties together with Alg 1 give a theoretical result that in conjunction with the learnability result of Muresan and Rambow (2007) show that LWFG is a learnable constraint-based grammar formalism that can be used for deep language understanding. Instead of writing grammar rules and constraints by hand, one needs to provide only a small set of annotated examples.4","4","The author acknowledges the support of the NSF (SGER grant IIS-0838801). Any opinions, findings, or conclusions are those of the author, and do not necessarily reflect the views of the funding organization. 892"]},{"title":"References","paragraphs":["Dzeroski, Saso. 2007. Inductive logic programming in a nutshell. In Getoor, Lise and Ben Taskar, editors, Introduction to Statistical Relational Learning. The MIT Press.","Ge, Ruifang and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of CoNLL-2005.","Kay, M. 1986. Algorithm schemata and data structures in syntactic processing. In Readings in natural language processing, pages 35–70. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.","Lloyd, John W. 2003. Logic for Learning: Learning Comprehensible Theories from Structured Data. Springer, Cognitive Technologies Series.","Muggleton, Stephen. 1995. Inverse Entailment and Progol. New Generation Computing, Special Issue on Inductive Logic Programming, 13(3-4):245–286.","Muresan, Smaranda and Owen Rambow. 2007. Grammar approximation by representative sublanguage: A new model for language learning. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).","Muresan, Smaranda. 2008. Learning to map text to graph-based meaning representations via grammar induction. In Coling 2008: Proceedings of the 3rd Textgraphs workshop on Graph-based Algorithms for Natural Language Processing, pages 9–16, Manchester, UK, August. Coling 2008 Organizing Committee.","Neumann, Günter and Gertjan van Noord. 1994. Reversibility and self-monitoring in natural language generation. In Strzalkowski, Tomek, editor, Reversible Grammar in Natural Language Processing, pages 59–96. Kluwer Academic Publishers, Boston.","Pollard, Carl and Ivan Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago, Illinois.","Shieber, Stuart, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson. 1983. The formalism and implementation of PATR-II. In Grosz, Barbara J. and Mark Stickel, editors, Re-search on Interactive Acquisition and Use of Knowledge, pages 39–79. SRI International, Menlo Park, CA, November.","Shieber, Stuart, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1-2):3– 36.","Steedman, Mark. 1996. Surface Structure and Interpretation. The MIT Press.","van Noord, Gertjan. 1993. Reversibility in Natural Language Processing. Ph.D. thesis, University of Utrecht.","Wong, Yuk Wah and Raymond Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-2007).","Zettlemoyer, Luke S. and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI-05.","Zettlemoyer, Luke and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Association for Computational Linguistics (ACL’09). 893"]}]}