{"sections":[{"title":"","paragraphs":["Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 632–643, Dublin, Ireland, August 23-29 2014. "]},{"title":"Interpretation of Chinese Discourse Connectives for Explicit Discourse Relation Recognition   Hen-Hsen Huang, Tai-Wei Chang, Huan-Yuan Chen, and Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan {hhhuang, twchang}@nlg.csie.ntu.edu.tw; {b00902057, hhchen}@ntu.edu.tw    Abstract","paragraphs":["This paper addresses the specific features of Chinese discourse connectives, including types (word-pair and single-word), linking directions (forward and backward linking), positions and ambiguous degrees, and discusses how they affect the discourse relation recognition. A semi-supervised learning method is proposed to learn the probability distributions of discourse functions of connectives from a small labeled dataset and a big unlabeled dataset. The statistics learned from the dataset demonstrates some interesting linguistic phenomena such as connective synonyms sharing similar distributions, multiple discourse functions of connectives, and couple-linking elements providing strong clues for discourse relation resolution."]},{"title":"1 Introduction","paragraphs":["Discourse relation labeling determines how two discourse units cohere to each other. A discourse unit may be a clause, a sentence, or a group of sentences. The labeled relation has many potential applica-tions. Coherence is considered as a metric to evaluate the essay writing by essay scorer (Lin et al., 2011). Discourse relations are used to order sentences in an event in a summarization system (Derczynski and Gaizauskas, 2013). Sentiment transition of two clausal arguments is identified based on their discourse relation in sentiment analysis (Hutchinson, 2004; Zhou et al., 2011; Wang et al., 2012; Huang et al., 2013).","The pioneer research of discourse has been established by Hobbs (1985), Polanyi (1988), Hovy and Maier (1992), and Asher and Lascarides (1995). Various discourse relation types have been defined in the frameworks such as Sanders et al. (1992), Hovy and Maier (1992), RST-DT (Carlson et al., 2002), Wolf and Gibson (2005), and PDTB (Prasad et al., 2008). Temporal, Contingency, Comparison, and Expansion, the four classes on the top level of PDTB sense hierarchy, are common used in the discourse relation labeling tasks. When two arguments are temporally related, they form a Temporal relation. The Contingency relation talks about the situation that the event in one argument casually affects the event in the other argument. Comparison is used to show the difference between two arguments. The last one relation, Expansion, is the most common. An Expansion relation either expands the information for one argument in the other one or continues the narrative flow.","In the recent years, discourse relation recognition has been studied for different languages (Afantenos et al., 2012, Cartoni et al., 2013). In explicit English discourse relation labeling tasks, the accuracy of the approach using just the connectives is already quite high, 93.67%, and incorporating the syntactic features raises performance to 94.15% (Pitler and Nenkova, 2009). In our previous work, we investigate Chinese intra-sentential relation detection and show an accuracy of 81.63% and an F-score of 71.11% in the two-way classification (Contingency vs. Comparison relations) when connectives are This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 632 introduced as features (Huang and Chen, 2012a). We also report an accuracy of 27.10% and an F-score of 24.27% in the four-way inter-sentential relation classification when only connectives are used (Huang and Chen, 2011). Sporleder and Lascarides (2008) point out some English connectives are often ambiguous between multiple discourse relations or between discourse and non-discourse usage, and Roze et al. (2010) report the ambiguity of French connectives. This issue also occurs in Chinese. Zhou et al. (2012) propose a framework to identify the ambiguous Chinese discourse connectives, and report an F-score of 74.81% in the four-way classification at the intra-sentential level.","The above discourse relation labeling tasks are done on the datasets of different size for different languages at the intra-/inter-sentential levels, thus the results cannot be compared directly. However, these works show a tendency: discourse connectives are useful clues for explicit discourse relation recognition, and the uses of Chinese connectives in discourse relation labeling are more challenging than those of English connectives. In comparison with English, the connectives in Chinese are more and their parts of speech are diverse. There are 100 English explicit connectives annotated in the PDTB 2.0. In Chinese, the linguists report a list of 808 discourse connectives (Cheng and Tian, 1989; Cheng, 2006). In addition, the Chinese discourse connectives have a variety of parts-of-speech. For example, 假設 (jiǎ shè, suppose) is a verb and listed as a discourse connective of the Contingency relation.","The following examples address some specific features of Chinese discourse connectives. On the one hand, the two words, “雖然” (suī rán, although) and “但是” (dàn shì, but), which form a word-pair connective, appear in the two discourse units shown in (S1), respectively. These two units demonstrate a Comparison relation. On the other hand, “雖然” (suī rán, although) and “但是” (dàn shì, but) can appear individually as single-word connectives shown in (S2)-(S6). The two discourse units have different discourse relations when the single-word connectives appear at different positions, i.e., (S2): Comparison, (S3): Comparison, (S4): Expansion, (S5): Comparison, and (S6): Expansion. Furthermore, the short word “而” (ér) can be an individual connective, which is interpreted as “而且” (and), “然而” (but), or “因而” (thus), and serves as functions of Expansion, Comparison, and Contingency, respectively. In addition, it can be linked with “雖然” (suī rán, although) and “因為” (yīn wèi, because) to be word-pair connectives, which are interpreted as Comparison and Contingency functions in (S7) and (S8), respectively. These examples demonstrate word-pair connectives composed of a same word and other words may have different discourse functions, so does the same single-word connective at different positions.","","(S1) 雖然湯姆很聰明,但是他並不用功。(Although Tom is smart, he doesn’t study hard.)","(S2) 雖然湯姆很聰明,他並不用功。(Although Tom is smart, he doesn’t study hard.)","(S3) 他流很多汗,雖然才走幾哩路。(He sweated a lot, although he went only a few miles.)","(S4) 我會好好閱讀,雖然我真的覺得蜘蛛好可怕。(I'll read, even if I really feel spider terrible.)","(S5) 湯姆很聰明,但是他並不用功。(Tom is smart, but he doesn’t study hard.)","(S6) 但是在巴黎,他放棄了學醫。(But in Paris, he gave up studying medicine.)","(S7) 雖然你不說,而我一聞就知道。(Although you did not say, I knew that smell.)","(S8) 他因為晚回家,而被媽媽罵了。(Because he came home late, he was scolded by his mother.)","","In this paper, we investigate special features of Chinese discourse connectives and apply the results to discourse relation labeling. A semi-supervised learning algorithm is proposed to estimate the probability distribution of the discourse functions of each connective. We address the issue of ambiguity between multiple discourse relations of Chinese connectives. The ambiguity between discourse and non-discourse usages is not our focus in this paper. This paper is organized as follows. Section 2 analyses the types of Chinese connectives and their forward/backward linking properties. Section 3 presents a semi-supervised method to deal with the probability distributions of discourse functions of Chinese connectives and discourse relation labeling. The experimental results are shown and discussed. In Section 4, we further introduce the discourse relation labeler to annotate 302,293 unlabeled sentences and analyze the linguistic phenomena of discourse connectives. We conclude this work in Section 5. 633"]},{"title":"2 Types of Discourse Connectives","paragraphs":["From the surface form, there are three kinds of linking elements in Chinese (Li and Thompson, 1981): forward-linking elements, backward-linking elements, and couple-linking elements. Discourse connectives are such kinds of linking elements. A discourse unit containing a forward-linking (backward-linking) element is linked with its next (previous) discourse unit. A couple-linking element is a pair of words that exist in two discourse units (Chen, 1994).","Figure 1 shows connectives and their linking direction. The word-pair connective “雖然...但是” (suī rán...dàn shì, although...but) in (S1) is a couple-linking element. A single-word connective may function as a forward-linking element and/or a backward-linking element. It may be a word appearing in a word-pair connective, e.g., “雖然” (suī rán, although), or a word existing individually, e.g., “以及” (yǐ jí, and). A single-word connective which is the first (the second) word of a word-pair connective may function as a forward-linking (backward-linking) element. The single-word connective “雖然” (suī rán, although) in (S2) is a typical example. It keeps the major discourse function, i.e., Comparison, of the word-pair connective that it belongs when it appears in the first discourse unit. In contrast, it may become ambiguous when its position is reversed from the first to the second (i.e., S3 and S4). It may link to the previous or the next discourse units. S5 and S6 have the similar behaviors. The single-word “但是” (dàn shì, but) in (S5) shows a backward-linking. In (S6), it is shifted to the first position and becomes ambiguous. It may be linked to the previous, or to the next discourse units. The correct interpretation depends on the context. These phenomena show a single-word connective may have different senses when it is not at its original position.",""," ","","Figure 1: Examples for forward linkging and backward linking.","","In this study, we collect 808 discourse connectives based on Cheng and Tian (1989), Cheng (2006),","and Lu (2007). The discourse connective lexicon contains 319 single-word and 489 word-pair connec-","tives. Initially, each connective is associated with only one discourse function manually by linguists. 634 For example, the word-pair connective, “雖然...但是” (suī rán...dàn shì, although...but), is assigned a Comparison function. The assignment is one-to-one mapping, thus it cannot capture the complete discourse functions of Chinese connectives. Table 1 shows an overview of the discourse connective lexicon. In this lexicon, Expansion is the majority, and Comparison is the minority. The percentages of Contingency and Expansion are close. Temporal is the third largest discourse function. Intuitively, the discourse connective lexicon cannot cover all their senses. To learn the probability distribution of the discourse functions of a connective needs a large-scale discourse corpus. Compared with RST-DT (Carlson et al., 2002) and PDTB (Prasad et al., 2008), Chinese discourse corpora are not publicly available (Zhou and Xue, 2012; Huang and Chen, 2012b).  Discourse Function Number of Connectives Examples of Single-Word and Word-Pair Discourse Connectives Temporal 151 (18.69%) 接著 (jiē zhe, then), 最初...現在 (zuì chū...xiàn zài, first...now) Contingency 261 (32.30%) 因為 (yīn wèi, because), 如...則 (rú...zé, if ... then) Comparison 87 (10.77%) 即使 (jí shǐ, even if), 儘管...但 (jǐn guǎn...dàn, although...but) Expansion 309 (38.24%) 另外 (lìng wài, besides), 不僅...而且 (bù jǐn...ér qiě, not only...but also)","Table 1: A Chinese discourse connective lexicon."]},{"title":"3 Learning Discourse Functions of Connectives","paragraphs":["This section proposes a semi-supervised learning method to learn the interpretation of discourse connectives from an incomplete and sparse dataset. 3.1 A Semi-Supervised Learning Algorithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1...c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. ","Length. This feature includes the word counts of ds1 and ds2.","Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma.","Words. The bags of words in ds1 and ds2 are considered.","Hypernym. The bags of hypernyms of the words in ds1 and ds2 are considered. A Chinese thesaurus, Tongyici Cilin1",", is consulted. The categorization scheme at the fourth level is adopted.","Shared Word. The number of words shared in ds1 and ds2 is considered as a feature.","Collocated Word. Collocated words are word pairs mined from the training set. The first and the second words of a pair come from ds1 and ds2, respectively.","POS. The bags of parts of speech in ds1 and ds2 are considered.","Polarity. Polarity and discourse relation may be related (Huang et al., 2013; Zhou, et al., 2011). For example, a Comparison relation implies its two discourse units are contrasting, and some contrasts are presented with different polarities. We estimate the polarity of ds1 and ds2 by a lexicon-based approach. The polarity score and the existence of negation are taken as features.","Discourse Connective. A discourse connective c is represented as a probability distribution of discourse functions denoted by a quadruple (P(c,temporal), P(c,contingency), P(c,comparison), P(c,expansion)), where P(c,temporal), P(c,contingency), P(c,comparison), and P(c,expansion) indicate the probabilities of the four discourse functions of c, such that P(c,temporal)+P(c,contingency)+P(c,comparison)+P(c,expansion)=1. Section 3.3 shows how we as-sign the probabilities to each connective in different experimental settings.","Position. The linguistic phenomena discussed in Section 2 show a single-word connective at different position may play different discourse function. Thus, the position of c is considered as a feature. 1 http://ir.hit.edu.cn/ 635 Because the number of Chinese connectives is large (e.g., 808 Chinese connectives in our lexicon)","and the large-scale labeled Chinese discourse corpus is not available, how to learn the probability dis-","tribution is a challenging issue. This paper proposes a semi-supervised learning method as follows. Its","pseudo code is shown in Algorithm 1.  (1) Train a 4-way discourse relation classifier drc with the training set and LIBSVM (Chang and","Lin, 2011). (2) Initialize probability distributions of unknown connectives in the test set (see experiments). (3) Use drc to label all the instances in the test set. (4) Compute the new probability distribution of discourse functions of each connective based on","the labeled results in the current run. Maximum likelihood estimation is adopted. (5) Repeat (3) and (4) until the number of label changes between two successive runs is below 1%. ","Algorithm 1. Probability Estimation for the Discourse Functions of Connectives","Input:","D={Temporal, Contingency, Comparison, Expansion}: a set of discourse relations and discourse functions for argument pairs and discourse connectives,","C={c1, c2, ..., cn}: a set of n discourse connectives,","S={s1, s2, ..., sp}: a set of p labeled argument-pairs [sa1, sa2] containing connective cCSC, each with a label dD, where CS is a set of connectives appearing in S,","T={t1, t2, ..., tq}: a set of q unlabeled argument-pairs [ta1, ta2] containing connective cCTC, where CT is a set of connectives appearing in T.","Output:","Q={q1, q2, ..., qn}: a probability distribution qi for connective ciC.","Method:","1. Initialization 1) Train a classifier drc using S. 2) Initialize the probability distribution with equal weight, (0.25, 0.25, 0.25, 0.25), for connec-","tive c  CT-CS, and build Q(0)",". 3) i ← 0","2. Relation labeling","For each t  T, estimate the probabilities of four discourse relations, P(t,temporal), P(t,contingency),","P(t.comparison), and P(t.expansion), using the classifier drc with Q(i)",".","3. Updating the probability distribution 1) For each c  C, compute the average probability of each discourse relation among the argu-","ment-pairs containing c in T:","P(c,tempora)l ← Average of P(t,temporal) for all t containing c in T.","P(c,contingency) ← Average of P(t,contingency) for all t containing c in T.","P(c,comparison) ← Average of P(t,comparison) for all t containing c in T.","P(c,expansion) ← Average of P(t,expansion) for all t containing c in T. 2) Form a new Q(i+1)"," 3) i ← i+1","4. Repeat steps 2-3 until the ratio of the number of label changes by previous and current runs is less than 1%.","5. Q ← Q(i)","  3.2 Experimental Setup For the corpus study of discourse connectives and discourse relations, we refer to a public available Chinese Web POS tagged corpus (Yu et al., 2012). This Chinese POS-tagged corpus is developed based on the ClueWeb09 dataset (CMU, 2009), where Chinese material is the second largest. To capture the discourse functions of individual connectives more accurately, the following three criteria are used to sample sentences:  1. A sentence should contain only two clauses. 2. A sentence should contain exact one discourse connective. 636 3. The lengths of both clauses in a sentence are no more than 20 Chinese characters. ","Total 7,601 sentences composed of two discourse units linked by a connective are sampled from a public available Chinese Web POS tagged corpus (Yu et al., 2012). Each sentence is annotated with a most likely discourse relation selected from {Comparison, Contingency, Comparison, Expansion} by three annotators guided by an instruction manual. The majority is taken as the ground truth. A mentor is involved to make a final decision for the tie conditions. The inter-agreement among the annotators is 0.41 in Fleiss’ Kappa values, which is a moderate agreement. The discourse category with the lowest inter-annotation agreement is Temporal, which annotators usually confuse with Expansion. It shows the difficulty to distinguish Temporal and Expansion even by human. Table 2 shows the statistics of the corpus. More than 50% of pairs are annotated with Expansion relation. The second largest group is Contingency relation. The percentages of Temporal and Comparison relations are near. Only 359 connectives appear in the corpus. That reflects the incompleteness issue."," Discourse Relation # Instances Percentage Temporal 846 11.13% Contingency 1,594 20.97% Comparison 926 12.18% Expansion 4,235 55.72%","Table 2: Statistics of the experimental discourse corpus.","","This Chinese discourse corpus is used for training and testing. We set up the experiments to simulate the scenario of estimating the probability distributions of discourse functions of the unknown connectives based on the information in the training set. We evaluate the experimental results by 5-fold cross-validation. To ensure the discourse connectives appearing in the test set are mutual exclusive of those connectives in the training set, we split the discourse connectives into 5 mutual exclusive sets and split all the 7,601 sentences into 5 folds according to the 5 sets of discourse connectives.","The kernel of our SVM classifier is the radial basis function. The two parameters, cost c and gamma g, are optimized by the grid-search algorithm within the range c  {2 -5 , 2 -3 , 2 -1 , ..., 2 15 } and g  {2","-15 ,","2-13",", 2-11",", ..., 23","}. 3.3 Results and Discussions To demonstrate the performance of our proposed semi-supervised learning methods, the following five models are experimented and compared. ","M0: Label the relation between two discourse units linked by a connective c based on the c’s discourse function defined in the connective lexicon. M0 is considered as a baseline model.","M1: Train a 4-way discourse relation classifier drc with the training set, then initialize the function probability distributions of the unknown connectives to (0.25, 0.25, 0.25, 0.25), and finally label all the pairs of discourse units by t