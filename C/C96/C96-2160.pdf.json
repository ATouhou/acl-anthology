{"sections":[{"title":"Computing Phrasal-signs in HPSG prior to Parsing Kentaro","paragraphs":["Torisawa and aun'ichi Tsujii","l)ci)artmenl; of Informat, ion S(:icncc, University of Tokyo, ltongo 7-3-1, Bunkyo-ku, Tokyo, 113, Japan {torisawa,tsujii}0is.o.u-'cokyo.ac.jp"]},{"title":"Abstract","paragraphs":["This t)ai)er deseril)es techniques to compile lexical entries in IIPSG (Pollard and Sag, 1.987; Poll;ml and Sag, 1993)-style grammar into a set of finite state automata. The states in automat~L are possible signs derived fl'om h',xical entries and contaili information"]},{"title":"raised","paragraphs":["fl'om the lexical entries. The automatt~ are augmented with feature structures use(l by a"]},{"title":"partial unification routine","paragraphs":["and de-"]},{"title":"layed/frozen","paragraphs":["definite el;rose programs."]},{"title":"1 introduction","paragraphs":["Our aim is to 1)uild an e, fli(:ient and robust ]tl)SG - based parser. IIPSG has 1)(;eu re, gar(led as a sophisticated but fl:;tgile and inettieient ff~mwwork. However, its"]},{"title":"principle-based","paragraphs":["al'(:hitecture enables a parser to handle real world texts only by giving concise core grammar, including principles and templates for lexicM entries, default lexical entries(Horiguchi et al., 1995). The architecture is different fl'om those of eonvelltional unification-l)t~sed ff)rmMisms which require hundreds of CFG skelet;ons to t)arse real world texl;s.","However, tiles(', design prin(:il)les of llI'SG have draw-backs in parsing cost. That is, signs/feature structures corresponding ~o non-termillal symbols ill CFG become"]},{"title":"vi,sible, only","paragraphs":["after applying t)l'incipies alld ~t t)&rs0r has to Cl'e~4te ~(;&tlIl;C stl'IlCtllleS ()lie by one using unification. ]in addition, identity checking of non-terlninM symbols used to eliminate spurious signs must be replaced with sub-sumption checking, which flHther detcrior~tes effi('ien(:y.","Our grammar eompih',r COlnputes skeleta.l 1)~rt of possible phrasal-signs froln individual h!xical enl;l'ies prior to parsing, and generates a set of finite state automata from h'~xical entries to ;tvoid the above draw-I)acks. We call this operation Oilline raising and an automaton thus"]},{"title":"generated is","paragraphs":["called a Lexieal Entry Automaton (LA). its states corresponds to 1)art of sigl,s and each transition between stales"]},{"title":"(;orrespon(ls","paragraphs":["to at)plication of a rule schema, which is a nonqexical comt)onent of grammar.","Our parsing algorithm adopts a two-i)hased l/arsing method. Phase 1 ]iottom-up (:hart-like 1)~rsing with LAs. Ilewriting lt,ule: MOTIIEI{([1]), IIEAI)-I)TR ([81) NON-IIEAt)-I)TI~([S]) FS: sign"]},{"title":"[' ...... 't t","paragraphs":["Syll b;I I } ) C ~)a,","content[4]","scm iIKlices [3]",", ...... ~ . ]","1 ...... l-dr,, [s] '~Y\" [ .~,l,,:~,: ( ,~ rl>","S(!lll indices indices ] J","goals arg2 2 arg2 fl'(!t!z(~ Figur(' 1 : AlL e.xalnph; of a rule s(:hema. Phase 2 Computing part of feature, structures","which cannot l)e (:omputcd at (;oml)ile-tinm.","We call tile tbature structures that are represented as states in automa,t;~ mtd are COml)uted at conlpih>time Core-structures, and the fcatllre strllCtlll'es whi(;h are t;o l)e (:Olnl)ut, ed in Phase 2, Sub-structures. In l)h~sc 1 parsing, t~ (:oresi, ructme. (:orr(,spond to a state in an ] ,A. The cost of comt)uting sub-structures at Phase 2 is ininimized by Dependency Analysis mM Partial Unification.","Tile next section describes rule schcmtm~, cen-"]},{"title":"tral","paragraphs":["eompouents of the. formalism, and gives ~ definition of Definite Clause Programs. Section 3 describes how to obtain LAs h'om lexical entries and how to perform the Phase i p;~rsing. Section 4 explMns the Phase 2 Parsing algorithm. A parsing exmnple is ln'es(;iLted in Section 5. The effectiveness of our method is exeinplified with a series (117 eXl)eriments in Section 6."]},{"title":"2 Rule Sch(:nmta and Definite Claus(; l)rograms","paragraphs":["Our fonmdism has only one type of compolmnt g-tS llOll-10xic&l (;ollll)Ollellt,q of ~rallllnar, i.e., rule schemata. I An example is showll in Figure 1. A ruh' s(:henl;~ consists of the following two items.","Ihl ()Ill' cilrr(!ilt, sysLeil/~ rill(', s(;h(2mltt~ ~l'('. goltc.ri~t('.(1 froul principh,.s and rewriting rules ~m(:ording (;c) ;L SlmCifical, ion given by ~ progr~mmter. 949 rule(R) a rewriting rule without specific syntac-","tic categories; fs(R) a feature structure.","A characteristic of HPSG is in the flexibility of principles which demands complex operations, such as append or subtraction of list-value feature structures. In our formalism, those operations are treated by a Definite Clause Program. A DCP can be seen as a logic program language whose arguments are feature structures. An auxiliary term, a query to a DCP augmenting a rule schema, is embedded in a feature structure of a rule schema as the value of goals. The rule schema in the exampie has an auxiliary term, append([1], [2], [3] ).","The bottom-up application of the rule schema R is carried out as follows. First, two daughter signs are substituted to the HEAD-DTP~ position and NOR-HEAD-DTI~ position of the rewriting rule rule(R). Then, the signs are unified with the head-dtr value and the non-head-dtr value of the feature structure of the schema, fs(R). Finally, the auxiliary term for DCPs given in the schema is evaluated•","Our definition of a DCP has a more operational flavor than that given by Carpenter(Carpenter, 1992)• The definition is crucial to capture the correctness of our method. 2 Definition 1 (DCP) A definite clause program (DCP) is a finite set of feature structures, each of which has the following form."]},{"title":"goals (HI 1-13) J","paragraphs":["next-steps [ goaSs (1~o, B1 ,''', ~, [1] ) ] a where 0 <_ n and H, Bo,\" • •, B,~ are feature structures.","A feature structure of the above form corresponds to a clause in Prolog. H, B0,...,B~ corresponds to literals in Prolog. H is the head and Bo,\"', B,~ are literals in the body of a clause. Definition 2 (Execution of DCP) Execution of a DCP P for the query,"]},{"title":"C2~e~y = [ go~Ss (qo, q~,'\" qz) ]","paragraphs":["is a sequence of unification, Query U rl U r2 U ... Urn where ri = [ (next-steps) i-1 Ci ], C, ¢ P or Ci = [ goals 0 ]. /f the execution is terminated, C~ must be unifiable with[ goals () ]. In this case, we call the sequence (rl,'\",r,~} a resolution sequence.","2Though, through the rest of the paper, we treat the definition as if it were used in an actual implementation, the actual implementation uses a more efficient method whose output is equivalent with the result obtained by the defiifition.","a (H0,.-', H~, [1]) is an abbreviation of rest \" ' rest [1]"]},{"title":"E21[ • N","paragraphs":["~,bcat 0 My colleague [ sig,! ~d ]","S 2 maJ subcat (","'ql ms.) V subcat ([2]NP) ln~i sutJcat (~ m aj V subcat ([lJNI',","[2]NP) wrote a good paper Figure 3: A parsing example","(next-steps) i-1 [goals of QueryHrl Hr2 H. • • H ri represents the goals which are to be solved in the steps following the i-th step. The goals are instantiated by the steps fi'om the first one to i-th one, through structure sharings. The result of execution in a Prolog-like sense appears in the query, Figure 2 is an example of execution for the query append([a], [b] ,X) , whose definition is based on a standard definition of append in Prolog.","Given this definition of DCPs, an application of a rule schema to two (laughter signs D1 and D2 can be expressed in the following form, where @1, r2,.\", r,~} is a resolution sequence: M= [ head-dtrnon_head_dtr D~D2 ]Ufs(R)U','tUr.2U...Ur,~"]},{"title":"3 Lexical Entry Automata","paragraphs":["This section presents a Lexical Entry Automaton (LA). The ineifieiency of parsing in HPSG is due to the fact that what kind of constituents phrasal-signs would become is invisible until the whole sequence of applications of rule schemata is completed. Consider the parse tree in Figure 3. The phrasal-signs $1 and $2 are invisible until a parser creates the feature structures describing them, using expensive unification.","Our parsing method avoids this on-line construction of phrasal-signs by computing skeletal part of parse trees prior to parsing. [n Figure 3, our compiler generates $1 and $2 only from the lexical entry \"wrote,\" without specifying the non-head daughters indicated by the triangles in Figure 3. Since the non-head daughters are tokenidentical with subcat values of the lexical entry for \"wrote\", the obtained skeletal parse tree contains the information that St takes a noun phrase as object and $2 selects another noun-phrase. Then unifying those non-head daughters with actual signs constructed from input, parsing can be done. An LA expresses a set of such skeletal parse trees. A state in an LA corresponds to a phrasal-sign suc h as Sj and $2. They are called core-structures. A transition arc is a domination link between a phrasal-sign and its head daughter, and its condition for transition on input is a non-head"]},{"title":"950","paragraphs":["Qtlery : P l\"Ogt'iUll: I,'~xecut iott : Q10 (J2 =: C; I =: goals llext-steps goals"]},{"title":"1 ..... / i~l'g, 1 (} / lll'g~ [2] I[]] , L ,,,.ga [21 m,,,l.~ [J]]","paragraphs":["next-steps ......"]},{"title":".-,,.~a [ (!~'q][l[~i) ]}","paragraphs":["argl 4 [ e-list ] arg'3 6","&rgl ( : goals arg2 (21 I I (;2 U [ nca:t-.sL,ep.s C I ] =:"]},{"title":"(","paragraphs":["goals ('2 = next-steps l[7][ (,-list 1} [ ;~, [[{![)e list ])"]},{"title":"[ (2 31[8] ]([q[ ,, }))","paragraphs":["a,'g2 arg, (f:~t[ [4])[[7] gl'ga ([3Jl[~})","goals arg2 4 i[r] re'g3 I[r)[ e-list ]) next-steps"]},{"title":"/ ,u~J 4 [ ,<i.~t ]","paragraphs":["goals / arg2 5 b .... xt-steps goals <) ]"]},{"title":"I[r][ ,!-li,~t ]) ]","paragraphs":["Figure 2: An examl)le of DCP's execution daughter, such as signs tagged [1] and [2] in Figure 3. Kasper c.t al. 1)resented an idea similar to this @line raising in their work on HPSG-TAG compiler(Kasper et al., 1995). The difference, is that our algorithm is based ou substitution, not adjoining, Furthermore, it is not clear in their work how offline raising is used to improve ef[icicncy of parsing.","Before giving the definition of LAs, we detine the notion of a quasi-sign, which is part of a sign and constitutes l~As. Definition 3 (quasi-sign(n)) For a given integer n, a fcatu,e structure S is a q'aasi-sign(n) if it has some of tile following four attributes:"]},{"title":"syn, sem, head-dtr,non-head-dtr","paragraphs":["and does not /Lave values for the paths (head-dtr +"]},{"title":"non-head-dtr)\"\".","paragraphs":["A qua,.si-sign('n) cannot rel)resent a parse tree whose height is inore than n, while a sign can express a parse tree with any height. Tlm)ugh the rest of this 1)aper, we often extract a quasi-sig\"n.(n) S from a sign or a quasi-sig',,(,n/) S' where '., < n'. This operation is denote(l by S' = c'x(S',,n). This means that 5' is equivMent to S' except ff)r the attributes head-dtr mM non-head-dtr whose root is the"]},{"title":"(head-dtr + non-head-dtr) '~","paragraphs":["value in S'. Note that S and S' are completely different entities. In other words, S and S' pose different scopes on structure sharing tags, in addition, we also extract a feature structure F reached by a path or an attribute 1) in a feature structure IP'. We denote this by F = val(F',p) and regard F and F' as different entities."]},{"title":"Definition 4","paragraphs":["(Lexical Entry"]},{"title":"Auton, aton(LA))","paragraphs":["A Lezical Entry Automaton is a tuplc (Q,A,qo}"]},{"title":"whel'e~","paragraphs":["Q: a set of states, where a .state is a quasi-sign(O).","A : a ,set of transition arcs between states, where a transition arc is a tuple (qd, q .... N,D,R) where qd, q,. 6 Q, N is a quasi-sign(O), D is a quasi-sign(I) and R is a rule schema.","qo : tile initial state, which corresponds to a lezical erLtry.","In a transition :-tt'(; < qd, q ..... N, D, 1~} , q,~ denotes the destination of the transition arc, and qd is the root of the arc. The N is a non-head daughter of a l)hrasal-sign, i.e., the destination state of the transition, and expresses the input condition for the transition. The D is used to represe, nt: the dependency 1)etween the nn)ther sign and the daughters through structure sharings. This is called a Dependency Feature Strueture(DFS) of the transition arc, the role of which will be discussed in Section 4. 1~, is the rule schema used to create this arc.","An LA is generated fl'om a lexieal entry l by the following recursive pro(:edure: 1. Let; ,~; 1)e {/}, A be an eml)ty set and sd = / 2. For ea(:h rule, schema 1~, and for each of its","ea(:h resolution sequence (rl,...,'r,~} obtain, 1) -"]},{"title":"[ head-dtr ,Sd ]","paragraphs":["uf.s(l~) u r, u .-. o r,~ and if l) is a feature structure, obtain s,, = ex(D,O) and N = ex(w~l(D, non-head-dtr), 0).","a. If D is a t~ature structure,","• If the, re is a state s~,~ 6 S such that s',~",",s .... 4 let s,~ be s~,~. Otherwise, add s,,~"]},{"title":"to 5\".","paragraphs":["* If there is no T'r = \\'/~\"d, '~,,~\"', N\", D\", 1~) A such that .%~ ~ s{',~, s,z ~ sSl, N","4For ~my feature structures f ~md f', f ~ f' iff f E f'~md f' E f 951 Phase2-proc-dcp(e : edge); assume e = (1, r, S, Dep) return S U sub-structure(e) sub-structure(e : edge); assmne e = (l, r, S, Dep) If Dep = 4) then return sub(S), else","for each (D, eh, e~, R) C Dep,","assume that el~ = (lh, rh, Sh, Deph)","and e~ : (In, r~, Sn, Dep,~)","Sh := sub-structure(eh),"]},{"title":"S,~","paragraphs":[":= Sn U sub-strueture(e~)","If neither of Sh and Sn is nil, s%bo :~"]},{"title":"fv(dep(D) u sub(fs(R)),","paragraphs":["[ head-dtr"]},{"title":"Sh ] non-head-dtr S n ' rs)","paragraphs":["............................... (A) for each resolution sequence"]},{"title":",rd,","paragraphs":["sub := .sub0 LIT 1 I~ • • • U Ti","............................... (B) If sub is not a feature structure or either of Sh or S~ is nil, then return nil else return sub Figure 4: A recursive procedure for tile Phase 2","N\" and D ~ D\", then, add the tuple","(s,t, s,,~, N, D, R) to A. 4. If the new quasi-sign(O) (s,~) was added to","S in the previous step, let sd be s,~ and go to","Step 2.","When this terminates, (S, A, l) is the LA for 1.","The major difference of Step 2 and the normal application of a rule schema is that non-head-dtr values are not specified in Step 2. In spite of this underspecification, certain parts of the non-head-dtr are instantiated because they are token-identicM with certain values of the head-d%r domain. By unifying non-head-dtr values with actual signs to be constructed fl'om input sentences, a parser can obtain parsing results. For more intuitive explanation, see (Torisawa and Tsujii, 1996).","However, this simple LA generation algorithm has a termination problem. There are two potential causes of non-termination. The first is the generative capacity of a feature structure of a rule schema, i.e., a rule schema can generate infinite variety of signs. The second is non-termination of the execution of DCP in Step 2 because of lack of concrete non-head daughters.","For the first case, consider a rule schema with the following feature structure. head-dtr syn [ counter [1] ] ]","Then, this can generate an infinite sequence of signs, each of which contains a part, [ counter <bar, ba, r,...,bar) l and is not equivalent to any previously generated sign. In order to resolve this difficulty, we apply tim restriction (Shieber, 1985) to a rule schemata and a lexical entry, and split the feature structure F = fs(R) of a rule schema R or a lexical entry F = l, into two, namely, core(F) and sub(F) such that F = core(F) U sub(F). The definition of the restriction here is given as follows."]},{"title":"Definition","paragraphs":["5 (paths) For arty node n in a feature structure F, paths(n,F) is a set of all the paths that reaches n from the root of F."]},{"title":"Definition 6 (Restriction Schema) A","paragraphs":["restriction schema rs is a set of paths."]},{"title":"Definition","paragraphs":["7 (Res) F' = Res(F, rs) is a ma.~;i-","real feature structure such that each node n in F ~","satisfies the following conditions. • The~ is a node no in f: such that","paths(no,F) = path.s(n,F') and type('n) ="]},{"title":"t?tpe(no).","paragraphs":["• For any p C paths('n,F'), there is no path","p,, 6 rs which prefixes p.","Res eliminates the feature structure nodes which is specified by a restriction schema. For a certMn given restriction schema rs, eore(fs(l~,)) -= Res(fs(R),rs) and sub(fs(R)) is a mini-mM feature structure such that core(fs(R))U sub(fs(R)) = fs(R). Tile nodes eliminated by Res must appear in sub(fs(R)). In tile example, if we add (syn, counter} to a restriction schema and replace fs(R) with eorc(fs(.R)) in the Mgorithm for generating LAs, the termination problenl does not occur because LAs can contain a loop and equivMent signs are reduced to one state in LAs. The sub(fs(R)) contains the synlcounter, and the value is treated at Phase 2.","The other problem, i.e., termination of DCPs, often occurs because of underspecification of the nork-head-dtr wines. Consider the rule schema in Figure 1. The append does not terminate at Phase 2 because the indices value of non-head (laughters is [ ± ]. (Consider the case of execut-ing append(X, (b),Y) in Prolog.) We introduce the .freeze Nnctor in Prolog which delays the evaluation of the second argument of the functors if the first arguruent is not instantiated. For instance, freeze (X, append(X, [b], Z) ) means to delay the ewfluation of append until X is instantinted. We introduce the functor in the following forln.","goals arg2 (fl arg3 [~ freeze ]","This means the resolution of this query is not performed if [1] is [±]. The delayed evaluation is considered later when tile non-head-dtr values are instantiated by an actual sign. Note that this change does not affect the discussion on the correctness of our parsing method, because the difference can be seen as only changes of order of unification.","Now, tile two phases of our parsing algorithm can be described in more detail. Phase 1 : Enumerate possible parses or edges in","a chart only with unifiability checking in a","bottom-up chart-parsing like manner."]},{"title":"952 Phase 2 : For comt)leted parse trees, compute","paragraphs":["sub-structures by DFSs, ,sub(fs(R)) for each","schema R and frozen 1)C1 ) programs.","Note that, in ['has(; 1, unification is replaced with nnifiability checking, which is more efficient than unification in terlns of space an(l time. The intended side effect by unification, such as build-ing up logical forms in sere values, is COmlntted at Phase 2 only for the parse trees covering the whole input. a.1 Phase 1 Parsing The Phase~ 1 parsing algorithm is quite similar to a bottom-up chart parsing for CFG. The Mgorithm has a chart and edges. Definition 8 (edge) An edge is a tupla (1, r, S, l)ep) where,","• 1 and r arc. vertexes in the chart.","• S"]},{"title":"is a slate of","paragraphs":["an LA.","• .l)ep i.s a .set of tuples in the form of (D, eh, c,,, ll} wh, e, rc. eh a7%d Cn aTY; (:dges, ]) i.s a quasi-.sign(I) and R is a rule .schema.","The intuition behind this definition is,","• £' l)lays the role of a non-/termimd in CFG, though it is actually a quasi-sign(O).","• ch and e,~ denote a head daughter edge and a non-head daughter edge, respectively.","• Dep represents the dependency of an edge and its daughter edges. Where"]},{"title":"(D, eh,c,~,l~} E Dcp, D is a DIeS of a","paragraphs":["transition arc. Basi(:ally, Phase 1 parsing creates these tuples, and ])hase 2 parsing uses them.","The Phase 1 parsing (:onsists of the folh)wing","steps. Assume that a word in input ]n~s a lexical","entry L~ and that an LA (Q,;,A,,q~) generated","fi'om Li is attached to the word:","1. Create an edge li -= (j.i,ji + 1,q~,()) in the chart for each Li, for at)propriate .ji.","2. For an edge e. 1 whose state is q~ in the chart, pick u t) an edge e2 which is adjacent to el and whose state is q~.","3. For a transition arc (ql, q, N, D, ll), check if N is unifiable with q2.","4. If the unifiability check is successful, find an edge (l = ('m,d,'n,d,q, Depd) strictly covering el and e2.","5. if there is, replace d with a new edge (m,,,'na,q, Dep,z U {(D,c,,eu,B)}) it) the. chart.","6. Otherwise, create a new edge (Tn, n, q, {(D, el, e2, R)}) strictly covering el and e2.","7. Go to steI) 2. 4 Phase 2 Parsing The algorithnl of Phase 2 parsing is given in Figure 4. The procedure sub-.structure is a recursive 1)rocedure which takes an edge as input and builds Ul) sub-structures, which is differ'ential feature structures representing modifications to core-structures, in a bottoln-U 1) nlanner. The obtained sub-structures are unified with core-structures when 1) the input edge covers a whole input or 2) the edge is a non-head daughter edge of sonm other edge. Note that the .~ub-struet'are treats sub(fs(R)), a feature structure eliminated l)y the restriction in the generation of LAs, (the (A) 1)art in Figure 4) and frozen goals of DCPs, by additional ewduation of DCPs. (the (B) part)","Here, we use two techniques: ()tie is dependency analysis which is eml)odied by the function dep in Figure 4. The other is a partiM unification routine expressed by p_nnify in the figure.","The del)endency analysis is represented with the function, dep(F,'rs), where F is a DFS and rs is"]},{"title":"a","paragraphs":["restriction schema used in generation of LAs: Definition 9 (dep) For a feature structure [\"' and the. restriction schema r.s, F = dep(l c~,r,s) is a maximal fc.atu're~ structure such O~,at any 'node 'n in F sati,~fies the conjunction of th, e. following two conditions:","t. There is a node n' in f i'' ,such, that v(tm.+,., P) - ~),,m.,+,,',F') a,.Z t:,mc.(7,0 := typc(n').","2. Where A) ha. = 'n or B) n,t is a descendan? of n, pa, ths(n,z,F) contains a path. prefixed by one of"]},{"title":"(head-dtr), (non-head-dtr) and <goa:ts>.","paragraphs":["3. The diajunetion of the following three conditions is satisfied where A) n,t = n or B) 'n(t is a descendant of n. • For .some p G pa, th, s(7t~l,F), there i.s a","path, p,,. E 'rs wh, ieh prefixes p. • Some p ~ p.,th,@n,t,F) is prefixed by"]},{"title":"(~m.,ls).","paragraphs":["• 7'here is no node 'n. in F .~'uch th, at i) there is paths Pi,7)'2 ~ paths('n<,., f;') such that Pi is prefixed by (syn) 07'"]},{"title":"(sere)","paragraphs":["aTtd P2 is 'p'r'efi;Le.d by"]},{"title":"(head-dtr)","paragraphs":["Or"]},{"title":"(non-head-dtr>,","paragraphs":["and i/) for a~ty p G paths(rid, F) there is p,~ E path..s(n,~, F) which prefixes p.","Roughly, dep eliminates 1) the descendant nodes of the node which apl)ears both in syn/sem domains and head-dtr/non-head-dtr domains and 2) the nodes at)peering only in syn/sem domains, excet)t for the node which el)pears in"]},{"title":"s'ab(fs(]¢))","paragraphs":["or goals domains. In other words, it removes the feature structures that have I)een already raised to core-structures or other DFSs, ex(:ept for the structure sharings, and leaves those which will be required by DCPs or xub(fs(R)).","p_uni f y( Fl , F.2 , r s ) is a partial unification routine where Fl and F2 are feature structures, and rs is a restriction schema used in generation of LAs. l{oughly, it performs unification of F, and l'12 only for common part of Ft, F.2, and it produces unified results only for the node 'n in Fl if","s'nj is ~t descendant of 'n2 in a feaiure structur{~ l,' ill 'nt"]},{"title":"#","paragraphs":["n2, and the.r('. ~u:e paths Pl 6 path, s(~,,l, [\") ~Hld I)'2 E pa, th, s(n.2, l\"), nnd p2 l)r('.fixes pl."]},{"title":"953","paragraphs":["phon \"wrote\" syn"]},{"title":",orv,] .... ....","paragraphs":["subcat <NI'[1],NP[2]) rein wrote","sere content agent object indices 0 Figure 5: A lexical entry for \"wrote\""]},{"title":"$2 ? A State","paragraphs":["Pb T:N"]},{"title":"T2:N S1 A Transition Arc","paragraphs":["TI:NP (N denotes"]},{"title":"L a non-head-dtr.)","paragraphs":["Figure 6: The LA derived from \"wrote\" n has a counter part in F~. More precisely, it produces the unification results for a nod(; n in Fj such that","• there is a path"]},{"title":"p ~ paths(n, I~)","paragraphs":["such that the node reached by 1) is also defined in F2, or","• there is a path p ~"]},{"title":"paths(n, F1)","paragraphs":["prefixed by some"]},{"title":"p,, C rs","paragraphs":["or"]},{"title":"(goals).","paragraphs":["Note that a node is unified if its structure-shared part has a counter-I)art in F2. Intuitively, the routing produces unified results for the part of"]},{"title":"Fi instantiated","paragraphs":["by"]},{"title":"/7'2.","paragraphs":["The other part, that is not produced by"]},{"title":"p_unify,","paragraphs":["is not required at Phase 2 because it is already computed in a state or DFSs in LAs when the LAs are generated. Then, a sign can be obtained by unifying a sub-structure and the corresponding core-structure."]},{"title":"5 Example","paragraphs":["This section describes the parsing process of the sentence \"My colleague wrote a good paper.\" The LA generated fronl the lexical entry for \"wrote\" in Figure 5 is given in Figure 6. The transition arc T1 between the states L and S1 is generated by the rule schema in Figure 1. Note thai; the query to DCP,"]},{"title":"freeze([1], append(Ill,","paragraphs":["[2], [3])), is used to obtain"]},{"title":"union","paragraphs":["of"]},{"title":"indices","paragraphs":["values of daughters and the result is"]},{"title":"written","paragraphs":["to the"]},{"title":"indices values","paragraphs":["of the mother sign. During the generation of the transition arc, since the first argument of the query is [ ± ], it is frozen. The core-structures arid the"]},{"title":"dependency-analyzed","paragraphs":["DFSs that augment the LA are shown in Figure 7. We assume that we do not use any restriction, i.e., for any lexical entry l and rule schenaata 2~,"]},{"title":"s,bb(1) ~-[±1","paragraphs":["and"]},{"title":"sub(fs(I{))","paragraphs":["= [±1. Note that, in the DFSs, the already raised feature structures are eliminated and, that the DFS of the transition arc T contains the"]},{"title":"frozen","paragraphs":["query as the goals.","Assmne that the noun phrases \"My colleague\" and '% good paper\" are already recognized by a parser. At phase 1, they are checked if they are unifiable to the condition of transition arcs T1 and T2, i.e., the"]},{"title":"NPs","paragraphs":["which are non-head daughters","$2 syn senl","5'1 syn head [ ..... ior V ] ] subcat 0"]},{"title":"]","paragraphs":["rehl wrote content agent ± object ± indices ± head [ major V ] ] subcat (NP[2])","rein wrote","senl content agent ]","object","indices"]},{"title":"A_","paragraphs":["The dependency-anMyzed"]},{"title":"DFS","paragraphs":["of T2 syn [~ ..... l [s] ] seln indices [a] - synSign 1 ]"]},{"title":".... ]","paragraphs":["h~ad-dtr [a] s~beat .[qN;'[r]) Z","[} t flllajor ..... syn subcttt ) non-head-dtr [5] sere indices","goals argl","arg2","arg2","The dependency-analyzed"]},{"title":"DFS","paragraphs":["of T1 head 8]","syn subcat {/10]±[9]} ] content [6] agent","sere object indices [3] • sign","sy 1 ..... I-dtr [3] 9]) ..... [ ic'::~itceet: t [2 t (,"]},{"title":"]","paragraphs":["syn [ t subcat ) non-head-dtr [5] sere indices"]},{"title":"1 ..... ]>","paragraphs":["freeze 1","goals argl a,-~2 () arg2"]},{"title":"Nll] N,I]","paragraphs":["Figure 7: States and DFSs in tim LA in Figure 6 The sub-structure for $2","content | agen~ 1]my_colleaque | ..... [ obje,:t"]},{"title":"[2]good_p.vJ4' J","paragraphs":["indices {[llmy_collea.o .... [2]good_paper) The sub-structure for S1 sere [ object [1]good_paper ]","indices ([1]good_paper)","The goals,head-dtr,non-head- dtr vMues are omitted. Figure 8: The sub-structures obtained in the parsing 954 __Parsing ~ithnl Phase 1 only Pheuse 1 & Phase 2 Phase 1 & Phase 2 ~f-~ naive application of rule sche, mata naive application of rule s<:hentata ~['ylTe. -6f sent;ences (# of sentences) glT 70 ---- <ufly successful~7~) enD. s~s~"]},{"title":"Ass)","paragraphs":["onTy ~fuT-- - _(~)_ ....... 19.2 19.2 18.8 17.13 3:[.4"]},{"title":"Av 3n< e& 1.25 ~L121_ 3.00 ~1.65)","paragraphs":["85.09","1093.22 ~2.1~ A bracketed time indicates non-(~(] execution tim(< ']'he eXl)erimeuts was l)(nformed on SparcStation 20 with 128 MI) IIAM Figure 9: Ext>eriments on a Japanese newsl)aper(Asahi Shinl)un) <)f $1 and $2. Since all l;he u,dfial)ility <:lwx:kings ,'/.1\"o successful, Phase 1 parsing produ(:es the parse tree whose form is presented in Figure 3. The Phase 2 1)arsing produces the sub-structures in Figure 8. Note that the frozen goals are evaluated and the indices wdues have al)prot)riate values. A l)arsing result is obtaine{l by unifying the sub-structure for 5\"2 with tim correspon<ling core-structllre.","The amount of the feature stru<:ture nodes generate(1 during t)arsing are r(~<lu(:e(1 (:<m~t>are(l to the case of the naive at)l)lication of rule schemata presented in Section 2. The important point is that they contMn only either the part iu the DFSs that was instantiated by head daughters' sub-structures, and non-head daughters' core-structures and sub-structures, or the part that contributes to the DCP's exaluation. The feature structure that does not al)pear i, a sub-structure appears in the corresponding core-structure. Se, e Figure 7. Because of these 1)rot>erties, the correctness of our parsing nmthod is guaranteed. ('lbrisawa and Tsujii, 1996). 7 Conclusion We have lu'esented a two-phased t)arsing nlethod tor HPSG. In the first l)hase,, our 1)arser produces parse trees using Lexical Entry Automnta compilcxl from lexical entries, in the second"]},{"title":"phase, only","paragraphs":["the feature structures whi<:h luust ])e (:ompute([ dynamically are (:omputed. As a resuit, amount of the fl;ature structures unifie<l at 1)arsing-time is reduce.d. We also showed the el'- feet of our optinfization te(:hniques by a series of exl)erinwats <m a real world text.","]t can l)e noticed that ea<:h transition arc of tim cOral)ileal l,As can be seen as a rewriting rule in CFG (or a dott;ed notation in a chart parser.) We belie.ve this can Ol)en the way to integrate severaJ n,et;hods deveh)l>ed for CI,'G, including the insideoutside algorithm tot grmmnar learning or disam biguation, into an HPSC, framework. We also 1)e-lieve that, by pursuing this direction for optimiz-ing ttl)SG parsers, we can reach the point whe.re grammar learning from corl)ora can be done with concise, and linguistically well-defined (:ore grant-Itt;tr. 6 ExI)eriments We have implenmnted our parsing metho<l in Common Lisp Ol)je<:t Systen~. hnprovenmnt by our method has /)een measured on 70 ra.ndonfly selected Japanese sentences from a newsl)at)er (Asahi Shinbun). The used grammar (',onsists of just 5 rule schemata, which are generated fl'om principles and rewriting rules, aim 55 default lexical entries given for each part of speech, with 44 manually tailored lexical entries. The total number of states in the LAs compiled fl'oln them was 1490. The grammar does not have a semantic part. The results arc. l)resented in Figure 9. Our grammar produ<:ed l>ossil)le parse trees for 43 senten<'.es (61.4%). We compared the. execution time of our I)arsing method and a more naive algorithm, which l)erforms Phase 1 parsing with LAs and al)- plys rule s(:hemata to (:olnph'.ted pars<; trees in the naive way described in Se<:tion 2. As the. naive algorithm caused thrashing for storage in GC, it is pointless to compare those tigures simply. However, it is obvious that our method is much fi~ster than the naive one. We could not measure the execution time for a totally naive algorithm which t)uilds parse trees without LAs because of"]},{"title":"Uwash-ing.","paragraphs":["References","Bob Carl>enter. 1992. The Looi<: of \"/]qped F.a,t..,.+'. Str'ucturcs. Cambridge University Press.","Keiko Horiguchi, Kentaro Torisawa, and Jun'ichi Tsujii. 1995. Automatic acquisition of cont(;nt words using an IIPSG.-based parser. In NL-1\"1~S'95.","Robert Kasper, Bernd Kiefer, Klaus Netter, and K. Vijay-Shanker. 1995. Compilation of IIPSG to TAG. In ACL 95.","Carl Pollard and Ivan A. Sag. 1987. h~,fovmatio,,- Based Syntaz and Semau, ties Vol. 1. CSLI lec-ture notes 11o.1 3.","(,'arl Pollard and Ivan A. Sag. 1993. lh..a.d-Driv<'.n Phrase Structure Grammar. University of Chicago Press an(l CSLI l)ul)li<:ations.","Stuart C. Shieber. 1985. Using restri<:tion to extend I)arsing algorithms for conq)lex feature based formalisms. In A CL85.","Kentaro Torisawa and Jun'ichi Tsujii. 1996. ()if-line raising, dei)endency analysis an{l l>artial unifieat;ion. In Third Iu, ternational Conference on HPSG. In the pr<)ceedings of TALN '96. 955"]}]}