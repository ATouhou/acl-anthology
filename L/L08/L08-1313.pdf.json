{"sections":[{"title":"On class\\b\\ty\\bng c\\fherent/\\bnc\\fherent R\\fman\\ban sh\\frt texts Anca D. D\\bnu","paragraphs":["Universi\\b\\t o\\f Buchares\\b, Facul\\b\\t o\\f Foreign Languages and Li\\bera\\bure","","S\\brada Edgar Quine\\b 5-7, Buchares\\b, Romania","E-mail: anca_d_dinu@\\tahoo.com Abstract In \\bhis paper we presen\\b and discuss \\bhe resul\\bs o\\f a \\bex\\b coherence experimen\\b per\\formed on a small corpus o\\f Romanian \\bex\\b \\from a number o\\f al\\berna\\bive high school manuals. During \\bhe las\\b 10 \\tears, an abundance o\\f al\\berna\\bive manuals \\for high school was produced and dis\\bribu\\bed in Romania. Due \\bo \\bhe large amoun\\b o\\f ma\\berial and \\bo \\bhe rela\\bive shor\\b \\bime in which i\\b was produced, \\bhe ques\\bion o\\f assessing \\bhe quali\\b\\t o\\f \\bhis ma\\berial emerged; \\bhis process relied mos\\bl\\t o\\f subjec\\bive human personal opinion, given \\bhe lack o\\f au\\boma\\bic \\bools \\for Romanian. Deba\\bes and claims o\\f poor quali\\b\\t o\\f \\bhe al\\berna\\bive manuals resul\\bed in a number o\\f examples o\\f incomprehensible / incoheren\\b paragraphs ex\\brac\\bed \\from such manuals. Our goal was \\bo crea\\be an au\\boma\\bic \\bool which ma\\t be used as an indica\\bion o\\f poor quali\\b\\t o\\f such \\bex\\bs. We crea\\bed a small corpus o\\f represen\\ba\\bive \\bex\\bs \\from Romanian al\\berna\\bive manuals. We manuall\\t classi\\fied \\bhe chosen paragraphs \\from such manuals in\\bo \\bwo ca\\begories: comprehensible/coheren\\b \\bex\\b and incomprehensible/incoheren\\b \\bex\\b. We \\bhen used di\\f\\feren\\b machine learning \\bechniques \\bo au\\boma\\bicall\\t classi\\f\\t \\bhem in a supervised manner. Our approach is ra\\bher simple, bu\\b \\bhe resul\\bs are encouraging.  "]},{"title":"1. Intr\\fduct\\b\\fn","paragraphs":["During \\bhe las\\b 10 \\tears, an abundance o\\f al\\berna\\bive manuals \\for primar\\t and high school was produced and dis\\bribu\\bed in Romania. Due \\bo \\bhe large amoun\\b o\\f ma\\berial and \\bo \\bhe rela\\bive shor\\b \\bime in which i\\b was produced, \\bhe ques\\bion o\\f assessing \\bhe quali\\b\\t o\\f \\bhis ma\\berial emerged; \\bhis process relied mos\\bl\\t o\\f subjec\\bive human personal opinion, given \\bhe lack o\\f au\\boma\\bic \\bools \\for Romanian. Deba\\bes and claims o\\f poor quali\\b\\t o\\f \\bhe al\\berna\\bive manuals resul\\bed in a number o\\f examples o\\f incomprehensible / incoheren\\b paragraphs ex\\brac\\bed \\from such manuals. Our goal was \\bo crea\\be an au\\boma\\bic \\bool \\for ca\\begoriza\\bion o\\f shor\\b Romanian \\bex\\b, which ma\\t be used as an indica\\bion o\\f poor quali\\b\\t o\\f such \\bex\\bs. The \\b\\tpical \\bex\\b ca\\begoriza\\bion cri\\beria comprise ca\\begoriza\\bion b\\t \\bopic, b\\t s\\b\\tle (genre classi\\fica\\bion, au\\bhorship iden\\bi\\fica\\bion), b\\t expressed opinion (opinion mining, sen\\bimen\\b classi\\fica\\bion), e\\bc. Ver\\t \\few approaches consider \\bhe problem o\\f ca\\begorizing \\bex\\b b\\t degree o\\f coherence, as in (Miller, 2004). We crea\\bed a small corpus o\\f represen\\ba\\bive \\bex\\bs \\from 6 Romanian al\\berna\\bive manuals. We manuall\\t classi\\fied \\bhe chosen paragraphs \\from such manuals in\\bo \\bwo ca\\begories: comprehensible/coheren\\b \\bex\\b and incomprehensible/incoheren\\b \\bex\\b. We \\bhen used di\\f\\feren\\b machine learning \\bechniques \\bo au\\boma\\bicall\\t classi\\f\\t \\bhem in a supervised manner. There are man\\t quali\\ba\\bive approaches rela\\bed \\bo coherence \\bha\\b could be applied \\bo English language. For example, segmen\\bed discourse represen\\ba\\bion \\bheor\\t (Lascarides, 2007) is a \\bheor\\t o\\f discourse in\\berpre\\ba\\bion which ex\\bends d\\tnamic seman\\bics b\\t in\\broducing rhe\\borical rela\\bions in\\bo \\bhe logical \\form o\\f discourses. A discourse is coheren\\b jus\\b in case: a) ever\\t proposi\\bion is rhe\\boricall\\t connec\\bed \\bo ano\\bher piece o\\f discourse, resul\\bing in a single connec\\bed s\\bruc\\bure \\for \\bhe whole discourse; b) all anaphoric expressions/rela\\bions can be resolved. Maximize Discourse Coherence is a guiding principle. In \\bhe spiri\\b o\\f \\bhe requiremen\\b \\bo maximize in\\forma\\biveness, discourses are normall\\t in\\berpre\\bed so as \\bo maximize coherence. O\\bher examples o\\f quali\\ba\\bive approaches rela\\bed \\bo coherence are la\\ben\\b seman\\bic anal\\tsis (Dumais e\\b al., 1988), lexical chains (Hirs\\b & S\\b.-Onge, 1997), cen\\bering \\bheor\\t (Beaver, 2004), discourse represen\\ba\\bion \\bheor\\t (Kamp & Re\\tle, 1993), veins \\bheor\\t (Cris\\bea, 2003), e\\bc. Never\\bheless, because o\\f \\bhe lack o\\f appropria\\be \\bools \\for Romanian language, we had \\bo choose a quan\\bi\\ba\\bive approach \\for au\\boma\\bicall\\t ca\\begorizing shor\\b Romanian \\bex\\b in\\bo coheren\\b /comprehensible and incoheren\\b /incomprehensible. An impor\\ban\\b ques\\bion \\for such ca\\begoriza\\bion is: are \\bhere an\\t \\fea\\bures \\bha\\b can be ex\\brac\\bed \\from \\bhese \\bex\\bs \\bha\\b can be success\\full\\t used \\bo ca\\begorize \\bhem? We propose a quan\\bi\\ba\\bive approach \\bha\\b relies on \\bhe use o\\f ra\\bios be\\bween morphological ca\\begories \\from \\bhe \\bex\\bs as discriminan\\b \\fea\\bures. We supposed \\bha\\b \\bhese ra\\bios are no\\b comple\\bel\\t random in coheren\\b \\bex\\b. Our approach is ra\\bher simple, bu\\b \\bhe resul\\bs are encouraging."]},{"title":"2. The c\\frpus","paragraphs":["We crea\\bed a small corpus o\\f \\bex\\bs \\from 6 Romanian al\\berna\\bive manuals wi\\bh di\\f\\feren\\b au\\bhors. We used 5 anno\\ba\\bors \\bo manuall\\t classi\\f\\t \\bhe chosen paragraphs \\from such manuals in\\bo \\bwo ca\\begories: comprehensible /coheren\\b \\bex\\b (\\bhe posi\\bive examples) and incomprehensible /incoheren\\b \\bex\\b (\\bhe nega\\bive examples). We selec\\bed 65 \\bex\\bs (paragraphs) which were unanimousl\\t labelled b\\t all \\bhe anno\\ba\\bors as incoheren\\b /incomprehensible. As some anno\\ba\\bors observed, \\bhe \\tes or no decision was"]},{"title":"2871","paragraphs":["overl\\t res\\bric\\bive; \\bhe\\t could have gave a more \\fine grained answer such as very di\\b\\bi\\tul\\f \\fo \\bollow, easy \\fo \\bollow, e\\bc, bu\\b we decided \\bo work wi\\bh 2 class ca\\begorisa\\bion \\from reasons o\\f simplici\\b\\t. We leave \\bhis \\for \\fur\\bher work, as well as crea\\bing a larger corpus. We also selec\\bed 65 coheren\\b / comprehensible \\bex\\bs \\from \\bhe manuals, b\\t \\bhe same me\\bhod."]},{"title":"3. Categ\\fr\\bzat\\b\\fn exper\\bments and results","paragraphs":["We used Balie s\\ts\\bem developed a\\b O\\b\\bawa Universi\\b\\t h\\b\\bp://balie.source\\forge.ne\\b/), which has a par\\b o\\f speech \\bagger \\for Romanian, named QTag. We onl\\t \\book in considera\\bion 12 par\\bs o\\f speech. We elimina\\bed \\bhe punc\\bua\\bion \\bags and we mapped di\\f\\feren\\b subclasses o\\f pos in\\bo a single uni\\f\\ting pos (\\for example all subclasses o\\f adverbs were mapped in\\bo a single class: \\bhe adverbs, all singular and plural common nouns were mapped in\\bo a single class: common nouns, e\\bc).We manuall\\t correc\\bed \\bhe \\bagging, because o\\f \\bhe poor accurac\\t ob\\bained b\\t \\bhe parser and because \\bhe size o\\f \\bhe corpus allowed us \\bo do so. We compu\\bed \\bhe pos \\frequencies in each o\\f \\bhe \\braining se\\b \\bex\\bs (bo\\bh \\from \\bhe posi\\bive and \\from \\bhe nega\\bive examples). We normalized \\bhem (divided \\bhe \\frequencies \\bo \\bhe \\bo\\bal number o\\f \\bagged words in each \\bex\\b), \\bo neu\\bralize \\bhe \\fac\\b \\bha\\b \\bhe \\bex\\bs had di\\f\\feren\\b leng\\bhs. We \\bhen compu\\bed all possible 66 ra\\bios be\\bween all 12 \\bags. In \\bhe process o\\f compu\\bing \\bhese ra\\bios we added a small ar\\bi\\ficial quan\\bi\\b\\t (equal \\bo 0.001) \\bo bo\\bh \\bhe numera\\bor and \\bhe denomina\\bor, \\bo guard agains\\b division b\\t zero. These 66 values become \\bhe \\fea\\bures on which we \\brained 3 ou\\b o\\f 5 \\b\\tpes o\\f machines we emplo\\ted (\\bhe o\\bher \\bwo needed no such pre-processing). Because o\\f \\bhe rela\\bive small number o\\f examples in our experimen\\b, we used leave one ou\\b cross valida\\bion (l.o.o.) (E\\fron & Tibshirani, 1997; Tsuda, 2001), which is considered an almos\\b unbiased es\\bima\\bor o\\f \\bhe generaliza\\bion error. Leave one ou\\b \\bechnique consis\\bs o\\f holding each example ou\\b, \\braining on all \\bhe o\\bher examples and \\bes\\bing on \\bhe hold ou\\b example. The \\firs\\b and \\bhe simples\\b \\bechnique we used was \\bhe linear regression (Duda e\\b al., 2001; Chen e\\b al., 2003; Schroeder e\\b al., 1986), no\\b \\for i\\bs accurac\\t as a classi\\fier, bu\\b because, being a linear me\\bhod, i\\b allows us \\bo anal\\tze \\bhe impor\\bance o\\f each \\fea\\bure and so de\\bermine some o\\f \\bhe mos\\b prominen\\b \\fea\\bures \\for our experimen\\b o\\f \\bex\\b ca\\begoriza\\bion. We also used \\bhis me\\bhod as a base line \\for \\bhe o\\bher experimen\\bs. For a \\braining se\\b:","S = (x1, y1), (x2, y2), ..., (xl, yl), \\bhe linear regression me\\bhod consis\\bs in \\finding \\bhe real linear \\func\\bion (i.e \\finding \\bhe weigh\\bs w)","g(x) =∑l i=1wixi such \\bha\\b","∑l","i=1 (yi - g(xi))2"," is minimized. I\\f \\bhe ma\\brix X’X is inver\\bible, \\bhen \\bhe solu\\bion is w = (X’X)-1X’y. I\\f no\\b (\\bhe ma\\brix X’X is singular), \\bhen one uses \\bhe pseudo-inverse o\\f \\bhe ma\\brix X’X, \\bhus \\finding \\bhe solu\\bion w wi\\bh \\bhe minimum norm. For \\bhis experimen\\b we used \\bhe pre-processed da\\ba as described above. I\\bs l.o.o accurac\\t was o\\f 67.48%, which we used \\fur\\bher as baseline \\for nex\\b experimen\\bs. We ordered \\bhe 66 \\fea\\bures (pos ra\\bios) in decreasing order o\\f \\bheir coe\\f\\ficien\\bs compu\\bed b\\t per\\forming regression. The \\bop 5 \\fea\\bures \\bha\\b con\\bribu\\be \\bhe mos\\b \\bo \\bhe discrimina\\bion o\\f \\bhe \\bex\\bs are linguis\\bicall\\t ver\\t in\\beres\\bing: ","• \\bhe ra\\bio be\\bween \\bhe pre-de\\berminer (such as all,","\\bhis, such, e\\bc) and adverbs, represen\\bing 15.8%","o\\f all \\fea\\bure weigh\\bs;","","• \\bhe ra\\bio be\\bween modal auxiliar\\t verbs and","adverbs, represen\\bing 13.29% o\\f all \\fea\\bure","weigh\\bs;","","","• \\bhe ra\\bio be\\bween pre-de\\berminer and","conjunc\\bion, represen\\bing 9.10% o\\f all \\fea\\bure","weigh\\bs;","","• \\bhe ra\\bio be\\bween modal verbs and conjunc\\bions,","represen\\bing 7.25% o\\f all \\fea\\bure weigh\\bs;","","","• \\bhe ra\\bio be\\bween common nouns and","conjunc\\bions, represen\\bing 6.98% o\\f all \\fea\\bure","weigh\\bs."," These \\bop 5 \\fea\\bures accoun\\bed \\for more \\bhan 50% o\\f da\\ba varia\\bion. The second ra\\bio ma\\t be explained b\\t \\bhe inheren\\b s\\brong correla\\bion be\\bween verbs and adverbs. The presence o\\f conjunc\\bion in 3 ou\\b o\\f \\bhe \\bop 5 ra\\bios con\\firms \\bhe na\\bural in\\bui\\bion \\bha\\b conjunc\\bion is an impor\\ban\\b elemen\\b wi\\bh regard \\bo \\bhe coherence o\\f a \\bex\\b. Also, \\bhe presence o\\f \\bhe pre-de\\berminers in \\bhe \\bop 5 ra\\bios ma\\t be rela\\bed \\bo \\bhe impor\\ban\\b role core\\ference pla\\ts in \\bhe coherence o\\f \\bex\\bs. As we said, we used \\bhe linear regression \\bo anal\\tze \\bhe impor\\bance o\\f di\\f\\feren\\b \\fea\\bures in \\bhe discrimina\\bion process and as baseline \\for s\\ba\\be o\\f \\bhe ar\\b machine learning \\bechniques. Nex\\b, we \\bes\\bed \\bwo kernel me\\bhods (Müller e\\b al., 2001; Schölkop\\f & Smola, 2002): ν suppor\\b vec\\bor machine (Saunders e\\b al., 1998) and Kernel Fisher discriminan\\b (Mika e\\b al., 1999; Mika e\\b al.,2001), bo\\bh wi\\bh linear and pol\\tnomial kernel. Kernel-based learning algori\\bhms work b\\t embedding \\bhe da\\ba in\\bo a \\fea\\bure space (a Hilber\\b space), and searching \\for linear rela\\bions in \\bha\\b space. The embedding is per\\formed implici\\bl\\t, \\bha\\b is b\\t speci\\f\\ting \\bhe inner produc\\b be\\bween each pair o\\f poin\\bs ra\\bher \\bhan b\\t giving \\bheir coordina\\bes explici\\bl\\t. Given an inpu\\b se\\b X (\\bhe space o\\f examples), and an embedding vec\\bor space F (\\fea\\bure space), le\\b φ : X → F be an embedding map called \\fea\\bure map. A kernel is a \\func\\bion k, such \\bha\\b \\for all x, z in X,","k (x, z) =< φ (x), φ (z) >, where < . , . > deno\\bes \\bhe inner produc\\b in F. In \\bhe case o\\f binar\\t classi\\fica\\bion problems, kernel-based"]},{"title":"2872","paragraphs":["learning algori\\bhms look \\for a discriminan\\b \\func\\bion, a \\func\\bion \\bha\\b assigns +1 \\bo examples belonging \\bo one class and -1 \\bo examples belonging \\bo \\bhe o\\bher class. This \\func\\bion will be a linear \\func\\bion in \\bhe space F, so i\\b will have \\bhe \\form:","\\b(x) = sign(< w, φ (x) > +b), \\for some weigh\\b vec\\bor w. The kernel can be exploi\\bed whenever \\bhe weigh\\b vec\\bor can be expressed as a linear combina\\bion o\\f \\bhe \\braining poin\\bs, ∑n","i=1 αi φ(xi), impl\\ting \\bha\\b \\f can be expressed as \\follows:","\\b(x) = sign(∑n","i=1 αi k(xi , x) + b). Various kernel me\\bhods di\\f\\fer b\\t \\bhe wa\\t in which \\bhe\\t \\find \\bhe vec\\bor w (or equivalen\\bl\\t \\bhe vec\\bor α). Suppor\\b Vec\\bor Machines (SVM) \\br\\t \\bo \\find \\bhe vec\\bor w \\bha\\b de\\fines \\bhe h\\tperplane \\bha\\b maximall\\t separa\\bes \\bhe images in F o\\f \\bhe \\braining examples belonging \\bo \\bhe \\bwo classes. Kernel Fisher Discriminan\\b (KFD) selec\\bs \\bhe w \\bha\\b gives \\bhe direc\\bion on which \\bhe \\braining examples should be projec\\bed such \\bha\\b \\bo ob\\bain a maximum separa\\bion be\\bween \\bhe means o\\f \\bhe \\bwo classes scaled according \\bo \\bhe variances o\\f \\bhe \\bwo classes in \\bha\\b direc\\bion. The kernel \\func\\bion cap\\bures \\bhe in\\bui\\bive no\\bion o\\f similari\\b\\t be\\bween objec\\bs in a speci\\fic domain and can be an\\t \\func\\bion de\\fined on \\bhe respec\\bive domain \\bha\\b is s\\tmme\\bric and posi\\bive de\\fini\\be. The op\\bimiza\\bion problems are solved in such a wa\\t \\bha\\b \\bhe coordina\\bes o\\f \\bhe embedded poin\\bs are no\\b needed, onl\\t \\bheir pairwise inner produc\\bs which in \\burn are given b\\t \\bhe kernel \\func\\bion k. De\\bails abou\\b SVM and KFD can be \\found in (Ta\\tlor and Cris\\bianini, 2004; Cris\\bianini and Ta\\tlor, 2000). The ν suppor\\b vec\\bor classi\\fier wi\\bh linear kernel (k (x, y) =< x, y >) was \\brained, as in \\bhe case o\\f regression, using \\bhe pre-processed 66 \\fea\\bures, exac\\bl\\t \\bhe same \\fea\\bures used \\for linear regression. The parame\\ber ν was chosen ou\\b o\\f nine \\bries, \\from 0.1 \\bo 0.9, \\bhe bes\\b per\\formance \\for \\bhe SVC being achieved \\for ν = 0.4. The l.o.o. accurac\\t \\for \\bhe bes\\b per\\forming ν parame\\ber was 77.34%, wi\\bh 9.86% higher \\bhen \\bhe baseline. The Kernel Fisher discrimina\\b wi\\bh linear kernel was \\brained on pre-processed da\\ba as i\\b was \\bhe case wi\\bh \\bhe regression and ν suppor\\b vec\\bor classi\\fier. I\\bs l.o.o. accurac\\t was 74.92 %, wi\\bh 7.44 % higher \\bhan \\bhe baseline. The \\flexibili\\b\\t o\\f \\bhe kernel me\\bhods allows us \\bo direc\\bl\\t use \\bhe pos \\frequencies, wi\\bhou\\b compu\\bing an\\t pos ra\\bios. Tha\\b is, \\bhe pol\\tnomial kernel relies on \\bhe inner produc\\b o\\f all \\fea\\bures: i\\b implici\\bl\\t embeds \\bhe original \\fea\\bure vec\\bors in a space \\bha\\b will con\\bain as \\fea\\bures all \\bhe monomial (up \\bo \\bhe degree o\\f \\bhe pol\\tnomial used) over \\bhe ini\\bial \\fea\\bures. For a pol\\tnomial kernel o\\f degree 2 \\for example, \\bhe implici\\b \\fea\\bure space will con\\bain apar\\b o\\f pos \\frequencies, all \\bhe produc\\bs be\\bween \\bhese \\frequencies, \\bhese produc\\bs pla\\ting \\bhe same role as \\bhe ra\\bios. The suppor\\b vec\\bor machine wi\\bh pol\\tnomial kernel was \\brained direc\\bl\\t on \\bhe da\\ba, needing no compu\\ba\\bion o\\f ra\\bios. The kernel \\func\\bion we used is:","k (x, y) = (< x, y > +1)2"," The l.o.o. accurac\\t o\\f \\bhe suppor\\b vec\\bor machine wi\\bh pol\\tnomial kernel \\for \\bhe bes\\b per\\forming ν = 0.4 parame\\ber was 81.13%, wi\\bh 13.65% higher \\bhan \\bhe baseline. The Kernel Fisher discriminan\\b wi\\bh pol\\tnomial kernel was \\brained direc\\bl\\t on \\bhe da\\ba, needing no ra\\bios. I\\bs l.o.o. accurac\\t was 85.12%, wi\\bh 17.64% higher \\bhen \\bhe baseline. All machine learning experimen\\bs were per\\formed in Ma\\blab, or using Ma\\blab as in\\ber\\face (Chang and Lin, 2001). We summarized \\bhese resul\\bs in \\bhe nex\\b \\bable.  Learning me\\bhod \\b\\tpe Accurac\\t Regression 67.48% linear Suppor\\b Vec\\bor Classi\\fier 77.34% quadra\\bic Suppor\\b Vec\\bor Machine 81.13% pol\\tnomial Kernel Fisher discriminan\\b 85.12%","","Table 1: Accurac\\t o\\f \\bhe learning me\\bhods."," As one can see \\from \\bable 1, \\bhe bes\\b per\\formance was achieved b\\t \\bhe Kernel Fisher discriminan\\b wi\\bh pol\\tnomial kernel, wi\\bh a l.o.o. accurac\\t o\\f 85.12%."]},{"title":"4. C\\fnclus\\b\\fns","paragraphs":["The bes\\b l.o.o. accurac\\t we ob\\bained, i.e. 85.12% is a good accurac\\t because using onl\\t \\bhe \\frequencies o\\f \\bhe par\\bs o\\f speech in \\bhe \\bex\\bs disregards man\\t o\\bher impor\\ban\\b \\fea\\bures \\for \\bex\\b coherence, such as, \\for example, \\bhe order o\\f phrases, core\\ferences resolu\\bion, rhe\\borical rela\\bions, e\\bc. Fur\\bher work: \\bhe \\bwo class classi\\fica\\bion, in \\bhe case o\\f Romanian al\\berna\\bive high school manuals, is a ra\\bher drama\\bic classi\\fica\\bion. I\\b would be use\\ful \\bo design a \\bool \\bha\\b produces as ou\\bpu\\b no\\b jus\\b a \\tes/no answer, bu\\b a score or a probabili\\b\\t \\bha\\b \\bhe inpu\\b (\\bex\\b) is in one o\\f \\bhe \\bwo ca\\begories, such \\bha\\b a human exper\\b ma\\t have \\bo judge onl\\t \\bhe \\bex\\bs wi\\bh par\\bicular high probabili\\b\\t \\bo be in \\bhe class o\\f incoheren\\b \\bex\\bs."]},{"title":"5. Ackn\\fwledgements","paragraphs":["Research suppor\\bed b\\t PNII-IDEI, projec\\b 228 and Universi\\b\\t o\\f Buchares\\b. "]},{"title":"6. Re\\terences","paragraphs":["David, B. (2004) The Op\\bimiza\\bion o\\f Discourse Anaphora. Linguis\\fi\\ts and Philosophy 27(1), pp. 3-56.","Cris\\bea, D. (2003): The rela\\bionship be\\bween discourse s\\bruc\\bure and re\\feren\\biali\\b\\t in Veins Theor\\t, in W. Menzel and C. Ver\\ban (eds.) a\\fural Language Pro\\tessing be\\fween Linguis\\fi\\t Inquiry and Sys\\fem Engineering, „Al.I.Cuza” Universi\\b\\t Publishing House, Iaşi."]},{"title":"2873","paragraphs":["Cris\\bianini and J. Shawe-Ta\\tlor (2000) An In\\frodu\\t\\fion \\fo Suppor\\f Ve\\t\\for Ma\\thines. Cambridge Universi\\b\\t Press, Cambridge, UK.","Chih-Chung Chang and Chih-Jen Lin (2001) LIBSVM: a librar\\t \\for suppor\\b vec\\bor machines. So\\f\\bware avalable a\\b h\\b\\bp://ww.csie.n\\bu.edu.\\bw/ cjlin/libsvm.","Chen, X., Ender, P., Mi\\bchell, M. and Wells, C. (2003). Regression wi\\fh SPSS h\\b\\bp://www.a\\bs.ucla.edu/s\\ba\\b/ spss/webbooks/reg/de\\faul\\b.h\\bm .","Duda, R.O., Har\\b, P.E., S\\bork, D.G. (2001) Pa\\f\\fern Classi\\bi\\ta\\fion (2nd ed.). Wile\\t-In\\berscience Publica\\bion.","Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Sco\\b\\b Deerwes\\ber, and Richard Harshman. (1988) Using La\\ben\\b Seman\\bic Anal\\tsis \\bo improve access \\bo \\bex\\bual in\\forma\\bion. In Human Fac\\bors in Compu\\bing S\\ts\\bems, in CHI'88 Con\\beren\\te Pro\\teedings (Washing\\fon, D.C.), pages 281- 285, New York, Ma\\t. ACM.","E\\fron and R.J. Tibshirani (1997) Improvemen\\bs on cross-valida\\bion: \\bhe .632+ boo\\bs\\brap me\\bhod. J. Amer. S\\fa\\fis\\f. Asso\\t, 92:548–560.","Hirs\\b, Graeme and David S\\b.-Onge (1997) Lexical chains as represen\\ba\\bion o\\f con\\bex\\b \\for \\bhe de\\bec\\bion and correc\\bion o\\f malapropisms. In Chris\\biane Fellbaum, edi\\bor, Wordne\\f: An ele\\t\\froni\\t lexi\\tal da\\fabase and some o\\b i\\fs appli\\ta\\fions. MIT Press, Cambridge, pages 305-332.","Lascarides, A., Asher, N. (2007) Segmen\\bed Discourse Represen\\ba\\bion Theor\\t: D\\tnamic Seman\\bics wi\\bh Discourse S\\bruc\\bure, in H. Bun\\b and R. Muskens (eds.) Compu\\fing Meaning: Volume 3, pp87--124, Springer.","Kamp, H. and Re\\tle, U. (1993) From Dis\\tourse \\fo Logi\\t. An In\\frodu\\t\\fion \\fo Model\\fheore\\fi\\t Seman\\fi\\ts o\\b a\\fural Language, Formal Logi\\t and Dis\\tourse Represen\\fa\\fion Theory, Kluwer Academic Publishers, Dordrech\\b Ne\\bherlands.","T. Miller (2004) Essa\\t Assessmen\\b wi\\bh La\\ben\\b Seman\\bic Anal\\tsis. Journal o\\b Edu\\ta\\fional Compu\\fing Resear\\th 28.","S. Mika, G. Rä\\bsch, J.Wes\\bon, B. Schölkop\\f, and K.-R. Müller (1999) Fisher discriminan\\b anal\\tsis wi\\bh kernels. In Y.-H. Hu, J. Larsen, E. Wilson, and S. Douglas, edi\\bors, eural e\\fworks \\bor Signal Pro\\tessing IX, pages 41–48. IEEE.","Mika, A.J. Smola, and B. Schölkop\\f (2001) An improved \\braining algori\\bhm \\for kernel Fisher discriminan\\bs. In T. Jaakkola and T. Richardson, edi\\bors, Pro\\teedings AISTATS 2001, pages 98–104, San Francisco, CA.","K.-R. Müller, S. Mika, G. Rä\\bsch, K. Tsuda, and B. Schölkop\\f (2001) An in\\broduc\\bion \\bo kernel-based learning algori\\bhms. IEEE Transa\\t\\fions on eural e\\fworks, 12 (2):181–201.","C. Saunders, M.O. S\\bi\\bson, J. Wes\\bon, L. Bo\\b\\bou, B. Schölkop\\f, and A.J. Smola (1998) Suppor\\b vec\\bor machine re\\ference manual. Te\\thni\\tal Repor\\f CSD-TR-98-03, Ro\\tal Hollowa\\t Universi\\b\\t, London.","Schölkop\\f and A.J. Smola (2002) Learning wi\\fh Kernels. MIT Press, Cambridge, MA.","Schroeder, Larr\\t D., David L. Sjoquis\\b, and Paula E. S\\bephan (1986) Unders\\banding regression anal\\tsis: An in\\broduc\\bor\\t guide. Thousand Oaks, CA: Sage Publica\\bions. Series: Quan\\fi\\fa\\five Appli\\ta\\fions in \\fhe So\\tial S\\tien\\tes, o. 57","John S. Ta\\tlor and Nello Cris\\bianini (2004) Kernel Me\\fhods \\bor Pa\\f\\fern Analysis. Cambridge Universi\\b\\t Press, New York, NY, USA.","K. Tsuda, G. Rä\\bsch, S. Mika, and K.-R. Müller (2001) Learning \\bo predic\\b \\bhe leave-oneou\\b error o\\f kernel based classi\\fiers. In G. Dor\\f\\fner, H. Bischo\\f, and K. Hornik, edi\\bors, Ar\\fi\\bi\\tial eural e\\fworks — ICA’01, pages 331–338. Springer Lec\\bure No\\bes in Compu\\ber Science, Vol. 2130.    "]},{"title":"2874","paragraphs":[]}]}