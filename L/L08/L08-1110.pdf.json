{"sections":[{"title":"Using Lgliner Mdels fr Selecting Best Mchine Trnsltin Output Michel Crl","paragraphs":["Institut r Anewandte Inormationswissenschat Martin Luther Str. 14, 66111 Saarbrcken, Germany E-mail: carl@iai.uni-sb.de Abstrct We describe a set o experiments to explore statistical techniques or rankin and selectin the best translations in a raph o translation hypotheses. In a previous paper (Carl, 2007) we have described how the hypotheses raph is enerated throuh shallow mappin and permutation rules . We have iven examples o its nodes consistin o vectors representin morpho-syntactic properties o words and phrases. This paper describes a number o methods or elaboratin statistical eature unctions rom some o the vector components. The eature unctions are trained o-line on dierent types o text and their lo-linear combination is then used to retrieve the best translation paths in the raph. We compare two lanuae modellin toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based eature unction models produce better results than token-based models, 2) addin a PoS-ta eature unction to the word-lemma model improves the output and 3) weihts or lexical translations are suitable i the trainin material is similar to the texts to be translated"]},{"title":"1. Intrductin","paragraphs":["Feature unctions are bein increasinly used in recent Machine Translation (MT) approaches to select and rank translation candidates. Developed within a statistical processin ramework (Och and Ney, 2002), the lo-linear combination o eature unctions provide a lexible ramework or discriminative modelin that allows to combine disparate and overlappin sources o inormation in a sinle model Accordin to Oepen et al. (2007), \"a lo-linear model is iven in terms o (a) a set o speciied eatures that describe properties o the data, and (b) an associated set o learned weihts that determine the contribution o each eature.\" Each pair o source sentence f and taret sentence e is represented as a real-valued eature vector h. A vector o weihts w is then trained to optimize some objective unction o the trainin data so as to allow a search procedure to ind the taret sentence ê with the hihest probability: A number o eature unctions have been explored in various system implementations (e.. Oepen et al. 2007, Liu et al. 2007, Quirk 2007), separatin the eatures rouhly into source eatures, channel eatures and taret eatures. Source eatures include probabilities o representations resultin rom the SL analysis such as likelihood o the parse tree and dictionary matchin. Channel models include SL-to-TL alinment and lexical translation probabilities. Taret eatures reer to probabilities o the enerated TL sentence, includin, amon other thins, n-gr lanuae models. In contrast to most purely statistical MT systems1",", we use rule-based methods to enerate an AND/OR raph o translation hypotheses (Carl 2007) and a beam search alorithm to traverse the raph and eature unctions to rank the paths, as e.. in the MISTRAL system (Patry et 1 A number o recent SMT architectures are described http://iwslt07.itc.it/menu/proram.html"]},{"title":"1140","paragraphs":["al, 2007). We report on two experimental settins, usin dierent types o eature unctions and dierent lanuae modellin toolkits2",". The experiments rely on our types o taret models and a statistical lexical translation model which are described in section 2. All o the lanuae models were trained on the BNC3",". The BNC is a collection o taed texts makin use o the CLAWS5 ta set which comprises rouhly 70 dierent tas. We also tested a lemma-ta co-occurrence model which was also trained on the BNC. In section 3 we describe additional eature unctions, The lexical translation models are trained on an excerpt o the EUROPARL corpus. The aim o the experimental settin was to iure out which combination o lanuae models and lexical weihts can enhance the quality o the translations, and whether the system can be tuned to a particular domain with lexical weihts."]},{"title":"2. Cmpring different lnguge mdels","paragraphs":["In a previous set o experiments (Carl 2007) we compared several lanuae models trained with the CMU lanuae modellin toolkit4",". The CMU lanuae modellin toolkit enerates n-gr lanuae models (LMs) rom tokenized texts. The CMU toolkit enerates a vocabulary o up to 65535 words which occur most requently in the trainin material. It supports open LMs which account or unknown words and closed LMs which assume all tokens to be known in the trainin material. A LM made up o CLAWS5 tas would be a closed lanuae model since there are less than 70 dierent tas in this ta set and all tas are likely to occur in the trainin material. The closed LMs assume that only items in the trainin data will occur in the test data, while open LMs save some o the probability mass or (unknown) words in the test data which did not occur in the trainin set. These 2 Due to time constraints, we did not compare our results with Moses (Koehn et.al, 2007), which also provides possibilities o deinin eature unctions. 3 The British National Corpus (BNC) consists o more than 100 million words in more than 6 million sentences http://www.natcorp.ox.ac.uk/ 4 The CMU SLM toolkit can be downloaded rom http://www.speech.cs.cmu.edu/SLM_ino.html words will be mapped on the item UNK. To ind most suited LMs or our application, we have experimented with the ollowin parameters: 1. open token-based LM: This lanuae model works on (lower-cased) surace word-orms. 2. closed mixed token-ta LM: The vocabulary o this model consists o word tokens. Unknown words are mapped on their CLAWS5 ta. 3. closed mixed lemma-ta LM: The vocabulary o this model consists o lemmas. Similar to the closed mixed lemma-ta model, unknown lemmas are mapped on their CLAWS5 ta. 4. orthoonal lemma-ta LM: In the orthoonal lemma-ta model we actually computed three LMs: 1. an n-gr CLAWS5 ta model 2. an -gr lemma model 3. a co-occurrence weiht o the lemmas, ta accordin to Laplace's law with the ollowin equation: Where N is hal the number o dierent tas, C(le) is the number o occurrences o the lemma in the BNC and C(le,tg) is the number o co-occurrences o a lemma and a ta. Dierent n-gr lanuae models were computed, based on 20K, 100K, 1M 5M and 6M sentences, rom the BNC and with:"]},{"title":"1141","paragraphs":["○ n={3,4,5} or the lemma and token models ○ n={3,4,5,6,7} or the ta models In a set o experiments (c. Carl 2007) we iured out that the orthoonal lemma-ta LM had the best perormance, usin a 3-gr lemma model rom 5M sentences and a 7-gr ta model also rom 5M sentences. These tests were run on a 200 sentences test corpus. There were three reerence translations or each test sentence. The sentences were selected (and partially constructed) so that they cover a rane o known translation problems includin: 1. lexical translation problems: separable preixes, ixed verb constructions, deree o adjectives and adverbs, lexical ambiuities, and others 2. syntactic translation problems: pronominalization, determination, word order, dierent complementation, relative clauses, tense/aspect, head switchin, prepositions, cateory chane, and others."]},{"title":"3. Testing Cnditins","paragraphs":["In another experiment we have extended the previous settin with the ollowin eatures: 1. Usin the SRI5","lanuae modellin toolkit instead o CMU","2. Usin an additional test corpus o 200 sentences rom the EUROPARL6","corpus 3. Addin a urther eature which takes into account weihts o lexical translations"]},{"title":"3.2 Perfrmnce f CMU nd SRI tlkits","paragraphs":["While both, CMU and SRI toolkits are open source projects, the reasonin or usin SRI was to see whether there is any dierence in perormance compared to the CMU-toolkit, based on the act that: ○ SRI is reerred to by most o the recent (MT) publications which make use o n-ram statistical 5 which can be downloaded rom http://www.speech.sri.com/projects/srilm/ 6 EUROPARL is available at http://people.csail.mit.edu/koehn/publications/europarl/ lanuae modellin ○ SRI is extensively maintained, while the development o the CMU-kit seems to have stopped around 10 years ao ○ SRI eatures numerous backo models and allows or unlimited vocabulary The SRI packae is somewhat easier to use, due to its C+ + implementation. However, it also seems to be a bit slower7",". We did not have the time to experiment with all the backo models, mainly stickin to the (enerally recommended) Kneser-Ney and Good-Turin Discount strateies. With these settins we did not ind a siniicant dierence between the SRI and the CMU toolkit. Notice, that we only experimented with the rthgnl lemmtg LM and an rthgnl tken tg mdel as discussed earlier."]},{"title":"3.2 Test Crpus","paragraphs":["The EUROPARL corpus was used or several reasons. First, we wanted to see how our results on the 200 test sentences compare with a set o sentences which have now become a standard or data-driven MT. We thereore extracted a set o 200 test translations rom the EUROPARL corpus and run a number o identical system settins on both texts. Second, we wanted to see whether knowlede o a hue number o translations could be proitably used to enhance the quality o the METIS translations. The results o the comparison is iven in section 4."]},{"title":"3.3 Trining dictinry weights","paragraphs":["For testin the adaptability o the system to EUROPARL terminoloy and text, we extracted a set o 10.000 translations rom another slide o the EUROPARL corpus which did not include our 200 test sentences. All sentences had at most 32 words. We did not consider alined sentences pairs where one lanuae side was 7 Koehn et al (2007) report that loadin and decodin times in Moses are much aster than with the SRI tool, due to more compact 8 bit data storae and eicient access to data."]},{"title":"1142","paragraphs":["empty. This trainin set was used to estimate and train weihts or our available dictionary entries. A urther eature unction would then take these weiht into account to compute the most likely translations, hence with a broader knowlede. Weihin o the lexicon is only crucial or ambiuous entries in order to discriminate between dierent translations or a German SL expression. The weihts o all other entries which have one sinle translation were set to 0.001. For entries with more than one translation option, weihts were computed as ollows. For each word in every SL sentence o the 10.000 sentences corpus we looked up the dictionary and checked whether an entry covers a word in the correspondin TL sentence. A hit h(g <-- ) was assined or entries where a German word g matches in the SL side and an Enlish word matches in the TL side o an alinment. We count as noise n(g <-- ) dictionary entries which match a word in the SL but with no realization o the translation in the TL side o the alinment. We then sum up hits and noise or all ambiuous entries over all 10.000 reerence sentences. Followin this, we compute the cumulated hit rate (g) or the German SL words, which amounts to summin the hit rate over all translation ambiuities o g. The weiht w(g <-- ) o a lexicon entry was inally computed as the ratio o the cumulated hit (g) o an entry g divided by the noise o the entry and the number o hits produced by the word . The weiht w is thus a number 0 < w <= 1. It is 1 i an entry has only hits and no other translation option o g was seen in the data, ire. i (g) equals h(g <-- ). It is close to 0 i a dictionary entry produces mainly noise accordin in our data."]},{"title":"4. Evlutin","paragraphs":["We started the evaluation experiments with usin only one eature unction, and then incrementally added urther eature unctions to see whether and how the system output improves. We started by runnin a coupe o tests on our irst test corpus and on the EUROPARL corpus usin an pen tkenbsed and an pen lemm bsed LM. Second we added various ta lanuae models. The results can be seen in Fiures 1 and 2. All iures represent BLEU scores. In Fiures 3 and 4 we added the lemmas-ta co-occurrence model (TTF) as described above, and a lexical weiht unction (Lex). These results are plotted in Fiures 3 and 4. The 'No Ta LM' raphs (lower lines in Fiures 1 to 4) plot the baseline model usin only one lanuae model. The let sides o these raphs show perormance when usin the open token-based LM. The riht side shows perormance with an open lemma-based LM8",". The token and lemma models use the same number o 100K, 1M and 2M sentences and 3, 4, 5 and 6-grs and they were enerated with the SRI toolkit. Surprisinly, in Fiure 1, or the 200 test sentences, the 3-gr model based on 2M sentences yields results which are worse than those produced by the 3 and 4-gr models based on a set o 1M sentences. This is dierent on the EUROPARL corpus in Fiures 2 and 4. The lemma-based models perorm consistently better than the token-based models. Best perormance is reported with a 3-gr model based on 2M sentences. For both test sets, the perormance is also consistently better when addin a ta model to this baseline. We compared seven ta models, and combined them with the lemma and token models. The ta models were CLAWS5 tas rom the BNC usin 100K, 1 M and 5M sentences and with n={3,4,5,6}. As a tendency, usin larer n provides in many cases better results than increasin the size o the trainin corpus. Best results are enerated by the 5-gr models with 100K and 1M sentences. When combinin the token /lemma models with the ta models, we tested various weihtin distributions. The weiht o a eature determines how much this unction contributes to the outcome o the ranker. Lower weihts would indicate a smaller contribution o that eature unction while hiher weiht would ive the eature more importance. We tested approximately 10 dierent weihts or each eature, so that every token-ta and lemma-ta lanuae model combination was tested on rouhly 100 dierent distributions o the eature shares. 8 The Open lemma-based model is based on lemmatised orms. In contrast to the CMU model the SRI lanuae models used here allow or an unrestricted size o the vocabulary."]},{"title":"1143 Figure 1: Test set of 200 sentences using vrious token, le nd tg lnguge odels. Figure 2: Test set of 200 EUROPAR sentences using vrious lnguge odels. 1144  Figure 3: Results for the 200 sentence corpus when dding lexicl nd token-tg cooccurrence functions. Figure 4: Results for the 200 EUROPAR sentences dding lexicl nd token-tg cooccurrence functions. 1145","paragraphs":["Weihts were between 0 and 1.0 and the best results shown in the Fiures 1 to 4 use an equal weiht o 0.6 or the token, lemma and ta models. Then we added urther eature unctions to this baseline scenario . Fiure 3 repeats the baseline rom Fiure 1, the 'No Ta LM' and the combination with the 100K, 5-ram ta model. In addition it shows the impact when addin lexical weihts and when addin the token-ta co-occurrence unction. Here also, we experimented with numerous weihtin distributions, but or none o them the overall perormance siniicantly increases. This is dierent or the EUROPARL test set in Fiure 4. Fiure 4 also plots the baseline systems rom Fiure 2, the ''No Ta LM” and the combination with the 100K, 5-ram ta model. As can be seen clearly in the raph, when addin the Lex unction, better results are produced, so that we obtain BLEU values o more than 0.09 on this set. However, here also, the TTF unction does not produce any additional improvement."]},{"title":"5. Cnclusin","paragraphs":["We resume our indins: 1. Lemma-based models produce better results than token-based models. We ind that (althouh not consistently) increasin the size o the trainin material or lemma models provides better results than increasin the lenth o the n-gr models. 2. Addin a ta model improves the output in any case. Contrary to the indins or the token and lemma models, larer values o n (in our case n=5) may be an easier way to increase perorm than to increase the size o the trainin set. 3. Addin token-ta co-occurrence statistics as a urther eature unction does not help. 4. Lexical weihts are suitable i the trainin material is similar to the texts to be translated (i.e. they are rom the same domain)."]},{"title":"6. References","paragraphs":["Allen, Jerey and Hoan, Christopher (2000) Toward the Development o a Posteditin Module or Raw Machine Translation Output: A Controlled Lanuae Perspective. In Proceedings of the Third Interntionl Workshop on Controlled nguge Applictions (CAW00). Seattle, Washinton: Association or Computational Linuistics, April 29-30, 2000, pp. 62-71. Carl, Michael (2007) METIS-II: The German to Enlish MT System, In Proceedins o the 11th Machine Translation Summit, Copenhaen, Denmark Font Llitjós, Ariadna and Carbonell, Jaime G. (2006) Automatin Post-Editin to improve MT systems. In Proc. of the Workshop on Autoted Post-Editing t AMTA-2006, Cambride, USA. Philipp Koehn, Hieu Hoan, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit or statistical machine translation. In Proceedins o the 45th Annual Meetin o the Association or Computational Linuistics Companion Volume Proceedins o the Demo and Poster Sessions, paes 177–180, Praue, Czech Republic, June 2007","Kniht, Kevin and Ishwar Chander (1994) Automated Post-editin o Documents, In Proceedings of the 12th Ntionl Conference on Artificil Intelligence, Seattle, Washinton, 779-784. Patry, Alexandre; Philippe Lanlais, Frédéric Béchet (2007), MISTRAL: A Lattice Translation System or IWSLT 2007, In Proceedings of Interntionl Workshop on Spoken nguge Trnsltion, Trento, Italy, available at: http://iwslt07.itc.it/menu/proram.html Phaholphinyo, Sitthaa, Modhiran, Teerapon, Kritsuthikul, Nattapol and Supnithi, Thepchai (2005) A Practical o Memory-based Approach or Improvin Accuracy o MT. In MT Suit X. Phuket Island, Thailand. Povlsen, C. & A. Bech: Ape (2001) Reducin the Monkey Business in Post-Editin by Automatin the Task intelliently. In: MT Suit VIII, Mchine Trnsltion in the Infortion Age, Santiao de Compostela, Spain, p. 283-286. Oepen, Stephan and Velldal, Erik and Lonnin, Jan Tore and Meurer, Paul and Rosen, Victoria (2007) Towards Hybrid Quality-Oriented Machine Translation -- On"]},{"title":"1146","paragraphs":["Linuistics and Probabilities in MT, In Proceedins o the 11th International Conerence on Theoretical and Methodoloical Issues in Machine Translation (TMI) Och, Franz J. (2002) Minimum Error Rate Trainin in Statistical Machine Translation, In Proceedin o ACL, Liu, Zhanyi and Wan, Haien and Wu Hua (2007) Example-Based Machine Translation Based on TSC and Statistical Generation, In Machine Translation (20). Quirk, Christopher and Menezes, Arul (2007) Dependency Treelet Translation: The converence o statistical and example-based machine translation?, In Machine Translation (20)."]},{"title":"1147","paragraphs":[]}]}