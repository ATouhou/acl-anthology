{"sections":[{"title":"The Context (not only) for Humans Barbora Hladká","paragraphs":["Institute of Formal and Applied Linguistics Charles University","Malostranskénáměstı́25","118 00 Prague Czech Republic e-mail: hladka@ufal.mff.cuni.cz","Abstract Our context considerations will be practically oriented; we will explore the specification of a context scope in the Czech morphological tagging. We mean by morphological tagging/annotation the automatic/manual disambiguation of the output of morphological analysis. The Prague Dependency Treebank (PDT) serves as a source of annotated data. The main aim is to concentrate on the evaluation of the influence of the chosen context on the tagging accuracy."]},{"title":"1. English and Czech Tagging Experiments","paragraphs":["The corpus-based approaches determine the amount of human work involved in the NLP tasks on the building of training data and on the coming up with an algorithm giv-ing results as precise as possible. The ideas of context specification cannot be left out in the formulation of the algorithm. The scope of context must be specified according to the character of the particular NLP task. As we consider the nature of context from the perspective of the tagging application, an elementary unit we process is a word token. In general, there is no strict rule saying how many preceding and following word tokens we should look at to be sure that we tag the word token properly. Thus, let us have a look at the empirical experience. STRATEGY TAGGER TRAINING TA","ID DATA (%) Trigram MM EN Associated 97.0 Markov model Press (Merialdo, 1994) (955Kw) Maximum Entropy ME EN WSJ 96.6 (Ratnaparkhi, 1996) (926Kw) Exponential model EXP EN WSJ 96.8 (Hajič, Hladká, 1998b) (1.2 Mw) Memory-based MB EN WSJ 96.4 (Daelemans, Zavrel, 1996) (2Mw) Rule-based RB EN WSJ 96.9 (Brill, 1993b) (600Kw) Neural Networks NE EN WSJ 96.2 (Schmid, 1994) (2Mw) Table 1: Tagging experiments on English","Let","","","","","","  ","","be an input text to be tagged.","As all the presented tagging strategies tag the input text","in the left-to-right direction, a word token","","is processed","when the word tokens","","","","","","","","have already been","tagged -","","","","","","","","    ","","","","","","","","1","For the currently 1 As the EXP tagger operates on a subtag level let ma","consist STRATEGY TAGGER TRAINING TA","ID DATA (%) Bigram MM CZ","PDT 92.50","Markov model (300Kw) Trigram MM CZ","PDT 93.38","Markov model (300Kw)","Exponential model EXP CZ PDT 93.85","(300Kw) Table 2: Tagging experiments on Czech","processed word token  , the context","","","","","of the repre-","sentative corpus-based tagging strategies for tagging Czech","(MM CZ","",",MM CZ , EXP CZ taggers - see Tab. 12",") and","English (MM EN, ME EN, EXP EN, MB EN, RB EN,","NE EN taggers - see Tab. 2) can be expressed as follows:","MM EN -","","","","","","","","","   ","","","","","","","","","","","MM CZ","","-","","","","    ","","","","","","","MM CZ","-","","","  ","","","","","","ME EN -","","","","","","","","","","","","  ","","","","","","","","","","","","","","","EXP CZ, EXP EN -","","","","    ","","","","","","","","","","","","","","","","","","","","","","","","","","","   ","","","","","","","","","","","","","","","","","","","","","  ","","","","","","","","",""," MB EN - not directly specified","RB EN -","","","","","","","","","","","   ","","","","","","","","","","","","","","","","","","","","","","","","","","","NE EN -","","","","","","","","","","","","","   ","","","","","","","","","","","","","","","","","","","","","","Observing the given description, the Markov models","are locally (processing","",") based only upon the left-hand of all possible values of i-th morphological category within the positional tag.","2","(Hladká, 2000) provides very detailed view on the issue of Czech language tagging. side context3","while the other strategies look not only at the left positions but consider also the right-hand side context. Some authors offer practical experience with a modifica-tion of the context scope. In the paper (Schmid, 1994), the author describes the context shrinking to two preceding and one following words together with their tags which causes accuracy reduction only by 0.1%. Enlarging the context gave no improvement. The authors (Daelemans, Zavrel, 1996) do not specify directly the context scope, but they construct a distance metrics between similar environments within modest contexts. We can conclude that the enumerated contexts as a whole are limited up to 4 positions to the left/right."]},{"title":"2. The Context for Humans","paragraphs":["At the starting point of the tagging procedure, all tagging strategies are given the same input text. The input text (as a whole) is understood as a whole text context. Consequently, the tagging strategies select from the whole text context any subcontext over which they process the given word token. Let us limit the subcontext to the word tokens (","","","","","","","","","","",") preceding the currently processed word token (","",") within the input text. For a vocabulary size n there are","","","","different subcontexts (for ex. n = 1,000 and i=4then","","","","","). The problem which immediately appears concerns the matrices (of","","","","order) represent-ing the counts of particular subcontexts within the training corpus. With regard to the astronomically large number of such subcontexts, a vast majority of the possible subcontexts will never occur in Czech (or other natural language) and that is the reason why the given matrices are sparse. Nevertheless, the computational linguists’ effort is directed to deal with sparseness of data being connected with context specification.","None of the representative corpus-based tagging methods do achieve the magic point of 100% performance (Table 1, 2). It is supposed that the context can reveal almost all the secrets of a language. We stress almost, in some cases the context is not enough to specify the function/meaning of a word form. As we are interested in context-based models of a language, the magic point of such models cannot be 100% because the world knowledge which is hidden somewhere between the lines cannot be read in the set of word forms and tags.","The bigram and trigram MMs employ the smallest left-hand side context size relatively to the other corpus-based methods; at the same time, their performances are the best (Table 1, 2). We believe that a further improvement of MMs lies in a better selection of the analysed context. Not to limit ourselves only to experiments modifying the context size and in order to discover certain guidelines we explore how people handle the information coming from the predefined left-hand side context. 3","In the end, the incorporation of the Viterbi algorithm to find the best tag sequence means the usage of the right-hand side context. 2.1. Prerequisites","The annotation of the test file was assigned to a group of 5 students: 2 undergraduate students (S1, S2) with rich experience learned during the annotation of the PDT; 3 computational linguistics graduate students (S3, S4, S5) - one of them (S5) with an experience with various tagging strategies and one of them (S4) being bilingual not educated in Czech. The test file that was given to the students comprised a 283 word token subset (141 unambiguous tokens and 142 ambiguous tokens) of the test file which we used in the tagging experiments mentioned above (MM CZ","",", MM CZ",", EXP CZ). For purposes of evaluation of the tagging and annotation, the given test file was annotated independently by another annotator upon an unlimited context.","2.1.1. Formalism","Let S =","",""," ","","be a sentence4","(a sequence of","word tokens) we tag/annotate in the left-to-right direction,","","","","","","","","","  be a list of word tokens occurring in","the sentence S. While tagging the i-th word, the i-1 preced-","ing word tokens are already tagged by tags","","","","","","","","","","","let T be a list of tags  ","","","","","","",".","The contexts which come into play during the experi-","ments of annotation (BC, TC, SC) and the experiments of","tagging (TTC, BTC) can be defined as functions:","","Bigram Context (BC) as a function","","   ","","","","","","","","","","","","","    ","","","","","","","","","Tag Bigram Context (TBC) as a function","","","   ","","","","","","","","","","","","    ","","","","","","","","","","Trigram Context (TC) as a function","","    ","","","","","","","","","","","","","","","","","","  ","","","","","","","","","","","","","  ","","","","","","Tag Trigram Context (TTC) as a function","","    ","","","","","","","","","","","","","","","   ","","","","","","","","","","","","","  ","","","","","","Sentence Context (SC) as a function","","   ","","","","","","","","","","","","","     ","","","","","","","","","","","","To illustrate the defined terms, let us assume a sample of the sentence fragment O dalšı́Střı́brné medvědy se podělily ... [lit. about – further – Silver – Bears – Refl. – they-shared ..., E. The remaining (Prizes of) Silver Bears were obtained by ...] and let us suppose that the first four word tokens are already tagged O","RR--4-- -------- dalšı́","AAMP4----1A---- Střı́brné","AAMP4----1A---- 4 We consider a context within a sentence, we do not cross the","sentence boundaries.","medvědy NNMP4-----A----. Then the word token se is to","be tagged/annotated. According to the chosen particular","context, the word token se is being processed within the","context information embodied in one of the sets BC(se) =","","medvědy , TC(se) =","Střı́brné, medvědy",", SC(se) =","O,","dalšı́, Střı́brné, medvědy",", TTC(se) =","AAMP4----1A----,","NNMP4-----A----",", BTC(se) =","NNMP4-----A----",". 2.2. How Humans Treat the Context Information","A specially developed tool for morphological annotation (Hajič, Hladká, 1998b), which offers an easy disambiguation of lemmas and tags (which are outputs of the automatic morphological analysis), was used as a disambiguation tool, which displays, for the currently annotated ambiguous word token, its morphological information and the whole text context. For our aims, we have modified the disambiguation tool in the sense of the visibility of a partial context; in case of Bigram Context only the previous word token is visible, for Trigram Context only two previous word tokens are, and finally, for Sentence Context the preceding word tokens up to the beginning of the sentence are at the annotator’s disposal. We have to stress that unambiguous word tokens remain obviously untouched by the annotator and while annotating the given ambiguous word token the annotators have no information on the assigned tags of the word tokens which are included in the context; annotators just suggest a hypothesis of the tags of the context word tokens themselves. On the other hand, the presented Markov models working over Tag Trigram/Bigram Context do not deal with the word tokens."]},{"title":"3. Discussion of the Results","paragraphs":["Table 3 provides information on the evaluation of the annotation and tagging of the given test file. Reading the table horizontally, we observe that all the students are getting better as the context enlarges. Reading the table vertically, we speculate that the learned experience in the course of the annotation over the whole context comes into play (students S1, S2 vs. students S3, S4, S5). On the other hand, the knowledge of the tagging methods seems not to be so important (student S5). The bigram MMs beat the students annotating over the bigram context TBC. However, the situation is inverse for the trigram contexts TTC, TC - annotation almost beats tagging.","Tables 4 and 5 give a detailed view on the annotation/tagging on a subtag level5",". A more interesting observation concerns the way how the error rate over these MCs changes as the context enlarges.","Looking at Tab. 6, the numbers represent decreas-ing/increasing (positive/negative numbers) of the error rates over the MCs gender, number, case for each student and the MM taggers. For example, for student S3, the","5","We present only the most problematic morphological categories (MCs) - gender, number, case - together with part of speech (POS) and sub-part of speech (SUBPOS).","context BC TC SC TTC TBC","annotator/ # of incorrectly processed ambiguous tagger word tokens out of 142 ambiguous S3 36 20 16 - - S4 47 32 27 - - S1 26 20 9 - - S2 16 13 7 - - S5 29 20 17 - -","MM CZ","","- - - 20 -","MM CZ","- - - - 24 Table 3: The evaluation of tagging and annotation over the predefined contexts annotator/ context POS SubPOS tagger BC 0.71 0.71","S3 TC 0.35 0.35 SC 0.00 0.35 BC 1.06 1.41","S4 TC 1.77 2.12 SC 0.35 0.71 BC 0.35 0.71","S1 TC 0.35 0.71 SC 0.00 0.35 BC 0.35 0.35","S2 TC 0.35 0.35 SC 0.35 0.35 BC 0.06 1.77","S5 TC 0.00 0.35 SC 0.35 0.71","MM CZ BTC 0.71 0.71","MM CZ"," TTC 0.71 0.71 Table 4: Error rates (%) over the POS, SubPOS error rate over gender decreases by 0.36% if the bigram context (BC) is enlarged to the trigram context (TC) and at the same time it decreases by 0.7% if the trigram context (TC) is enlarged to the sentence context (SC). Given the Czech typical word order and given the assumed left-hand side contexts, the improvement of the case error rate is more expressive than the changes of the gender and number error rates. Again, given the Czech word order, it is necessary to include the right-hand side context to identify the gender and number of the word token.","The strategy of human annotation described above can be understood only as a simulation of the MMs. The humans work with the left-hand side context from the beginning till the end; the MMs assign to the currently processed word token tags with regard to the left-hand side context as well, but the incorporation of the Viterbi algorithm to find the best tag sequence which means, in fact, the usage of the right-hand side context in fact.","Putting together this fact and the insufficiently represenannotator/ context g n c tagger BC 4.95 3.18 8.13","S3 TC 4.59 1.77 2.83 SC 3.89 1.41 2.47 BC 6.36 3.53 13.43","S4 TC 4.24 3.18 8.83 SC 4.95 2.47 6.36 BC 4.59 1.77 5.65","S1 TC 2.83 1.77 4.24 SC 2.12 1.06 1.41 BC 2.83 0.35 3.53","S2 TC 1.06 0.35 3.53 SC 1.41 0.35 1.06 BC 6.36 2.47 6.01","S5 TC 4.95 2.12 3.89 SC 4.59 2.47 3.89","MM CZ BTC 2.47 0.71 6.71","MM CZ"," TTC 2.12 0.35 5.30 Table 5: Error rates (%) over the gender, number, case","morphological g n c","category","annotator/ context the error rate","tagger enlarging improvement (%) TC","BC 0.36 1.41 5.3","S3 SC","TC 0.7 0.36 0.36 TC","BC -0.71 0.35 4.6","S4 SC","TC 1.41 0.71 2.47 TC","BC 1.76 0.00 1.41","S1 SC","TC 0.71 0.71 2.83 TC","BC 1.77 0.00 0.00","S2 SC","TC -0.35 0.00 2.47 TC","BC 1.68 0.35 2.12","S5 SC","TC 0.36 -0.35 0.00","MM CZ TTC","TBC 0.35 0.36 1.41 Table 6: The error rate changes (%) due to the context enlarging tative size of the test sample we cannot make any definite conclusions. On the other hand, the presented results offer the idea that the sentence context (SC) can be sufficient for successful context-based approaches. We speculate that it is not necessary to take the sentence context (SC) as a whole, but dynamically to select a trigram subcontext from the sentence context. The next step toward the specification of dynamic selection strategy will concern the type of information were used in human deciding limited by the SC (like for the human improvement of the speech recognizer’s output, see (Brill et al., 1998)."]},{"title":"4. Acknowledgement","paragraphs":["The results described herein have been obtained partially within the individual grant of the OSF/HESP No. 135/1998 and partially within the Ministry of Education Project No. VS9615."]},{"title":"5. References","paragraphs":["E. Brill. Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach. In Proceedings of the 3rd International Workshop on Parsing Technologies, Tilburg, Netherlands, 1993.","E. Brill, R. Florian, J. C. Henderson and L. Mangu. Beyond N-Grams: Can Linguistic Sophistication Improve Language Modeling? In Proceedings of the AC/COLING Conference, pp. 186-190, Montreal, Canada, 1998.","W. Daelemans and J. Zavrel. MBT: A Memory-Based Part of Speech Tagger-Generator. In Proceedings of the Workshop on Very Large Corpora, pp. 14-27, Copenhagen, Denmark, 1996.","J. Hajič. Disambiguation of Rich Inflection (Computational Morphology of Czech). Charles University Press - Karolinum, in press.","J. Hajič and B. Hladká. Probabilistic and Rule-Based Tag-ger of an Inflective Language - a Comparison. In Proceedings of the 5th Conference on Applied Natural Language Processing, pp. 111-118, Washington, USA, 1997.","J. Hajič and B. Hladká. Czech Language Processing - PoS Tagging. In Proceedings of the First International Conference on Language Resources & Evaluation, pp. 931-936, Granada, Spain, 1998.","J. Hajič and B. Hladká. Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset. In Proceedings of COLING-ACL Conference, pp. 483-490, Montreal, Canada, 1998.","B. Hladká. Czech Language Tagging. PhD Thesis at the Institute of Formal and Applied Linguistics, Charles University, prague, Czech Republic, 2000.","B. Merialdo. Tagging English Text with a Probabilistic Model. In Computational Linguistics, 20(2), pp. 155-171, 1994.","A. Ratnaparkhi. A Maximum Entropy Model for Part-of-Speech Tagging. In Proceedings of the First Empirical Methods in Natural Language Processing Conference, pp. 133-141, Philadelphia, USA, 1996.","H. Schmid. Part-Of-Speech Tagging with Neural Networks. In Procedings of the 15th COLING Conference, pp. 172-176, Kyoto, Japan, 1994."]}]}