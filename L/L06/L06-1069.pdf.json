{"sections":[{"title":"A Conditional Random Field Framework for Thai Morphological Analysis Canasai Kruengkrai, Virach Sornlertlamvanich, Hitoshi Isahara","paragraphs":["Thai Computational Linguistics Laboratory","National Institute of Information and Communications Technology","112 Paholyothin Road, Klong 1, Klong Luang, Pathumthani 12120, Thailand","{canasai,virach}@tcllab.org, isahara@nict.go.jp","Abstract This paper presents a framework for Thai morphological analysis based on the theoretical background of conditional random fields. We formulate morphological analysis of an unsegmented language as the sequential supervised learning problem. Given a sequence of characters, all possibilities of word/tag segmentation are generated, and then the optimal path is selected with some criterion. We examine two different techniques, including the Viterbi score and the confidence estimation. Preliminary results are given to show the feasibility of our proposed framework."]},{"title":"1. Introduction","paragraphs":["Morphological analysis is the process of segmenting text into morphemes and performing some tasks such as word formation analysis or part-of-speech (POS) tagging. Morphological analysis is often an initial step for many kinds of text analysis of any languages. In English and other Western languages, text can be tokenized into words by whitespace or punctuation, and morphological analysis can start by considering words as primitive units. In unsegmented languages, such as Chinese, Japanese, and Thai, words are not explicitly delimited by whitespace. As a result, the problem of morphological analysis is more difficult for these languages. More specifically, in the context of Thai morphological analysis, a major challenge is how to solve ambiguities of both word boundary detection and POS tagging simultaneously. The morphological analysis task can be thought of as the sequential supervised learning problem (Dietterich, 2002), where text is formulated as a sequence of characters. In sequential supervised learning, one of the most widely used techniques is hidden Markov models (HMMs). Based on the concept of generative models, HMMs typically define the joint probability distribution p(y, x) over an observation sequence x and a label sequence y. The limitation of generatively-trained models is that they must make independent assumptions among elements of the observation sequence in order to make inference tractable. In the morphological analysis task, non-independent elements of the observation sequence, such as prefixes, suffixes, or surrounding words, are useful features for learning and predicting. One solution to relax the independent assumptions is to formulate the model with the conditional probability distribution p(y|x). McCallum et al. (2000) proposed maximum entropy Markov models (MEMMs) that can learn an observation-dependent transition from the previous label to the current label. However, MEMMs and other discriminative models with independently trained next-state classifiers suffer from a serious problem called the label-bias. To deal with this problem, Lafferty et al. (2001) proposed conditional random fields (CRFs) that globally normalize the probability of the label sequence to avoid the label-bias problem. CRFs also provide the flexibility to use non-independent features. Recently, CRFs have been successfully applied in many tasks in natural language process-ing, including morphological analysis (Kudo et al., 2004), noun-phrase chunking (Sha and Pereira, 2003), and name entity recognition (McCallum and Li, 2003). In this paper, we propose a unified framework for dealing with ambiguities in Thai morphological analysis based on the theoretical background of CRFs. As mentioned earlier, the Thai writing system has no word boundary indicators. This leads to a problem where elements of the observation sequence are inconsistent due to word boundary ambiguity. Unlike the Chinese writing system which is monosyllabic, the Thai writing system is alphabetic. Each Chinese character can function as a single morpheme, so the ambiguity of the observation sequence can be avoided by performing word segmentation and POS tagging at the character level (Peng et al., 2004). However, Thai word formation is similar to English in which it is composed of consonants and vowels but without tense and inflection. Thus, tagging at the character level for Thai is not a practical way. Our framework is more closely related to morphological analysis for Japanese (Kudo et al., 2004). Given a sequence of characters, all possibilities of word/tag segmentation are first generated. We present the combination of the longest matching algorithm and the backtracking technique for constructing the word/tag lattice. Then, the optimal path is selected with some criterion. In our work, we examine two different techniques for selecting the most likely word/tag path, including the Viterbi score and the confidence estimation. Preliminary results on a standard benchmark for the Thai POS tagging task named the ORCHID corpus are provided. To the best of our knowledge, a study of Thai morphological analysis based on the concept of CRFs has not been reported on the literature. The rest of the paper is organized as follows. In Section 2, we discuss related work on Thai morphological analysis, consisting of various techniques for tackling word segmentation and POS tagging. Section 3 reviews some important concepts of conditional random fields. In Section 4, we describe how to obtain all possible word/tag segmentation patterns, and how to select the optimal path with the Viterbi algorithm and the confidence estimation. Section"]},{"title":"2419","paragraphs":["5 provides details of our experiments, including data sets, evaluation methods, and results. Finally, conclusion and future work are given in Section 6."]},{"title":"2. Related Work","paragraphs":["In this section, we briefly review related work on Thai morphological analysis. Most of previous researches have focused on the word segmentation and POS tagging problems separately. Sornlertlamvanich (1993) introduced the maximum matching algorithm that splits a sequence of characters into all possibilities of segmentation using a word list. The word list can be derived from unique head words in a lexicon. The algorithm attempts to minimize the occurrence of unknown words in each candidate, and selects the best segmentation with the lowest number of segmented tokens. Despite of the simple idea, the maximum matching algorithm performs reasonably well for the word segmentation problem. Kawtrakul et al. (1997) proposed a language modeling technique to select the optimal segmentation rather than using heuristics. A trigram Markov model is used to estimate the probabilities of word clusters from a segmented text corpus. However, solving word boundary ambiguity often requires some higher levels of linguistic knowledge. Meknavin et al. (1997) combined word segmentation with POS tagging based on a generatively-trained model. The optimal segmentation is selected by the highest marginal probability of word sequences. Feature-based approach is utilized to deal with segmentation ambiguity. Charoenpornsawat et at. (1999) also applied the Winnow algorithm to learn contextual features for handling the unknown word problem. Murata et al. (2002) examined other machine learning methods for solving the Thai POS tagging problem, including decision lists, maximum entropy, and support vector machines. Another direction of researches focuses on a more finegrained level of word formation. Morphological rules are first applied for syllable/morpheme segmentation, and then the process of word recovery is performed. Jaruskulchai (1998) used a model selection technique called minimum description length (MDL) to select the most likely morpheme combination. Aroonmanakun (2002) exploited statistics of collocation to merge syllables into a word. However, these proposed methods cannot directly integrate the POS tagging task into a single framework."]},{"title":"3. Conditional Random Fields 3.1. Basic Definition","paragraphs":["We describe CRFs through the concept of graphical models. Let y be a linear-chain graph structure, consisting of nodes y1, . . . , y|y|. In the case of undirected graphical models, the probability distribution can be factorized according to the definition on cliques of the graph. Each clique Ci ∈ C is a fully connected subset of nodes yCi, which can be parameterized by using a clique potential ψCi. Thus, we can express the probability distribution of the graph as the product of overall clique potentials: p(y) = 1 Z ∏ Ci∈C ψCi(yCi) , (1) where Z =","∑ y ∏","Ci∈C ψCi(yCi) is a normalization term","called the partition function to ensure that","∑","y p(y) = 1. By","applying the idea of log-linear models, the clique potential","can be written in the form of feature functions: ψCi(yCi) = ∏ k exp{λkfk(yCi)} = exp{ K ∑ k=1 λkfk(yCi)} ,","(2) where K is the number of all features, and λ1, . . . , λK are the weight parameters corresponding to local feature functions fk. In our context, we formulate a linear-chain CRF with the conditional probability distribution pλ(y|x) of the label (tag) sequence y given the observation (word) sequence x. For simplicity, we assume that both y and x have the same length T , where y = (y1, . . . , yT ) and x = (x1, . . . , xT ). We also assume the first-order Markov process on y. Each local feature function fk for a clique can be uniformly defined as a state feature s(yt, x, t) and a transition feature t(yt−1, yt, x, t) at a position (or time) t. We compactly denote the feature function by fk(yt−1, yt, x, t). Thus, the conditional probability distribution for a linear-chain CRF becomes: pλ(y|x) =","1 Zλ(x)","exp{ T ∑ t=1 K ∑ k=1 λkfk(yt−1, yt, x, t)} , (3) where Zλ(x) = ∑ y exp{ T ∑ t=1 K ∑ k=1 λkfk(yt−1, yt, x, t)} . (4) 3.2. Parameter Estimation and Inference Given a set of training data D = {x(i)",", y(i)","}N","i=1, where N is the number of all training samples, the objective is to find a set of weight parameters λ = {λ1, . . . , λK}. A common method is to use maximum likelihood estimation (MLE). We can express the log likelihood as follows: l(λ; D) = log p(D|λ) = log N ∏ i=1","pλ(y(i) |x(i)",") = N ∑ i=1","log pλ(y(i) |x(i)",") . However, MLE often overfits the the training data. Thus, we choose to maximize the penalized log-likelihood (or maximum a posteriori) instead, where log p(λ|D) = log p(D|λ) + log p(λ). The term log p(λ) is defined by a spherical Gaussian prior, so we obtain: log p(λ|D) = N ∑ i=1","log pλ(y(i) |x(i)",") − K ∑ k=1 λ2 k 2σ2 . (5) Let F(y, x) be a global feature for the label sequence. We can compactly write: exp","{ T ∑ t=1 K ∑ k=1 λkfk(yt−1, yt, x, t)} = exp { λ · F(y, x)} . (6)"]},{"title":"2420","paragraphs":["\"เทคนิคปัญญาประดิษฐ์ในการแปลภาษา\" Artificial Intelligent technique in Machine Translation"]},{"title":"ประดิษฐ์/VACT เทคนิค/NCMN ปัญญาประดิษฐ์/NCMN ใน/RPRE การ/FIXN แปลภาษา/VACT เทคนิค/NCMN ปัญญาประดิษฐ์/NCMN ใน/RPRE การ/FIXN แปลภาษา/VACT 0 เ̀ทคนิค|ปัญญาประดิษฐ์|ใน|การ|แปลภาษา' 0.0097094480 เท/VACT ค/NCMN นิค/NCMN ปัญญาประดิษฐ์/NCMN ใน/RPRE การ/FIXN แปลภาษา/VACT 1 เ̀ท|ค|นิค|ปัญญาประดิษฐ์|ใน|การ|แปลภาษา' 0.0002493113 เทคนิค/NCMN ปัญญา/NCMN ประดิษฐ์/VACT ใน/RPRE การ/FIXN แปลภาษา/VACT 2 เ̀ทคนิค|ปัญญา|ประดิษฐ์|ใน|การ|แปลภาษา' 0.0016840507 เทคนิค/NCMN ปัญญาประดิษฐ์/NCMN ใน/RPRE กา/NCMN ร/NCMN แปลภาษา/NCMN 3 เ̀ทคนิค|ปัญญาประดิษฐ์|ใน|กา|ร|แปลภาษา' 0.0012505042 เทคนิค/NCMN ปัญญาประดิษฐ์/NCMN ใน/RPRE การ/FIXN แปล/VACT ภาษา/NCMN","paragraphs":["นิค/NCMN ปัญญา/NCMN ค/NCMN แป/VACT เทคนิค/NCMN เท/VACT ปัญญาประดิษฐ์/NCMN ใน/RPRE ประดิษฐ์/VACT การ/FIXN ล/NCMN ภาษา/NCMN แปลภาษา/VACT แปล/VACT"]},{"title":"• •","paragraphs":["Figure 1: An example of the generated word/tag lattice from a Thai phrase Using Eq. (3) and Eq. (6), we can rewrite Eq. (5) as: log p(λ|D) = N ∑ i=1","[λ·F(y(i) , x(i)",")−log Zλ]− ∥λ∥2 2σ2 . (7) One can apply several numerical methods to optimize Eq. (7). In our implementation, we use a limited-memory quasi-Newton method (L-BFGS) that is known to be a very efficient technique for training CRFs (Sha and Pereira, 2003). To find the most likely label sequence given an observation sequence, we can apply the Viterbi algorithm as performed in generative models."]},{"title":"4. Thai Morphological Analysis Framework","paragraphs":["In this section, we describe the framework for dealing with ambiguities of Thai morphological analysis. We divide the task into two subproblems: (a) how to obtain all possible word/tag segmentation patterns, and (b) how to select the most likely path. 4.1. Constructing All Possible Paths Given a character sequence x, all possibilities of word segmentation are produced. This results a set of candidate paths X that can be thought of as a lattice of words. We can efficiently generate X by using the combination of the longest matching algorithm and the backtracking technique. Our method is reminiscent of Sornlertlamvanich’s approach (1993) but attempts to build all possible word/tag paths instead of only word paths. The process starts by constructing an initial segmentation with the longest matching algorithm. The algorithm tries to search the longest prefix occurred in a word list. If the current prefix matches any token in the word list, the algorithm inserts a boundary at the end of prefix. It continues the longest prefix search starting at the character following the match. If no match is found, the algorithm skips that character and begins the new search starting at the next character. The longest matching algorithm iterates this procedure until the input character sequence is exhausted. Consequently, we can obtain a list of segmented tokens, which is considered to be the initial path. We then proceed by finding all possible paths using the backtracking technique. This technique can greatly reduce the amount of work in performing all exhaustive searches. Backtracking performs on each segmented token in the initial path by retracing from the left side to the right side. If the considered token cannot be segmented into smaller lexical morphemes, we keep it as it is. On the other hand, if the considered token can be segmented further, we store the first match and allow the longest matching algorithm to run forward again starting at the character following the match. However, backtracking can also produce unknown tokens composed of one or more characters. After we obtain the candidate paths from the previous process, we then assign each word in the paths with all possible part-of-speech tags. Unfortunately, this leads to a very large number of word/tag patterns. In our current work, we limit the generated paths by constraining each candidate word path with the most likely tag path found by the Viterbi algorithm. The details are described in the following section. Figure 1 shows an example of the generated word/tag lattice. 4.2. Finding The Optimal Path So far, the question is how to select the optimal path from the word/tag lattice. As mentioned earlier, the most likely tag path for the word path can be found through the Viterbi algorithm. Based on the learnt model, the Viterbi algorithm is capable of finding the optimal solution, reflecting the most probable label sequence for a given observation sequence. We can formally write:","y∗ = argmaxypλ(y|x) . (8) The Viterbi algorithm is an efficient dynamic programming technique that can avoid an exponential-time search over all possible settings of the label sequence y. The idea is to store the probability (δ) of the most likely path that leads to the the considered label yi. In the CRF framework, we can define the recursion for computing the probability of yi at each position in the sequence by:","δt+1(yi) = max y′","[","δt(y′",") exp ( ∑","k","λkfk(y′ , y i, x, t))] .","(9) In the termination step (t = T ), we find the label with the highest score:","p∗ = argmaxiδT (yi) , (10) and backtrack through the dynamic programming table to recover the most probable label sequence y∗",". To select the global best path on the word/tag lattice, we can exploit the probability score produced by the Viterbi search at the termination step as a criterion. Thus, we choose the global best path with the highest Viterbi score. However, in our preliminary tests, we find that the global Viterbi score sometimes indicates incorrect word/tag paths."]},{"title":"2421","paragraphs":["# of sentences 23,125 # of tags 46 # of training/test sentences 18,500/4,625 # of training/test tokens 274,469/68,168 # of training/test ambiguous tokens 205,271/45,559 # of (state+transition) features 19,708 # of words (LEXiTRON) 32,363 Table 1: Statistics of ORCHID used in our experiments We think that these situations may occur from the ambiguity in the observation sequence. Thus, we examine an alternative score to measure the confidence of candidate word/tag paths. The confidence estimation (CE) is equal to the normalized value of a constrained lattice (Culotta and McCallum, 2004): CE = Z′ λ(x) Zλ(x) (11) In our context, the constraints C correspond to a set of segmented tokens with their specified tags. We estimate the confidence of the entire word/tag path. For example, in Figure 1, the constrained lattice is marked by the bold path. In order to obtain the normalization term Zλ(x), we have to marginalize out the label selection probabilities, which can be computed by applying the Constrained Forward algorithm. Instead of selecting the optimal label sequence as performed in the Viterbi algorithm, the Constrained Forward algorithm evaluates all possible label sequences given the observation sequence. Only the max term in Eq. (9) is replaced by the summation, so we can compute the forward value by: αt+1(yi) = ∑ y′","[","αt(y′",") exp ( ∑","k","λkfk(y′ , y i, x, t))] . (12) In the termination step, we obtain: Zλ(x) = ∑ i αT (yi) . (13) Let C = ⟨yt, yt+1, . . .⟩ be a constrained path. The constrained forward value is defined by: α′","t+1(yi) =","{ ∑ y′","[","α′ t(y′ ) exp ( ∑","k λkfk(y′",", y i, x, t))]","if yi = yt+1 , 0 otherwise,","(14)","and we finally obtain the constrained lattice value in the","termination step as follows: Z′ λ(x) = ∑ i α′ T (yi) . (15)"]},{"title":"5. Preliminary Experiments 5.1. Data Sets and Evaluation Methods","paragraphs":["We performed experiments on a standard benchmark for the Thai POS tagging task named the ORCHID corpus. The ORCHID corpus was constructed at the Linguistics and Knowledge Science Laboratory under the National Electronics and Computer Technology of Thailand. The statistics of the ORCHID corpus used in our experiments are given in Table 1. We randomly split the corpus into 80% for training and the remaining 20% for testing. We de-segmented the test set by removing all tags from words. We then merged all the words in each sentence into a character sequence. Thus, the task is to recover the word boundaries and assign the words with the most likely tags. In our preliminary experiments, we only trained a linear-chain CRF with basic features. Transition features are based on the first-order Markov assumption, and state features are composed of tokenized words found in the training set. In particular, for the state features, we also added unknown features that are generated when the frequencies of the tokenized words are less than 4 times. The word list for the lattice construction process was taken from an online lexicon called LEXiTRON1",". For the propose of comparison, we experimented with two different techniques of the decoding process (Viterbi and CE) as described in Section 4.2. We also compared our techniques with another two methods: the longest matching (LM) algorithm and the maximum matching (MM) algorithm. After obtaining the segmented tokens, both the LM and MM algorithms performed POS tagging with the unigram baseline (UB) technique. The idea of this technique is just to assign each token with the most likely tag that can be estimated from the training set (Jurafsky and Martin, 2000). We used precision, recall, and F1 for evaluation. For word segmentation, precision is defined as the percentage of tokens recovered by the algorithm that also occurred in the test set in the same sentence, whereas recall is defined as the percentage of tokens in the test set recovered by the algorithm. For POS tagging, a token is considered to be a correct one only if both the word boundary and its corresponding POS tag are correctly identified. In order to get a single measure of effectiveness, we employ F1 that is a combination of precision and recall. These measures can be summarized as follows: Precision =","# of correct tokens # of tokens recovered by the algorithm , Recall =","# of correct tokens # of tokens in the test set , F1 = 2 · Precision · Recall Precision + Recall . We implemented a parallel version of the CRF trainer with C language and MPI (Massive Passing Interface) library. We observed that the process of computing the log likelihood function and its gradient is inherently data-parallel, and this process runs iteratively. Thus, the loop can be parallelized by evenly distributing the training samples across processors, and computing the log likelihood function and its gradient simultaneously. At the end of each iteration, 1 http://lexitron.nectec.or.th."]},{"title":"2422 Method Precision Recall F1","paragraphs":["LM 82.717% 82.059% 82.389% MM 83.305% 82.392% 82.846% CRF-Viterbi 74.727% 84.147% 79.157% CRF-CE 83.991% 83.149% 83.568% Table 2: Results of word segmentation on ORCHID Method Precision Recall F1 LM-UB 75.715% 75.113% 75.413% MM-UB 76.272% 75.436% 75.851% CRF-Viterbi 70.955% 79.900% 75.162% CRF-CE 79.744% 78.945% 79.342% Table 3: Results of POS tagging on ORCHID all the values are summed up and redistributed to all processors. We ran the training process on a Linux cluster, consisting of 10 nodes. Each node is an Intel®","XeonTM 3 GHz with 4 Gbytes of RAM. 5.2. Results We now provide experimental results with some discussion. Table 2 shows the summary of the word segmentation results produced by LM, MM, CRF-Viterbi, and CRF-CE. We can see that the two baseline methods give acceptable results. However, the CRF-Viterbi yields poor results in terms of precision and F1, while CRF-CE provides the best results on F1 score. We check the segmentation results to see why CRF-Viterbi performs worse on the word segmentation task. We observe that CRF-Viterbi often prefers paths that have greater numbers of segmented tokens. This corresponds to the segmentation results in which CRF-Viterbi achieves the highest recall value. In contrast, CRF-CE can select better paths. It is interesting to note that the path selection of CRF-CE seems to be independent from the number of segmented tokens. Table 3 shows the summary of the POS tagging results. The two baseline methods perform well even with the simple tagging technique. CRF-Viterbi still yields unsatisfactory results, while CRF-CE outperforms other methods for the POS tagging task."]},{"title":"6. Conclusion and Future Work","paragraphs":["This paper has described our initial effort to deal with ambiguities in Thai morphological analysis based on the concept of conditional random fields. We present a unified framework for performing word segmentation and POS tagging at the same time. The word/tag lattice is efficiently constructed, and then the optimal segmentation path on the lattice is selected. Preliminary experiments find that the path selection with the Viterbi score often prefers paths that contain greater numbers of segmented tokens. This can yield unsatisfactory segmentation results. To alleviate this problem, we apply an alternative path selection criterion called the confidence estimation. This criterion is the normalized value of the constrained lattice, which can be computed by the Constrained Forward algorithm. The evaluation on the ORCHID corpus shows that selecting the optimal path with the confidence estimation is very promising. Several issues of future work remain. In the current work, we use only basic features for learning and predicting. However, one of strengths of CRFs is to allow using arbitrary, overlapping, and non-dependent features. The feature induction technique will be explored (McCallum, 2003). The unknown word problem is also an issue in morphological analysis. We will integrate this problem into our framework. In (Peng et al., 2004), the authors show that the confidence estimation is very useful for unknown word detection."]},{"title":"7. References","paragraphs":["Wirote Aroonmanakun. (2002). Collocation and thai word segmentation. In Proc. of the 5th SNLP & 5th Oriental COCOSDA Workshop, pages 68–75.","Paisarn Charoenpornsawat. (1999). Feature-based Thai Word Segmentation. Master’s Thesis, Computer Engineering, Chulalongkorn University.","Aron Culotta and Andrew McCallum. (2004). Confidence estimation for information extraction. In Proc. of HLT-NAACL 2004 (short paper).","Thomas G. Dietterich. (2002). Machine Learning for Sequential Data: A Review. In T. Caelli (Ed.) Structural, Syntactic, and Statistical Pattern Recognition, Lecture Notes in Computer Science, Vol. 2396, Springer-Verlag.","Chuleerat Jaruskulchai. (1998). An automatic thai lexical acquisition from text. In Proc. of PRICAI’98, pages 289– 296.","Daniel Jurafsky and James H. Martin. (2000). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall, Inc.","Asanee Kawtrakul and Chalatip Thumkanon. (1997). A statistical approach to thai morphological analyzer. In Proc. of the 5th Workshop on Very Large Corpora, pages 289–296.","Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. (2004). Applying conditional random fields to japanese morphological analysis. In Proc. of EMNLP 2004.","John Lafferty, Andrew McCallum, and Fernando Pereira. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML 2001.","Andrew McCallum and Wei Li. (2003). Early results for name entity recognition with conditional random fields, feature induction and web enhanced lexicon. In Proc. of CoNLL 2003.","Andrew McCallum, Dayne Freitag, and Fernando Pereira. (2000). Maximum entropy markov model for information extraction and segmentation. In Proc. of ICML 2000.","Andrew McCallum. (2003). Efficiently inducing features of conditional random fields. In Proc. of the 19th Annual Conference on Uncertainty in Artificial Intelligent (UAI-03).","Surapant Meknavin, Paisarn Charoenpornsawat, and Boonserm Kijsirikul. (1997). Feature-based thai word segmentation. In Proc. of NLPRS’97, pages 289–296."]},{"title":"2423","paragraphs":["Masaki Murata, Qing Ma, and Hitoshi Isahara. (2002). Comparison of three machine-learning methods for thai part-of-speech tagging. ACM Trans. Asian Lang. Inf. Process., 1(2):145–158.","Fuchun Peng, Fangfang Feng, and Andrew McCallum. (2004). Chinese segmentation and new word detection using conditional random fields. In Proc. of COLING 2004.","Fei Sha and Fernando Pereira. (2003). Shallow parsing with conditional random fields. In Proc. of HLT-NAACL 2003.","Virach Sornlertlamvanich. (1993). Word Segmentation for Thai in Machine Translation System. Machine Translation, National Electronics and Computer Technology Center, Bangkok."]},{"title":"2424","paragraphs":[]}]}