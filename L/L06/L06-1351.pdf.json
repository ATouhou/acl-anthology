{"sections":[{"title":"Word Knowledge Acquisition for Computational Lexicon Construction Thatsanee Charoenporn†,††, Canasai Kruengkrai†, Thanaruk Theeramunkong††, Virach Sornlertlamvanich†, and Hitoshi Isahara†††","paragraphs":["†Thai Computational Linguistics Laboratory, Thailand","††Sirindhorn International Institute of Technology (SIIT), Thammasat University, Thailand †††National Institute of Information and Communications Technology, Japan","{thatsanee, canasai, virach}@tcllab.org, thanaruk@siit.ac.tu.th, isahara@nict.go.jp","","Abstract The growing of multilingual information processing technology has created the need of linguistic resources, especially lexical database. Many attempts were put to alter the traditional dictionary to computational dictionary, or widely named as computational lexicon. TCL’s Computational Lexicon (TCLLEX) is a recent development of a large-scale Thai Lexicon, which aims to serve as a fundamental linguistic resource for natural language processing research. We design either terminology or ontology for structuring the lexicon based on the idea of computability and reusability."]},{"title":"1. 2. Introduction","paragraphs":["Researchers in the area of computational linguistics and natural language processing have been interested in the problem of acquiring large semantic knowledge for natural language understanding systems. The availability of lexical databases or machine-readable of several languages, such as English WordNet (Miller et al, 1993), English FrameNet (Baker et al, 1998), and Japanese EDR Dictionary (EDR, 1990), and Chinese HowNet (Dong, online) appears to be useful for many different research areas, including word sense disambiguation, machine translation, information retrieval, etc. More recently, the reemergence of ontology researches in both theories and applications has activated researchers to reuse and extend these linguistic resources in many other domains. Among few Thai machine-readable dictionaries, Multilingual Machine Translation: MMT (CICC, 1995) and LEXiTRON (Lexitron, online) are dominant. MMT dictionary is a dictionary designed and developed for using in analysis and generation modules in the multilingual machine translation system. The dictionary then contains as much detailed information as needed for concept disambiguation and word selection. LEXiTRON dictionary is designed and developed for human use. It is extracted from the MMT dictionary to add more human friendly information, as well as to reduce the unnecessary information. Part-of-speech categories are simplified. Due to these different purposes, one may contain information not existing in the other. Moreover, currently these machine-readable dictionaries cope with only limited semantic aspects of word entries. To implement in more practical applications, a more expressive dictionary with various types of information is required.","In this paper, we propose an approach to construct a content-rich computational lexicon by reusing information from the two existing dictionaries, i.e., the MMT dictionary and LEXiTRON, and then enriching the result with additional semantic information extracted from texts on the web. Toward this end, we design the specification of the lexicon and propose a method to acquire semantic information in automatic and semi-automatic ways rather than by only manual annotation. A number of practical approaches are applied for such tasks. As a by-product, we",""," obtain a methodology of word sense representation, in contrast to the descriptive manner in general lexicons.","This paper is organized as follows. Section 2 describes the existing Thai dictionaries and reference thesaurus, MMT dictionary and LEXiTRON. The structure of our lexicon and its construction is proposed in Section 3. The method to acquire constraints from corpora is illustrated in section 4. In section 5, experimental results are reported together with some discussion. Finally, section 6 gives a conclusion and some future works.",""]},{"title":"Thai Dictionaries and Thesaurus","paragraphs":["Two large-scale dictionaries, namely Multilingual Machine Translation (MMT) Dictionary for language processing and LEXiTRON for human use were developed and publicized. The MMT dictionary contains 68,860 entries with 53,759 unique words while LEXiTRON covers 40,844 entries with 35,192 unique words. The MMT dictionary was originally constructed by National Electronics and Computer Technology Center (NECTEC), for the Multilingual Machine Translation project, which is a six-year (1987-1992) cooperative project between groups of research institutes from five countries, i.e. China, Indonesia, Japan, Malaysia, and Thailand, organized by the Center of the International Cooperation for Computerization (CICC) in Japan.","The MMT dictionary consists of three levels of linguistic information; morphological, syntactic and semantic information. It was created based on some linguistic theories, such as phrase structure grammar, case grammar and dependency grammar.","LEXiTRON by NECTEC is the first Thai-English corpus-based electronic dictionary, which can be accessed via the internet. LEXiTRON dictionary is designed and developed for human use. It is extracted from the MMT dictionary to add more human friendly information, as well as to reduce unnecessary information. LEXiTRON includes the following information; word entry, English equivalent word, definition, related words (synonymantonym), and sample sentence.",""]},{"title":"949 3. TCLLEX’s Structure and its Construction ","paragraphs":["The syntactic information contains information of the entry’s syntactic structure, i.e. grammatical categories (CAT) and subcategories (SUBCAT). In case of a verb, its verb pattern is also included. In TCLLEX, there are 11 categories with 44 subcategories defined. Each category is divided into subcategories according its sub characteristic, such as its occurring position, composition, and reference. For example, the category ‘determiner’ is divided into 9 subcategories by its occurring position, such as after noun, after noun and classifier, between noun and classifier, etc. There are totally 11 verb patterns, classified according to the position of the obligatory arguments and its contextual environment.","Eventhough the MMT dictionary and LEXiTRON are very useful for the Thai language processing, pros and cons of both depend on purpose of the design. Some problems occur when we consider combining these two dictionaries.","- Information in these two dictionaries is individually defined for different purpose. Consistency of the information in word entry is crucial.","- The semantic information that is useful for word sense disambiguation (i.e. semantic constraint) is not explicitly defined in both of these two dictionaries, for example, the possible semantic class of the agent (AGT) or the object (OBJ) of the verb ‘pay’.","TCL’s Computational Lexicon (TCLLEX) adopts the semantic constraints to generate a word frame. Word arguments and semantic groups are defined to indicate the meaning that can be differentiated from others. Based on the designed framework, the word entries are selected from LEXiTRON. The corresponding word information especially the syntax-semantic mapping and AKO are extracted from the MMT dictionary. As a result, 68,860 word entries of 13,781 verbs, 35,688 nouns are extracted and redefined in TCL lexicon.","Since the MMT dictionary includes much more information than LEXiTRON, the word entries taken from LEXiTRON are revised using information from the MMT dictionary. Some information comes from LEXiTRON. For instance, the synonym of LEXiTRON is selected for Equal in TCLLEX, such as “ใหเงิน” ,“จายเงิน”, the synonyms of “จาย” (to pay). On the other hand, arguments in the MMT dictionary are relied for the selection of semantic constraints. Among 35,192 word entries in LEXiTRON and 53,759 word entries in MMT dictionary, there are 22,173 word entries co-existing in both dictionaries. The information from both dictionaries is selected to compose TCLLEX. The ones that exist in either dictionary will be added. As a result, the union set of both dictionaries will be generated as the TCLLEX. Moreover, TCLLEX relies on the word entries, part-of-speech category and synonym from LEXiTRON while it relies on part-of-speech sub-category, syntax-semantic mapping from the MMT dictionary. TCLLEX is then generated to cover both word syntax and semantic.","The TCLLEX’s structure consists of four types of information, including general, morphological, syntactic, and semantic information. The general information of an entry conveys its Thai entry, corresponding English word, entry definition, and example sentence. The morphological information indicates the type of word composition (TYPE), which is of two types: single word and compound word. A single word is a lexical unit that cannot be divided into smaller units, such as โทรศัพท (telephone). In contrast, a compound word is a combination of more than one single word. For example, the word โทรศัพทมือถือ (mobile phone) consists of two lexical units: โทรศัพท (telephone), and มือถือ (mobile).","The semantic information provides a set of logical and semantic constraints, which is useful for discriminating word senses. The logical constraints can be attached to a word of any category type. They illustrate the logical relationship among word senses in the lexicon. The semantic constraints are attached to a verb or an adjective. They represent the relationship among thematic roles in a verb or adjective pattern.","There are five types of logical constraints proposed in the work. They are ISA (a-kind-of), EQU (synonym), NEQ (antonym), POF (meronym), and WOF (holonym). The ISA constraint indicates a-kind-of relation of words in the semantic hierarchy which is currently composed of 189 conceptual classes. There are 16 kinds of semantic constraints or case relations, constructed by analyzing a set of Thai sentences by Thai linguists. They are Agent, Object, Patient, Experiencer, Result, Goal, Location, Commutative, Measure, Complement, Cause, Time, Source, Instrument, Manner, and Beneficiary. Syntax-semantic mapping information (MAPS) is a key for extracting the constraint arguments. BIC and Agglomerative Merging algorithms are finally used to provide the candidates of classes for each constraint.","","TCL MMT LEXiTRON Corpus General Information Thai entry Header Header - Corresponding English Header Header - Entry Definition - Definition - Example Sentence - Sample - Morphological Information Word-type TYPE - - Syntactic Information Category CAT Header - Sub-category SUBCAT - - Verb Pattern (for verbs) VPPAT - - Semantic Information Logical Constraints Is-a (ISA) AKO - - Equal (EQU) - Synonym - Not-Equal (NEQ) - Antonym - Part-Of (POF) - - Corpus* Whole-Of (WOF) - - Corpus* Semantic Constraints Syntactic-semantic Mapping MAPS - - Agent - - Corpus Object - - Corpus Instrument - - Corpus* Location - - Corpus* Time - - Corpus*","... ... ... ... ","Table 1: The list of TCLLEX entries and their sources","","Furthermore, we classify the constraints into two types: obligatory and optional. While the obligatory constraints should be filled as much as possible, the optional constraints can be left empty. In the logical constraints, there is only one obligatory constraint, i.e."]},{"title":"950","paragraphs":["ISA. It is also considered to be the core structure of the TCLLEX. In the semantic constraints, AGT and OBJ are the obligatory constraints. The remaining constraints are categorized to be the optional constraints.",""]},{"title":"4. 4.1. 4.2. Constraints Acquisition","paragraphs":["In this section, we describe how to acquire the semantic constraints. The main idea of our approach relies on reusing existing linguistic resources and annotating the structure in automatic and semi-automatic ways."]},{"title":"Semantic Constraint Acquisition","paragraphs":["The semantic constraints can be acquired by identifying selectional preferences of verbal predicates. Here we focus on the obligatory constraints, including AGT and OBJ. We propose a method to identify selectional preferences of a verb, a kind of semantic constraints, by using a large number of documents (or texts) gathered from the Web. By the MAPS in the MMT dictionary, we know that the subject of the verb"]},{"title":"‘จาย","paragraphs":["’ (pay) is the agent, but we do not know which semantic class (concept) of the agent should be. Typically, one may think that the subject of the verb"]},{"title":"‘จาย","paragraphs":["’ (pay) prefers to be humans. By parsing through text corpora, we can obtain examples of context nouns that are considered to be the subjects of the verb. A method to create a set of semantic constraints from these examples using Bayesian Information Criteria is presented in the next section."]},{"title":"Bayesian Information Criteria","paragraphs":["A model selection technique called the Bayesian Information Criteria (BIC) (Wasserman, 1999) is applied in order to obtain an optimal set of selectional preferences for a given verb. The BIC is a model selection based on Bayesian theory. The problem of model selection is to choose the best model among a set of candidate models mi ∈ M. The BIC of a model mi can be approximated as follows:"]},{"title":")1(||log 2)()( Dp DlBIC","paragraphs":["i i"]},{"title":"−=im","paragraphs":["where li(D) is the log-likelihood of the data D according to the model mi and pi is the number of independent parameters. The BIC is independent of the prior and related to the minimum description length (MDL). The details of BIC criterion can be found in (Chickering, 1997). As the probabilistic model for the semantic hierarchy, the tree cut model [13] is adopted.","The tree cut model is introduced to characterize the probabilistic model of the semantic hierarchy. Let m = (Γ,Θ) be the model, including a partition in the semantic hierarchy Γ, and a set of parameters Θ. Given the noun class C ∈ Γ, the verb v ∈ V, and the syntactic relationship r ∈ R, the sum of the conditional probability distribution of P(C|v,r) must be 1 as follows."]},{"title":")2(1),|( =∑","paragraphs":["Γ∈C"]},{"title":"rvCP","paragraphs":["Two main assumptions for estimating probabilities in this model are: (1) the probability of a class C can be calculated from all nouns in the class n ∈ C, each of which is estimated using the maximum likelihood estimation (MLE), and (2) after calculating the class probability, the probability of each noun in the class noun n ∈ C is assumed to be uniform distribution."]},{"title":")3( || ),|( ),|( D rvnfreq rvCP","paragraphs":["Cn"]},{"title":"∑","paragraphs":["∈"]},{"title":"= )4( || )( ),|( CCPrvnP =","paragraphs":["where freq(n|v,r) is the frequency of the noun n co-occurring with the verb v and the syntactic relationship r, |D| is the data size, i.e., the total frequency of all nouns, and |C| is the number of classes in the current partition. Based on this, the log-likelihood of class C according to mi is:"]},{"title":")5(),|(log),|(log)( ∑∏","paragraphs":["∈∈"]},{"title":"==","paragraphs":["Cn i Cn ii"]},{"title":"rvnPrvnPDl )6(||log 2),|()( DprvnPBIC","paragraphs":["i C i"]},{"title":"−= ∑","paragraphs":["Γ∈ i"]},{"title":"m ","paragraphs":["where the number of parameter pi is equivalent to the number of classes in Γ minus one, i.e. |C|-1. Pi is the probability distribution of the model i. Finally, the objective function is defined as follows. "]},{"title":")7()(maxarg","paragraphs":["* i m"]},{"title":"mm i BIC","paragraphs":["M∈"]},{"title":"= 4.3. The Agglomerative Merging Algorithm","paragraphs":["An iterative algorithm for selectional preference generalization is now described. Our algorithm searches the appropriate levels of noun classes on the semantic hierarchy by performing agglomerative merging in a bottom-up manner. One may consider the behavior of the algorithm as a simplified agglomerative clustering algorithm. We assume that all nouns are pre-classified onto their hierarchical classes according to the semantic information indicated by AKO. As a result, the algorithm does not have to make any decision about assigning nouns to the most probable classes. What it has to do is to repeatedly merge subclasses into a single class if the structure of the semantic hierarchy improves. We consider this structure as a model for representing selectional preferences. The improvement of the model can be measured by using the BIC as described in the previous section. The more the BIC increases, the more the model improves. The agglomerative merging algorithm tries to increase the objective function value in Equation 7 at every step. Thus, the BIC is used to test the improvement of the model both locally and globally.","Our algorithm starts by initializing the region of noun classes on the semantic hierarchy. The input data are given in the form of the co-occurrence tuple, <v, r, n, freq>, where v is the verb, r is the syntactic relationship, n is the noun, and freq is the co-occurring frequency. The co-occurrence tuples can be obtained by extracting and analyzing the snippets. It then finds appropriate leaf nodes having the same AKO to merge up into the parent node. Focusing on this partition, the BIC is measured locally. If the BIC score of the parent node is not greater than the BIC score of the children nodes, the algorithm keeps the structure of leaf nodes as it is. Otherwise, the BIC is measured globally to guarantee the overall improvement."]},{"title":"951","paragraphs":["These processes are implemented by MERGECLASSES and AGGLOMERATIVEMERGING algorithms shown in Figure 1. The algorithm iterates until it cannot find leaf nodes to merge or there remains one class. ANIMAL BIRD INSECT bird 4 crow 2 eagle 2","swallow 0 bee 2  bug 0  insect 0  ParentBIC > ChildrenBIC (-18.58) (-19.56) NewGlobalBIC > OldGlobalBIC (-28.20) (-29.17) (a) ANIMAL BIRD INSECT bee 2  bug 0  insect 0  ParentBIC > ChildrenBIC (-7.81) (-7.97) NewGlobalBIC > OldGlobalBIC (-28.05) (-28.20) (b) ANIMAL BIRD INSECT  NewGlobalBIC < OldGlobalBIC (-28.07) (-28.05) (c) ANIMAL BIRD INSECT (d)   ","Algorithm 1: MERGECLASSES({ci}k","i=1)","begin c’ ← ∅ ; for i from i = 1, . . . , k do c’ ← c’∪ ci; endFor if BIC(c’) > BIC({ci}k","i=1) then","return c’; else","return ∅; endif","Algorithm 2: AGGLOMERATIVEMERGING input: Semantic hierarchy Γ containing a","set of initial leaf nodes ci, where","i = 1, ..., m. output: Generalized Γ with leaf nodes forming the optimal noun classes. begin repeat","Find remaining nodes to merge, {ci}k","i=1; if k = 0 then","break; endif  c’ = MERGECLASSES({ci}k","i=1);","if c’ ≠ ∅ ; then","Ψ = Γ \\ {ci}k","i=1 ∪ c’;","if BIC(Ψ) > BIC(Γ) then Re-distribute P(n) for n∈ c’ according to Equation 3; DELETE(Γ, {ci}k","i=1); APPEND(Γ, c’); m = m − k + 1;","endif","endif Figure 2: An Example of Agglomerative Merging","until m < 1;","end",""]},{"title":"5. 5.1. Experimental Methodology","paragraphs":["Figure 1: MERGECLASSES and AGGLOMERATIVEMERGING Algorithms"]},{"title":"Data Collection","paragraphs":["Figure 2 illustrates an example of how the algorithm works, (originally, Li and Abe, 1998). Given the verb fly with the syntactic relationship subject, the co-occurring nouns are: crow (2), eagle (2), bird (4), and bee (2), where numbers in the parentheses indicate the co-occurring frequency of nouns. Let us focus on Figure 2a, which is the initial semantic hierarchy of the data. The algorithm starts by finding possible leaf nodes to merge. Since the local BIC score increases, it further measures the global BIC score by comparing the overall structure. The global BIC score also increases, it decides to merge the children nodes into the parent node. Figure 2b performs the same process. In Figure 2c, since the local BIC score decreases, it is not necessary to measure the global BIC score. Finally, we obtain the generalized semantic hierarchy in Figure 2d, whose remaining leaf nodes are considered to be selectional preferences (Kruengkrai, 2004).","As mentioned earlier, we view the Web as large and free corpora. Below we describe how to retrieve examples for selectional preference generalization through search engines. Common search engines usually return results, including a number of relevant links and their short descriptions. Since our objective is to extract the co-occurrence tuples, what we anticipate from the search engines is that, given a verb as a query, the returned short descriptions may contain the verb and its context. We refer to these short descriptions as snippets.","We implemented a simple web robot that sends the target verb to the search engines, and retrieves all the search results kept into a repository. Two major Thai search engines were used, including www.sansarn.com and www.siamguru.com. Then, we parsed HTML documents in the repository to extract only snippets. We obtained about 800-1000 snippets for each verb query. Each snippet contains 100-150 words on average.","The benefits of using the snippets from the search engines are two folds. Firstly, we can use the efficient"]},{"title":"952","paragraphs":["search mechanism to get the context of the target word without implementing any string-pattern matching algorithms. Secondly, we obtain the large databases of the search engines, reflecting natural language usage in the society. One problem we faced is that the snippets are too heterogeneous. For example, since the descriptions of the web pages were produced from table data containing lists of items or bullets, the snippets did not contain grammatical features and were less meaningful. Consequently, we limited our web robot to crawl particularly on news sites, which are already categorized by both search engines. The search results from the news categories seem to contain more useful phrases having the target verb with its context.",""]},{"title":"5.2. Co-occurrence Extraction","paragraphs":["Since we need the final input data of the algorithm in the form of the co-occurrence tuple, <v, r, n, freq>, linguistic tools for analyzing morphological and syntactic structure of Thai text are required. However, we only have a parts-of-speech tagger called Swath. A syntactic relationship r between a target verb v and its co-occurring noun n is manually assigned. In this section, we describe an approach that assists human subjects to do such task.","After retrieving snippets containing the target verb and its context, word segmentation and parts-of-speech tagging are performed using Swath. Note that Thai text has no explicit word boundaries like English text, so we have to segment it into meaningful tokens. We consider ±3 words of context around the target verb. This window size is enough to capture syntactic relationships. As the result, we obtain a set of tuples in the form of <v, context relationship, n, freq>.","We observe that the co-occurring frequencies have small different values. In order to filter out nouns which have insignificant dependence of the target verb, we measure dependence between words by using statistics taken from all the snippets. We apply the log likelihood ratio (LLR) (Dunning, 1994) for selecting the most optimal nouns. Given the verb v and the noun n occurring within window z, a fast version of the LLR can be calculated as follows (Tanaka, 2002)."," ,, ,, , ,)( )( ),,( ,loglogloglog),( 2212221111 2221212111 21121122 1121 1112 11 22 22 22 12 21 21 21 12 12 11 11 11 kkRkkR kkQkkQ kkkNk knfreqk kvfreqk nvfreqk RQ Nk k RQ Nk k RQ Nk k RQ Nk knvLLRZ +=+= +=+= −−−= −= −== +++= ","where freq(v,n) is the co-occurring frequency between v and n, freq(v) and freq(n) are frequencies of v and n, respectively. Only nouns with their LLR values greater than a pre-defined threshold are left. Table 2 shows the top 14 co-occurring nouns within window size +3 for a given verb ตรวจ ‘check’. The second and third columns show their co-occurring frequencies and LLR values, respectively. The nouns within the window size -3 are considered in the similar way. Once the candidate nouns are produced, we ask human subjects to analyze and assign the most suitable syntactic relationships between the verb and candidate nouns. For example, from Table 2, we get co-occurrence tuples <ตรวจ ‘check’, obj, รางกาย ‘body’, 9>, <ตรวจ ‘check’, obj, หนังสือเดินทาง ‘passport’, 2>, and so on."," Word Freq LLR+3 รางกาย ‘body’ หนังสือเดินทาง ‘passport’ กลามเนื้อ ‘muscle’ พยาธิ ‘worm’ ปาไม ‘forest’ คฤหาสน ‘mansion’ รถบรรทุก ‘truck’ บานพัก ‘home’ กระเปา ‘bag’ สุขภาพ ‘health’ ผลิตภัณฑ ‘product’ รถ ‘car’ โรงงาน ‘factory’ กะโหลก ‘skull’ 9 2 1 1 2 2 2 2 2 1 1 8 1 2 24.6864 6.4391 4.5825 4.5825 4.3856 4.3856 3.7537 2.8196 2.8196 2.6056 1.4067 1.3848 1.0653 0.9742","","Table 2: Co-occurring nouns of verb ตรวจ ‘check’ within window size +3"]},{"title":"5.3. Result and Discussion","paragraphs":["Evaluation of selectional preference generalization is a difficult task. To this end, it requires a gold standard for checking the appropriateness of the acquired results. This gold standard can be produced by using the majority of the human agreements. At the present, there is no such gold standard for the Thai language. However, in order to observe the behavior of our algorithm, we selected Thai verbs, including ตรวจ ‘check’, สราง ‘build’, ซื้อ ‘buy’, and จาย ‘pay’ for evaluation. We considered two syntactic structures, including subject-verb and verb-direct object relationships. Table 3 and 4 show some results of generalization.  Class Prob. Word Example","Subject of ตรวจ ‘check’ PEOPLE 1.00 ตํารวจ ‘police’","Subject of สราง ‘build’ ABSTRACT_THING ORGANIZATION PERSON 0.69 0.04 0.03 สังคม ‘society’ รัฐบาล ‘government’ นักทองเที่ยว ‘tourist’","Subject of ซื้อ ‘buy’ PERSON CONSTRUCTION ORGANIZATION 0.40 0.35 0.25 ชาวบาน ‘villager’ โรงพยาบาล ‘hospital’ บริษัท ‘company’","Subject of จาย ‘pay’ PERSON CONSTRUCTION CULTURAL_AB_THING 0.54 0.39 0.08 นักเรียน ‘student’ ธนาคาร ‘bank’ ประธาน ‘chairman’ ","Table 3: Generalization results with subject-verb-relationship ","Class Prob. Word Example Direct Object of ตรวจ ‘check’ ARTIFACT ABSTRACT_THING ANIMAL_PART 0.34 0.22 0.18","รางวัล ‘prize’","เอกสาร ‘document’","รางกาย ‘body’ Direct Object of สราง ‘build’","ABSTRACT_THING","ARTIFACT","ATTRIBUTE 0.65 0.16 0.10 มาตรการ ‘measure’ สะพาน ‘bridge’ สถานการณ ‘situation’ "]},{"title":"953","paragraphs":["Direct Object of ซื้อ ‘buy’ ABSTRACT_THING ARTIFACT GRAIN 0.40 0.27 0.03","ธุรกิจ ‘business’","ของที่ระลึก ‘souvenir’","ขาว ‘rice’ Direct Object of จาย ‘pay’","IMMATERIAL_THING","SOCIAL_AB_THING","RESULT_OF_ACT 0.31 0.22 0.01 คาเชา ‘rental fee’ คา ‘fee’ ดอกเบี้ย ‘interest’ ","Table 4 Generalization results with verb-direct object relationship","","From these tables, it is noted that the result matches well with human intuition. For example, the subject of the verb ตรวจ ‘check’ falls into the class PEOPLE, which its children classes are PERSON and ORGANIZATION. The class ANIMAL_PART can be discovered to be the object of this verb. The computational time is very short, which is less than one second running on a personal computer with Pentium processor 2GHz and memory 512 KB. In addition, we observe that the noun sense ambiguity can lead to irrelevant results in some cases. For example, the noun โรงพยาบาล ‘hospital’ has two senses, which are categorized into two classes: CONSTRUCTION and ORGANIZATION.However, the class CONSTRUCTION is unlikely to be the subject of the verb ตรวจ ‘check’. Since the tree cut model just deals with this problem by equally dividing the frequency of a noun among all the classes containing that noun, more sophisticated approach is needed for further improvement.",""]},{"title":"6. 7. 8. Conclusion and Future Work ","paragraphs":["In this paper, we have described the ongoing work on developing the TCL's computational lexicon (TCLLEX). We redefine the structured form of the semantic information by using the logical and semantic constraints. Our specification attempts to cover all possible semantic representation based on the concept of the simplicity and re-usability. The acquisition schemes of the semantic information are given.","In future work, we plan to explore approaches to extract other logical and semantic constraints. Additionally, we further study how the computational operations among word senses can be performed by using TCLLEX.",""]},{"title":"Acknowledgements","paragraphs":["This study is supported by Thailand Graduate Institute of Science and Technology (TGIST) of National Science and Technology Development Agency (NSTDA) under the grant number TGIST-TG-B-11-44-13-418D. The authors would like to thank the anonymous reviewers for giving a lot of useful comments."]},{"title":"References","paragraphs":["Baker , Collin F., Fillmore, Charles J., and Lowe, John B. (1998). The Berkeley FrameNet Project, In Proceedings of the COLING-ACL. Montreal, Canada,","Center of the International Cooperation for Computerization (CICC) (1995). Thai Basic Dictionary: Technical Report. Tokyo, Japan.","Chickering, D.M., and Heckerman, D. (1997). Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables. Machine Learning, 29, pp.181-212.","Dong, Zhendong, and Dong, Qiang. Hownet [online]. Available at http://www.keenage.com/zhiwang/e_zhiwang.html.","Dunning, T. (1994). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1), pp. 61-74.","EDR (1990). EDR Electronic Dictionary Technical Guide. Japan Electronic Dictionary Research Institute, Ltd., Japan.","Kruengkrai, C., Charoenporn, T., Sornlertlamvanich, V., Isahara,H. (2004). Acquiring selectional preferences in a thai lexical database. In Proceedings of the 1st Joint Conference on Natural Language Processing (IJCNLP-04), China.","Lexitron, Available at http://lexitron.nectec.or.th","Li, H., and Abe, N. (1998). Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2), pp. 217—244.","Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D. and Miller, K. (1993). Introduction to WordNet: An on-line lexical database. CSL Report 43.","Tanaka, T. (2002). Measuring the similarity between compound nouns in different languages using nonparallel corpora. In Proceedings of the 19th"," International Conference on Computational Linguistics.","Wasserman, L., (1999). Bayesian model selection and model averaging. Journal of Mathematical Psychology."]},{"title":"954","paragraphs":[]}]}