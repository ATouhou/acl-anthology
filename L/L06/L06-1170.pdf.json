{"sections":[{"title":"Bilingual Machine-Aided Indexing Jorge Civera and Alfons Juan","paragraphs":["Departamento de Sistemas Inform·aticos y Computaci·on Universitat Politecnica de Valencia jcivera,ajuan","@dsic.upv.es","Abstract The proliferation of multilingual documentation in our Information Society has become a common phenomenon. This documentation is usually categorised by hand, entailing a time-consuming and arduous burden. This is particularly true in the case of keyword assignment, in which a list of keywords (descriptors) from a controlled vocabulary (thesaurus) is assigned to a document. A possible solution to alleviate this problem comes from the hand of the so-called Machine-Aided Indexing (MAI) systems. These systems work in cooperation with professional indexer by providing a initial list of descriptors from which those most appropiated will be selected. This way of proceeding increases the productivity and eases the task of indexers. In this paper, we propose a statistical text classication framework for bilingual documentation, from which we derive two novel bilingual classiers based on the naive combination of monolingual classiers. We report preliminary results on the multilingual corpus Acquis Communautaire (AC) that demonstrate the suitability of the proposed classiers as the backend of a fully-working MAI system."]},{"title":"1. Introduction","paragraphs":["The proliferation of multilingual documentation in our Information Society has become a common phenomenon in many ofcial institutions (EU parliament, the Canadian Parliament, UN sessions, Catalan and Basque Parliaments in Spain, etc.) and private companies (user’s manuals, newspapers, books, etc.). In many cases, this textual information needs to be categorised by hand, entailing a time-consuming and arduous burden. This fact is particularly true in keyword assignment, in which a list of keywords (descriptors) from a thesaurus is assigned to a document, without requiring the keywords to be explicitly present in the document. This task can be efciently done using MAI tools (Hodge, 1998; Pouliquen and others, 2003). MAI tools assign to a document a list of keywords (descriptors) from a controlled vocabulary (thesaurus) for indexing purposes. This list of descriptors suggested by the system is reviewed by expert indexers to add and select those descriptors that are the most suitable. The interest behind the development of indexing systems is not only the document classication capabilities per se, but also the cross-lingual information access possibilities (Pouliquen and others, 2003) through multilingual thesaurus, as EuroVoc (EC, 1995), AgroVoc (FAO, 1998), etc. However, current MAI systems do not take full advantage of multilinguality since they are based on monolingual text classiers, both rule-based systems (Hlava and Hainebach, 1996; Loukachevitch and Dobrov, 2002) and statistical methods (Lin and Hovy, 2000; Pouliquen and others, 2003). A more sophisticated approach is to develop new classication models that make prot of multilingual information in order to boost the performance of MAI systems. In this paper, we will focus on bilingual text classication, even though the extension of our classication model to the multilingual case is straightforward. The structure of this paper is as follows. Section 2. introduces the basic probabilistic framework for bilingual classi-","Work supported by the Agencia Valenciana de Ciencia i Tecnologia under grant GRUPS03/031, the Spanish project TIC2003-08682-C02-02 and the Ministerio de Educaci·on y Ciencia. cation, together with two possible instantiations of bilingual classiers. Section 3. is devoted to the EM parameter estimation of one of the models proposed. In Section 4., some preliminary monolingual and bilingual results obtained on the multilingual Acquis Communautaire (AC) corpus are presented. Finally, some conclusions and thoughts for future work are stated in Section 5.."]},{"title":"2. Bilingual text classication","paragraphs":["Given a bilingual document t",", in which","and","are","documents in different languages and translations of each","other, we assign t","to that class (descriptor):  t","    t","","","","t","","   ","t","  ","t","","  t","(1) where","","","t","is the a priori probability of class","and  ","","t","is the probability of observing the bilingual document",",t","in class",". This last term can be better understood when decomposed as a class-dependent language model","","","t",", and a class-dependent translation model  ","","t",". Language models express the idea of how likely is a given sequence of words, while translation models represent the degree of correlation between sequence of words across languages. Note that the classication rule proposed in Eq. 1 can be easily extended to the multiclass case by considering the","most probable classes. However, it is common the case that documents labeled with the same class tend not only to devise about different topics, but also may consist of different kinds of sublanguages (legal texts, communications, questions, etc.) (Steinberger, 2001). For these reasons, it is appealing to consider the so-called mixture model, in which a class may contain documents from several unknown topics:  t    ","    t","  ","t","","","","t","","  t","(2)","where","  t","and","","","","t","are class and topic depen-","dent language and translation models, respectively."]},{"title":"1302","paragraphs":["In the present work, we postpone the usage of translation models in bilingual text classication by considering","and ","to be independent. Therefore, our simplied classication rules can be expressed as follows:  t","    t","  ","t","","  t","(3)","    ","    t","  ","t","","","","t","","  t","(4) In this work, Eq. 3 will be instantiated as a language-independent smooth n-gram model. Smooth n-gram models has been successfully applied in many different areas related to natural language processing. The parameter estimation of the smooth n-gram models is performed according to the maximum likelihood estimation paradigm along with powerful and well-founded smoothing techniques. These models were trained with the well-known and publicly available SRILM toolkit (Stolcke, 2002). Conversely, Eq. 4 will be represented by a language-independent multinomial (unigram) mixture model. Mixture modelling is a standard pattern classication technique. In text classication, the use of multinomial mixtures (Novovicov·a and Mal·k,2003) can be seen as a generalisation of the Naive Bayes text classier by relaxing its feature independence assumption. Maximum likelihood estimation of mixture parameters can be reliably accomplished by the well-known Expectation-Maximisation (EM) algorithm (Dempster et al., 1977). Monolingual classiers can be easily derived from bilingual models in Eqs. 3 and 4 by ignoring one of the terms associated to one of the languages."]},{"title":"3. EM mixture parameter estimation","paragraphs":["This section is devoted to the presentation of an instance of the EM algorithm that estimates the parameters of the mixture model presented in Eq. 4 for a given class",". Let t ","  ","","tt","be a set of samples","to learn the parameters in Eq. 4. The only information re-","tained is two vectors of word counts","","t","and","","","t , where","and","are the number of oc-","currences of word","and","in the input and output sentences,","respectively. and","are the size of the input and output","vocabularies.","The vector of unknown parameters is:","","","","t  t","","tt","(5) for all",",","and",". We are excluding the number of components from the estimation problem, as it is a crucial parameter to control model complexity and it is discussed in Section 4.. Following the maximum likelihood principle, the best parameter values maximise the log-likelihood function,     t        ","","","t  ","","","t","","   t","(6)","In order to nd these optimal values, it is useful to think","of each sample pair  ","","t","as an incomplete component-","labelled sample, which can be completed by an indicator","vector","","","","","  ","t","with","in the position corre-","sponding to the component generating","","","","t","and zeros","elsewhere. In doing so, a complete version of the log-","likelihood function (6) can be stated as     t            ","","","t  ","","","t","","   t","(7) where ","","","","is the so-called missing data. The form of the log-likelihood function given in (7) is generally preferred because it makes available the EM optimisation algorithm (for nite mixtures). This algorithm proceeds iteratively in two steps. The E(xpectation) step computes the expected value of the missing data given the incomplete data and the current parameters. The M(aximisation) step nds the parameter values which maximise (7), on the basis of the missing data estimated in the E step. In our case, the E step replaces each   ","by the posterior probability of  ","","t","being actually","generated by the","-th component,   ","","","t  ","","t","  ","t   ","","","t  ","","t","  ","t (8)","for all","and",", while the M step","nds the maximum likelihood estimates for the priors,  t         ","t (9) and the component prototypes,  t ","   ",""," ","   ","           ","  ","(10)  t ","   ",""," ","   ","              ","(11)","for all",",","and","."]},{"title":"4. Experimental results 4.1. Dataset","paragraphs":["Experiments were carried out on the Acquis Communautaire (AC) corpus (Steinberger et al., 2006). This large text collection contains documents selected from the European Union (EU) legislation in all the EU languages. Most of these documents have been manually classied according to the EuroVoc thesaurus. Each document is assigned a set of EuroVoc descriptors out of 6645 possible, even though only those 990 descriptors ocurring at least 5 times were considered in this work for evaluation purposes. Before training our text classier, the AC corpus underwent a basic preprocessing consisting in downcasing, isolation of punctuation marks and replacement of numbers by a generic label. Some statistics of the preprocessed French-English partition of this corpus are shown in Table 1. However, we prefered not to apply any language-dependent preprocessing, such as lemmatisation, multi-word mark-up or stopword lists, since our models are intended to deal with multilingual text. This linguistic preprocessing would improve the accuracy of classiers and we plan to consider it in future work (Pouliquen and others, 2003)."]},{"title":"1303","paragraphs":["Fre Eng documents 5108 average length 1819 1564 vocabulary 36.6K 32.5K singletons 10.6K 10.5K running words 9.3M 8.0M Table 1: Basic statistics of the preprocessed French-English partition of the AC corpus. 4.2. Experimental results We evaluated the bilingual classier discussed above, and also its monolingual counterpart, on random 80%-20% train-test splits of the French-English AC partition. Classiers were assessed in terms of precision and recall on a per-document basis, since this measure is closer to user needs. Also, it should be considered that the number of EuroVoc descriptors varies from one document to another, therefore a strategy to select the right number of descriptors for each document is required. As a rst, preliminary approach, we have simply extract ve descriptors per document, which is the average number of descriptors in the whole corpus. The preliminary results obtained for the smooth n-gram (straight lines) and mixture (curves) multinomial classiers are shown in Figure 1, both for the best monolingual (English-only) and the bilingual classier. In the case of mixture-based classiers, an analysis of the evolution of the precision and recall values as a function of the number of mixture components per class was performed. Each plotted point along mixture curves is an average over values obtained from 6 randomised trials. In the case of smooth n-gram models, a single experiment for each parameter set-ting was considered. From the results in Figure 1, we can observe that the trigram (3g) classier performs the best on its monolingual and bilingual versions, followed by the bigram (2g) classier, the mixture multinomial (mix 1g) classier and the unigram (1g) classier. This perfomance directly correlates with the increasing complexity of the models that support these classiers. Additional experiments demonstrated that smooth n-gram models beyond trigrams provides no accuracy improvement at all. When analysing the behaviour of multinomial mixture on monolingual and bilingual classiers clearly outstands the advantages of multiple component over single component modelling. Indeed, we could consider that the optimal number of unkown topics (components) in our mixture model is about 10, thereafter the precision and recall values seem to follow a steady trend. Nevertheless, these results surprisingly reect that there is little difference between the performance of the monolingual and bilingual classiers. Even though, this is not the rule but the exception, as revealed in previous work (Juan and Civera, 2005; Civera and Juan, 2005). 4.3. Discussion The excellence of these results should be assessed bearing in mind the complexity of this task and how MAI systems work. On the one hand, professional indexers do not com-44.0 46.0 48.0 50.0 52.0 54.0 1 2 5 10 20 50 1g P 1g R 2g P 2g R English-only classifier 3g P 3g R mix 1g P mix 1g R Mixture components Precision & Recall (%) 44.0 46.0 48.0 50.0 52.0 54.0 1 2 5 10 20 50 Bilingual classifier Mixture components Precision & Recall (%) 1g1g P 1g1g R 2g2g P 2g2g R 3g3g P 3g3g R mix 1g1g P mix 1g1g R Figure 1: Precision (P) and recall (R) curves as a function of the number of mixture components for the English-only (top) and bilingual (bottom) multinomial (mix) classiers. Precision and recall straight lines are plotted for the English-only and bilingual single component n-gram (ng) classier. pletely agree on the most suitable descriptors for a given document. Indeed, previous studies (Pouliquen and others, 2003) on annotator agreement mantain that keyword over-lapping among indexers is about 70% to 80%. On the other hand, MAI systems work by providing a lengthy list of descriptors from which an indexer would select those ones considered most appropriated. For evaluation purposes, we decided that our MAI system should provide only 5 descriptors for each document, seeking a balance between precision and recall. However, in a MAI scenario, we would be more interested in recall, since we would like that our system provides a longer list of descriptors, from which a indexer would lter out those unsuitable descriptors. Taking this into account, we conducted some additional experiments to evaluate the recall values that we would obtain if we considered a longer list of descriptors. These experiments revealed that our MAI system would be offering up to 68.9% of the correct descriptors for a list of 10 descriptors, and up to 78.7% for a list of 20 descriptors. These gures clearly convey the possibility of a MAI system which suggests most of the desired descriptors."]},{"title":"1304 5. Conclusions and future work","paragraphs":["In the current work, we have presented two bilingual text classiers and their corresponding monolingual counterparts based on multinomial mixture and smooth n-gram models. The performance of these classiers was assessed on the recently released preliminary version of the multilingual AC corpus. Three outstanding conclusions can be stated from the results presented. First, multinomial (unigram) mixture-based classiers surpass single component unigram classiers. In fact, we have taken advantage of the exibility of the mixture modelisation over the single component approach to further improve the precision and recall values achieved. Second, smooth n-gram models clearly out-perform multinomial mixture models. This is so, because smooth n-gram models go beyond the bag-of-words representation and make prot of the context information (Peng and others, 2003; Scheffer and Wrobel, 2002). Third, bilingual classiers show similar performance to their monolingual counterparts, although previous work on simpler datasets exhibit different behaviour (Juan and Civera, 2005; Civera and Juan, 2005). As said above, we think that this may be due to the relatively high complexity of the AC task. Nonetheless, the accuracy of our smooth n-gram classier is good enough to support a MAI system, that would be providing on average about 80% of the correct descriptors associated with a document. As a future work, there are several research lines that would be interesting to explore. First of all, the accuracy of multinomial mixture classier may be signicantly boosted by incorporating some of the techniques proposed in (Rennie and others, 2003; Pavlov and others, 2004). Extensions of smooth","-gram models provide an interesting starting point for more versatile language models, as mixtures of smooth n-gram models (Iyer and Ostendorf, 1999) or smooth n-gram models that incorporate automatically learned word classes (Brown and others, 1992). Other appealing approaches consider the problem of text classication under the maximum entropy framework (Nigam et al., 1999) or the application of multi-label text classiers (McCallum, 1999). All in all, the two bilingual classiers described in this work are relatively simple models for the statistical distribution of bilingual texts. More sophisticated models, such as IBM statistical translation models (Brown and others, 1990), may be better in describing the statistical distribu-tion of bilingual, correlated texts."]},{"title":"6. References","paragraphs":["P. F. Brown et al. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):7985.","P. F. Brown et al. 1992. Class-based n-gram models of natural language. Comput. Linguistics, 18(4):467479.","J. Civera and A. Juan. 2005. Multinomial Mixture Modelling for Bilingual Text Classication. Technical report DSIC-II/10/05, UPV.","A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Soc., 39(B):138.","EC. 1995. Thesaurus eurovoc - volume 2: Subject-oriented version. Annex to the index of the Ofcial Journal of the EC, Ofce for Ofcial Publications of the EC. http://europa.eu.int/celex/eurovoc.","FAO. 1998. Multilingual agricultural thesaurus. World Agricultural Information Center. http://www.fao.org/ scripts/agrovoc/frame.htm.","M. Hlava and R. Hainebach. 1996. Multilingual Machine Indexing. In Proc. of NIT’96, pages 105121.","G. Hodge. 1998. CENDI agency indexing system descriptors: A Baseline Report. Technical report, Information International Associates, Inc.","R. M. Iyer and M. Ostendorf. 1999. Modelling long distance dependence in language: Topic mixtures versus dynamic cache models. IEEE Transactions on Speech & Audio Processing, 7(1):3039.","A. Juan and J. Civera. 2005. Parallel Multinomial Mixtures for Bilingual Text Classication. Technical report DSIC-II/09/05, DSIC, Polytechnical Univ. of Valencia.","C. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proc. of CoLing’00, pages 495501.","N. Loukachevitch and B. Dobrov. 2002. Crosslingual IR based on Multilingual Thesaurus specically created for Automatic Text Processing. In Proc. of SIGIR’02, pages 105121.","A. McCallum. 1999. Multi-label text classication with a mixture model trained by EM. In Proceedings of the AAAI’99 Workshop on Text Learning.","K. Nigam, J. Lafferty, and A. McCallum. 1999. Using maximum entropy for text classication. In Proc. of IJCAI-99, pages 6167.","J. Novovicov·a and A. Mal·k. 2003. Application of Multinomial Mixture Model to Text Classication. In Proc. of IbPRIA 2003, pages 646653.","D. Pavlov et al. 2004. Document Preprocessing For Naive Bayes Classication and Clustering with Mixture of Multinomials. In Proc. of KDD’04, pages 829834.","F. Peng et al. 2003. Augmenting Naive Bayes classiers with statistical language models. Information Retrieval, 7(3):317345.","B. Pouliquen et al. 2003. Automatic annotation of multilingual text collections with a conceptual thesaurus. In Proc. of EUROLAN’03.","J. Rennie et al. 2003. Tackling the Poor Assumptions of Naive Bayes Text Classiers. In Proc. of ICML’03, pages 616623.","T. Scheffer and S. Wrobel. 2002. Text Classication Beyond the Bag-of-Words Representation. In Proc. of ICML’02.","Ralf Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tus, A. Ceausu, and D. Varga. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proc. of LREC’06.","R. Steinberger. 2001. Crosslingual keyword assignment. In Proc. of SEPLN’01, pages 273280.","A. Stolcke. 2002. SRILM an extensible language model-ing toolkit. In Proc. of ICSLP’02."]},{"title":"1305","paragraphs":[]}]}