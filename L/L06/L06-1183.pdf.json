{"sections":[{"title":"Fear-ty\\be\\temot\\fons\\tof\\tthe\\tSAFE\\tCor\\bus:\\tannotat\\fon\\t\\fssues\\t Chloé\\tClavel","paragraphs":["1,2,3"]},{"title":",\\tIoana\\tVas\\flescu\\t","paragraphs":["2"]},{"title":",\\tLaurence\\tDev\\fllers","paragraphs":["2"]},{"title":",\\tTh\\fbaut\\tEhrette","paragraphs":["1"]},{"title":"\\t\\tand\\tGaël\\tR\\fchard","paragraphs":["3"]},{"title":".\\t","paragraphs":["(1) Tha\\b\\ts R\\ts\\ta\\fch & T\\tchno\\bogy F\\fanc\\t, RD 128, 91767 Pa\\bais\\tau C\\td\\tx (2) LIMSI-CNRS, BP 133, 91403 O\\fsay C\\td\\tx, F\\fanc\\t. (3) ENST-TSI, 46 \\fu\\t Ba\\f\\fau\\bt, 75634 Pa\\fis, C\\td\\tx 13, F\\fanc\\t E-mai\\b: ch\\bo\\t.c\\bav\\t\\b@tha\\b\\tsg\\foup.com"]},{"title":"Abstract\\t","paragraphs":["Th\\t p\\f\\ts\\tnt \\f\\ts\\ta\\fch focus\\ts on annotation issu\\ts in th\\t cont\\txt of th\\t acoustic d\\tt\\tction of f\\ta\\f-typ\\t \\tmotions fo\\f su\\fv\\ti\\b\\banc\\t app\\bications. Th\\t \\tmotiona\\b sp\\t\\tch mat\\t\\fia\\b us\\td fo\\f this study com\\ts f\\fom th\\t p\\f\\tvious\\by co\\b\\b\\tct\\td SAFE Databas\\t (Situation Ana\\bysis in a Fictiona\\b and Emotiona\\b Databas\\t) which consists of audio-visua\\b s\\tqu\\tnc\\ts \\txt\\fact\\td f\\fom movi\\t fictions. A g\\tn\\t\\fic annotation sch\\tm\\t was d\\tv\\t\\bop\\td to annotat\\t th\\t va\\fious \\tmotiona\\b manif\\tstations contain\\td in th\\t co\\fpus. Th\\t annotation was ca\\f\\fi\\td out by two \\bab\\t\\b\\b\\t\\fs and th\\t two annotations st\\fat\\tgi\\ts a\\f\\t conf\\font\\td. It \\tm\\t\\fg\\ts that th\\t bo\\fd\\t\\f\\bin\\t b\\ttw\\t\\tn emotion and ne\\bt\\tal va\\fy acco\\fding to th\\t \\bab\\t\\b\\b\\t\\f. An acoustic va\\bidation by a thi\\fd \\bab\\t\\b\\b\\t\\f a\\b\\bows at ana\\bysing th\\t two st\\fat\\tgi\\ts. Two human st\\fat\\tgi\\ts a\\f\\t th\\tn obs\\t\\fv\\td: a fi\\fst on\\t, cont\\txt-o\\fi\\tnt\\td which mix\\ts audio and cont\\txtua\\b (vid\\to) info\\fmation in \\tmotion cat\\tgo\\fization; and a s\\tcond on\\t, bas\\td main\\by on audio info\\fmation. Th\\t k-m\\tans c\\bust\\t\\fing confi\\fms th\\t \\fo\\b\\t of audio cu\\ts in human annotation st\\fat\\tgi\\ts. It pa\\fticu\\ba\\f\\by h\\t\\bps in \\tva\\buating thos\\t st\\fat\\tgi\\ts f\\fom th\\t point of vi\\tw of a d\\tt\\tction syst\\tm bas\\td on audio cu\\ts. "]},{"title":"1. Introduct\\fon\\t","paragraphs":["Th\\t \\tmotiona\\b info\\fmation conv\\ty\\td by sp\\t\\tch has b\\t\\tn igno\\f\\td fo\\f a \\bong tim\\t and sp\\t\\tch and \\banguag\\t studi\\ts hav\\t most\\by focus\\td on th\\t \\txp\\bicit m\\tssag\\t p\\fovid\\td by th\\t \\b\\txica\\b \\b\\tv\\t\\b. R\\tc\\tnt\\by th\\t t\\f\\tnd has b\\t\\tn chang\\td and a fad fo\\f th\\t \\tmotiona\\b ph\\tnom\\tnon has \\tm\\t\\fg\\td (Cowi\\t \\tt a\\b, 2001). Th\\t \\tmotiona\\b cont\\tnt inf\\bu\\tnc\\ts th\\t s\\tmantic d\\tcoding of human int\\t\\factions and can in this way sha\\f\\t in th\\t imp\\fov\\tm\\tnt of sp\\t\\tch p\\foc\\tssing syst\\tms. Fu\\fth\\t\\fmo\\f\\t, in dia\\bog syst\\tms app\\bications th\\t id\\tntification of th\\t sp\\tak\\t\\f's \\tmotiona\\b stat\\t aims at adapting th\\t dia\\bog st\\fat\\tgy in o\\fd\\t\\f to p\\fovid\\t a mo\\f\\t \\f\\t\\b\\tvant answ\\t\\f to th\\t sp\\tak\\t\\f's \\f\\tqu\\tst (L\\t\\t \\tt a\\b, 2001, D\\tvi\\b\\b\\t\\fs \\tt a\\b., 2003). In this study, w\\t a\\f\\t int\\t\\f\\tst\\td in audio-vid\\to su\\fv\\ti\\b\\banc\\t app\\bications. Sinc\\t cu\\f\\f\\tnt syst\\tms a\\f\\t most\\by vid\\to-bas\\td, on\\t of th\\t main cha\\b\\b\\tng\\ts is to us\\t th\\t audio cont\\tnt as comp\\b\\tm\\tnta\\fy info\\fmation to vid\\to to automatica\\b\\by d\\tt\\tct an abno\\fma\\b situation (situation du\\fing which th\\t human \\bif\\t is in dang\\t\\f). Th\\t human o\\fa\\b communication in such situations is st\\fong\\by bas\\td on th\\t \\tmotiona\\b chann\\t\\b. Th\\t\\f\\t is, as a cons\\tqu\\tnc\\t, a st\\fong int\\t\\f\\tst to automatica\\b\\by d\\tt\\tct symptomatic \\tmotions occu\\f\\fing in abno\\fma\\b situations. This g\\fowing int\\t\\f\\tst fo\\f \\f\\ts\\ta\\fch on \\tmotions has \\fais\\td th\\t qu\\tstion of co\\b\\b\\tcting and annotating co\\fpo\\fa of \\tmotiona\\b sp\\t\\tch. Th\\t \\tmotiona\\b ph\\tnom\\tnon is subt\\b\\t\\f, \\fa\\f\\t\\f and mo\\f\\t subj\\tctiv\\t than th\\t ph\\tnom\\tna p\\f\\tvious\\by studi\\td in dia\\bog (C\\faggs, 2004). Obvious\\by, th\\t p\\t\\ffo\\fmanc\\ts of an automatic \\tmotion d\\tt\\tction syst\\tm st\\fong\\by d\\tp\\tnd on th\\t qua\\bity of th\\t \\tmotiona\\b mat\\t\\fia\\b us\\td to bui\\bd th\\t acoustic mod\\t\\bs. Th\\t cha\\b\\b\\tng\\t is to d\\t\\bimitat\\t accu\\fat\\t \\tmotiona\\b cat\\tgo\\fi\\ts both in t\\t\\fms of p\\t\\fc\\tiv\\td c\\bass\\ts fo\\f th\\t annotation st\\fat\\tgy and in t\\t\\fms of acoustic mod\\t\\bs fo\\f th\\t d\\tt\\tction syst\\tm. Co\\b\\b\\tcting app\\fop\\fiat\\t \\tmotiona\\b data in th\\t cont\\txt of su\\fv\\ti\\b\\banc\\t app\\bications is a difficu\\bt task. Th\\t \\tmotions ta\\fg\\tt\\td by su\\fv\\ti\\b\\banc\\t app\\bications b\\t\\bong ind\\t\\td to th\\t sp\\tcific c\\bass of \\tmotions \\tm\\t\\fging in abno\\fma\\b situations. Th\\ty occu\\f in dynamic situations, du\\fing which th\\t matt\\t\\f of su\\fviva\\b is \\fais\\td. Abno\\fma\\b situations a\\f\\t how\\tv\\t\\f \\fa\\f\\t and unp\\f\\tdictab\\b\\t and \\f\\ta\\b-\\bif\\t \\f\\tco\\fdings of such situations a\\f\\t fo\\f th\\t most confid\\tntia\\b. Existing \\f\\ta\\b-\\bif\\t co\\fpo\\fa (Doug\\bas-Cowi\\t, 2003), i\\b\\bust\\fat\\t \\tv\\t\\fyday \\bif\\t cont\\txts in which socia\\b \\tmotions cu\\f\\f\\tnt\\by occu\\f. Th\\t typ\\t of \\tmotiona\\b manif\\tstations and th\\t d\\tg\\f\\t\\t of int\\tnsity of such \\tmotions a\\f\\t d\\tt\\t\\fmin\\td by po\\bit\\tn\\tss habits and cu\\btu\\fa\\b b\\thaviou\\fs. How\\tv\\t\\f, th\\t \\back of co\\fpo\\fa i\\b\\bust\\fating st\\fong \\tmotions in \\f\\ta\\b abno\\fma\\b situations has \\tncou\\fag\\td us to bui\\bd th\\t SAFE Co\\fpus (Situation Ana\\bysis in a Fictiona\\b and Emotiona\\b Co\\fpus). A \\fea\\t-type \\tmotions d\\tt\\tction syst\\tm bas\\td on acoustic cu\\ts has b\\t\\tn d\\tv\\t\\bop\\td using this co\\fpus (C\\bav\\t\\b \\tt a\\b, 2006). Th\\t ta\\fg\\tt\\td \\fea\\t c\\bass is a g\\boba\\b c\\bass containing a high va\\fiabi\\bity in t\\t\\fms of \\tmotiona\\b \\f\\tp\\f\\ts\\tntations. In pa\\fticu\\ba\\f, \\fea\\t is \\ba\\fg\\t\\by \\f\\tp\\f\\ts\\tnt\\td in t\\t\\fms of \\tmotiona\\b int\\tnsity. How\\tv\\t\\f, th\\t annotation of this \\tmotiona\\b int\\tnsity is quit\\t subj\\tctiv\\t and d\\tp\\tnds on th\\t \\bab\\t\\b\\b\\t\\f annotation st\\fat\\tgy. It is shown in this pap\\t\\f that th\\t appa\\f\\tnt disc\\f\\tpanci\\ts b\\ttw\\t\\tn th\\t two \\bab\\t\\b\\b\\t\\fs can b\\t, to a \\ba\\fg\\t \\txt\\tnt, \\txp\\bain\\td by th\\t diff\\t\\f\\tnt st\\fat\\tgi\\ts adopt\\td. Th\\t pap\\t\\f is o\\fganis\\td as fo\\b\\bows. In th\\t n\\txt s\\tction, th\\t SAFE Co\\fpus is b\\fi\\tf\\by d\\tsc\\fib\\td. Th\\tn, in s\\tction 3, a compa\\fativ\\t in-d\\tpth ana\\bysis of th\\t annotations of th\\t two \\bab\\t\\b\\b\\t\\fs is p\\fovid\\td inc\\buding an \\txp\\t\\fim\\tnt on s\\tgm\\tnts that w\\t\\f\\t \\f\\t-annotat\\td using on\\by th\\t audio data (without acc\\tss to th\\t co\\f\\f\\tsponding vid\\to data). In s\\tction 4, th\\t pot\\tntia\\b inf\\bu\\tnc\\t of th\\t va\\fious human annotation st\\fat\\tgi\\ts on th\\t automatic d\\tt\\tction syst\\tm p\\t\\ffo\\fmanc\\ts is discuss\\td."]},{"title":"2. The\\tSAFE\\tCor\\bus\\t 2.1\\t Global\\tContent\\t\\t","paragraphs":["Th\\t SAFE Co\\fpus consists of 400 audio-visua\\b sequences f\\fom 8s to 5min \\txt\\fact\\td f\\fom a co\\b\\b\\tction of"]},{"title":"1099","paragraphs":["30 \\f\\tc\\tnt movi\\ts in Eng\\bish. Th\\t fiction p\\fovid\\ts an int\\t\\f\\tsting \\fang\\t of pot\\tntia\\b \\f\\ta\\b-\\bif\\t abno\\fma\\b cont\\txts and of typ\\t of sp\\tak\\t\\fs that wou\\bd hav\\t b\\t\\tn v\\t\\fy difficu\\bt to co\\b\\b\\tct in \\f\\ta\\b \\bif\\t. Emotions a\\f\\t \\tm\\t\\fging in int\\t\\fp\\t\\fsona\\b int\\t\\factions in th\\t h\\ta\\ft of th\\t action. Ev\\tn if th\\t audio and vid\\to acquisitions a\\f\\t conduct\\td und\\t\\f b\\ttt\\t\\f conditions than it wou\\bd b\\t in a \\f\\ta\\b audio and vid\\to su\\fv\\ti\\b\\banc\\t, th\\t fiction a\\b\\bows th\\t co\\b\\b\\tction of \\tmotiona\\b data with th\\ti\\f \\tnvi\\fonm\\tnta\\b nois\\t. A tota\\b of 7 hou\\fs of \\f\\tco\\fdings was co\\b\\b\\tct\\td in which sp\\t\\tch \\f\\tp\\f\\ts\\tnts 76% of th\\t data. Each s\\tqu\\tnc\\t co\\f\\f\\tsponds to a pa\\fticu\\ba\\f situation, no\\fma\\b o\\f abno\\fma\\b. Emotions a\\f\\t consid\\t\\f\\td in th\\t t\\tmpo\\fa\\b cont\\txt of th\\t seq\\bence."]},{"title":"2.2. Annotat\\fon\\tScheme","paragraphs":["A gene\\b\\tc annotation sch\\tm\\t was d\\tv\\t\\bop\\td with th\\t vi\\tw to b\\t \\txpo\\ft\\td to oth\\t\\f co\\fpo\\fa and to a \\f\\ta\\b \\bif\\t su\\fv\\ti\\b\\banc\\t app\\bication (C\\bav\\t\\b \\tt a\\b, 2004, 2006). Va\\fious asp\\tcts of th\\t s\\tqu\\tnc\\t cont\\tnt w\\t\\f\\t tak\\tn into account: the\\f emot\\tonal\\f substance,\\f the\\f s\\ttuat\\tonal\\f context (typ\\t of th\\f\\tat, sp\\tak\\t\\f g\\tnd\\t\\f and id\\tntity, \\bocation \\ttc.) and th\\t acoustic cont\\txt (audio qua\\bity). Vid\\to was us\\td as h\\t\\bp fo\\f th\\t annotation via th\\t us\\t of a too\\b fo\\f mu\\btimoda\\b annotation ANVIL (Kipp, 2001). Th\\t s\\tgm\\tntation and th\\t annotation of th\\t co\\fpus w\\t\\f\\t ca\\f\\fi\\td out by a fi\\fst Eng\\bish nativ\\t \\bab\\t\\b\\b\\t\\f. Each s\\tqu\\tnc\\t was s\\tgm\\tnt\\td into a basic annotation unit, th\\t segment. It co\\f\\f\\tsponds to a sp\\tak\\t\\f tu\\fn o\\f a s\\tction of a sp\\tak\\t\\f tu\\fn po\\ft\\faying th\\t sam\\t annotat\\td \\tmotion. 4724 s\\tgm\\tnts of sp\\t\\tch with a du\\fation va\\fying f\\fom 40ms to 80s a\\f\\t thus obtain\\td f\\fom th\\t 400 s\\tqu\\tnc\\ts of th\\t co\\fpus. Th\\t d\\tsc\\fiption of \\tmotiona\\b substanc\\t is consid\\t\\f\\td at th\\t s\\tgm\\tnt \\b\\tv\\t\\b and is b\\fok\\tn into two typ\\ts of d\\tsc\\fipto\\fs: dim\\tnsiona\\b and cat\\tgo\\fica\\b. D\\tmens\\tonal\\f desc\\b\\tpto\\bs a\\f\\t bas\\td on th\\t th\\f\\t\\t abst\\fact dim\\tnsions p\\f\\tvious\\by \\txp\\boit\\td in th\\t \\bit\\t\\fatu\\f\\t (Osgood, 1975): activation, \\tva\\buation and cont\\fo\\b. Th\\t cont\\fo\\b dim\\tnsion has b\\t\\tn h\\t\\f\\t adapt\\td acco\\fding to th\\t app\\bication and \\f\\tnam\\td \\f\\tactivity. Abst\\fact dim\\tnsions a\\f\\t \\tva\\buat\\td on disc\\f\\tt\\t sca\\b\\ts. Th\\t p\\t\\fc\\tptua\\b sa\\bi\\tnc\\t of thos\\t d\\tsc\\fipto\\fs was \\tva\\buat\\td in a fo\\fm\\t\\f study (C\\bav\\t\\b \\tt a\\b, 2004). Catego\\b\\tcal\\fdesc\\b\\tpto\\bs a\\f\\t a\\bso \\tmp\\boy\\td fo\\f th\\t cha\\fact\\t\\fisation of th\\t \\tmotiona\\b cont\\tnt of \\tach s\\tgm\\tnt. Fou\\f majo\\f \\tmotion c\\bass\\ts hav\\t b\\t\\tn s\\t\\b\\tct\\td: g\\boba\\b c\\bass f\\ta\\f, oth\\t\\f n\\tgativ\\t \\tmotions, n\\tut\\fa\\b, positiv\\t \\tmotions. G\\boba\\b c\\bass f\\ta\\f co\\f\\f\\tsponds to a\\b\\b f\\ta\\f-\\f\\t\\bat\\td \\tmotiona\\b stat\\ts. A s\\tcond \\bab\\t\\b\\b\\t\\f ind\\tp\\tnd\\tnt\\by annotat\\td th\\t \\tmotiona\\b cont\\tnt of th\\t p\\f\\t-s\\tgm\\tnt\\td s\\tqu\\tnc\\ts. F\\fom now on, th\\t Eng\\bish nativ\\t \\bab\\t\\b\\b\\t\\f and th\\t bi\\bingua\\b \\bab\\t\\b\\b\\t\\f a\\f\\t \\f\\tsp\\tctiv\\t\\by nam\\td Lab1 and Lab2. "]},{"title":"3. Human\\tannotat\\fon\\tstrateg\\fes\\t","paragraphs":["In this s\\tction, th\\t \\b\\tv\\t\\b of ag\\f\\t\\tm\\tnt b\\ttw\\t\\tn th\\t two \\bab\\t\\b\\b\\t\\fs, Lab1 and Lab2, in \\tmotiona\\b annotations is \\tva\\buat\\td and th\\t two human annotation st\\fat\\tgi\\ts a\\f\\t conf\\font\\td."]},{"title":"3.1. Categor\\fzat\\fon\\tProcess","paragraphs":["Th\\t int\\t\\f-\\bab\\t\\b\\b\\t\\f ag\\f\\t\\tm\\tnt fo\\f th\\t fou\\f \\tmotiona\\b cat\\tgo\\fi\\ts is \\tva\\buat\\td thanks to th\\t t\\faditiona\\b kappa statistics (S\\tiga\\b, 1988). Th\\t kappa sco\\f\\t b\\ttw\\t\\tn th\\t two \\bab\\t\\b\\b\\t\\fs is at 0.46 which is an acc\\tptab\\b\\t \\b\\tv\\t\\b of ag\\f\\t\\tm\\tnt fo\\f subj\\tctiv\\t ph\\tnom\\tna such as \\tmotions (C\\faggs, 2004). Tab\\b\\t 1 shows th\\t \\f\\tpa\\ftition of th\\t s\\tgm\\tnts among th\\t \\tmotiona\\b cat\\tgo\\fi\\ts acco\\fding to th\\t \\bab\\t\\b\\b\\t\\f. It \\tm\\t\\fg\\ts that Lab1 \\tva\\buat\\ts mo\\f\\t s\\tgm\\tnts (75%) as non-n\\tut\\fa\\b, than Lab2 (58%). ","F\\ta\\f Oth\\t\\f n\\tgativ\\t \\tmotions","N\\tut\\fa\\b Positiv\\t \\tmotions","Lab1 32% 35% 25%\\t 8%","Lab 2 27% 24% 42%\\t 7% Tab\\b\\t 1: Emotiona\\b cat\\tgo\\fi\\ts \\f\\tpa\\ftition acco\\fding to","\\tach \\bab\\t\\b\\b\\t\\f."," In o\\fd\\t\\f to high\\bight wh\\t\\f\\t th\\t disag\\f\\t\\tm\\tnts a\\f\\t \\bocat\\td, figu\\f\\t 1 i\\b\\bust\\fat\\ts th\\t confusions b\\ttw\\t\\tn th\\t two \\bab\\t\\b\\b\\t\\fs fo\\f \\tach \\tmotiona\\b cat\\tgo\\fy. Fo\\f \\txamp\\b\\t th\\t histog\\fam associat\\td with \\fea\\t on th\\t x-axis tak\\ts into account th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t by Lab2. It shows th\\t dist\\fibution of th\\t \\bab\\t\\bs s\\t\\b\\tct\\td by Lab1 fo\\f th\\ts\\t s\\tgm\\tnts. Lab1 ag\\f\\t\\ts on 78% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td f\\ta\\f by Lab2. On th\\t oth\\t\\f hand 53 % of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td ne\\bt\\tal by Lab2 a\\f\\t \\bab\\t\\b\\b\\td emotion (\\fea\\t, othe\\t negative emotions o\\f positive emotions) by Lab1. Th\\t\\f\\tfo\\f\\t th\\t majo\\f caus\\t of disag\\f\\t\\tm\\tnt b\\ttw\\t\\tn th\\t two \\bab\\t\\b\\b\\t\\fs \\f\\t\\bi\\ts on th\\t emotion v\\t\\fsus ne\\bt\\tal cat\\tgo\\fisation. Th\\t s\\tcond caus\\t of disag\\f\\t\\tm\\tnt is du\\t to confusion b\\ttw\\t\\tn \\fea\\t and othe\\t negative emotions (15% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t by th\\t Lab2 a\\f\\t \\bab\\t\\b\\b\\td othe\\t negative emotions by Lab1 and 22% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td othe\\t negative emotions a\\f\\t \\bab\\t\\b\\b\\td as \\fea\\t). W\\t can a\\bso notic\\t a f\\tw confusions b\\ttw\\t\\tn positiv\\t \\tmotions and n\\tgativ\\t \\tmotions which a\\f\\t du\\t to mix\\td \\tmotions, difficu\\bt to annotat\\t.  Figu\\f\\t 1: Confusion histog\\fam fo\\f \\tmotiona\\b cat\\tgo\\fi\\ts"]},{"title":"3.2. Intens\\fty\\tscale\\tlabell\\fng","paragraphs":["Th\\t bo\\bde\\bl\\tne\\tb\\ttw\\t\\tn\\temot\\ton\\tand\\tneut\\bal\\tstate is not th\\t sam\\t fo\\f th\\t two \\bab\\t\\b\\b\\t\\fs. This finding \\tncou\\fag\\ts us to co\\f\\f\\t\\bat\\t \\tmotion cat\\tgo\\fi\\ts with th\\ti\\f dim\\tnsiona\\b d\\tsc\\fiptions and with th\\t int\\tnsity dim\\tnsion in pa\\fticu\\ba\\f. "]},{"title":"1100","paragraphs":["Th\\t kappa is a\\bso comput\\td fo\\f th\\t int\\tnsity sca\\b\\t. Th\\t kappa sco\\f\\t b\\ttw\\t\\tn th\\t two \\bab\\t\\b\\b\\t\\fs is at 0.24. This sco\\f\\t is quit\\t \\bow. How\\tv\\t\\f it m\\tasu\\f\\ts th\\t \\b\\tv\\t\\b of ag\\f\\t\\tm\\tnt b\\ttw\\t\\tn th\\t fou\\f \\b\\tv\\t\\bs (0, 1, 2, 3) of th\\t int\\tnsity sca\\b\\t without consid\\t\\fing th\\t \\b\\tv\\t\\b p\\foximity. Th\\t C\\fonbach’s a\\bpha m\\tasu\\f\\t (C\\fonbach, 1951) is anoth\\t\\f m\\tasu\\f\\t of int\\t\\f-\\bab\\t\\b\\b\\t\\f \\f\\t\\biabi\\bity, mo\\f\\t suitab\\b\\t than kappa fo\\f \\bab\\t\\bs on a num\\t\\fica\\b sca\\b\\t such as thos\\t us\\td fo\\f th\\t int\\tnsity axis. Th\\t C\\fonbach’s a\\bpha sco\\f\\t is h\\t\\f\\t at 0.82. This sco\\f\\t shows that, \\tv\\tn though th\\t d\\tg\\f\\t\\t of ag\\f\\t\\tm\\tnt on th\\t fou\\f int\\tnsity \\b\\tv\\t\\bs int\\tnsity b\\ttw\\t\\tn th\\t two \\bab\\t\\b\\b\\t\\fs is quit\\t \\bow, th\\t two \\bab\\t\\b\\b\\t\\fs annotation st\\fat\\tgi\\ts s\\t\\tms to b\\t co\\f\\f\\t\\bat\\td. Figu\\f\\t 2: Confusion histog\\fam fo\\f th\\t int\\tnsity sca\\b\\t  In o\\fd\\t\\f to \\bocat\\t th\\t disag\\f\\t\\tm\\tnt, th\\t confusion histog\\fam fo\\f th\\t int\\tnsity sca\\b\\t is \\f\\tp\\f\\ts\\tnt\\td on figu\\f\\t 2. As \\txp\\tct\\td, th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td ne\\bt\\tal (int\\tnsity \\b\\tv\\t\\b = 0) by Lab2 and emotion by Lab1 a\\f\\t fo\\f th\\t majo\\f pa\\ft \\bab\\t\\b\\b\\td \\b\\tv\\t\\b 1 on th\\t int\\tnsity sca\\b\\t. G\\boba\\b\\by Lab1 \\tva\\buat\\ts th\\t s\\tgm\\tnts as mo\\f\\t int\\tns\\t with on\\t \\b\\tv\\t\\b high\\t\\f than Lab2. 77% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\b\\tv\\t\\b 3 by Lab2 a\\f\\t \\tva\\buat\\td with th\\t sam\\t \\b\\tv\\t\\b by Lab1. On th\\t oth\\t\\f hand 42% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td 2 by Lab2 a\\f\\t \\bab\\t\\b\\b\\td 3 by Lab1."]},{"title":"3.3. \\t“Bl\\fnd”\\tannotat\\fon\\t","paragraphs":["Th\\t annotation of a giv\\tn s\\tgm\\tnt is inf\\bu\\tnc\\td both by audio (acoustic and s\\tmantic cont\\tnt) and vid\\to info\\fmation contain\\td in th\\t who\\b\\t s\\tqu\\tnc\\t. Th\\t SAFE Co\\fpus is ind\\t\\td audio-visua\\b and th\\t audio annotation und\\t\\f ANVIL is don\\t using th\\t vid\\to st\\f\\tam. This choic\\t is d\\fiv\\tn by th\\t fina\\b p\\t\\fsp\\tctiv\\t of this \\f\\ts\\ta\\fch. Th\\t audio modu\\b\\t is aim\\td to b\\t ins\\t\\ft\\td in a mu\\btimoda\\b su\\fv\\ti\\b\\banc\\t syst\\tm. How\\tv\\t\\f th\\t goa\\b of th\\t cu\\f\\f\\tnt \\f\\ts\\ta\\fch is to d\\tv\\t\\bop an \\tmotion d\\tt\\tction syst\\tm bas\\td on acoustic cu\\ts. Th\\t acoustic d\\tt\\tction syst\\tm p\\f\\tvious\\by d\\tv\\t\\bop\\td (C\\bav\\t\\b \\tt a\\b, 2006) \\f\\t\\bi\\ts on a \\fea\\t v\\t\\fsus ne\\bt\\tal c\\bassification syst\\tm. Fo\\f \\tach c\\bass an acoustic mod\\t\\b is bui\\bt f\\fom th\\t data \\bab\\t\\b\\b\\td as inc\\bud\\td in this c\\bass. Th\\t annotation of som\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t may on\\by b\\t du\\t to vid\\to o\\f cont\\txtua\\b cu\\ts. Th\\ts\\t s\\tgm\\tnts don’t contain any acoustic cu\\ts of \\fea\\t. If th\\t p\\fopo\\ftion of such s\\tgm\\tnts in th\\t c\\bass \\fea\\t is too high, th\\t acoustic mod\\t\\b of c\\bass \\fea\\t cou\\bd b\\t too c\\bos\\t to th\\t mod\\t\\b of c\\bass ne\\bt\\tal.  W\\t p\\fopos\\t in this s\\tction to \\tva\\buat\\t th\\t audio cu\\ts w\\tight to d\\tt\\tct a situation which p\\fovok\\ts f\\ta\\f. P\\t\\fc\\tptiv\\t t\\tsts ca\\f\\fi\\td out in a fo\\fm\\t\\f study (C\\bav\\t\\b \\tt a\\b, 2004) hav\\t a\\b\\f\\tady shown that \\tmotions a\\f\\t p\\t\\fc\\tiv\\td as mo\\f\\t int\\tns\\t with th\\t h\\t\\bp of vid\\to suppo\\ft. W\\t thus aim\\td at s\\tpa\\fating s\\tgm\\tnts annotat\\td with th\\t cat\\tgo\\fica\\b \\bab\\t\\b \\fea\\t in th\\t basis of audio OR vid\\to cu\\ts. Cons\\tqu\\tnt\\by, a supp\\b\\tm\\tnta\\fy “b\\bind” annotation bas\\td on th\\t audio suppo\\ft on\\by (i.\\t. by \\bist\\tning to th\\t s\\tgm\\tnts with no acc\\tss to th\\t cont\\txtua\\b info\\fmation conv\\ty\\td by vid\\to and by th\\t g\\boba\\b cont\\tnt of th\\t s\\tqu\\tnc\\t) has b\\t\\tn ca\\f\\fi\\td out by a thi\\fd \\bab\\t\\b\\b\\t\\f on a subco\\fpus. F\\fom, now on, this thi\\fd \\bab\\t\\b\\b\\t\\f wi\\b\\b b\\t nam\\td Lab3. Th\\t subco\\fpus is compos\\td of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t o\\f ne\\bt\\tal by at \\b\\tast on\\t of th\\t two p\\f\\tvious \\bab\\t\\b\\b\\t\\fs (s\\t\\t tab\\b\\t 1). Lab3 has to c\\bassify th\\t s\\tgm\\tnts into th\\t cat\\tgo\\fi\\ts, \\fea\\t o\\f ne\\bt\\tal. G\\boba\\b\\by, Lab3 annotat\\ts mo\\f\\t s\\tgm\\tnts as n\\tut\\fa\\b than th\\t two initia\\b \\bab\\t\\b\\b\\t\\fs. 54 % of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t by Lab1, and 43% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t by Lab2 a\\f\\t \\bab\\t\\b\\b\\td ne\\bt\\tal by Lab3. Th\\t annotation st\\fat\\tgy of Lab3 is c\\bos\\t\\f to th\\t annotation st\\fat\\tgy of Lab2 than Lab1. Figu\\f\\t 3 and 4 show th\\t confusion histog\\fam b\\ttw\\t\\tn Lab3 and Lab1 (fig3), and b\\ttw\\t\\tn Lab3 and Lab2 (fig4). Th\\t annotation of \\fea\\t c\\bass by th\\t two initia\\b \\bab\\t\\b\\b\\t\\fs is h\\t\\f\\t co\\f\\f\\t\\bat\\td with th\\t int\\tnsity dim\\tnsion, in o\\fd\\t\\f to \\bocat\\t th\\t confusions with Lab3. As \\txp\\tct\\td th\\t confusion is high\\t\\f b\\ttw\\t\\tn \\fea\\t1 (int\\tnsity \\b\\tv\\t\\b = 1) and ne\\bt\\tal than b\\ttw\\t\\tn \\fea\\t3 (int\\tnsity \\b\\tv\\t\\b = 3) and ne\\bt\\tal. S\\tgm\\tnts with a \\bow \\b\\tv\\t\\b int\\tnsity a\\f\\t \\f\\tmov\\td f\\fom th\\t c\\bass \\fea\\t wh\\tn th\\t “b\\bind” annotation is consid\\t\\f\\td. Th\\t diff\\t\\f\\tnc\\t b\\ttw\\t\\tn th\\t two annotation st\\fat\\tgi\\ts \\tm\\t\\fg\\ts fo\\f th\\t annotations of subc\\bass \\fea\\t2 (int\\tnsity \\b\\tv\\t\\b = 2): 64% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t2 by Lab1 a\\f\\t \\bab\\t\\b\\b\\td ne\\bt\\tal by Lab3. 43 % of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t2 by Lab2 a\\f\\t \\bab\\t\\b\\b\\td ne\\bt\\tal by Lab3. Th\\t subc\\bass \\fea\\t2 contains f\\tw\\t\\f s\\tgm\\tnts with audio cu\\ts wh\\tn consid\\t\\fing Lab1’s annotation than wh\\tn consid\\t\\fing Lab2’s annotation. W\\t can thus assum\\t that th\\t annotation st\\fat\\tgy of Lab2 is mo\\f\\t\\f aud\\to-o\\b\\tented than th\\t st\\fat\\tgy of Lab1 which is mo\\f\\t context-o\\b\\tented.  Figu\\f\\t 3: Confusion histog\\fam b\\ttw\\t\\tn Lab1 and Lab3\\t  "]},{"title":"1101","paragraphs":["Figu\\f\\t 4: Confusion histog\\fam b\\ttw\\t\\tn Lab2 and Lab3"]},{"title":"4. Perce\\btual\\tversus\\tacoust\\fc\\tclasses\\t","paragraphs":["In th\\t p\\f\\tvious s\\tction, w\\t compa\\f\\td th\\t two initia\\b \\bab\\t\\b\\b\\t\\fs’ annotation st\\fat\\tgi\\ts. It \\tm\\t\\fg\\ts in pa\\fticu\\ba\\f that th\\t bo\\fd\\t\\f\\bin\\t b\\ttw\\t\\tn global \\fea\\t and ne\\bt\\tal d\\tp\\tnds on th\\t \\bab\\t\\b\\b\\t\\fs st\\fat\\tgy. In this s\\tction, w\\t \\tva\\buat\\t th\\t pot\\tntia\\b inf\\bu\\tnc\\t of th\\t va\\fious human annotation st\\fat\\tgi\\ts on th\\t d\\tt\\tction syst\\tm. With this pu\\fpos\\t, w\\t high\\bight th\\t co\\f\\f\\tspond\\tnc\\t b\\ttw\\t\\tn th\\t p\\t\\fc\\tptiv\\t spac\\t d\\t\\bimit\\td by \\tach \\bab\\t\\b\\b\\t\\f and th\\t acoustic spac\\t consid\\t\\f\\td by th\\t d\\tt\\tction syst\\tm."]},{"title":"4.1. Acoust\\fc\\tcluster\\fng","paragraphs":["W\\t consid\\t\\f h\\t\\f\\t a\\b\\b th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td global \\fea\\t o\\f ne\\bt\\tal and \\tva\\buat\\t th\\t acoustic p\\foximity b\\ttw\\t\\tn th\\t s\\tgm\\tnts without consid\\t\\fation of th\\t \\bab\\t\\bs. This \\tva\\buation is ca\\f\\fi\\td out by using th\\t unsup\\t\\fvis\\td k-m\\tans c\\bust\\t\\fing (Duda & Ha\\ft, 1973). Th\\t c\\bust\\t\\fing is bas\\td on th\\t minimisation of th\\t sum, ov\\t\\f a\\b\\b c\\bust\\t\\fs, of th\\t within-c\\bust\\t\\f sums of point-to-c\\bust\\t\\f-c\\tnt\\foïd distanc\\ts. Th\\t distanc\\t us\\td to m\\tasu\\f\\t th\\t acoustic p\\foximiti\\ts b\\ttw\\t\\tn th\\t c\\bust\\t\\f and th\\t s\\tgm\\tnt is th\\t squa\\f\\td Euc\\bid\\tan distanc\\t. Th\\t \\f\\tsu\\bts of this c\\bust\\t\\fing a\\f\\t th\\t two c\\bust\\t\\fs \\tm\\t\\fging f\\fom th\\t acoustic cont\\tnt \\f\\tga\\fd\\b\\tss of th\\t \\bab\\t\\bs. W\\t ca\\b\\b th\\ts\\t c\\bust\\t\\fs th\\t aco\\bstic cl\\bste\\ts."]},{"title":"4.2. Acoust\\fc\\tfeatures\\tvector","paragraphs":["Th\\t \\tmotiona\\b cont\\tnt is h\\t\\f\\t cha\\fact\\t\\fiz\\td by p\\fosodic and voic\\t qua\\bity f\\tatu\\f\\ts (C\\bav\\t\\b \\tt a\\b, 2006). Th\\t p\\fosodic f\\tatu\\f\\ts a\\f\\t \\f\\t\\bat\\td to pitch (F0) and int\\tnsity contou\\fs which a\\f\\t \\txt\\fact\\td with P\\faat (Bo\\t\\fsma & W\\t\\tnink, 2005). Pitch is comput\\td using a \\fobust a\\bgo\\fithm fo\\f p\\t\\fiodicity d\\tt\\tction bas\\td on signa\\b autoco\\f\\f\\t\\bation with 40 ms f\\fam\\t ana\\bysis. Th\\t \\bast p\\fosodic f\\tatu\\f\\t tak\\tn into account is th\\t du\\fation of th\\t voic\\td t\\faj\\tcto\\fy. Th\\t va\\fiations in t\\t\\fms of voca\\b \\tffo\\ft a\\f\\t \\f\\tp\\f\\ts\\tnt\\td by th\\t jitt\\t\\f (pitch modu\\bation), th\\t shimm\\t\\f (amp\\bitud\\t modu\\bation), th\\t unvoic\\td \\fat\\t (co\\f\\f\\tsponding to th\\t p\\fopo\\ftion of unvoic\\td f\\fam\\ts in a giv\\tn s\\tgm\\tnt) and th\\t ha\\fmonic to nois\\t \\fatio. Voic\\t qua\\bity is a\\bso cha\\fact\\t\\fiz\\td by sp\\tct\\fa\\b f\\tatu\\f\\ts such as th\\t fi\\fst two fo\\fmants and th\\ti\\f bandwidths comput\\td by a LPC (Lin\\ta\\f P\\f\\tdiction Coding) ana\\bysis. P\\t\\fc\\tption-bas\\td sp\\tct\\fa\\b and c\\tpst\\fa\\b f\\tatu\\f\\ts such as Standa\\fd M\\t\\b F\\f\\tqu\\tncy C\\tpst\\fa\\b Co\\tffici\\tnts, c\\bassica\\b\\by us\\td in automatic sp\\t\\tch \\f\\tcognition and us\\td mo\\f\\t \\f\\tc\\tnt\\by fo\\f \\tmotion d\\tt\\tction (Shaf\\fan \\tt a\\b, 2003), Ba\\fk band \\tn\\t\\fgy and sp\\tct\\fa\\b c\\tnt\\foïd (Eh\\f\\ttt\\t \\tt a\\b, 2003) a\\f\\t a\\bso consid\\t\\f\\td. F\\tatu\\f\\ts a\\f\\t comput\\td \\tv\\t\\fy 10 ms fo\\f \\tach s\\tgm\\tnt. In o\\fd\\t\\f to mod\\t\\b th\\t t\\tmpo\\fa\\b \\tvo\\bution of \\tach f\\tatu\\f\\t, d\\t\\fivativ\\ts and statistics (min, max, \\fang\\t, m\\tan, standa\\fd d\\tviation, ku\\ftosis, and sk\\twn\\tss) a\\f\\t comput\\td at a g\\boba\\b \\b\\tv\\t\\b, i.\\t. th\\t s\\tgm\\tnt \\b\\tv\\t\\b. Each s\\tgm\\tnt is th\\tn \\f\\tp\\f\\ts\\tnt\\td by a tota\\b of 174 f\\tatu\\f\\ts. A\\b\\b th\\t f\\tatu\\f\\ts a\\f\\t no\\fma\\bis\\td by th\\ti\\f g\\boba\\b maximum so that th\\ty a\\f\\t put on a sing\\b\\t sca\\b\\t b\\ttw\\t\\tn -1 and 1. Th\\t f\\tatu\\f\\t spac\\t is \\f\\tduc\\td by combining th\\t diff\\t\\f\\tnt f\\tatu\\f\\ts to fo\\fm 40-dim\\tnsion v\\tcto\\f (P\\fincipa\\b Compon\\tnt ana\\bysis). S\\t\\b\\tction f\\tatu\\f\\t a\\bgo\\fithms a\\f\\t not us\\td h\\t\\f\\t, as th\\t goa\\b is to ca\\f\\fy out a b\\bind c\\bust\\t\\fing without any consid\\t\\fation on th\\t p\\t\\fc\\tptua\\b \\bab\\t\\bs of th\\t s\\tgm\\tnts."]},{"title":"4.3. Protocol\\tand\\tresults","paragraphs":["Th\\t cat\\tgo\\fica\\b \\bab\\t\\bs a\\f\\t co\\f\\f\\t\\bat\\td with th\\t int\\tnsity dim\\tnsion so that th\\t p\\t\\fc\\tptua\\b c\\bass\\ts consid\\t\\f\\td a\\f\\t ne\\bt\\tal, \\fea\\t1, \\fea\\t2, \\fea\\t3. Th\\t dist\\fibution of th\\t s\\tgm\\tnts of \\tach \\bab\\t\\b (o\\f pe\\tcept\\bal class) among th\\t aco\\bstic cl\\bste\\ts p\\fovid\\ts an \\tva\\buation of th\\t acoustic p\\foximiti\\ts insid\\t a pe\\tcept\\bal class. This dist\\fibution is \\tva\\buat\\td fo\\f \\tach annotation st\\fat\\tgy, that is, fo\\f \\tach \\bab\\t\\b\\b\\t\\f’s annotation choic\\t. This ana\\bysis is p\\t\\ffo\\fm\\td on a subco\\fpus containing on\\by good q\\bality s\\tgm\\tnts \\bab\\t\\b\\b\\td global \\fea\\t and ne\\bt\\tal. Th\\t qua\\bity of th\\t sp\\t\\tch in th\\t s\\tgm\\tnts has b\\t\\tn \\tva\\buat\\td by th\\t \\bab\\t\\b\\b\\t\\fs. Ov\\t\\f\\baps hav\\t b\\t\\tn avoid\\td. Tab\\b\\t 2 sto\\f\\ts th\\t numb\\t\\f of s\\tgm\\tnts us\\td fo\\f th\\t c\\bust\\t\\fing fo\\f \\tach \\bab\\t\\b\\b\\t\\f Lab1 and Lab2. 1073 s\\tgm\\tnts a\\f\\t us\\td fo\\f Lab1 and 1215 s\\tgm\\tnts fo\\f Lab2. ","F\\ta\\f 3 F\\ta\\f 2 F\\ta\\f 1 N\\tut\\fa\\b","Lab1 184 261 191 437","Lab2 70 226 190 729","Tab\\b\\t 2: R\\tpa\\ftition of th\\t s\\tgm\\tnts acco\\fding to th\\t","\\bab\\t\\b\\b\\t\\f","","Th\\t \\f\\tpa\\ftitions of th\\t s\\tgm\\tnts of \\tach p\\t\\fc\\tptua\\b c\\bass","among th\\t two acoustic c\\bust\\t\\fs, cl\\bste\\t1 and cl\\bste\\t2,","a\\f\\t consid\\t\\f\\td in th\\t two tab\\b\\ts 3 (fo\\f Lab1) and 4 (fo\\f","Lab2). This \\f\\tpa\\ftition is p\\fovid\\td consid\\t\\fing diff\\t\\f\\tnt","divisions of th\\t p\\t\\fc\\tptua\\b spac\\t, gath\\t\\fing fo\\f \\txamp\\b\\t","th\\t c\\bass\\ts \\fea\\t2 and \\fea\\t1 o\\f th\\t c\\bass\\ts \\fea\\t3 and \\fea\\t2."," F\\ta\\f 3 F\\ta\\f 2 F\\ta\\f 1 N\\tut\\fa\\b","52% 54% 41% C\\bust\\t\\f1 50% 46%","48% 46% 59% C\\bust\\t\\f2 50% 54%","Tab\\b\\t 3: Lab1. "]},{"title":"1102 ","paragraphs":["F\\ta\\f 3 F\\ta\\f 2 F\\ta\\f 1 N\\tut\\fa\\b 81% 49% 40%","C\\bust\\t\\f1 51% 36%","19% 51% 60% C\\bust\\t\\f2 49% 64% Tab\\b\\t 4: Lab2.  Th\\t acoustic bo\\fd\\t\\f\\bin\\t b\\ttw\\t\\tn th\\t two c\\bust\\t\\fs is \\bocat\\td b\\ttw\\t\\tn \\fea\\t2 and \\fea\\t3 fo\\f Lab2 and b\\ttw\\t\\tn \\fea\\t1 and \\fea\\t2 fo\\f Lab1. Th\\t t\\f\\tnd of Lab1 is to \\tva\\buat\\t th\\t s\\tgm\\tnts as mo\\f\\t int\\tns\\t than Lab2. It m\\tans fo\\f \\txamp\\b\\t that a sam\\t s\\tgm\\tnt is \\tva\\buat\\td \\fea\\t3 by Lab1 and \\fea\\t2 by Lab2. This div\\t\\fg\\tnc\\t of st\\fat\\tgy is c\\bos\\t\\by akin to th\\t \\tm\\t\\fging position of th\\t acoustic bo\\fd\\t\\f\\bin\\t co\\f\\f\\tsponding to \\tach \\bab\\t\\b\\b\\t\\f. In addition, th\\t position of this bo\\fd\\t\\f\\bin\\t is mo\\f\\t p\\f\\tcis\\t wh\\tn consid\\t\\fing Lab2’s annotation. 81% of \\fea\\t3 s\\tgm\\tnts a\\f\\t g\\foup\\td in th\\t sam\\t acoustic c\\bust\\t\\f. In a\\b\\b cas\\ts th\\t g\\boba\\b c\\bass \\fea\\t \\tm\\t\\fg\\ts a\\bways \\b\\tss c\\b\\ta\\f\\by in on\\t c\\bust\\t\\f than th\\t c\\bass \\fea\\t3. Th\\t fi\\fst c\\bust\\t\\f contains 51% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t by Lab2 and 64%of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td ne\\bt\\tal by Lab2. In o\\fd\\t\\f to imp\\fov\\t th\\t co\\f\\f\\tspond\\tnc\\t b\\ttw\\t\\tn th\\t acoustic spac\\t and th\\t p\\t\\fc\\tptiv\\t spac\\t, th\\t “b\\bind” annotation is consid\\t\\f\\td. W\\t s\\t\\b\\tct th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t o\\f ne\\bt\\tal by Lab1, Lab2, and Lab3 at onc\\t. Tab\\b\\t 5 shows th\\t \\f\\tpa\\ftition of th\\ts\\t \\bab\\t\\bs among th\\t two c\\bust\\t\\fs. Th\\t co\\f\\f\\tspond\\tnc\\t b\\ttw\\t\\tn th\\t acoustic c\\bust\\t\\fs and th\\t p\\t\\fc\\tptua\\b c\\bass\\ts is h\\t\\f\\t b\\ttt\\t\\f. Now, c\\bust\\t\\f 1 contains 55% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td \\fea\\t and 66% of th\\t s\\tgm\\tnts \\bab\\t\\b\\b\\td ne\\bt\\tal.  F\\ta\\f N\\tut\\fa\\b C\\bust\\t\\f1 55% 34% C\\bust\\t\\f2 45% 66% Tab\\b\\t 5: Lab3 "]},{"title":"5. Conclus\\fon\\t ","paragraphs":["In this pap\\t\\f w\\t compa\\f\\t p\\t\\fc\\tptua\\b \\tmotiona\\b c\\bass\\ts to an unsup\\t\\fvis\\td acoustic c\\bassification obtain\\td by th\\t k-m\\tans d\\tsc\\fiptiv\\t m\\tthod. P\\t\\fc\\tptua\\b c\\bass\\ts a\\f\\t d\\t\\bimit\\td thanks to th\\t human annotation conduct\\td by two initia\\b \\bab\\t\\b\\b\\t\\fs. Th\\t st\\fat\\tgi\\ts of \\tach \\bab\\t\\b\\b\\t\\f a\\f\\t fi\\fst compa\\f\\td. It app\\ta\\fs that th\\t bo\\fd\\t\\f\\bin\\t b\\ttw\\t\\tn emotion and ne\\bt\\tal d\\tp\\tnds on th\\t annotation st\\fat\\tgy. A thi\\fd “b\\bind” annotation high\\bights th\\t \\fo\\b\\t of audio info\\fmation in th\\t p\\t\\fc\\tption of f\\ta\\f-typ\\t \\tmotions and h\\t\\bps at ana\\bysing th\\t annotation st\\fat\\tgi\\ts of th\\t two initia\\b \\bab\\t\\b\\b\\t\\fs. Two human st\\fat\\tgi\\ts a\\f\\t obs\\t\\fv\\td: a fi\\fst on\\t, cont\\txt-o\\fi\\tnt\\td which mix\\ts audio and cont\\txtua\\b (vid\\to an t\\tmpo\\fa\\b) info\\fmation in \\tmotion cat\\tgo\\fization; and a s\\tcond on\\t, bas\\td main\\by on audio info\\fmation. Th\\t k-m\\tans c\\bust\\t\\fing confi\\fms th\\t \\fo\\b\\t of audio cu\\ts in human annotation st\\fat\\tgi\\ts. It pa\\fticu\\ba\\f\\by h\\t\\bps at \\tva\\buating thos\\t st\\fat\\tgi\\ts f\\fom th\\t point of vi\\tw of a d\\tt\\tction syst\\tm. Fi\\fst of a\\b\\b, th\\t d\\tt\\tction syst\\tm is bas\\td on th\\t d\\tt\\tction of th\\t g\\boba\\b c\\bass \\fea\\t which contains a high va\\fiabi\\bity of manif\\tstations in t\\t\\fms of int\\tnsity \\b\\tv\\t\\bs. Th\\t g\\boba\\b c\\bass contains both f\\ta\\f-typ\\t \\tmotions with a \\bow \\b\\tv\\t\\b int\\tnsity such as anxi\\tty and f\\ta\\f-typ\\t \\tmotions with a high \\b\\tv\\t\\b of int\\tnsity such as t\\t\\f\\fo\\f. It \\tm\\t\\fg\\ts h\\t\\f\\t that f\\ta\\f with a high \\b\\tv\\t\\b of int\\tnsity (\\fea\\t3) is st\\fong\\by \\txp\\f\\tss\\td at acoustic \\b\\tv\\t\\b, succ\\tssfu\\b\\by cha\\fact\\t\\fiz\\td by th\\t acoustic f\\tatu\\f\\ts. Cons\\tqu\\tnt\\by mo\\f\\t wo\\fk n\\t\\tds to b\\t don\\t to mod\\t\\b bo\\fd\\t\\f\\bin\\t \\tmotions, i.\\t. f\\ta\\f1 typ\\t o\\f som\\t of th\\t f\\ta\\f2 typ\\t.  S\\tcond\\by, audio-o\\fi\\tnt\\td d\\tsc\\fiptions a\\f\\t mo\\f\\t \\f\\t\\b\\tvant to an \\tmotion d\\tt\\tction syst\\tm bas\\td on audio cu\\ts. How\\tv\\t\\f, \\tmotions a\\f\\t conv\\ty\\td by oth\\t\\f chann\\t\\bs as w\\t\\b\\b. Th\\t combination of acoustic info\\fmation with oth\\t\\f \\binguistic \\b\\tv\\t\\bs (\\b\\txica\\b, dia\\bogic, and cont\\txtua\\b) wi\\b\\b \\f\\tqui\\f\\t cont\\txt-o\\fi\\tnt\\td d\\tsc\\fiption of \\tmotiona\\b sp\\t\\tch. Fina\\b\\by, in th\\t p\\t\\fsp\\tctiv\\t of a mu\\btimoda\\b su\\fv\\ti\\b\\banc\\t syst\\tm, th\\t vid\\to cu\\ts wi\\b\\b hav\\t to b\\t consid\\t\\f\\td and int\\tg\\fat\\td in th\\t annotation. "]},{"title":"6. B\\fbl\\fogra\\bh\\fcal\\treferences\\t ","paragraphs":["Bo\\t\\fsma P., W\\t\\tnink, D. (2005). P\\faat: doing phon\\ttics by comput\\t\\f [Comput\\t\\f p\\fog\\fam], f\\fom http://www.p\\faat.o\\fg/  C\\bav\\t\\b C., Vasi\\b\\tscu I, D\\tvi\\b\\b\\t\\fs L., Eh\\f\\ttt\\t T. (2004) Fiction databas\\t fo\\f \\tmotion d\\tt\\tction in abno\\fma\\b situation, ICSLP, J\\tju.","","C\\bav\\t\\b C., Vasi\\b\\tscu I., Richa\\fd G. and D\\tvi\\b\\b\\t\\fs L. (2006) Du co\\fpus émotionn\\t\\b au systèm\\t d\\t d\\tt\\tction : \\b\\t point d\\t vu\\t app\\bicatif d\\t \\ba su\\fv\\ti\\b\\banc\\t dans \\b\\ts \\bi\\tux pub\\bics. Acc\\tpt\\td fo\\f pub\\bication in th\\t F\\f\\tnch R\\tvu\\t in A\\ftificia\\b Int\\t\\b\\big\\tnc\\t (RIA)","","C\\bav\\t\\b C., Vasi\\b\\tscu I., Richa\\fd G. and D\\tvi\\b\\b\\t\\fs L. (2006) Voic\\td and Unvoic\\td cont\\tnt of f\\ta\\f-typ\\t \\tmotions in th\\t SAFE Co\\fpus. Sp\\t\\tch P\\fosody, D\\f\\tsd\\tn – to b\\t pub\\bish\\td.","","Cowi\\t, R., Doug\\bas-Cowi\\t, E., Tsapatsou\\bis, N., Votsis, G., Ko\\b\\bias, S., F\\t\\b\\b\\tnz, W., & Tay\\bo\\f, J. (2001). Emotion \\f\\tcognition in human-comput\\t\\f int\\t\\faction. IEEE Signa\\b P\\foc\\tssing Magazin\\t: 18 (1), 32–80.  C\\faggs, R. (2004). Annotating \\tmotion in dia\\bogu\\t – issu\\ts and app\\foach\\ts. 7th Annua\\b CLUK R\\ts\\ta\\fch Co\\b\\boquium.  C\\fonbach, L. J. (1951). Co\\tffici\\tnt a\\bpha and th\\t int\\t\\fna\\b","st\\fuctu\\f\\t of t\\tsts. Psychom\\tt\\fika. 16, 297-334.","","Doug\\bas-Cowi\\t, E., Campb\\t\\b\\b, N, Cowi\\t R., Roach R., (2003), Emotiona\\b sp\\t\\tch : Towa\\fds a n\\tw g\\tn\\t\\fation of databas\\ts, Sp\\t\\tch Communication, vo\\b 40, pag\\ts 33-60.  Duda, R. O., & Ha\\ft, P. E. (1973). Patt\\t\\fn c\\bassification","and sc\\tn\\t ana\\bysis. N\\tw Yo\\fk: Wi\\b\\ty & Sons.  Eh\\f\\ttt\\t, T., Chat\\tau, N., d’A\\b\\b\\tssand\\fo, C., Maffio\\bo V., (2003). P\\f\\tdicting th\\t P\\t\\fc\\tptiv\\t Judgm\\tnt of Voic\\ts in a T\\t\\b\\tcom Cont\\txt: S\\t\\b\\tction of Acoustic Pa\\fam\\tt\\t\\fs."]},{"title":"1103","paragraphs":["Eu\\fosp\\t\\tch, G\\tn\\tva.  Kipp, M., (2001). Anvi\\b a g\\tn\\t\\fic annotation too\\b fo\\f","mu\\b-timoda\\b dia\\bogu\\t. Eu\\fosp\\t\\tch.","","L\\t\\t, C.M., Na\\fayanan, S., Pi\\t\\faccini, R. (2001), R\\tcognition of N\\tgativ\\t Emotions f\\fom th\\t Sp\\t\\tch Signa\\b. In P\\foc\\t\\tding of th\\t IEEE Automatic Sp\\t\\tch R\\tcognition and Und\\t\\fstanding.  Osgood, C., May, W.H., Mi\\fon M. S. (1975), C\\foss-cu\\btu\\fa\\b univ\\t\\fsa\\bs of aff\\tctiv\\t m\\taning, Univ\\t\\fsity of I\\b\\binois P\\f\\tss, U\\fbana.  S\\tiga\\b (1988) “Non-pa\\fam\\tt\\fic statistics”. S\\tcond","Edition. Mc-G\\faw-Hi\\b\\b.  Shaf\\fan, I., Ri\\b\\ty, M., Moh\\fi, M., (2003). Voic\\t Signatu\\f\\ts. Automatic Sp\\t\\tch R\\tcognition and Und\\t\\fstanding Wo\\fk-shop."]},{"title":"1104","paragraphs":[]}]}