{"sections":[{"title":"UAM Text Tools a exible NLP architecture Tomasz Obr\\bebski, MichaStolarski","paragraphs":["Adam Mickiewicz University","Umultowska 87, 61-614 Pozna ·n, Poland {obrebski,mstolar}@amu.edu.pl","Abstract The paper presents a new language processing toolkit developed at Adam Mickiewicz University. Its functionality includes currently tokenization, sentence splitting, dictionary-based - morphological analysis, heuristic morphological analysis of unknown words, spelling correction, pattern search, and generation of concordances. It is organized as a collection of command-line programs, each performing one operation. The components may be connected in various ways to provide various text processing services. Also new user-dened components may be easily incorporated into the system. The toolkit is destined for processing raw (not annotated) text corpora. The system was originally intended for Polish, but its adaptation to other languages is possible."]},{"title":"1. Introduction","paragraphs":["Automatic extraction of linguistic information from text corpora has for many years been an important research direction within natural language processing area. A number of computer systems and data resources have been developed. Two examples of such systems, which may be used to process Polish corpora, are Poliqarp (Przepirkowski and et al., 2004) and Intex (Silberztein, 1993). Poliqarp is a search engine accompanying IPI PAN Corpus1","a large (300 million segments) corpus of Polish (Przepirkowski, 2004). Intex is a corpus processor, which in contrast to Poliqarp is able to work with arbitrary text data supplied by the user. Systems of this type are addressed to end-users: linguists or computational linguists. The functionality and the range of possible applications of such systems is xed by their architects and difcult or impossible to extend. In the present time, applications of natural language processing technologies become more and more common, and text corpora become more and more accessible. Therefore, the need emerged for tools offering similar functionality, but addressed towards language engineering community: easily congurable, independent of language, annotation format and contents, extensible with user-dened components, and easily connectible to other software. The project called UAM Text Tools (UTT) started at the end of 2004 as an attempt to respond to this need. UTT is meant to supply a set of basic instruments for building various applications related to text corpus processing. It is a package of language processing tools. Its functionality includes currently tokenization, sentence splitting, dictionary-based morphological analysis, heuristic morphological analysis of unknown words, spelling correction, pattern search, and generation of concordances. One more component, which is almost ready to be integrated with the package, is a dependency parser. The package is freely available for research and educational use."]},{"title":"2. The architecture","paragraphs":["The system is organized following the software engineering principles adopted in UNIX environments: as a collection 1 http://www.korpus.pl of independent components, each providing one language processing service (tokenization, lemmatization, search,...). The components are independent command-line programs working as lters, communicating through pipes. The unifying element of the whole is the uniform i/o le format, in which annotated corpus data is stored and exchanged be-tween component programs. The UTT le format was designed to meet two principal requirements:","simplicity of processing with standard widely available text processing utilities and programming languages (grep, sed, AWK, Perl, lex), human readability. Therefore, simple line-based text format with space-separated elds was chosen. Each line of a UTT-formatted le describes a continuous segment of the input text le, usually a token. There are four mandatory elds: start position in the input le, the length of the segment, the segment type (word-form, number, space), and the orthographic form of the segment. Any number of annotation elds may then follow. These elds contain annotation introduced by processing programs. Below is the sample of the UTT le with morphological annotation. 0000 04 W sza lem:i ·s·c,V/AiVpMdTaNsP3Gf 0004 01 B _ 0005 10 W dzieweczka lem:dzieweczka,N/CnGfNs 0015 01 B _ 0016 02 W do lem:do,P 0018 01 B _ 0019 08 W laseczka lem:laseczka,N/GfNsCn 0019 08 W laseczka lem:laseczek,N/GiNsCg 0027 01 P . 0028 01 B \\n Figure 1: Example of UTT le Annotation elds are composed of the eld name, in this case lem: , and the value, which is an arbitrary string of non-white characters. The eld name in the above example is the name of the component which introduced the eld (this is the default2","). 2 This is one of the reasons for using very short, usually 3-letter","long, names for component programs."]},{"title":"2259","paragraphs":["Format features:","1. ambiguous annotation (ambiguous interpretation of a segment, ambiguous segmentation) may be represented 2. parallel annotation also may be represented","3. the result of independent processing of a le by two programs may be merged (using eg. the standard UNIX sort -m command), 4. reference to the original text is directly accessible A component program reads a sequence of segments from input and writes another sequence of segments to output. The parameters determine which segments the program is supposed to process, which eld(s) contain the input data to the program, what should be output (successfully processed segments, unsuccessfully processed segments, all). The results of processing are added as a new annotation eld. There are two ways of representing ambiguity: either the segment appears multiple times with different annotations (laseczka in the example in Fig. 1), or ambiguous annotation is presented as one complex value (Fig. 2). ... 0018 01 B _ 0019 08 W laseczka lem:laseczka,N/GfNsCn;laseczek,\\ N/GiNsCg 0027 01 P . ... Figure 2: Ambiguous annotation in one eld The rst format is more universal. It directly corresponds to a DAG encoding of ambiguous text data, allowing for example for representing ambiguous segmentation in natural way. This format is convenient e.g. as the input to syntactic parser. The second format is more compact and especially in case of unambiguous segmentation allows for performing efcient search."]},{"title":"3. Component programs","paragraphs":["In this section we will shortly describe the UTT components currently available. 3.1. tok a tokenizer tok is a simple program which reads a text le and identies tokens on the basis of their orthographic form. The type of the token is printed as the type eld. In standard conguration ve types of tokens are distinguished: word (type W ) continuous sequence of letters, number (type N ) continuous sequence of digits, space (type B ) continuous sequence of white-space characters, punctuation mark (type P ) single printable characters not belonging to any of the other classes, unprintable character (type H ) single unprintable character (see Fig. 3) 3.2. lem a lemmatizer lem performs morphological analysis of a simple orthographic word, returning all its possible morphological annotations, disregarding the context. The result of the analysis is added as the value of the new annotation eld, which 0000 04 W sza 0004 01 B _ 0005 10 W dzieweczka 0015 01 B _ 0016 02 W do 0018 01 B _ 0019 08 W laseczka 0027 01 P . 0028 01 B \\n Figure 3: Example of tok output default name is lem. In case of ambiguity either the segment is muliplicated or ambiguous description in the format lemma1, tag1, : : :, tagn ; : : :; lemmam, tag1, : : :, tagn is used as the value (cf. Fig. 2). lem may work with a text format dictionary or with a dictionary compiled into FSA representation. 3.3. gue a guesser Morphological analysis performed by lem component is based on a dictionary. Therefore it is limited to the words included in that dictionary. The gue component is designed to assist lem when it fails to recognize a word form. It proposes the most likely description(s) of a word. The output is based on the information on the frequency a specic morphological description is assigned to word-forms with a given sufx and/or prex. 3.4. cor a corrector The spelling corrector applies Kemal Oazer’s dynamic programming algorithm (Oazer, 1996) to the FSA representation of word-forms extracted from the lem’s dictionary. Given an incorrect word-form it returns all word-forms present in the dictionary which edit distance is smaller than the threshold given as a parameter. 3.5. ser a pattern search tool The ser program is the UTT component for locating text fragments matching a pattern. The pattern is a regular expression over terms corresponding to corpus segments. Several examples of terms follow: seg any segment, word any word, word(nie.+<N>) a word beginning with ’nie’ and tagged as noun, space(. * \\n. * ) a space segment containing a new-line character, lexeme(pomoc) a form of the lexeme ’pomoc’, tag(<N/Ca>) a noun in accusative. Term arguments may be arbitrary regular expressions. For example the following invocation of ser: ser -e ’tag(<ADJ>) space (word space)? lexeme(praca)’ locates occurrences of adjectives followed by any form of the lexeme ’praca’ (work), optionally separated by another word. Matches are indicated by inserting 0-length segments at the beginning and at the end of the match. ser performs the search by matching character-level regular expressions against fragments of UTT le using the flex program. The expansion of the pattern into the coresponding character-level regular expression is implemented with the use of m4 macroprocessor. Terms used in patterns are simply m4 macro invocations. The special form <...> represents a partial specication of a tag (constraints on tag form) and is expanded into a"]},{"title":"2260","paragraphs":["regular expression matching all tags meeting this specication (e.g. <N/Ca> is expanded into a regular expression matching all tags for nouns in accusative case). The processing speed of ser is over 350 000 corpus segments/sec3","(segments = visible tokens: words, punctuation marks, numbers) ... 180467 06 W b\\bed \\bacy lem:b\\bed \\bacy,ADJPRP/NpCnvGp,ADJPRP /. .. 180473 01 S _ 180474 11 W przedmiotem lem:przedmiot,N/GiNsCi 180485 01 S _ 180486 00 BOM ser:1 180486 10 W niniejszej lem:niniejszy,ADJ/DpNsCgdlGf 180496 01 S _ 180497 05 W pracy lem:praca,N/GfNsCd,N/GfNsCg,N/GfN sC l 180502 00 EOM ser:1 180502 01 P . 180503 02 S _\\n 180505 09 W Rwnowaga lem:rwnowaga,N/GfNsCn ... Figure 4: Example of ser output 3.6. grp - a grep-like tool The grp component is similar to ser with the difference that, instead of flex , text scanning is performed by grep on a slightly modied UTT le (as grep operates on single lines, all segments making up a sentence are merged in one line). Regular expressions passed to grep are generated by grp using the same set of m4 macrodenitions as ser. grp atteins the speed of over 1.5 mln corpus segments/sec for arbitrarily large corpora. The basic use of grp is the reduction of the corpus size being passed to subsequent processing phases (e.g. to ser which will extract the matches), by extracting sentences in which the searched expression appears or is likely to appear. For more details on ser and grp, see (Obr\\bebski, 2006). The complete list of components includes also: sen - the sentensizer, kot - reconstructs original text from given UTT le, con - a tool for displaying concordances in humanfriendly format."]},{"title":"4. Usage examples","paragraphs":["Below several examples are presented, showing how UTT may be used to perform different text processing tasks. 4.1. Annotation The following sequence of commands: tok | lem | cor -S lem | lem -I cor | gue -S lem causes the input text to be processed as follows: tokenize the text (tok), perform morphological analysis of words (lem), try to correct words for which lem produced no description (cor -S lem ), perform morphological analysis of words corrected by cor (lem -I cor ), guess descriptions for words which still have got no annotation from lem (gue -S lem ). For example, the result of processing the sentence Piszemy gitesowe progromy.4","will be: 3 on a PC with Celeron 1.8 GHz, 256 MB RAM, disk read time","22MB/sec. 4 piszemy ([we] write) is a correct word and present in the dic-","tionary, gitesowe (cool) is a slang adjective, absent in the dictio-","nary, progrumy (programs) contains a spelling error. 0000 07 W Piszemy lem:pisa ·c,V/AiVpMdTrNpP1 0007 01 B _ 0008 08 W gitesowe gue:gitesowy,ADJ/CanvDpGafinNp 0008 08 W gitesowe gue:gitesowy,ADJ/CanvDpGnNs 0016 01 B _ 0017 08 W progromy cor:pogromy lem:pogrom,N/GiNpCa 0017 08 W progromy cor:pogromy lem:pogrom,N/GiNpCn 0017 08 W progromy cor:pogromy lem:pogrom,N/GiNpCv 0017 08 W progrumy cor:programy lem:program,N/GiNpCa 0017 08 W progrumy cor:programy lem:program,N/GiNpCn 0017 08 W progrumy cor:programy lem:program,N/GiNpCv 0025 01 P . 0026 01 B \\n Figure 5: Example of lem/cor/gue annotation 4.2. Spelling correction The following command: tok | lem -p W -o /dev/null | cor --one prints out all spelling errors found in text, annotated with correction suggestions. For example, for the sentence Pizsemy dobre progromy. the following output is produced: 0000 07 W Pizsemy cor:Piszemy 0014 08 W progromy cor:pogromy;programy First, dictionary-based morphological analysis is performed and successfully processed segments are discarded. The segments the lem failed to analyze are sent to cor . 4.3. Concordancer In this example we are looking for an adjective followed by a form of the nominal lexeme ’praca’ (work). The matches are displayed by the con component with left and right con-text. The command: tok | lem --one \\ | ser -e \"tag(<ADJ>) space lexeme(praca)\" \\ | con -t -l 20 -r 15 produces: b\\bed \\bacy przedmiotem [niniejszej pracy]. Rwnowaga","W rozwa zanym w [niniejszej pracy] wyadowaniu Rezultaty wy zej [wspomnianych prac] s \\ba w peni jest przedmiotem [tej pracy]. Metody","do zrozumienia [niniejszej pracy]. Profil","Z tego powodu autor [niniejszej pracy] wraz ze","jednej z nast\\bepnych [swoich prac] Devoto (Devoto Z punktu widzenia [niniejszej pracy] podstawowe","zarwno autora [tej pracy], jak i w tej pracy, jak i w [innych pracach] zaobserwowano Figure 6: Example of con output 4.4. UTT and standard UNIX text tools Thanks to the fact that UTT les may be easily processed by standard UNIX text tools, more sophisticated computations may be performed without additional programming. For example, to obtain the frequency list of adjective lexemes preceding a form of the nominal lexeme praca (work), the following command may be used: tok | lem --one \\ | ser -m -e \"tag(<ADJ>) space lexeme(praca)\" \\ | egrep ’,ADJ/’ \\ | sed -r ’s/.̂ *[:;]([[:alpha:]]+),ADJ\\/. *$/ \\1/ ’ \\ | sort | uniq -c | sort -n -r ser with -m option outputs only matching fragments of the input le. The lines containing adjectives are selected by egrep , the base-forms are then extracted by sed , sorted, counted, and arranged in desired order."]},{"title":"2261 5. Languages and tagsets","paragraphs":["The UTT package has originally been developed for Polish. Polish dictionary data for lem, gue, and cor is derived from the large-coverage morphological dictionary Polex/PMDBF (Vetulani, 2000) and is made available for use with the package. The adaptation of UTT to other languages is possible, provided that dictionaries in appropriate format are supplied (scripts for compiling the dictionaries into the binary format used by component programs make part of the package). Recently UTT has been successfully ported to Portuguese. The dictionary data for lem, gue, and cor components was derived from the UNITEX-PB dictionary (Muniz et al., 1995), which is available under GNU Licence. The format of tags (Intex/Unitex-type) was retained. A single multi-language installation of UTT, equipped in dictionary data for several languages (Polish and Portuguese at the moment) is now being in test phase. The set of macrodenitions used by the search tools to expand high-level terms into character-level regular expressions may by freely extended or modied by the user. This feature allows to modify the syntax of search patterns and allows for processing les with different types of annotation. Adaptation of the search components (ser and grp) to another tag format is accomplished by redening the expansion of <...> -expressions into character-level regular expressions matching appropriate sets of tags. Technically this problem reduces to implementation of a short script performing the expansion. The only restriction on the tag form, which can be handled by UTT programs is that space, comma and semicolon characters are not allowed in tags. So far, UTT has worked with PMDB-type (eg. N/NsGpCa ) and Intex/Unitex-type (eg.N+Pr:ms ) tags."]},{"title":"6. Conclusion","paragraphs":["We have presented a language processing toolkit, called UTT. Thanks to the adopted architecture, which permits to connect the components in various ways, UTT may be used to perform numerous different text processing tasks. The system is also easily extensible: new components may be incorporated into the system provided that they respect the i/o le format structure. The price paid for exibility and openness of the UTT package is its difcult portability to other, non-UNIX-like, systems. Moreover, users not accustomed to command line work-style may nd UTT’s interface unfriendly. Development of a graphical user interface is planned in or-der to make the UTT functionality more accessible to less technically-oriented users. This, however, will take place after the system attains stability. The rst confrontation of the toolkit with serious LRrelated research was its use in the SyntLex project ((Vetulani et al., 2006)) for retrieving candidates for support verbs for a list of already identied predicative nouns on the basis of IPI PAN Corpus. The feature which proved to be crucial while using UTT to support lexicographical investigations was the possibility to automatically run the search processes from the level of shell scripts (eg. when a series of 50 000 search tasks for different pairs predicativenounsyntactic-pattern had to be performed)."]},{"title":"7. Acknowledgements","paragraphs":["The authors wish to thank the following people: Jorge Gilberto, Joao Gomes and Luis Pedro for their collaboration in porting UTT to Portuguese, Justyna Walkowska for the implementation of the con component, and Filip Grali·nski for his valuable remarks and suggestions."]},{"title":"8. References","paragraphs":["Marcelo Muniz, Maria das Grac\\bas Volpe Nunes, and Eric Laporte. 1995. Unitex-PB, a set of exible language resources for brazilian portuguese. In Proceedings of the III Workshop em Tecnologia da Informac\\bao e da Linguagem Humana - TIL, XXV Congresso da SBC, Sao Leopoldo.","Tomasz Obr\\bebski. 2006. Searching text corpora with grep. In (to appear) Intelligent Information Processing and Web Mining Proceedings of the International IIS: IIPWM 06 Conference, Ustronie, Advances in Soft Computing. Springer-Verlag.","Kemal Oazer. 1996. Error-tollerant nite state recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22(1):7389.","Adam Przepirkowski and Zygmunt Krynicki et al. 2004. Search tool for corpora with positional tagsets and ambiguities. In The Proceedings of LREC 2004, pages 1235 1238.","Adam Przepirkowski. 2004. The IPI PAN Corpus. IPI-PAN.","Max Silberztein. 1993. Dictionnaires Ølectroniques et analyse automatique de textes. Le systŁme INTEX. MASSON, Paris.","Grazyna Vetulani, Zygmunt Vetulani, and Tomasz Obr\\bebski. 2006. Syntactic lexicon of polish predicative nouns. In this volume.","Zygmunt Vetulani. 2000. Electronic language resources for polish: POLEX, CEGLEX and GRAMLEX. In Gavrilidou et al: Second International Conference on Language Resources and Evaluation, Athens, 30.05-2.06.2000, pages 367374, ELRA, Paris."]},{"title":"2262","paragraphs":[]}]}