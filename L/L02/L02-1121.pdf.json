{"sections":[{"title":"Recordingtechniquesforcapturingnaturalevery-dayspeech NickCampbell","paragraphs":["ATRHumanInformationScienceLaboratories Kyoto619-022,Japan","nick@atr.co.jp","Abstract Thispaperdescribestechniquesforthecollectionofnaturalspontaneousspeechfromdailyconversationalinteractionsforalarge corpusthatiscurrentlybeingproducedbytheJapanScienceandTechnologyAgency.Thiscorpuswillformthebasisforfurther developmentoftoolsandsoftwarefortheimprovementofconcatenativespeechsynthesisandforthedevelopmentofspoken-language interfacesforinformation-providingdevicesthatwillbesensitivenotonlytothecontentofanutterance,butalsotothemannerin whichitisspoken,soastobeabletodetectspeakeremotionsandattitudes.  "]},{"title":"1.Introduction","paragraphs":["Thereisnowagrowinginterestinmodellingthe characteristicsofemotionalspeechandattitudinallymarkedspeech,butasthemajorityofpresentationsand thefewnotableexceptionsattherecentISCASpeech& Emotiontutorialandresearchworkshop[1]showed,there arestillveryfewcorporaofrealspontaneousspeechbeing usedincurrentresearch[2].","","Thisismaybeduetoan‘observer’sparadox’effect, inthatassoonasapersonrealizesthatthereisa microphoneinfrontofthem,theyswitchintoapublicor self-consciousmodeofbehaviourinwhichtheexpression ofemotionisconsideredtaboo,oratleastpotentially embarrassing.","","Asaresult,wehavelittleknowledgeofhowpeople actuallyperformspokenlanguageinthewild,orofhow theyexpresstheirfeelingsandattitudesthroughvariations inspeakingstyle.Wehavesubjectiveexperience,but littledata,andthemajorityofanalysesofemotional speech,forexample,arestillbasedontheintuitive performancesofactorsorofsubjectssimulatingemotional speechinthereadingofotherwisebalancedtexts.","","Thispaperreportsonourexperiencesintryingto produceacorpusthatincludesrepresentativeexamplesof spontaneouslyemotionalspeechandonthetechniques thatwedevelopedinordertoovercometheobserver effectsonthenaturalnessofthespeech.."]},{"title":"2.ExpressiveSpeechProcessing","paragraphs":["TheJapanScience&TechnologyAgencyrecently providedfundingforafive-yearprojecttoproducespeech technologyinterfacesforan“AdvancedMediaSociety”, undertheauspicesofCRESTCoreResearchfor EvolutionalScience&Technology.","","Thegoalofthisresearchistoprovidetheknowledge, software,tools,anddatabasesforthedevelopmentof spoken-languageinterfacesthatarepeople-friendly.To improvespeechsynthesissothatitcanemulatethevariety ofwaysthatpeopleusetone-of-voiceandspeakingstyle toconveynon-verbalinformation,andtodevelopspeech recognitionmodulesthatareabletodistinguishbetween differentintentionsdisplayedonthesameunderlyingtext, inordertodetecthowthespeakerisrelatingtothespoken discoursetofacilitateinteractionwithinformationprovidingdevices.","","Althoughtheprojectstartedoutaimingtostudy emotionalspeech,itsoonbecameobviousfromtheinitial datathatwecollectedthattheexpressionofspeakerstate andattitudeisbynomeanslinkedtoemotionalone,and thatthereisawiderangeofspeakingstyleswhichare usedtoshowcommitmentofthespeakertotheutterance andtoindicatetherelationshipwiththelistener[3]."]},{"title":"2.1.Projectgoals","paragraphs":["Thisresearchisbeingcarriedoutinconjunctionwith severaluniversitylabsandincludesmodulesfordiscourse analysis,speechprocessing,linguisticstructure,interface design,andsystemintegration.Theircommonlinkisa largecorpusofexpressivespeech,whichformsthecoreof theanalysisanddevelopment.Thefocusisonimproving concatenativespeechsynthesis,bothinqualityandin rangeofspeakingstyles.","","Thegoaloftheprojectistocollect1000hoursof spontaneousinteractivespeechinthefirstthreeyearsand tospendtheremainingtwoyearsonprototype developmentandsystemevaluation.Todatewehave collectedmorethan250hoursofspeechandplanto collecttheremaining750hoursduringthecomingyear.","","Itisessentialthatthespeechbeofhighsignalquality, sothatautomatictechniquesforsegmentationand annotationmaybeapplied,andthatitisatthesametime representativeofthefullrangeofspokenbehaviourthat information-processingdevicesarelikelytoencounterin thenearfuture.Ifthesedevicesaretobeusedindomestic aswellasbusinessenvironments,thentheywilllikelybe exposedto‘private-mode’speech.","","Ourscheduleforspeechdatacollectionwasoriginally asfollows:10hoursinthefirstyear,100inthesecond, and1000inthethird.Weassumedthateachyear’sdata wouldreplacethatofthepreviousyearaswelearntfrom ourmistakes.Infactmuchoftheearlydatacanstillbe madeuseof,althoughwenowhaveabetterunderstanding bothofthecollectiontechniquesandofthetypesof speechthatwewishtocollect.","Mostofourearlyexperimentsconcernedchoiceof microphonetype,anditsplacementwithrespecttothe speaker,alongwithchoiceofdeviceforrecordingthedata, withlightnessandwearabilitybeingoneofthemain considerations,sinceweneedtocapturespeechinawide varietyofcontexts."]},{"title":"2.2.Speechdatacollection","paragraphs":["Inordertocapturenaturalspeech,itisimportantto imposeasfewconstraintsonthespeakeraspossible.","","Whenpreviouslyrecordingspeechcorporaasasource ofunitsforconcatenativespeechsynthesis,weprepared balancedtextsthatensuredevencoverageofallphone combinationsinmostprosodicenvironments,butthe resultingsentences,beinggeneratedbyagreedyalgorithm, werelexicallydenseandphoneticallycomplicatedforthe readertoproduce.Theresultingstressinthevoice remainedthroughoutthespeechsynthesisprocess,andthe resultswerelessthansatisfactorytolistento.","","Developmentsinthesynthesiscorpusrecordingledto theuseoflongertextssuchasnovelsorshortstories, whichhadsimplerandsequentially-relatedsentencesbut which,whenreadforlongenough,providedsimilar prosodicandphonemicbalancetothepreviously-used sentencelists.Thestories,however,hadtheadvantageof producingmuchmorerelaxedandnatural-sounding speech.","","Theresultingcorporawerebothnatural-soundingand phonetically/prosodicallybalanced,buttheyexhibitedthe characteristicsofonlyonefixedspeakingstyle.Ifthe sourcetextwassad,forexample,thenthewholecorpus wouldbereadinasadvoice,andanysynthesisproduced usingthatspeechwouldalsosoundsad.Synthesisofe.g., aweatherforecastusingasadvoicecanintroducealevel ofinterpretationofthetextthatisquitedifferentfrom whatwasintended.","","Byextension,ifwearetocollectnatural-sounding speech,thenweneedtowidenthetask-specificationstill furthertoincludeallthevarietiesofspeakingstyle encounteredindailyinteractivecommunication.When weconsiderthenumberandtypesofinterlocutortypically encounteredinaday’sinteraction,thenthatbecomesa verywiderangeindeed.Ourcurrentgoalisthereforeto producerecordingsofdailyspokenactivity,frommorning tonight,includinginteractionswithfamilymembers, friends,strangers,traders,andcasualacquaintances."]},{"title":"2.3.ESPRecording","paragraphs":["Volunteerswearlighthead-mountedstudio-quality microphones,suspendedfromtheearsandlargelyhidden bythehair.Theytaketimetobecomeaccustomedto theseandlearntoadapttheirbehavioursothattheydonot accidentallyknockthemicrophoneordragthecableto createunwantedmechanicalnoise.","","Somevolunteersuseradiotransmitters,someconnect directlytoDATrecorders,andsometoaportable minidiskrecorder.AlthoughthequalityoftheDAT recordingishigh,therecordersarestillbulky,andthe radiomicrophonescansufferfromrangeproblems."]},{"title":"3.ComparingDAT&MD","paragraphs":["AlthoughDATrecordersarenowverysmall,theyare notyetpocket-sized,andarestilltooheavytowear comfortablyonthebodyduringeverydayactivities. PortableMinidiscWalkmantechnologyisconsiderably smallerandlighter,butmakesusesignalcompressionto reducetheamountofdatatobestoredondisc.","","TheATRACperceptual-maskingcompression[4] usedintheSonyMinidiscrecordersmayrenderthe recordedspeechunsuitableforconventionalsignal processingtechniques.Wethereforecarriedoutteststo determinetheextenttowhichtraditionalmethodsofe.g. voicepitchestimation,formant-tracking,spectralanalysis, andcepstralencodingmaybedegradedasaresultof usingspeechdatawhichhasundergoneperceptualmaskingforcompressionoftherecordedsignal[5]."]},{"title":"3.1.Microphoneatttachment","paragraphs":["TheSonyECM-77Bstudio-qualityminiaturelavalier microphonesareprovidedwitha3-pinXLRplugabout15 cminlength,whichcontainsatype-31.5Vbatteryand transformercircuitrytopowerthemicrophone.By substitutingthiswithasmall1.5VSR44camerabattery andholder,thepowerunitcanbemadesmallenoughto fitintotheextensionbatterypackoftheSONYMZR900 MDWalkman.FittingthemicrophoneintoaSennheiser NB2adjustableear-mountprovidesacomfortable,light, portable,andunobtrusiverecordingunit.","","Tomeasurethedifferencebetweenrecordingquality onDAT(DigitalAudioTape)andMinidisc,weusedthe abovemicrophonearrangementtorecorda5vowel sequence(a-i-u-e-o)fromamaleandafemalespeaker, takingthesignaltoaDATrecorder(SonyDATTCD-100) andaMinidiscrecorder(SonyMZ-R900)simultaneously.","","Wealsorecordeda1kHz-10kHzsweeptoneanda 200Hz-800Hzchirptonewithasinusoidalwaveform producedbyanNFElectronicInstrumentsDF-194A variablephasedigitalfunctionsynthesiser.Therecording levelsofthetwodeviceswereadjustedtoan approximatelyequivalentsettingusingthesetones.The signalsweretransferreddirectlytocomputerdiscusing opticalfibreviaaCanopusMD-Port,anddown-sampled to16kHz16-bitusingWavesurfersoftware[6].Both WavesurferandEntropic’sESPSsoftware[7]wereused forpitch-estimation,spectraldisplay,andformantanalysis. WeusedtheHTK-3.0program[8]tocalculatetheMelscaledcepstralcoefficients.."]},{"title":"3.2.Comparisonoftherecordings","paragraphs":["Inallcases,thevisiblesignalswereperceptually equivalentbutnotexactlyidentical(compareFigures1 and2),norwerethenumericalvaluesidentical.Table1 showsF0andformantstatistics.Sincethestartpointsof thedifferentwaveformswerealignedmanuallyand processedusingidentical(default)settingsofthesoftware, wecouldexpectthevaluestobeidentical,exceptfor smalldifferencesinsignalpowerarisingfromdifferences intherecordinglevelsofthetwodevices.Figure3shows veryclosespectralsimilarities,revealingidenticalpeaks, butwithslightlylessenergyinthetroughsfortheMD data. Table1.Comparisonofprosodic(top:fundamental frequency)andspectralparameters(bottom:formantsand their bandwidths) derived from signals recorded simultaneouslyfromthesamemicrophonetobothDAT andMinidisc(MD)recorders. ","Male female","Meanf0 Sd Meanf0 sd","DAT 98.69 9.77 171.06 34.7","MD 98.73 9.68 169.31 38.6","","F1 F2 F3 F4 DAT 701(371) 1615(390) 2726(451) 3771(403) MD 678(365) 1603(380) 2683(455) 3750(402)","B1 B2 B3 B4 DAT 336(273) 389(217) 451(253) 493(224) MD 336(268) 392(244) 439(237) 478(237)  ","WecanseefromTable1thatalthoughthevaluesfor thevoicefundamentalfrequencyarenotidenticalforthe datarecordedsimultaneouslytobothDATandMD,they dofallwithinapproximatelythesamerange.Similarly fortheformantsandtheirbandwidths.Thus,ifthe purposeofthiscomparisonweretoprovethatthereisno differencebetweenthetwomedia,thenwecouldconclude immediatelythatthisisnotthecase.However,ourgoalis todeterminewhetherspeechdatafromthetwomediacan betreatedequivalently,anditappearsthatthedifferences, mainlyintherangeofafewHz,canbeconsideredasboth beingdifferentbutsimilarlyaccurateestimatesofthe underlyingprosodicprocesses.","","Similarly,Figure3,whichplotstwospectralslices overlapping,showsthatthedifferencesinthespectraare limitedtooccasionalvalleys,andthatthestructureofthe spectralpeakscanbeconsideredalmostidentical.These spectralsectionsweretakenfromthesamepointinthe vowel/a/,atasteadypartofthevowelcenter.figure4 showsapairofspectralslices(asabove,onefromthe DATrecording,andonefromtheMD)butheretaken fromamoreactivepartofthespeechsignal.Wenotice thatthereisagreaterdifferencebetweenthetwospectra, particularlyintheareaaround6kHz,andperhapsaslight differenceinthepeaksataroundthispoint,butthis examplerepresentsthebiggestlevelofdifferencethatwe foundinthetestdata.","","Thus,ifourgoalissimplytousetheprosodicor spectralinformationderivedfromthespeechsignals,we canfreelyusedatarecordedoneithermedium.However, asweshallseebelow,thenumericaldifferencesbecome significantinthecaseofcepstraltransformeddata. Table2.Mean-squareddifferencesincepstraldistance betweenpairsofvowels,forMDandDATspeechdata. Top:samemedia,differentvowel;bottom:samevowel, differentmedia.  "]},{"title":"/i/-/a/ /i/-/u/ /a/-/u/ mean","paragraphs":["MD-MD"]},{"title":"0.0381 0.0297 0.0267 0.0315","paragraphs":["DAT-DAT"]},{"title":"0.0422 0.0268 0.0261 0.0317 /i/-/i/ /a/-/a/ /u/-/u/","paragraphs":["MD-DAT"]},{"title":"0.0207 0.0186 0.0060 0.0151 ","paragraphs":["Figure1.Spectrogramandf0ofthe5-vowelsequence (fromthemalespeaker)recordedusingaMinidisc. "," .Figure2.Spectrogramandf0ofthe5-vowelsequence (fromthemalespeaker)recordedonDAT. "]},{"title":"3.3.Cepstraldifferences","paragraphs":["Table2showsdifferencesmeasuredfromcepstra calculatedbyHTK’sHCodeprogramfromthesameDAT andMD-recordedspeechdata,usingdefaultparametersto produce16Mel-scaledcepstraper10msecofspeech. Thecepstraldistanceisfrequentlyusedinspeech recognitiontomeasuresimilaritybetweenspeechsignals, e.g.,forautomaticspeechsegmentationorphonemic alignment.","","Sincethecepstraldistancemeasureinitselfcanbe difficulttointerpret,weestablishedabaselinecepstral distanceforeachmediumbytakingmeasuresbetween pairsofvowels;/a/-/i/,/a/-/u/,and/i/-/u/,andthen averagingthisdistancetoproduceavaluewhichmight representthetypicalinter-voweldistance.This(possibly unitless)distanceisabout0.03inthecaseofbothDAT andMD.Wethenmeasuredcepstraldistancesbetween same-vowelcepstralsequences,butvaryingthecoding; DATvs.MD(seebottomrowoftable2).Thecalculated distancesweredifferentdependingonthetypeofvowel, greatestfor/i/andleastfor/u/,buttheyaveraged0.015. Sincethisishalftheinter-voweldistance,thereis probablycauseforsomeconcern.  Figure3.Spectralsectionthroughthevowel/a/. TheplotshowsbothDATandMDdataoverlapped.   Figure4.Spectralsectionthroughthediphthong/ai/. TheplotshowsbothDATandMDdataoverlapped. "]},{"title":"3.4.Discussion","paragraphs":["Thereareundeniablydifferencesinthespeechsignal betweenDATandMDrecordings,buttherecorded speechofbothsoundsidenticaltotheearevenwhen playedoverhighqualityheadphones.Thedifferencein recordingqualitybetweenthetwomediabecomesobvious whenlisteningtomusic,butinthefrequencyrangeof humanspeechitcanbeconsideredimperceptible.","","Althoughthenumericaldataofourcomparisonreveal differences,presumablyarisingasaresultofthe perceptually-masking-basedcompressionusedinthe MiniDidc,thederivedestimatesofformants,fundamental frequency,andglottalparametersrevealonlysmall differencesandthetworecordingmediacanbe consideredequivalentforthepurposesofprosodic analysis."]},{"title":"4.Microphoneplacement","paragraphs":["Inmanycasesthesignal-to-noiserationismore importantforspeechsignalprocessingthancompression, soweconsiderthataslongasthemicrophoneplacement iscorrect,andthetypeofmicrophoneusedissufficiently powered,thenwewillbeabletocollectdatawhichwill notonlyserveourownpurposesforspeechsynthesisbut alsobenefitthewidercommunityofspeechresearchers.","","Weexperimentedbetweenhead-mountingandlapelmountingforthelavaliermicrophones,butconcludedthat althoughlapel-mountingismorewidelyusedinstudio recordings,thereistoomuchheadmovementindailyinteractivespeechandthemic-to-mouthdistancechanges considerablyunlesshead-mountingisused.","","Weexperimentedwithtwoformsofear-mounted setting–usingaboom(e.g.,SennnnheiserNB2)or attachingdirectlytotheearasanearring.Thelatter methodismuchlessobtrusive(andquitecomfortable)but resultsinadropinhigh-frequencyspeechsignalandin spiteoftheinconvenience,wenowprefertheear-mounted boomarrangement.Withcarefulplacementtoavoidnasal airflowandoralplosiveair-bursts,itcanproduceahighqualityconstant-levelsignalthatallowsfaithfulrecording ofthesubject’sspeech.","","Wearestillundecidedbetweendynamicand condensermicrophoneshowever,aseachhasbothgood andbadpoints,butbyreducingthesizeofthepower supply,ithasbecomemucheasiertomakemoreuseof thelatter."]},{"title":"5.Conclusion","paragraphs":["Thispaperhasdescribedourtechniquesforcollecting spontaneousspeechdataandpresentedtheresultsofa comparisonbetweenDATandMDrecordedspeech.We foundthatalthoughtherearedifferencesasaresultofthe signalcompression,theMDspeechdatacanbe consideredequivalenttothatcollectedonDATtapefor useinlarge-scalerecordings.WeareusingMinidisc recorderswithhigh-qualityhead-mountedmicrophonesto collectcontinuousspontaneousconversationalspeech fromarangeofvolunteersubjectsthroughoutJapan."]},{"title":"6.Acknowledgements ","paragraphs":["PartofthisworkwassponsoredbytheJST/CREST underProjectNumber131.Theauthorisgratefultoan anonymousreviewerforhelpfulcomments.Ashorter versionofthispaperwaspresentedattheSpringMeeting oftheAcousticalSocietyofJapaninMarchthisyear."]},{"title":"7.References ","paragraphs":["[1]ISCATutorialandResearchWorkshoponSpeechand Emotion,Belfast1999:www.isca-speech.org/archive.","[2]Douglas-CowieE,Campbell,N.,Cowie,R.,Roach,P. “Emotionalspeech:towardsanewgenerationofspeech databases”,inSpeechCommunicationSpecialIssueon Speech&Emotion,forthcoming.","[3]Campbell,N.,“Collectingreallyspontaneousspeech”, iProcMombushow/sonSpeechProsody,Tokyo,2002.","[4]ATRAC:www.minidisc.org/aes_atrac.htm","[5]Campbell,N&Mokhtari,P.,“DATvs.Minidisc-Is MDrecordingqualitygoodenoughforprosodic analysis?”,ProcASJSpringMeeting2002,1-P-27","[6]Wavesurfer:seehttp://www.speech.kth.se/wavesurfer","[7]EntropicESPSSignalProcessingSoftware–nolonger availableafterbeingboughtbyMicrosoft","[8]HTKSpeechRecognitionToolkit,webpagesand downloadsiteathtk.eng.cam.ac.uk"]}]}