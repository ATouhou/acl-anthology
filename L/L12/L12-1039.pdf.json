{"sections":[{"title":"A Tool for Extracting Conversational Implicatures Marta Tatu and Dan Moldovan","paragraphs":["Lymba Corporation Richardson, TX, United States","marta@lymba.com, moldovan@lymba.com","Abstract Explicitly conveyed knowledge represents only a portion of the information communicated by a text snippet. Automated mechanisms for deriving explicit information exist; however, the implicit assumptions and default inferences that capture our intuitions about a normal interpretation of a communication remain hidden for automated systems, despite the communication participants’ ease of grasping the complete meaning of the communication. In this paper, we describe a reasoning framework for the automatic identification of conversational implicatures conveyed by real-world English and Arabic conversations carried via twitter.com. Our system transforms given utterances into deep semantic logical forms. It produces a variety of axioms that identify lexical connections between concepts, define rules of combining semantic relations, capture common-sense world knowledge, and encode Grice’s Conversational Maxims. By exploiting this rich body of knowledge and reasoning within the context of the conversation, our system produces entailments and implicatures conveyed by analyzed utterances with an F-measure of 70.42% for English conversations. Keywords: Conversational implicature, Knowledge representation, Natural language reasoning"]},{"title":"1. Introduction","paragraphs":["The term implicature was coined by Grice in 1975 to de-note the aspects of meaning that are communicated by an utterance in a conversational context without being part of the literal meaning of the utterance (Grice, 1975). It is useful to distinguish between explicit and implicit information, and between implicit and implicated information. Explicit information is what a reader gathers only from the strict meaning of words. It rarely reflects the meaning of an utterance. Implicit information is built up from the explicit content of the utterance by conceptual strengthening or “enrichment”, which yields what would have been made fully explicit if lexical extensions had been included in the utterance. Implicated information, called implicature, goes beyond what is said (“the coded content” (Grice, 1975)). It is heavily dependent on the context of the situation. For example, within the following conversation held between two users of twitter.com, A: Dinner’s ready! prawns, grouper in some sauce, veg-","etables, rice and shark’s fin melon soup! Still waiting","for lotus root soup this week! B: Eeeeeee lotus root? A: so what you having for dinner? several facts are stated explicitly and their logical inferences can easily be identified (the dinner is ready, a list of dishes where the ingredients of the soup include shark’s fin and melon, lotus root soup for later in the week, A’s question about what B will have for dinner). However, a rich body of implicated information is conveyed as well (A has prepared a dinner which includes the list of mentioned dishes; A is excited of having prepared this gourmet dinner, B dislikes lotus root and cannot believe that A would choose to eat it; A has a poor opinion of B’s gastronomic knowledge). These conversational implicatures are derived from cultural contexts. They go beyond the communication’s semantic content, contrasting with its logical implications. In order to recognize them, communication participants rely on common sense knowledge gathered by observation of successful social interactions. More specifically, they make use of world knowledge about one’s culture, about what is socially or ethically allowed in general as well as what are the expected reactions in a particular situation, and the use of language for cooperative interactions. Communication participants have an inherent understanding of language and its use and are able to make certain inferences based on implicit assumptions rather than what is explicitly stated. Language philosophers analyzed these phenomena and put forward principles of rational human communication behavior (Grice, 1975; Gazdar, 1979; Mc-Cafferty, 1987; Grice, 1989; Hirschberg, 1991; Kasher, 1998; Levinson, 2000). For instance, Grice proposed a Cooperative Principle with associated Maxims of Conversation, which he used to explain how implicatures arise during conversations. • Maxim of Quality: be truthful","• Maxim of Quantity: make your contribution as informative as is required, but not more informative than is required • Maxim of Relation: be relevant","• Maxim of Manner: be clear, by avoiding obscurity of expression and ambiguity and being brief and orderly In this paper, we describe our solution for automatic discovery of implicatures conveyed by English and Arabic utterances. For this purpose, we exploit Grice’s Conversational Maxims, which make explicit the assumptions humans make when interpreting an utterance. By converting these maxims into a rich set of default macro-axioms, our"]},{"title":"2708","paragraphs":["abductive natural language reasoner is able to derive default inferences that link the analyzed utterance to the conversational context, making explicit the speaker’s implicature s."]},{"title":"2. Model for Conversational Implicatures","paragraphs":["Without any means to represent and store derived implicatures, their automatic identification is impracticable. Therefore, we created an implicature model that captures not only the implicit information conveyed by the speaker but also the explicit information transmitted to the hearer. The 8tuple {SU , SI , HT M , HT E, HI , C, GM, K}, where","• the speaker S is characterized by his utterance (SU ) and his intentions (SI ),","• the hearer H’s characterization captures his understanding of the utterance (HT M ), its entailments (HT E) and conveyed implicatures (HI ), • the C component captures the context","• Grice Maxims (GM) indicates whether there was no maxim violation, a clash between maxims has occurred or one or more maxims have been flouted, and","• K denotes the common sense knowledge needed by H to derive S’s implicatures establishes a standard semantic representation for conversational implicatures that facilitates the consumption of implicatures. Our model captures the complete meaning of an utterance, thus enabling advanced application systems to make use of extracted implicatures and produce highly accurate results."]},{"title":"3. Data Sets","paragraphs":["Implicatures are prevalent in conversations. We used the Microsoft Research Conversation (MRC)1","corpus for our analysis of implicatures conveyed by English conversations. It consists of 1.3 million open-domain co-operative conversations gathered from twitter.com (Twitter) (Ritter et al., 2010). Their linguistic styles vary greatly from spoken language, which often includes misspelled words, shorthands, interjections, context describing words, emoticons, etc. to more formal language. For Arabic, we followed the main steps of the MRC generation process and created a similar corpus of conversations held among Arabic Twitter users. Our goal was to be able to easily compare our findings across languages and cultures. We identified a set of Arabic native speakers that tweet in Arabic by searching Twitter for various Arabic words and used their most recent tweets that were posted as replies to other tweets to build conversation threads. Iteratively, if the tweet that was replied to was itself a reply to another tweet, the conversation was augmented accordingly, more specifically, built in reverse chronological order, one tweet at a time. Our Arabic conversation corpus contains 4,000 conversations involving 2,067 Twitter users. 1 http://research.microsoft.com/en-us/downloads/8f8d5323-","0732-4ba0-8c6d-a5304967cc3f/default.aspx In order to be able to evaluate our approach, we annotated implicatures conveyed by real-data examples extracted from these datasets. We manually identified the components of implicature models for 253 English utterances and 75 Arabic utterances. Most implicatures exploit the Relevance Maxim. Floutings of the Manner Maxim are also an important source of implicatures."]},{"title":"4. Reasoning for Implicatures","paragraphs":["Implicatures exhibit certain properties that must be taken into account when attempting to select the best reasoning framework for identifying implicatures. More specifically, implicatures are cancelable, non-detachable, calculable, non-conventional, reinforceable, and universal (Grice, 1975; Gazdar, 1979; Levinson, 1983; Levinson, 2000; Horn, 2004). Therefore, non-monotonic (defeasible) reasoning frameworks that support a default mode of reasoning, such as induction, abduction, practical reasoning, and default logics, are the best candidates for automatic implicature derivation (McCafferty, 1987; Hobbs et al., 1990; Wainer, 1991; Harnish, 1991; Green and Carberry, 1993; Green and Carberry, 1994; Levinson, 2000; Allan, 2000). The solution presented in this paper makes use of abduc-tion as well as default knowledge to identify implicatures. Our system processes the natural language of a conversation one turn at a time and derives the logical implications and implicatures communicated by each speaker utterance, updating the conversation’s common ground after each in-dividual analysis. In Figure 1, we show the architecture of the system, highlighting the roles and interactions of our implicature model components ({SU , SI , HT M , HT E, HI , C, GM, K}). Our implicature derivation engine was implemented on top of Lymba’s natural language reasoner, C OGEX (Tatu and Moldovan, 2006; Tatu and Moldovan, 2007; Moldovan et al., 2010). The series of modifications needed to trans-form COGEX from a recognizing textual entailment (RTE) system into an automated system that derives the implicatures conveyed by natural language conversations focus on (1) developing an accurate knowledge representation of dialogs for both English and Arabic, (2) generating semantic axioms to be used during the reasoning process, and (3) altering the existing reasoning framework to allow for nonmonotonicity. 4.1. Logical form transformation Our first order logical representation of text captures the rich semantic information extracted by Lymba’s NLP pipeline (Moldovan et al., 2010). Unlike natural language texts, conversations are rich in indexicals, such as I and you, which are resolved to their corresponding references, before being represented in logical form. Therefore, I like pizza is represented as speaker USER(x1) & human NE(x1) & like VB(e1) & pizza NN(x2) & EXP SR(x1,e1) & THM SR(x2,e1)2",", where the name of the predicate speaker USER(x1) will be 2 EXP SR(x1,e1) denotes that x1 is the experiencer of e1.","Similarly, x2 is the theme of e1 in THM SR(x2,e1)."]},{"title":"2709","paragraphs":["Figure 1: System architecture replaced with the appropriate value of the speaker given by the analyzed turn. We note that the values of speaker and hearer change with each conversation turn. Furthermore, we enhanced our logical form representation with several special predicates that describe the utterance being represented (e.g., type: question TYPE, statement TYPE, or a capitalized words flag: capitalized CHARS). These predicates are needed to ensure that our reasoning engine will be able to make use of certain types of axioms that use similar predicates. 4.2. Axioms for implicatures Various types of axioms are needed to identify implicatures. Each axiom uses logical predicates that match the semantic representation of the conversation language. 4.2.1. XWN lexical chain axioms The XWN lexical chain axioms link WordNet concepts by exploiting the semantic relationships present in the eXtended WordNet (XWN) (Tatu and Moldovan, 2006; Tatu and Moldovan, 2007; Moldovan et al., 2010). This valu-able resource stores semantic representations of WordNet’ s plain text glosses, which can be mined for their world knowledge. For Arabic, lexical chain axioms are generated using the Arabic WordNet relations only. Within our system, these axioms are used to link a speaker utterance (SU ) to the established common ground (the context C). This is particularly important for apparent floutings of the Relation Maxim, where there exists a semantic disconnect between the two. The XWN lexical chain axioms are generated on demand and derive concepts semantically related with “source” concepts mentioned in the speaker utterance currently under analysis. Examples include:","• sauce NN(x1) → dish NN(x2) & PW SR(x1,x2) [WordNet’s P ART-WHOLE (sauce,dish) relation]","• praise VB(e1) & AGT SR(x1,e2) & THM SR(x2,e1) → express VB(e2) & ISA SR(e1,e2) & approval NN(x3) & THM SR(x3,e2) & AGT SR(x1,e2) & THM SR(x2,x3) [WordNet gloss for praise: express approval of]","• asay 1 NN(x1) → salobiy 1 JJ(x2) & ueuwr 1 NN(x3) & VAL SR(x2,x3) & ISA SR(x1,x3) [English translation: sorrow NN(x1) → negative JJ(x2) & feeling NN(x3) & VAL SR(x2,x3) & ISA SR(x1,x3)] 4.2.2. Semantic calculus Semantic calculus axioms identify the semantic relationship (R0) that defines the combination of two semantic relations (R1 and R2) (Tatu and Moldovan, 2006; Tatu and Moldovan, 2007; Moldovan et al., 2010) (i.e., R1(c1,c3) & R2(c3,c2) → R0(c1,c2)). These axioms greatly increase the semantic connectivity between concepts. This is particularly important when no immediate semantic link can be found between two concepts of interest. The 86 semantic calculus axioms used within our system were manually derived on empirical observations. The accuracy of each axiom was measured on a large corpus. These axioms are language independent. Examples include:","• PW SR(x1,x2) & PW SR(x2,x3) → PW SR(x1,x3) [PART-WHOLE is transitive]","• PW SR(x1,x2) & LOC SR(x3,x2) → LOC SR(x3,x1) [if x2 is located at x3, then its parts, x1, are also located at x3]","• QNT SR(x1,x2) & INS SR(x2,x3) → QNT SR(x1,x3) [frequency x1 of x3’s instrument x2 becomes the frequency of x3] 4.2.3. Common sense world knowledge This type of axioms encode the common sense knowledge required by an automated system to derive unstated implications. These axioms describe not only various properties of concepts, but also how the concepts interact in the world and how people speak about them. Most of these axioms are universal. However, culture-dependent information is mostly encoded as common sense knowledge axioms. The sources used for this type of axioms include (1)"]},{"title":"2710","paragraphs":["domain-specific and open-domain ontologies built to complement WordNet (first axiom shown below), (2) semantic associations (selectional restrictions) learned from large corpora by generalizing the arguments of semantic relation instances extracted from text (second sample axiom), and (3) manual encoding (third axiom shown below). Examples include:","• lotus root NN(x1) → lotus NN(x2) & root NN(x3) & PW SR(x3,x2) & ISA SR(x1,x3) [lotus root is the root of the lotus]","• meal NN(x1) :→ cook VB(e1) & THM SR(x1,e1) [meals are usually cooked]","• create VB(e1) & THM SR(x1,e1) & PW SR(x2,x1) → THM SR(x2,e1) [if one creates a whole, then one creates its parts] We are currently using 482 common sense knowledge axioms during the processing of English conversations and 251 axioms for Arabic dialogs. 4.2.4. Grice maxims axioms These axioms act as macro rules that capture the essence of Grice’s Maxims. The 63 axioms currently used by the system exploit the components of the model we defined for conversational implicatures (Section 2.). Before being used by an automated system for the analysis of a particular utterance, each Gricean maxim axiom must be instantiated with the actual values of the various components of the current conversational model. This set of axioms was manually derived. They describe implicatures conveyed both by obeying the maxims as well as by apparent floutings (exploitation) of the conversational maxims. Although most axioms are language independent, they needed to be rewritten to use the logical predicates corresponding to the analyzed language. Examples of Relevance Maxim axioms include RELEVANCE GM → (predicatei(xj)∈LF(SU ) & predicatei(xk)∈LF(C) :→ xj = xk) [there must be at least one common predicate between the (enhanced) logical forms of SU and C, given that the speaker’s utterance must be relevant to the established common ground]. In the case of floutings of the Relevance Maxim, this axiom can be used to assume the unification of two identically named predicates from SU and C. The Quality Maxim dictates a certain degree of sincerity from the speaker. Thus, our Quality Maxim axioms exploit the type of the speaker’s utterance. Examples include:","• QUALITY GM & SU (x1) & exclamation(x1) → S(x2) & show VB(e1) & AGT SR(x2,e1) & strong JJ(x3) & feeling NN(x4) & VAL SR(x3,x4) & THM SR(x4,e1) [speakers show strong feelings with exclamations]","• QUALITY GM & SU (x1) & question(x1) → S(x2) & -(know VB(e1) & EXP SR(x2,e1) & THM SR(x1,e1)) [speakers do not know the answer to their questions] Other Quality Maxim axioms exploit the type of speech act performed by the speaker and his utterance. For instance, by uttering an apology, the speaker implicates that he regrets having caused trouble for someone. Floutings of the Quality Maxim can be identified using the axiom QUALITY GM & (SU (x1) → $F) → S(x2) & ironic JJ(x3) & VAL SR(x3,x2) [if the speaker’s utterance is (blatantly) false, S is ironic]. Manner implicatures conveyed by utterances assumed to be respecting the Manner sub-Maxim “be orderly” can be derived using the axiom MANNER GM & predicate1(e1)∈SU & predicate2(e2)∈SU & syntactically coordinated(e1,e2) → (BEFORE SR(e1,e2) | IMMEDIATELY BEFORE SR(e1,e2)) [within a speaker utterance, events are recounted in the order in which they happened]. Floutings of Grice’s Manner Maxim and their corresponding implicatures are captured by various axioms, including","• MANNER GM → (( capitalized chars(x1) & SU (x1)) :→ (S(x2) & excited JJ(x3) & VAL SR(x3,x2))) [capitalized texts within speaker utterances indicate the speaker’s excitement]","• MANNER GM & SU (x1) & statement repetition(x1) :→ S(x2) & show VB(e1) & AGT SR(x2,e1) & strong JJ(x3) & feeling NN(x4) & VAL SR(x3,x4) & THM SR(x4,e1) & TPC SR(x1,x4) [the repetition of a statement within the speaker utterance indicates the speaker’s strong feelings about the statement] Implicatures derived from the speaker’s observance of the Quantity Maxim are identified using a macro axiom that exploits implicational scales and contrasting sets (Levinson, 2000): QUANTITY GM → (predicatei(xi)∈SU & predicate0(x0) & SCALE GM(x0,xi) → -SU (x0,xi)) [if the speaker utterance contains a predicate that is part of an informational scale and there is a stronger item part of the same scale, then the negation of the modified speaker utterance where the “weaker” predicate is replaced by the stronger one holds]. If the speaker utterance mentions a term/expression found to be weaker than others with respect to its informativeness, then the speaker was not in the position to state the strong term/expression and implicates that the alternate utterance is not true. Grice’s Quality Maxim is flouted when a speaker is uttering tautologies (SU → $T), which, by being necessarily true, should lack informativeness. However, depending on the form of the speaker utterance, certain implicatures are conveyed:","• QUANTITY GM & SU :(P(xi) & =(xi,xi)) → predicate0(x0)∈LF(C) & (PRO SR(x0,xi) | VAL SR(x0,xi)) & always TMP(x0) & -(predicatex(xx) → -(PRO SR(x0,xi) | VAL SR(x0,xi))) [if the speaker utterance is of the form X is X, then one of X’s"]},{"title":"2711","paragraphs":["(essential) properties relevant to the existing common ground always happens in X and nothing can change that]","• QUANTITY GM & SU :(P(xi) | -P(xi)) → -(worry VB(e1) & (H(x1) | S(x1)) & EXP SR(x1,e1) & THM SR(xi,e1)) & -(predicatex(ex) & (H(x1) | S(x1)) & AGT SR(x1,ex) & CAU SR(ex,xi)) [if the speaker utterance is of the form X or not-X, then neither the hearer nor the speaker should worry about X because nothing can be done to influence it]","• QUANTITY GM & SU :(P(xi) → P(xi)) → -(worry VB(e1) & (H(x1) | S(x1)) & AGT SR(x1,e1) & THM SR(xi,e1)) & -(predicatex(ex) & (H(x1) | S(x1)) & AGT SR(x1,ex) & CAU SR(ex,xi)) [if the speaker utterance is of the form if X, X, then neither the hearer nor the speaker should worry about X because nothing can be done to influence it] 4.3. Reasoning framework Lymba’s natural language reasoner, C OGEX, is a heavily modified version of the Otter theorem prover3",", which uses the Set of Support (SoS) strategy to prove by contradic-tion: a hypothesis is proved by showing that it is impossible for it to be false in the face of the provided evidence and background knowledge (BackgroundKnowledge, Evidence & ¬Hypothesis → ⊥). 4.3.1. Question answering (QA) and Recognizing","textual entailment (RTE) COGEX has been successfully employed within Lymba’s QA engine to re-rank the final list of candidate answer passages based on the degree of entailment between each passage and the given question (Moldovan et al., 2010). For the RTE task, COGEX computes the extent to which a text snippet entails a hypothesis as a normalized score between 0 and 1 and compares this value to a threshold learned during training to determine whether the given hypothesis is entailed by the given text (Tatu and Moldovan, 2006; Tatu and Moldovan, 2007). Within these settings, the initial usable list contains various natural language axioms, which can be used to infer new information (BackgroundKnowledge), while the logical clauses corresponding to the candidate passage/text snippet (Evidence) as well as the negated question/hypothesis (¬Hypothesis) are added to the SoS list. Given Otter’s best-first clause selection mechanism, the heavily weighted question/hypothesis clauses are the last clauses to be processed by the system. Thus, when resolutions using these clauses are attempted, it is guaranteed that all other inferences ([BackgroundKnowledge & Evidence]+) have been made and are stored in the usable list. 4.3.2. Implicature extraction For the implicature identification task, COGEX exploits the first order logical representations of the implicature model components (Section 2.). It makes use of the SU , C, and 3 http://www.cs.unm.edu/m̃ccune/otter/ K components to derive the values of SI , HT M , HT E, and HI , which, in turn, will be used to update the context of future conversational turns (Figure 1). We note that our system is currently operating under the assumption that the communication channel is noise-free and, thus, HT M = SU . Futhermore, SI = HI since it is difficult to determine whether the speaker intended all the implicatures SU conveyed at the time of SU ’s analysis. These values are revised if future conversational turns cancel some of the implicatures derived during SU ’s analysis. When the analysis of a conversation begins, the context C is empty, the set of axioms described in Section 4.2. form the knowledge component (K), and the semantic representation of the speaker utterance make up the model’s S U component. Furthermore, the first order clauses of the C and K model components form the usable list and the logical clauses of SU serve as the initial SoS. As the reasoning process unfolds, all SU predicates and the inferences they produce (entailments as well as implicatures) are moved to the usable list, where they become part of the context C of future conversational turns. The reasoning process terminates when no more inferences can be made from SU (i.e., SoS is empty). In this situation, the clauses inferred from a given utterance using non-default axioms and/or context clauses that were explicitly stated or entailed by previously analyzed utterances (i.e., previous SU /HT M and HT E components) are marked as logical entailments (HT E). All clauses inferred from default axioms as well as abductive rules and/or context clauses that were previously labeled as implicatures (i.e., previous SI /HI components) are marked as the implicatures conveyed by the current SU (SI , HI ). This is the most expected outcome of an analysis of a conversational turn. However, if a refutation is found during the reasoning process, the clauses that caused the inconsistency may indicate that (1) the speaker made a false statement (the contradic-tion stems from SU clauses alone), which carries certain Quality-flouting implicatures or (2) a previously identified implicature must be canceled (information from current SU contradicts previous SI /HI ), in which case, the implicature clauses are removed and the reasoning process is restarted. 4.4. Evaluation In order to assess our system’s performance, we manually compared the SI /HI components of automatically generated implicature models with their human annotated counterparts. An automatically derived implicature was deemed correct if a sufficient semantic overlap exists between itself and one of the gold implicatures annotated for the input utterance. The amount of meaning overlap between a system implicature and a human implicature required to consider the system implicature correct was left at the assessor’s di scretion. We adopted this evaluation approach due to large differences between the surface form of annotated implicatures expressed in natural language English or Arabic and the highly simplified logical form of automatically derived implicatures. We note that the utterance entailment (HT E), context (C), Grice Maxims (GM), and common sense knowledge (K) components of system returned models are far richer than"]},{"title":"2712","paragraphs":["their corresponding components of human annotated models. Although, in our current evaluation, these components are not taken into account, they should influence the overall performance of the system.","English Arabic Test data size 50 35 Implicatures/turn (average; gold) 1.32 1.08 Precision 75.75 57.89 Recall 65.79 45.83 F-measure 70.42 51.16 Table 1: System performance Our system derived many of the implicatures identified by human annotators (Table 1). We attribute its high precision to the various axioms it employs, mainly Grice Maxim axioms as well as common sense knowledge rules that are employed by its reasoning mechanism when deriving new inferences. The automatically identified implicatures that were deemed incorrect were not utterly wrong, but highly unlikely for the given speaker utterance in the given context. 4.4.1. Error analysis As noted above, the system relies heavily on the set of axioms it uses to derive implicatures. Our current set of Grice Maxim axioms produces novel and interesting implicatures. However, more axioms must be created to account for various floutings of the Manner Maxim. The lack of sufficient common sense knowledge caused the highest number of errors, 66% of the unidentified conversational implicatures conveyed by English conversations. For Arabic conversations, most errors are caused by the lack of a complete understanding of the speaker utterance. Lymba’s NLP pipeline for the Arabic language is not as rich as the suite of tools we developed for English and, altough, the reasoning engine’s entailment and implicature clause generation process is highly accurate, the quality of its output depends on the accuracy of its inputs, the logical representation of the speaker utterance as well as the knowledge-representing axioms. 4.4.2. Example Let us consider the sample dialog showed in Section 1. The implicature models generated by the analysis of the first and second conversational turns are shown in Tables 2 and 3, which display a simplified logical representation of the model components (as returned by our implicature derivation system) together with a simple natural language conversion of the logical predicates (manually derived). The semantic representation of A’s utterance (S U /HT M – Dinner’s ready! prawns, grouper in some sauce, vegetables, rice and shark’s fin melon soup! Still waiting for lotus root soup this week!) captures the statement’s explicit meaning (e.g., value(ready,dinner), part-whole(grouper,sauce), etc.). The unification and resolution of these logical clauses with some of K’s lexical chain and Semantic Calculus axioms produces logical clauses that make explicit the entailments carried by A’s utterance. For instance, sauce (part of SU ) and sauce → dish, part-whole(sauce,dish), dish → meal, part-whole(dish,meal) (lexical chain axioms) and part-whole(x1,x2), part-whole(x2,x3) → part-whole(x1,x3) (Semantic Calculus axiom) generate the entailments dish, part-whole(sauce,dish), meal, part-whole(dish,meal), part-whole(sauce,meal), which show that, since there is some sauce, a dish as well as a meal, which include that sauce, must exist. Assuming A is rational, the list of dishes and ingredients mentioned in A’s second sentence is relevant to his/her first sentence (Dinner’s ready!). The link found between the two statements is a conversational implicature given by the Relevance Maxim macro axiom shown in Section 4.2.4., which indicates that the meal described by its various dishes is the ready dinner (=(dinner,meal), since WordNet’s ISA(dinner,meal) is converted into the axiom dinner → meal, or, more specifically dinner NN(x1) → meal NN(x1)). We note that the GM component of the implicature model is derived based on the set of Grice Maxim axioms used during the reasoning process (e.g., Quality Maxim; Relevance Maxim flouting, for the first conversational turn)."]},{"title":"5. Conclusion","paragraphs":["Our findings provide useful insights into the problem of conversational implicature identification. We have defined a conversational implicature model that captures the implicit information conveyed by the speaker as well as the explicit information transmitted to the hearer. Based on previous research and analysis of real-world conversations, we implemented an automated system that performs well on this task. The knowledge that humans use to fully under-stand an utterance is captured within the rich set of axioms used by our system. Furthermore, we identified and annotated real-world conversations that convey implicatures for both English and Arabic languages. 5.1. Future work The broad scope of this difficult task requires more effort from computational linguists that aim to automatically identify conversational implicatures conveyed by natural language texts. Possible extensions of the work presented in this paper include:","• Politeness. Polite utterances conversationally implicate the bald on record contribution (where no sooth-ing layer exists),","• Clarifications. Clarification requests provide good evidence of implicatures because they make implicatures explicit, • Figures of speech (e.g., metaphor, scarcasm, irony),","• Social context. In addition to the information exchanged during the course of a conversation, the context may include the knowledge shared by the conversation participants, their social relationship, their mood, the physical setting of the conversation, etc."]},{"title":"2713","paragraphs":["SU HTM Dinner’s ready! prawns, grouper in some sauce, vegetables, rice and shark’s fin melon soup! Still waiting for lotus root soup this week! dinner, ready, value(ready,dinner), exclamatory type, prawn, grouper, sauce, part-whole(grouper,sauce), vegetable, rice, shark, fin, part-whole(fin,shark), melon, soup, part-whole(melon,soup), part-whole(fin,soup), still, wait, manner(still,wait), lotus, root, part-whole(root,lotus), soup, part-whole(root,soup), theme(soup,wait), week, date(week), during(wait,week), A, agent( A,wait)","C ∅","K sauce → dish, part-whole(sauce,dish); dish → meal, part-whole(dish,meal); soup → dish, isa(soup,dish) cook → create, isa(cook,create); wait → expect; dinner → meal isa(x1,x2), theme(x3,x1) → theme(x3,x2); isa(x1,x2), agent(x3,x1) →(x3,x2); isa(x1,x2), during(x3,x1) → during(x3,x2) part-whole(x1,x2), part-whole(x2,x3) → part-whole(x1,x3); isa(x1,x2), part-whole(x2,x3) → part-whole(x1,x3) create, theme(x1,create), part-whole(x2,x1) → theme(x2,create); meal → cook, theme(meal,cook) quality gm, exclamatory type → A, show, agent( A,show), strong, feeling, value(strong,feeling), theme(feeling,show) relevance gm → =(dinner,meal); relevance gm, cook → A, agent( A, cook)","HTE dish, part-whole(sauce,dish), meal, part-whole(dish,meal), part-whole(sauce,meal), dish, isa(soup,dish), meal, part-whole(dish,meal), part-whole(soup,meal), expect sauce is part of a dish; soup is a dish; dish is part of a meal; sauce and soup are part of a meal; wait is expect SI HI show, agent( A,show), strong, feeling, value(strong,feeling), theme(feeling,show), =(dinner,meal), cook, theme(meal,cook), agent( A,cook), create, theme(meal,create), agent( A,create) A shows strong feeling; the ready dinner is the meal with all the dishes; A cooked that meal Table 2: Implicature model of the first conversational turn","SU Eeeeeee lotus root?","HTM eeeeeee, lotus, root, part-whole(root,lotus), question type","C dinner, ready, value(ready,dinner), exclamatory type, prawn, grouper, sauce, part-whole(grouper,sauce), vegetable, rice, shark, fin, part-whole(fin,shark), melon, soup, part-whole(melon,soup), part-whole(fin,soup), still, wait, manner(still,wait), lotus, root, part-whole(root,lotus), soup, part-whole(root,soup), theme(soup,wait), week, date(week), during(wait,week), A, agent( A,wait); dish, part-whole(sauce,dish), dinner, part-whole(dish,dinner), part-whole(sauce,dinner), dish, isa(soup,dish), dinner, part-whole(dish,dinner), part-whole(soup,dinner), meal, expect; show, agent( A,show), strong, feeling, value(strong,feeling), theme(feeling,show), =(dinner,dinner), cook, theme(meal,cook), agent( A,cook), create, theme(meal,create), agent( A,create) a dinner made by A is ready; this dinner includes prawns, grouper in some sauce, vegetables, rice and shark’s fin melon soup; A is expecting to make lotus root soup this week; A is showing strong feelings","K eeeeeee → interjection type; interjection type → exclamatory type eeeeeee → disgust, value(eeeeeee,disgust); disgust → dislike; dislike → feeling quality gm, question type → B, -believe, experiencer( B,believe), theme( question type,believe) relevance gm → =(feeling,feeling); relevance gm → =(root,root) quality gm, exclamatory type → B, show, agent( B,show), strong, feeling, value(strong,feeling), theme(feeling,show)","HTE interjection type, exclamatory type, disgust, value(eeeeeee,disgust), dislike, feeling eeeeeee is interjection, which indicates exclamation, and value of disgust; disgust is dislike, which is a feeling SI HI show, agent( B,show), strong, feeling, value(strong,feeling), theme(feeling,show), =(feeling,feeling), =(root,root), -believe, experiencer( B,believe), theme( question type,believe) B shows a strong feeling of dislike; B does not believe A will create soup Table 3: Implicature model of the second conversational turn"]},{"title":"2714","paragraphs":["These aspects are used by humans when interpreting an utterance and should be exploited by automated systems as well.","• Social utterance. Written discussions lack the expresiveness of oral dialogues where participants may use non-verbal communicative actions, e.g., winking, laughing, coughing, etc. as well as various speech at-tributes, such as, tone, pitch, accent, stress, volume, etc. The identification and representation of such fea-tures as part of the SU model component is vital for the automatic derivation of Relevance, Manner and Quality implicatures.","• Parallel interpretations. Given that a hearer cannot be 100% sure of the speaker’s implicatures (these are only implicated and may be retracted ar any time) and because clarifications interfere with politeness, a hearer will allow the conversation to continue while maintaining a set of likely analyses in hopes of disambiguating past speaker utterances based on future statements. A similar mechanism is desired for automated systems that derive contradicting competing implicatures from a given speaker utterance. Extensions to other languages depend on the implementa-tion of (1) natural language understanding tools that derive the referential meaning of an utterance in the native language (not by translating to another language); (2) mechanisms of converting the meaning into logical forms that can be manipulated by a default reasoning engine; and (3) methods for generating the various types of natural language axioms described above."]},{"title":"6. References","paragraphs":["Keith Allan, 2000. Quantity Implicatures and the Lexicon, pages 169–218. Amsterdam:Elsevier.","G. Gazdar. 1979. Pragmatics: Implicature, Presupposi-tion, and Logic Form. New York: Academic.","Nancy Green and Sandra Carberry. 1993. A Discourse Plan-Based Approach to a Class of Particularized Conversational Implicature. In Proceedings of the 10th Annual Meeting of the Eastern States Conference on Linguistics, pages 117–128.","Nancy L. Green and Sandra Carberry. 1994. A Hybrid Reasoning Model for Indirect Answers. In James Pustejovsky, editor, Proceedings of the Thirty-Second Meeting of the Association for Computational Linguistics, pages 58–65, San Francisco. Association for Computational Linguistics, Morgan Kaufmann.","H. P. Grice. 1975. Logic and Conversation. In Syntax and Semantics: Speech Acts. Volume 3.","H. P. Grice. 1989. Studies in the Way of Words. Harvard University Press, Cambridge, MA.","Robert M. Harnish. 1991. Logical Form and Implicature. In Steven Davis, editor, Pragmatics: A Reader, pages 316–364. Oxford University Press, Oxford.","Julia Hirschberg. 1991. A Theory of Scalar Implicature. Garland Publishing Company, New York.","J. Hobbs, M. Stickel, D. Appelt, and P. Martin. 1990. In-terpretation as Abduction. Artificial Intelligence.","L. R. Horn. 2004. Implicature. In The Handbook of Pragmatics.","Asa Kasher, editor. 1998. Pragmatics: Critical Concepts. Volume IV: Presupposition, Implicature, and Indirect Speech Acts. Routledge, London.","S. C. Levinson. 1983. Pragmatics. Cambridge, England: Cambridge University.","S. C. Levinson. 2000. Presumptive Meanings: The Theory of Generalized Conversational Implicature. Cambridge, MA: MIT Press.","A. S. McCafferty. 1987. Reasoning about Implicature: a Plan-based Approach. UMI.","D. Moldovan, M. Tatu, and C. Clark. 2010. Role of Semantics in Question Answering. In Semantic Comput-ing, pages 373–419. John Wiley & Sons, Inc.","A. Ritter, C. Cherry, and B. Dolan. 2010. Unsupervised Modeling of Twitter Conversations. In Proceedings of NAACL.","M. Tatu and D. Moldovan. 2006. A Logic-based Semantic Approach to Recognizing Textual Entailment. In Proceedings of COLING/ACL 2006.","M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.","Jacques Wainer. 1991. Uses of Nonmonotonic Logic in Natural Language Understanding: Generalized Implicatures. Ph.D. dissertation, Department of Computer Science, Pennsylvania State University, University Park, Pennsylvania."]},{"title":"2715","paragraphs":[]}]}