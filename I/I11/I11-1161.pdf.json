{"sections":[{"title":"","paragraphs":["Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1397–1402, Chiang Mai, Thailand, November 8 – 13, 2011. c⃝2011 AFNLP"]},{"title":"A Semantic Relatedness Measure Based on Combined Encyclopedic, Ontological and Collocational Knowledge Yannis Haralambous Institut Télécom – Télécom Bretagne Département Informatique UMR CNRS 3192 Lab-STICC Technopôle Brest Iroise CS 83818, 29238 Brest Cedex 3, France yannis.haralambous@telecom-bretagne.eu Vitaly Klyuev University of Aizu Aizu-Wakamatsu Fukushima-ken 965-8580, Japan vkluev@u-aizu.ac.jp Abstract","paragraphs":["We describe a new semantic relatedness measure combining the Wikipedia-based Explicit Semantic Analysis measure, the WordNet path measure and the mixed collocation index. Our measure achieves the currently highest results on the WS-353 test: a Spearman ρ coefficient of 0.79 (vs. 0.75 in (Gabrilovich and Markovitch, 2007)) when applying the measure directly, and a value of 0.87 (vs. 0.78 in (Agirre et al., 2009)) when using the predic-tion of a polynomial SVM classifier trained on our measure. In the appendix we discuss the adaptation of ESA to 2011 Wikipedia data, as well as various unsuccessful attempts to enhance ESA by filtering at word, sentence, and section level."]},{"title":"1 Introduction 1.1 Semantic Relatedness and Corpora","paragraphs":["Semantic relatedness describes the degree to which concepts are associated via any kind of semantic relationship (Scriver, 2006). Its evaluation is a fundamental NLP problem, with applications in word-sense disambiguation, text classification, information retrieval, automatic summarization and many other fields. In re-cent decades, a great variety of relatedness measures have been defined, based on corpora such as Wikipedia, Wiktionary, WordNet, etc.","Wikipedia is one of the most successful collaborative projects of all time. By a constantly growing number of additions, corrections and verifications, its contents grows in both quantity and quality, and is considered by many linguists as the corpus they had always dreamed of (Medelyan et al., 2008).","By measuring the normalized tfidf values of words in a page, we can consider the page to be a weighted vector in the space of words. Inverting the matrix of these vectors we obtain weighted vectors of words in the space of pages. As every page deals with a single topic, we consider these vectors as being concept vectors. The ESA (Explicit Semantic Analysis) measure between two words is obtained by taking the cosine of their concept vectors (Gabrilovich and Markovitch, 2007).","Unlike Wikipedia, WordNet (Miller, 1995), a semiformal lexical ontology (Huang et al., 2010), has a fine and carefully-crafted ontological structure: word senses are represented by sets of synonyms (“synsets”), and there is a graph structure on synsets based on hypernymic relations. Several WordNet-based semantic relatedness measures have been defined, based on distances in the hypernymic graph, and often combined with word distribution in sense-tagged corpora. 1.2 Evaluation of Results, WS-353 Test (Finkelstein et al., 2001) introduce WS-353, a semantic relatedness test set consisting of 353 word pairs1","and a gold standard defined as the mean value of evaluations by up to 17 human judges. Although this test suite contains some quite controversial word pairs,2","it has been widely used in literature and has become the de facto standard for semantic relatedness measure evaluation.","Technically, the final result of the test is the Spearman ρ rank correlation coefficient (Spearman, 1904) between the relatedness ranking of pairs by human judges and that by the tested algorithm. So, in fact, it is not the value obtained for each pair that counts, but only the ranks. 1.3 Our Approach By closely examining word pairs that failed to be ranked correctly by ESA, we came to the conclusion that the WS-353 word pairs belong (non-exclusively) to four classes, corresponding to different kinds of semantic relatedness and requiring different kinds of knowledge: 1. encyclopedic: see Section 2; 2. ontological: see Section 3; 3. collocational: see Section 4; 4. pragmatic: see Section 6. In this paper, we define a new semantic relatedness","measure by combining knowledge related to these four","classes. 1 Actually 352 pairs, since “money / cash” appears twice. 2 For example: “Arafat / terror” (0.765), “Arafat / peace”","(0.673), “Jerusalem / Israel” (0.846), “Jerusalem / Pales-","tinian” (0.765), etc."]},{"title":"1397 2 Encyclopedic Knowledge","paragraphs":["This class contains pairs that are best sorted by ESA. We note that (Agirre et al., 2009) qualify ESA as a distributional approach. Indeed, technically two words are semantically related in ESA if they appear together frequently in Wikipedia pages. But since pages are descriptions of topics (= concepts), words are ESA-close when they appear frequently in common concept descriptions, and therefore in common semantic domains. Hence, ESA is semantically richer than a merely distributional approach.","ESA is the first and most important component of our combined relatedness measure. By adapting our implementation of the ESA algorithm to 2011 Wikipedia data (see App. A), we obtain a Spearman ρ = 0.7394. In the following sections we describe the components added to ESA in order to optimize its performance even further."]},{"title":"3 Ontological knowledge","paragraphs":["To get a better insight into the shortcomings of ESA on WS-353, we calculate Spearman ρ for the WS-353 set minus a single pair, for every pair. In Fig. 1 one can see the top 40 “most problematic” pairs: those whose removal increases ρ the most. By taking a closer look at them we can get hints for further improvements of the measure. 0 10 20 30 40 0.740 0.742 0.744 0.746 Rang Sp e a rma n  ρ  w h e n  re mo","vi","n","g","","o","n","e","","p","a","i","r","dol","l","ar buck","mil e kil omet er hot el  reservat","i","on school  cent","er cup t","abl","eware boy l","ad","maradona","f","oot","bal","l","secret","ary senat","e","summe","r drought","basebal","l","","season","money l","aundering","bread","but","t er","closet"," clot hes","day dawn hundred percent canyon","l","andscape money possession benchmark i","ndex","cent","ury year","f","uck sex","wednesday news","t","i","ger f","el","i","ne","psych","ol","ogy di","scipl","i","ne","cucumber pot","at","o","l","obster f","ood","bi","shop","rabbi","practi","ce i nsti t ut i on","energy l aborat ory","t","ype kind game round t i ger zoo start","year","mini","ster party","consumer energy","book paper","viewer serial","game","series","deat","h","i","nmat","e","admission","t","i","cket","t","i","ger carni","vore Spearman ρ of the complete set Figure 1: Spearman ρ when removing a single pair from WS-353.","First of all, we see pairs having a relation that is ontological in nature: “tiger / feline” (hyponym), “mile / kilometer” (coordinate terms, or “classmates” (Kuroda et al., 2010b)), “dollar / buck” (synonyms), etc. These relations are strong enough to justify the presence of the pairs in the test set, but do not necessarily imply high frequency of terms in common Wikipedia pages.","A good place for information of an ontological nature is WordNet. There have been several WordNet-based measures defined in the literature. When applying them3","to the WS-353 test set we get the following ρ: 3","In fact, these measures apply to synsets rather than to words. To avoid going through a sense-disambiguation process, we take the optimistic approach of using for each pair WNP (Path-based) 0.2873 WUP (Wu and Palmer, 1994) 0.1356 RES (Resnik, 1995) 0.2112 JCN (Jiang and Conrath, 1997) 0.3172 LCH (Leacock and Chodorow, 1998) 0.1437 HSO (Hirst and St-Onge, 1998) 0.1598 LIN (Lin, 1998) 0.1987 LESK (Banerjee and Pedersen, 2002) 0.1304","Despite the fact that JCN (which combines WordNet-graph calculations and word frequencies from a corpus4",") rates best when used alone, the measure which we are going to use is WNP, which gives the best results when combined with ESA (see below). This measure is based exclusively on the shortest-path distance in WordNet and hence is purely ontological. For example, the WNP-measure of “wood / forest” is 1 (synonyms), “bird / cock” is 0.5 (hypernym), “century / year” is 0.33, “bishop / rabbi” is 0.25, etc.","We found that this measure provides bad results in its lower range (since the path length between distant nodes strongly depends on the density of WordNet for each knowledge domain). To understand the behavior of ESA and WNP measures in their low ranges, we progressively remove pairs from WS-353 in order of increasing relatedness. 0 50 100 150 200 250 300 350 0.2 0.4 0.6 0.8 1.0 Nb of pairs removed (in order of increasing relatedness) Sp e a rma n  ρ WNP ESA Figure 2: The effect on Spearman ρ of the progressive removal of pairs in order of increasing relatedness, for ESA and WNP measures.","As we can see in Fig. 2, removing pairs in the small-value range of the measure strongly decreases ESA (which, after half of the pairs are removed, becomes chaotic), while the same operation steadily increases WNP. In other words, small-value pairs are crucial positive contributors for ESA, but rather negative contributors for WNP. For this reason, we use only the up-per range of WNP, and ignore its results for low-valued pairs. To achieve a smooth “fade-out” of WNP’s lower range we multiply it by a sigmoid logistic function. We","of words, the pair of senses which are the most closely re-","lated. Hence, if μ̂ is a synset-measure, s, s′","are synsets","and w, w′","words, we define the induced word-measure μ as","μ(w, w′",") := maxs∋w,s′","∋w′","μ̂(s, s′","). 4 For the distributional part of Jiang & Conrath, Resnik","and Lin, we use the Wikipedia 2011 corpus."]},{"title":"1398","paragraphs":["hence define a new measure","μEW(w1, w2) = μESA(w1, w2) · (1 + λσm,s(μWNP(w1, w2))), (1) where λ weights WNP with respect to ESA, m is the sigmoid inflection point (= a soft boundary of WNP’s lower range), s is the steepness of the sigmoid (small s makes the central part of the sigmoid closer to a vertical line), and “EW” stands for “ESA and WordNet.”","Calculations give the following optimal result: λ = 4.665, m = 0.26, s = 0.05 ρ = 0.7779 which surpasses the (Gabrilovich and Markovitch, 2007) ESA result of 0.75 by 5.2%. The parameter values have been obtained by gradient descent. In the next section we will further enhance this result by taking collocations into account."]},{"title":"4 Collocational Knowledge","paragraphs":["Returning to Fig. 1, we see that many “problematic” pairs are in fact collocations: “baseball / season,” “money / laundering”, “hundred / percent,” etc. We claim that the collocational nature of these word pairs has motivated their inclusion in WS-353. To show this, we calculated the collocation index (defined as","2#(w1w2) #(w1)+#(w2) ) of all WS-353 pairs5",". C o l l o ca t i o n  i n d e x (2  # (w w ' ) /  # (w ) + # (w ' )) 0.00 0.02 0.04 0.06 0.08 0.10 0.12 soap opera credi t  card","movie","t","heat","er","drug","abuse","movie","star","stock market","cel","l","","phone","t","enni","s racket","l","i","abi","l","i","t","y i","nsurance","comput","er sof t ware","gender equal i t y f amil y pl anni","ng weat","her f","orecast","champi","onshi","p","t","ournament","f","ood","preparat","i","on","energy crisis","money l","aundering","movie","crit","i","c","hundred","percent","deat","h","row 0.00 0.02 0.04 0.06 0.08 0.10 0.12","weapon","secret","cup","cof","f ee","compet i t i on price change","at","t","i","t","ude day summe","r","company stock","closet  clot hes f oot bal l  soccer book l","i","brary Figure 3: Top twenty direct (in gray) and inverse (in red) collocation indices for WS-353.","The primary goal of WS-353 is to evaluate relatedness measures, and these are symmetric by defini-tion (we always have μ(w1, w2) = μ(w2, w1)). If the word pairs were chosen on strictly semantic criteria, and if collocations were purely accidental, then we would have a roughly equal number of pairs (w1, w2) where w1w2 is a collocation and pairs where w2w1 is a collocation.","Fig. 3 shows that this is not the case: for the word pairs concerned, WS-353 developers have almost systematically chosen to write the words in the order in which they form a collocation. 5","We obtained WS-353 pair and word frequencies from the 53.45 billion-word GoogleBooks corpus (Michel et al., 2011). We considered only books published after 1970.","But neither ESA nor WNP recognize collocations: the former because of the bag-of-words principle underlying tfidf, and the latter only in the case where the collocational pair is a concept on its own. Indeed, most of the collocations in Fig. 3 are WordNet concepts (the exceptions being: “gender / equality,” “food / preparation,” “secret / weapon,” “energy / crisis,” etc.) but knowledge of that fact is not sufficient for ranking, since there is no mention in WordNet of the strength of the collocational relation.","We use the collocation index to further enhance our EW relatedness measure. -0.002 0.000 0.002 0.004 0.006 -1 5 -1 0 -5 log(1 + Spearman ρ difference when removing one pair) l o g (mi xe d  co l l o ca t i o n  i n d e x) f o r ξ =0 . 1 a d mi ssi","o","n","","t","i","cke","ta","l","u","mi","n","u","m","me","t","a","l","a","n","n","o","u","n","ce","me n t  n e w sa","n n o u n ce me","n","t","","w","a","rn","i","n","ga rch i t e ct u re","","ce","n","t","u","ry","a","t t e mp t  p e a ceb","a b y mo t h e r b a n k mo n e","y","b","a se b a l l  se a so","n b e d  cl o se","t","b","e","n","ch ma rk i n d","e","x","b","i","rd","","co","ck","b o a rd  re co mme","n","d","a","t","i","o","n","b o o k l i b ra ry b o o k p a p","e","r","b o xi n g  ro u n","d b o y l a d b re a d  b u t t e","r","b","ro","t","h","e","r","mo","n","k","ca n yo n  l a n d","sca","p","e","ca r a u t o mo b","i","l","e ce l l  p h o n e","ce","me","t e ry w o o d l a","n","d","ce","n t u ry n a t i o n ce n t u ry ye","a","r","ch","a","mp","i","o n sh i p  t o u rn","a","me","n","t","ch","a","n","g","e"," a t t i t u d e","cl","o","se","t","","cl","o","t","h","e","s co a st ","f","o","re","st","co","a","st","","sh","o","re","co mp a n y st o ck","co","mp e t i t i o n  p ri","ce","co","mp u t e r i n t e rn","e","t","co","mp u t e r ke yb","o","a","rd","co","mp u t e r l a b o ra","t","o","ry","co","mp u t e r n e w","s","co","mp u t e r so f t w","a","re","co n ce rt  vi rt u o","so","co","n","su","me r co n f i d e n","ce","co","n","su me r e n e rg","y","co u n t ry ci t i ze","n cre d i t  ca rd","cre d i t  i n f o rma","t","i","o","n","cu p  co f f e e cu p  d ri n k cu p  f o o d cu p  l i q u","i","d","cu","rre","n","cy","ma","rke t d a y d a w n d a y su mme","r","d e a t h  i n ma","t","e d e a t h  ro w d e l a y n e w","s","d","e ve l o p me n t ","i","ssu","e","d","i","re ct i o n  co mb","i","n","a","t","i","o","n","d i sa st e r a re","a","d","i","sco ve ry sp a","ce","d","i","vi d e n d  ca l cu","l","a","t","i","o","n","d","i vi d e n d  p a yme","n","t d o ct o r n u rse d o l l a r l o ssd o l l a r p ro f","i","t d ri n k e a r d ri n k e a t","d ri n k mo t h e","r d ri n k mo u t","h d ru g  a b u se e n e rg y cri si","s e n e rg y l a b","o","ra","t","o","rye","n","e","rg","y","se cre t a ry","e","q u i p me n t  ma","ke","r","e","xp e ri e n ce  mu","si","c","f","a","mi l y p l a n n i n","g f e rt i l i t y e g g f i ve  mo n t h f o cu s l","i","f","ef","o","o d  f ru i t","f","o o d  p re p a ra","t","i","o","n f o o t b a l l ","b","a","ske","t","b","a","l","lf","o","o","t","b","a","l","l","","so","cce","r","football tennis","f o re st  g ra ve","ya","rd f u rn a ce  st o","ve","g","a me  d e f e a t","g a me  ro u n d","g a me  se ri e s","g","a","me  t e a m g a me","","vi","ct","o","ry","g","e","n d e r e q u a l i t","y g l a ss me","t","a","lg","o","ve","rn","me n t  cri si s","g","o","ve rn o r o f f i ce","g","ro ce ry mo n","e","y","h","o","l","y","se","x","h o sp i t a l  i n f ra","st","ru","ct","u","re","h o t e l  re se rva","t","i","o","n","h","u n d re d  p e rce","n","t","i","ma g e  su rf a","ce","i","n","ve st i g a t i o n  e f","f","o","rt","j","o","u rn a l  a sso ci","a","t","i","o","n","j","o","u","rn","e","y","ca","r","ki","n","g","","q","u","e","e n ki","n","g","","ro","o","k","l","a w  l a w ye r l i a b i l i t y i n su","ra","n","ce life death","l","i","f","e"," l e sso n","l","i","f","e","","t","e","rm l i n e  i n su ra","n","ce liquid water","l i st i n g  ca t e g","o","ry l u xu ry ca","r","ma n  g o ve rn","o","r","ma n  w o ma n","me","d i a  g a i n me","d","i","a","","ra","d","i","o mi l e  ki","l","o","me","t","e","r","mi","n","i","st","e","r","p","a","rt","y","mi n i st ry cu l t u","re mo","n","e","y","b","a","n","kmo","n e y ca sh","mo","n","e","y cu rre n cy","mo n e y d e p o si","t","mo n e y d o l l a r","mo","n e y l a u n d e ri","n","g","mo","n e y o p e ra t i o","n","mo n e y p o sse ssi","o","n","mo n e y p ro p e rt","y","mo n e y w e a l t h","mo ra l i t y ma rri","a","g","e","mo vi e  cri t i c","mo vi e  p o p co","rn","mo vi e  st a r","mo vi e  t h e a t e r mu se u m","t","h","e","a","t","e","r","mu","si","c","p","ro","j","e","ct","n","a t u re  e n vi ro","n","me","n","t n a t u re  ma","n n e t w o rk","h","a","rd","w","a","ren","e","w","s","re p o rt o i l  st o ck","o p e ra  i n d u st","ry","o","p e ra  p e rf o rma","n","ce p e a ce  a t mo","sp","h","e","rep","e","a","ce  i n su ra n ce","p e a ce  p l a n","p","h o n e  e q u i p me","n","t","p","h","ysi cs ch e mi","st","ry p l a n e  ca r","p","l a n e t  mo o n","planet peoplep l a n e t  st","a","r p l a n e t  su n","p","o","p","u l a t i o n  d e ve","l","o","p","me","n","t","p","ra","ct i ce  i n st i t u t i","o","n","p","re ce d e n t  l a w","p","re","se rva t i o n  w o","rl","d","p ro b l e m a i rp","o","rt","p","ro","b","l e m ch a l l e n","g","e","p","ro d u ct i o n  cre","w","p","ro f e sso r d o ct","o","r p ro f i t  l o","ss","p","ro","f","i","t"," w a rn i n g","p","sych o l o g y cl i n","i","c","p","sych o l o g y d e p re","ssi","o","n","p","sych","o l o g y d i sci p","l","i","n","e","p","sych o l o g y f e a","r","p","sych o l o g y h e a","l","t","h","p","sych o l o g y mi","n","d","p","sych o l o g y sci","e","n","ce","re co rd  n u mb","e","r","re","p","o rt  g a i n ro ck j a zz sch","o","o","l","","ce","n","t","e","rse","ve n  se ri e s sh o re","","w","o","o","d","l","a","n","d","si","t","u","a","t","i","o n  i so l a t i o nski n  e ye","sma rt  st u d e n t so a p  o p e ra","sp a ce  ch e mi","st","ry","sp a ce  w o rl d st a rt  ma t ch st a rt  ye","a","rst","o","ck","l","i","f","e st o ck l i ve st o ck ma rke","t","st re e t  a ve n u","e","st re e t  b l o ck","st re e t  ch i l d re","n","st","re","e","t","","p","l","a","ce st ro ke ","h","o","sp","i","t","a","l","su","mme r d ro u g h","t","su","mme r n a t u re t e l e p h o n e ","co","mmu","n","i","ca","t","i","o","n","t","e","l","e","vi","si o n  f i l m","t","e l e vi si o n  ra d","i","o t e n n i s ra cke","t t e rri t o ry su","rf","a","ce","t h e a t e r h i st","o","ry t i g e r ca t t i g e r t i g e r t ra i n  ca r","t","ra","ve l  a ct i vi t y t re a t me n t ","re","co","ve","ry","t","yp","e","","ki","n","d vi ct i m","e","me","rg","e","n","cy vi d e o  a rch i","ve w a r t ro o p s","w a t e r se e p a","g","e","w e a p o n  se cre","t","w e a t h e r f o re","ca","st w o o d  f o re st","w","o rd  si mi l a ri t","y Figure 4: Collocation index vs. Spearman stability of EW. The red line is LOWESS polynomial regression (Cleveland, 1981).","Note that this index is not a measure (for example, the collocation index of “tiger / tiger” is not 1) and cannot be used directly as such.","How do collocational pairs contribute to the WS-353 Spearman ρ value? In Fig. 4 one can compare collocation index and Spearman stability (that is, the effect on ρ of the removal of a single word pair). Pairs located on the green vertical line are those whose removal does not affect Spearman ρ. Those on the right increase ρ when removed. We observe that most collocations are on the right; in other words, they are negative contributors. The most problematic ones are collocations which are not individual WordNet concepts (typical examples: “school / center,” “hotel / reservation,” “canyon / landscape,” etc.).","On the other hand, on the left side we find collocations that contribute positively to ρ: in many cases these have a strong ontological relation (“tiger / tiger,” “street / avenue,” “football / soccer,” etc.) which is probably the main reason for their positive contribution. The LOWESS polynomial regression line is quasi-horizontal, so we cannot infer whether or not collocation index is correlated with ρ.","An auxiliary question is whether collocation index values (at least in the high range) are correlated with the actual values of the WS-353 gold standard. Fig. 5 compares these two quantities. As we can see, LOWESS polynomial regression is almost steadily monotonically"]},{"title":"1399","paragraphs":["increasing, which shows that, although not a measure per se, (high-range) collocation index could be useful for relatedness measurement. -5 -4 -3 -2 -1 -7 -6 -5 -4 -3 log(WS-353 golden standard) l o g (mi xe d  co l l o ca t i o n  i n d e x) f o r ξ =0 . 1 soap opera credit card","movie theater drug abuse","movie star","stock market","cell phone tennis racket","liability insurance","computer software","gender equality family planning weather forecast championship tournament food preparation","energy crisis","money laundering movie critic","hundred percent death row","baseball season","dividend payment","consumer confidence aluminum metal","tiger cat computer keyboard liquid water","news report","admission ticket","hotel reservation","luxury car","peace plan network hardware telephone communication company stock","record number","summer drought train car movie popcorn television film Figure 5: Collocation index vs. WS-353 gold standard. The red line is LOWESS polynomial regression.","We combine the previously defined EW measure with collocation index, by defining measure EWC (= “ESA + WordNet + collocations”) as follows:","μEWC(w1, w2) = μESA(w1, w2) · (1 + λσm,s(μWNP(w1, w2))) · (1 + λ′","σ","m′ ,s′","(Cξ(w1, w2))),","where λ, m, s are as in (1), λ′",", m′","and s′","are similar,","and the mixed collocation index Cξ is defined as fol-","lows: Cξ(w1, w2) =","2#(w1w2) #(w1) + #(w2) + ξ","2#(w2w1) #(w1) + #(w2)","where #(.) is the frequency in the corpus. Calculations give the following optimal result: λ = 5.16, μ = 0.25, λ′","= 48.7, μ′","= 0.19, s = s′","= 0.05, ξ = 0.55 ρ = 0.7874 which is 1.2% higher than EW and, to the best of our knowledge, currently the highest result for WS-353 by a direct measure (not using a support vector machine). The parameter values have been obtained by gradient descent.","We can interpret this result as follows: the EWC measure works best when the lower fourth of WordNet measure and the lower fifth of collocation index values are ignored, and when inverse collocations count half as much as direct ones."]},{"title":"5 Supervised Approach Using an SVM","paragraphs":["(Agirre et al., 2009, p. 25) train an SVM on pairs of WS-353 pairs; this allows them to get an insight on performance increase obtained by combining various measures. By combining knowledge from a Web corpus and from WordNet, they obtain a highest value of Spearman ρ = 0.78. We calculated predictions of (4th degree polynomial) SVMs based on our EW and EWC","measures, and obtained the following results, using 10-","fold cross-validation: Measure Result EW (ESA + WNP) ρ = 0.7996 EWC (ESA + WNP + collocations) ρ = 0.8654","We observe that even without collocations we already get a better value than (Agirre et al., 2009), and also that the collocation component increases this value significantly, hence validating our choice of using collocational knowledge to enhance semantic relatedness measurement."]},{"title":"6 Pragmatic Knowledge","paragraphs":["This class contains pairs not captured by the previous methods. The typical example is “hotel / reservation”: its ESA value is very low, there is no ontological relation, and the collocation index is quite low as well. To capture the relatedness of such a pair, we need specific knowledge domain ontologies, providing relations such as “A is part of a functional process of system B” (in this case: “a ‘reservation’ is part of the process of renting a room in a ‘hotel’ ”). We leave this as an open task for future development."]},{"title":"7 Conclusion","paragraphs":["By combining two pre-existing semantic relatedness measures and by adding a component based on frequency of collocations, we have obtained a new measure that surpasses the one given in (Agirre et al., 2009) by 11% (when comparing results obtained by SVMs). We conjecture that this measure can further be enhanced by using pragmatic knowledge taken, for example, from specialized domain ontologies."]},{"title":"Appendices A Adapting ESA to 2011 Wikipedia","paragraphs":["The original (and unreleased) C++ ESA implementation (Gabrilovich and Markovitch, 2007) is based on 2005 Wikipedia data (2.2 GB) and achieves a Spearman ρ = 0.75. A later implementation in Python and Java (Çallı, 2010), based on the same corpus, achieves ρ = 0.74. We implemented ESA in Perl and similarly obtained ρ = 0.7404 when based on 2005 data. The same algorithms applied to 2011 data (31 GB), produced a disappointing ρ = 0.7047. Indeed, between 2005 and 2011, Wikipedia has evolved as follows:","2005 2011","#concepts 866,881 4,178,454","#terms/concept 96.1971 97.4243 where by “concepts” we mean Wikipedia pages in the main namespace, and by “terms,” distinct stemmed words.","Following advice by Gabrilovich (personal communication), we increased the generality of concepts by filtering Wikipedia pages by two criteria: minimum number of terms, and minimum number of in- and outgoing links. The original values were: 100 terms and 5 links; by requiring a minimum of 200 terms and 14"]},{"title":"1400","paragraphs":["links, we have attained the 2005 ρ value (more precisely: ρ = 0.7394). Fig. 6 displays ρ as a function of our two criteria. 100 150 200 250 300 0102030 0.7 0.705 0.71 0.715 0.72 0.725 0.73 0.735 0.74 0.745 Min # of words per page max=0.7394 orig=0.7047 Min # of links per page Spearman coefficient Figure 6: Adapting ESA to 2011 Wikipedia data by increasing the minimum number of distinct (stemmed) words and of in- and outgoing links per page.","In the following table, the column 2011 displays the results with original ESA setting, 2011* the ones with modified settings, df is mean document frequency of terms and term density is df","#concepts :","2005 2011 2011* #concepts 132,689 311,209 155,767 #terms/concept 165 279 414 #terms 187,971 503,368 408,299 df 116.3307 173.7199 159.0395 term density 0.00088 0.00056 0.00102","As we see, terms are less densely distributed in the 2011 corpus, since the increase of their mean document frequency, though important, is overruled by an even more important increase in the number of concepts. By more efficiently pruning concepts and leaving df relatively stable, we manage to increase term density anew and hence, enhance performance."]},{"title":"B Experiments","paragraphs":["(Giraud-Carrier and Dunham, 2010) emphasize the importance of sharing negative results. Responding to their call, here are some of our failed attempts at increasing ESA performance on the 2005 corpus. Note that the standard ESA value we challenge is ρ = 0.7404.","B.1 At the Word Level: Lemmatization and POS Filtering ESA removes stop words and words with fewer than three letters before applying the Porter stemmer thrice. Instead of stemming, we lemmatized and then applied two strategies: keeping only nouns and proper names (Penn tags NN, NNP, and plurals), or also verbs and adjectives (tags starting with NN, NNP, VB, and JJ). Here are the results obtained: Penn tags NN, NNS, NNP, NNPS ρ = 0.7194 Penn tags NN*, NNP*, VB*, JJ* ρ = 0.7178 The performance loss is due to lemmatization, proving once again that while Porter stemming may seem a brutal technique, it works better than anything else. Note that, surprisingly, when adding verbs and adjectives we get a (slightly) smaller ρ. B.2 Filtering at the Sentence Level We attempted to triple the weight of sentences containing either the page title, or one of the (non stop- )words of the page title, or one of the anchors point-ing to the page. This operation affected 1,399,165 sentences. Here are the results obtained: Tripling weight of selected sentences ρ = 0.7293 B.3 Filtering at the Section Level The idea is to avoid “historical sections” in pages describing current notions or objects. Historical sections are detected by a higher frequency of past-tense verbs, unless of course the whole page is of a historical nature, and hence using primarily the past tense. Let π = # past-tense verbs","# verbs for each Wikipedia page. We pruned sections of π ≥ 0.8 when the page had π < 0.8. We also pruned sections named “History,” “External links,” “References,” “See also,” “Further reading,” and “Bibliography.” This affected 111,028 sections out of 470,948. Here are the results obtained: Pruning of “historical” and other sections ρ = 0.6608"]},{"title":"C Implementation Details","paragraphs":["Implementation of ESA was done from scratch in Lex and Perl. To access WordNet v3, we used the Perl module WordNet::Similarity (Pedersen et al., 2004). SVM calculations as well as 2D figures were done in R, and the 3D figure in Matlab. For lemmatizing and POS-tagging, we used Tree-Tagger (Schmid, 1994). Our code is publicly available at http://omega2.enstb.org/yannis/ similarity.php."]},{"title":"D Acknowledgments","paragraphs":["We wish to thank Evgeniy Gabrilovich and Çağatay Çallı for their help in implementing ESA, and Sophia Ananiadou for her helpful advice."]},{"title":"References","paragraphs":["Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Paşca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19–27.","Eneko Agirre, Montse Cuadros, German Rigau, and Aitor Soroa. 2010. Exploring knowledge bases for similarity. In Proceedings of IJCAI, pages 373–377.","Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk algorithm for word sense disambigua-tion using WordNet. Springer Lecture Notes in Computer Science, 2276:136–145."]},{"title":"1401","paragraphs":["Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Comput. Linguist., 32:13–47, March.","Çağatay Çallı. 2010. Improving search result clustering by integrating semantic information from Wikipedia. Master’s thesis, Middle East Technical University, Ankara.","W.S. Cleveland. 1981. LOWESS: A program for smoothing scatterplots by robust locally weighted regression. The American Statistician, 35:54.","Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In Tenth International World Wide Web Conference (WWW10), Hong Kong, pages 406– 414.","Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In IJCAI’07: Proceedings of the 20th International Joint Conference on Artificial Intelligence.","Evgeniy Gabrilovich and Shaul Markovitch. 2009. Wikipedia-based semantic interpretation for natural language processing. Journal of Artificial Intelligence Research, 34(1):443–498.","Christophe Giraud-Carrier and Margaret H. Dunham. 2010. On the importance of sharing negative results. SIGKDD explorations, 12(2):3–4.","Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detec-tion and correction of malapropisms. In Christiane Fellbaum, editor, WordNet: An electronic lexical database, pages 305–332. The MIT Press.","Cha-Ren Huang, Nicoletta Calzolari, Aldo Gangemi, Alessandro Lenci, Alessandro Oltramari, and Laurent Prévot, editors. 2010. Ontology and the lexicon. Studies in Natural Language Processing. Cambridge.","Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings on International Conference on Research in Computational Linguistics, Taiwan 1997.","Kow Kuroda, Francis Bond, and Kentaro Torisawa. 2010a. Why Wikipedia needs to make friends with WordNet. In The 5th International Conference of the Global WordNet Association (GWC-2010).","Kow Kuroda, Jun’ichi Kazama, and Kentaro Torisawa. 2010b. A look inside the distributionally similar terms. In Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 40–49.","Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An electronic lexical database, pages 265– 283. The MIT Press.","Dekang Lin. 1998. An information-theoretic defini-tion of similarity. In Proceedings of 15th International Conference On Machine Learning, Madison WI, 1998.","Olena Medelyan, Catherine Legg, David Milne, and Ian H. Witten. 2008. Mining meaning from Wikipedia. Technical report, Department of Computer Science, University of Waikato, New Zealand.","Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and Erez Lieber-man Aiden. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176–182.","George A. Miller. 1995. WordNet: a lexical database for English. Commun. ACM, 38:39–41, November.","Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the relatedness of concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04), pages 1024–1025.","Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 192–199.","Simone Paolo Ponzetto and Michael Strube. 2007. Knowledge derived from Wikipedia for computing semantic relatedness. Journal of Artificial Intelligence Research, 30:181–212.","R Development Core Team, 2011. R: A Language and Environment for Statistical Computing. R Founda-tion for Statistical Computing, Vienna, Austria.","Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: Computing word relatedness using temporal semantic analysis. In WWW 2011.","Daniel Ramage, Anna N. Rafferty, and Christopher D. Manning. 2009. Random walks for text semantic similarity. In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, TextGraphs-4, pages 23–31, Stroudsburg, PA, USA. ACL.","Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, Montréal, pages 448–453.","Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, Manchester, UK.","Aaron D. Scriver. 2006. Semantic distance in WordNet: A simplified and improved measure of semantic relatedness. Master’s thesis, University of Waterloo.","Charles Spearman. 1904. The proof and measurement of association between two things. Amer. J. Psychol., 15:72–101.","Zhi-Biao Wu and Martha Palmer. 1994. Verb semantics and lexical selection. In Proceedings of the 32nd Annual Meetings of the Association for Computational Linguistics, pages 133–138.","Torsten Zesch, Iryna Gurevych, and Max Mühlhäuser. 2007. Analyzing and accessing Wikipedia as a lexical semantic resource. Preprint of the Technische Universität Darmstadt.","Torsten Zesch, Christof Müller, and Iryna Gurevych. 2008. Extracting lexical semantic knowledge from Wikipedia and Wiktionary. In Proceedings of the 6th International Conference on Language Resources and Evaluation."]},{"title":"1402","paragraphs":[]}]}