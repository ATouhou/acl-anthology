{"sections":[{"title":"Combining Context Features by Canonical Belief Network for Chinese Part-Of-Speech Tagging Hongzhi Xu and Chunping Li School of Software, Tsinghua University Key Laboratory for Information System Security, Ministry of Education China xuhz05@mails.tsinghua.edu.cn cli@tsinghua.edu.cn Abstract","paragraphs":["Part-Of-Speech(POS) tagging is the essential basis of Natural language processing(NLP). In this paper, we present an algorithm that combines a variety of context features, e.g. the POS tags of the words next to the word a that needs to be tagged and the context lexical information of a by Canonical Belief Network to together determine the POS tag of a. Experiments on a Chinese corpus are conducted to compare our algorithm with the standard HMM-based POS tagging and the POS tagging software ICTCLAS3.0. The experimental results show that our algorithm is more effective."]},{"title":"1 Introduction","paragraphs":["Part-Of-Speech(POS) tagging is the essential basis of Natural language processing(NLP). It is the process in which each word is assigned to a correspond-ing POS tag that describes how this word be used in a sentence. Typically, the tags can be syntactic categories, such as noun, verb and so on. For Chinese language, word segmentation must be done before POS tagging, because, different from English sentences, there is no distinct boundary such as white space to separate different words(Sun, 2001). Also, Chinese word segmentation and POS tagging can be done at the same time(Ng, 2004)(Wang, 2006).","There are two main approaches for POS tagging: rule-based and statistical algorithms(Merialdo, 1994). Rule based POS tagging methods extratct rules from training corpus and use these rules to tag new sentences(Brill, 1992)(Brill, 1994). Statistic-based algorithms based on Belief Network(Murphy, 2001) such as Hidden-Markov-Model(HMM)(Cutting, 1992)(Thede, 1999), Lexicalized HMM(Lee, 2000) and Maximal-Entropy model(Ratnaparkhi, 1996) use the statistical information of a manually tagged corpus as background knowledge to tag new sentences. For example, the verb is mostly followed by a noun, an adverb or nothing, so if we are sure that a word a is a verb, we could say the word b following a has a large probability to be a noun. This could be helpful specially when b has a lot of possible POS tags or it is an unknown word.","Formally, this process relates to P r(noun|verb), P r(adverb|verb) and P r(nothing|verb), that can be estimated from the training corpus. HMM-based tagging is mainly based on such statistical information. Lexicalized HMM tagging not only considers the POS tags information to determine whether b is noun, adverb or nothing, but also considers the lexical information a itself. That is, it considers the probabilities P r(noun|a, verb), P r(adverb|a, verb) and P r(nothing|a, verb) for instance. Since combining more context information, Lexicalized HMM tagging gets a better performance(Lee, 2000).","The main problem of Lexicalized HMM is that it suffers from the data sparseness, so parameter smoothing is very important. In this paper, we present a new algorithm that combines several context information, e.g. the POS tags information and lexical information as features by Canonical Belief Network(Turtle, 1991) to together determine the tag"]},{"title":"907","paragraphs":["of a new word. The experiments show that our algorithm really performs well. Here, we don’t explore Chinese word segmentation methods, and related information can be found in(Sun, 2001).","The rest of the paper is organized as follows. In section 2 and section 3, we describe the standard HMM-based tagging and Lexicalized HMM tagging respectively which are relevant to our algorithm. In section 4, we describe the Belief Network as a preliminary. In section 5, we present our algorithm that is based on Canonical Belief Network. Section 6 is the experiments and their results. In section 7, we have the conclusion and the future work."]},{"title":"2 Standard Hidden Markov Model","paragraphs":["The problem of POS tagging can be formally defined as: given an observation(sentence) w = {w1, w2, ..., wT } and a POS tag set T S = {t1, t2, ..., tM }, the task is to find a tag sequence t = {t1, t2, ..., tT }, where ti ∈ T S, that is the most possible one to explain the observation. That is to find t to maximize the probability P r(t|w). It can be rewritten by Bayesian rule as follows. P r(t|w) =","P r(w|t) × P r(t) P r(w) As for any sequence t, the probability P r(w) is constant, we could ignore P r(w). For P r(t), it can be decomposed by the chain rule as follows.","P r(t) = P r(t1, t2, ..., tT ) = P r(t1) × P r(t2|t1) × P r(t3|t1, t2)×","... × P r(tT |t1, t2, ..., tT −1) Through this formula, we could find that the calculation is impossible because of the combination explosion of different POS tags. Generally, we use a n-gram especially n = 2 model to calculate Pr(t) approximately as follows.","P r(t) = P r(t1|t0) × P r(t2|t1) × P r(t3|t2)× ... × P r(tT |tT −1) where t0 is nothing. For P r(w|t), with an independent assumption, it can be calculated approximately as follows.","P r(w|t) = P r(w1|t1) × P r(w2|t2) × P r(w3|t3) ... × P r(wT |tT ) Usually, the probability P r(ti|ti−1) is called transition probability, and P r(wi|ti) is called the emission probability. They both can be estimated from the training set. This means that the tag ti of word wi is only determined by the tag ti−1 of word wi−1. So, we could find the best sequence through a for-ward(left to right) process.","If we state all possible POS tags(stats) of each word and connect all possible ti−1 with all possible ti and each edge is weighted by P r(ti|ti−1), we could get a Directed Acyclic Graph(DAG). The searching process(decoding) that is involved in finding t that maximizes P r(t|w) can be explained as finding the path with the maximal probability. For this sub task, Viterbi is an efficient algorithm that can be used(Allen, 1995)."]},{"title":"3 Lexicalized Hidden Markov Model","paragraphs":["Lexicalized HMM is an improvement to the standard HMM. It substitutes the probability P r(ti|ti−1) with P r(ti|ti−J,i−1, wi−L,i−1), and the probability P r(wi|ti) with P r(wi|ti−K,i, wi−I,i−1). In other words, the tag of word wi is determined by the tags of the J words right before wi and L words right before wi. It uses more context information of wi to determine its tag.","However, it will suffer from the data sparseness especially when the values of J , L, K and I are large, which means it needs an explosively larger training corpus to get a reliable estimation of these parameters, and smoothing techniques must be adopted to mitigate the problem. Back-off smoothing is used by Lexicalized HMM. In the back-off model, if a n-gram occurs more than k times in training corpus, then the estimation is used but discounted, or the estimation will use a shorter n-gram e.g. (n-1)-gram estimation as a back-off probability. So, it is a recursive process to estimate a n-gram parameter."]},{"title":"4 Belief Network","paragraphs":["Belief Network is a probabilistic graphical model, which is also a DAG in which nodes represent random variables, and the arcs represent conditional independence assumptions. For example, the probability P r(A, B) = P r(A) × P r(B|A) can be depicted as Figure 1(a), and if we decompose"]},{"title":"908","paragraphs":["% $ & D % $ & E % $ $ % F G Figure 1: Some Belief Networks. P r(A, B) = P r(B) × P r(A|B), it can be depicted as Figure 1(b). Similarly, the probability P r(A, B, C) = P r(A) × P r(B|A) × P r(C|A, B) can be depicted as Figure 1(c).","As we have analyzed above, such decomposition would need us to estimate a large amount of parameters. In the belief network, a conditional independence relationship can be stated as follows: a node is independent of its ancestors given its parents, where the ancestor/parent relationship is with respect to some fixed topological ordering of the nodes. For example, if we simplify the graph Figure 1(c) to graph Figure 1(d), it is equivalent to the decomposition: P r(A, B, C) = P r(A) × P r(B|A) × P r(C|B), which is actually the same as that of HMM. More details about Belief Network can found in(Murphy, 2001)."]},{"title":"5 Canonical Belief Network Based Part-Of-Speech Tagging 5.1 Canonical Belief Network","paragraphs":["Canonical Belief Network was proposed by Turtle in 1991(Turtle, 1991), and it was used in information retrieval tasks. Four canonical forms are presented to combine different features, that is and, or, wsum and sum to simplify the probability combination further. With the and relationship, it means that if a node in a DAG is true, then all of its parents must be true. With the or relationship, it means that if a node in a DAG is true, then at least one of its parents is true. With the wsum relationship, it means that if a node in a DAG is true, it is determined by all of its parents and each parent has a different weight. With the sum relationship, it means that if a node in a DAG is true, it is determined by all of its parents and each parent has an equal weight.","For example, we want to evaluate the probability P r(D|A) or P r(D = true|A = true), and $ &% ’ DQG D $ &% ’ RU E $ &% ’ ZVXP F $ &% ’ VXP G Figure 2: Canonical Belief Networks for P r(A, B, C, D). node D has two parents B and C, we could use the four canonical forms to evaluate P r(D|A) as shown in Figure 2. Suppose that P r(B|A) = p1 and P r(C|A) = p2, with the four canonical form and, or, wsum and sum, we could get the following estimations respectively. Pand(D|A) = p1 × p2 Por(D|A) = 1 − (1 − p1) × (1 − p2) Pwsum(D|A) = w1p1 + w2p2 Psum(D|A) = (p1 + p2)/2 The standard Belief Network actually supposes that all the relationships are and. However, in real world, it is not the case. For example, we want to evaluate the probability that a person will use an umbrella, and there are two conditions that a person will use it: raining or a violent sunlight. If we use the standard Belief Network, it is impossible to display such situation, because it could not be raining and sunny at the same time. The or relationship could easily solve this problem. 5.2 Algorithm Description Definition: A feature is defined as the context information of a tag/word, which can be POS tags, words or both. For example, {Ti−J , ..., Ti−1} is a feature of tag ti, {Ti−J , ..., Ti} is a feature of word wi, {Ti−J , ..., Ti−1, Wi−L, ..., Wi−1} is a feature of tag ti, {Ti−K , ..., Ti, Wi−I , ..., Wi−1} is a feature of word wi.","In our algorithm, we select 6 features for tag ti, and select 2 features for word wi, which are shown in Table 1. We can see that f 1","t , f 2","t and f 3","t are actually","the n-gram features used in HMM, f 4","t , f 5","t and f 6","t are actually features used by lexicalized HMM.","We adopt the canonical form or to combine them as shown in Figure 3, and use the canonical form"]},{"title":"909","paragraphs":["Features f 1 t : Ti−3, Ti−2, Ti−1 f 2 t : Ti−2, Ti−1","ti f 3 t : Ti−1 f 4 t : Ti−3, Ti−2, Ti−1, Wi−3, Wi−2, Wi−1 f 5 t : Ti−2, Ti−1, Wi−2, Wi−1 f 6 t : Ti−1, Wi−1","wi f 1 w: Ti−1, Ti f 2 w: Ti Table 1: Features used for ti and wi. and to combine features of ti and wi. Because we think that the POS tag of a new word can be determined if any one of the features can give a high confidence or implication of a certain POS tag. The probabilities P r(f i","t |ti−1), i = 1, ..., 6. are all 1, which means that all the features in the Canonical Belief Network are considered to estimate the tag ti of word wi when we have already estimated the tag ti−1 of word wi−1. So, the transition probability could be calculated as follows. ptrans i−1,i = 1 − 6∏ j=1 [1 − P r(ti|f j t )] In the same way, the probabilities P r(f i","w|ti), i = 1, 2. are all 1. The emission probability could be calculated as follows. pomit i = 1 − 2∏ j=1","[1 − P r(wi|f j w)]","Let’s return to the POS tagging problem which needs to find a tag sequence t that maximizes the probability P r(t|w), given a word sequence w defined in Section 2. It is involved in evaluating two probabilities P r(t) and P r(w|t). With the Canonical Belief Network we just defined, they could be calculated as follows. P r(t) =","∏T i=1 ptrans","i−1,i P r(w|t) = ∏T i=1 pomit","i","P r(w, t) = P r(t) × P r(w|t) The canonical form or would not suffer from the data sparseness even though it refers to 4-gram, because if a 4-gram feature(f 1","t for example) doesn’t appear in the training corpus, the probability IW RU","IW IW"," I","W IW"," I W WL WL RU IZ ZL IZ D E WL Figure 3: Canonical Belief Networks used in our algorithm. P r(ti|f 1","t ) is estimated as zero, which means the feature contributes nothing to determine the probability that word wi gets a tag ti, which is actually determined by a lower n-grams. Cases are the same for 3-gram, 2-gram and so on. In a special case, when a 4-gram (f 4","t for example) appears in the training corpus and appears only once, the probability P r(ti|f 1","t ) will be 1, which means that the sentence or phrase we need to tag may have appeared in the training corpus, so we can tag the sentence or phrase with reference to the appeared sentence or phrase in the training corpus. This is an intuitional comprehension of our algorithm and its motivation. Decoding: The problem of using high n-gram is the combination explosion especially for high grams. For example, consider the feature , suppose one word has 3 possible tags on average, then we have to evaluate 33","= 27 cases for f 1","t , further, different features could get different combinations and the number of combinations will be 272","× 92","× 32","= 531441. To solve the problem, we constrain all features to be consistent. For example, the tag ti−1 of feature f 1","t must be same as that of feature f 2","t , f 3","t ,","f 4","t , f 5","t and f 6","t at one combination. The following","features are not consistent, because the ti−1 in f 1","t is","V BP , while the ti−1 in f 4 t is N N .","f 1","t = J J, N N S, V BP","f 4","t = J J, N N S, N N, little, boys, book This will decrease the total combination to 33","= 27. We use a greedy search scheme that is based on the classic decoding algorithm Viterbi. Suppose that the Viterbi algorithm has reached the state ti−1, to calculate the best path from the start to ti, we only use the tags on the best path from the start to ti−1 to calculate the probability. This decreases the total com-"]},{"title":"910","paragraphs":["bination to 3(the number of possible tags of ti−1), which is the same as that of standard HMM."]},{"title":"6 Experiments Dataset","paragraphs":[": We conduct our experiments on a Chinese corpus consisting of all news from January, 1998 of People’s Daily, tagged with the tag set of Peking University(PKU), which contains 46 POS tags1",". For the corpus, we randomly select 90% as the training set and the remaining 10% as the test set. The corpus information is shown in Table 2, where unknown words are the words that appear in test set but not in training set. The experiments are run on a machine with 2.4GHZ CPU, and 1GB memory.","Training set Test set Words 1021592 112321 Sentences 163419 17777","Unknow words 2713 Table 2: Chinese corpus information.","Unknown Words: In our experiments, we first store all the words with their all possible POS tags in a dictionary. So, our algorithm gets all possible tags of a word through a dictionary. As for the word in the test set that doesn’t appear in the training set, we give the probability P r(wi|f j","w) value 1, with all j. This processing is quite simple, however, it is enough to observe the relative performances of different POS taggers.","For Chinese word segmentation, we use the segmentation result of ICTCLAS3.02",". The segmentation result is shown in Table 3. Sen-Prec is the ratio of the sentences that are correctly segmented among all sentences in the test set. Precision Recall F1 Sen-Prec 0.9811 0.9832 0.9822 0.9340 Table 3: Segmentation Result by ICTCLAS.","Open Test: We compare the POS tagging performance of our algorithm with the standard HMM, 1 http://icl.pku.edu.cn/Introduction/corpustagging.htm 2 ICTCLAS3.0 is a commercial software developed by Insti-","tute of Computing Technology, Chinese Academy of Science,","that is used for Chinese word segmentation and POS tagging. and ICTCLAS3.0. The experimental result is shown in Table 4. Prec-Seg is the POS tagging precision on the words that are correctly segmented. Prec-Sen is the ratio of the sentences that are correctly tagged among all sentences in the test set. Prec-Sen-Seg is the ratio of sentences that are correctly tagged among the sentences that are correctly segmented.","With the experiments, we can see that, our algorithm always gets the best performance. The ICT-CLAS3.0 doesn’t perform very well. However, this is probably because of that the tag set used by ICT-CLAS3.0 is different from that of PKU. Even though it provides a mapping scheme from their tags to PKU tags, they may be not totally consistent. The published POS tagging precision of ICTCLAS3.0 is 94.63%, also our algorithm is a little better. This has proved that our algorithm is more effective for POS tagging task.","ICTCLAS HMM CBN Precision 0.9096 0.9388 0.9465","Recall 0.9115 0.9408 0.9485","F1 0.9105 0.9398 0.9475 Prec-Seg 0.9271 0.9569 0.9647 Prec-Sen 0.6342 0.7404 0.7740","Prec-Sen-Seg 0.6709 0.7927 0.8287 Table 4: Open test comparison result on Chinese corpus.","Close Test: As we have analyzed above in Section 5.2 that our algorithm takes advantage of more information in the training set. When a sentence or a phrase appears in the training set, it will help a lot to tag the new sentence correctly. To test whether this case really happens, we conduct a new experiment that is the same as the first one except that the test set is also added to the training set. The experimental result is shown in Table 5. We can see that the performance of our algorithm is greatly improved, while the HMM doesn’t improve much, which further proves our analysis.","Even though our algorithm gives a satisfying performance, it may be able to be improved by adopt-ing smoothing techniques to take advantage of more useful features, e.g. to make the probabilities such as P r(ti|f 1","t ), P r(ti|f 2","t ) not be zero. In addition, the adoption of techniques to deal with unknown words"]},{"title":"911","paragraphs":["ICTCLAS HMM CBN Precision 0.9096 0.9407 0.9658","Recall 0.9115 0.9427 0.9678","F1 0.9105 0.9417 0.9668 Prec-Seg 0.9271 0.9588 0.9843 Prec-Sen 0.6342 0.7476 0.8584","Prec-Sen-Seg 0.6709 0.8004 0.9191 Table 5: Close test comparison result on Chinese corpus. and techniques to combine with rules may also improve the performance of our algorithm. If we have a larger training corpus, it may be better to remove some confusing features such as f 3","t and f 2","w, because they contain weak context information and this is why a higher n-gram model always performs better than a lower n-gram model when the training corpus is large enough. However, this should be validated further."]},{"title":"7 Conclusion and Future Work","paragraphs":["In this paper, we present a novel algorithm that combines useful context features by Canonical Belief Network to together determine the tag of a new word. The ’or’ node can allow us to use higher n-gram model although the training corpus may be not sufficient. In other words, it can overcome the data sparseness problem and make use of more information from the training corpus. We conduct experiments on a Chinese popular corpus to evaluate our algorithm, and the results have shown that it is powerful even in case that we don’t deal with the unknown words and smooth the parameters.","We think that our algorithm could also be used for tagging English corpus. In addition, we only extract simple context information as features. We be-lieve that there exists more useful features that can be used to improve our algorithm. For example, the syntax analysis could be combined as a new feature, because a POS sequence may be illegal even though it gets the maximal probability through our algorithm. Yet, these will be our future work.","Acknowledgement This work was supported by Chinese 973 Research Project under grant No. 2002CB312006."]},{"title":"References","paragraphs":["Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Proc. of the Empirical Methods in Natural Language Processing Conference(EMNLP’96), 133-142.","Bernard Merialdo. 1994. Tagging English Text with a Probabilistic Model. Computational Linguistics, 20(2):155–172.","Doug Cutting, Julian Kupied, Jan Pedersen and Penelope Sibun. 1992. A Practical part-of-speech tagger. In Proceedings of the 3rd Conference on Applied Natural Language Processing(ANLP’92), 133-140.","Eric Brill. 1992. A simple rule-based part of speech tag-ger. In Proc. of the 30th Conference on Applied Computational Linguistics(ACL’92), Trento, Italy, 112-116.","Eric Brill. 1994. Some Advances in Transformation-Based Part of Speech Tagging. In Proc. of the 12th National Conference on Artificial Intelligence(AAAI’94), 722-727.","Howard Turtle and W. Bruce Croft. 1991. Evaluation of an Inference Network-Based Retrieval Model. ACM Transactions on Information Systems, 9(3):187-222.","Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once? Word-Based or Character-Based?. In Proc. of the Empirical Methods in Natural Language Processing Conference(EMNLP’04).","James Allen. 1995. Natural Language Understanding. The Benjamin/Cummings Publishing Company.","Kevin P. Murphy. 2001. An introduction to graphical models. Technical report, Intel Research Technical Report.","Maosong Sun and Jiayan Zou. 2001. A critical appraisal of the research on Chinese word segmentation(In Chinese). Contemporary Linguistics, 3(1):22-32.","Mengqiu Wang and Yanxin Shi. 2006. Using Part-of-Speech Reranking to Improve Chinese Word Segmentation. In Proc. of the 5th SIGHAN Workshop on Chinese Language Processing, 205-208.","Sang-Zoo Lee, Jun-ichi Tsujii and Hae-Chang Rim. 2000. Lexicalized Hidden Markov Models for Part-of-Speech Tagging. In Proc. of 18th International Conference on Computational Linguistics(COLING’00), Saarbrucken, Germany, 481-487.","Scott M. Thede and Mary P. Harper. 1999. A Second-Order Hidden Markov Model for Part-of-Speech Tagging. In Proc. of the 37th Conference on Applied Computational Linguistics(ACL’99), 175-182."]},{"title":"912","paragraphs":[]}]}