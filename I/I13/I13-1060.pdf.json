{"sections":[{"title":"","paragraphs":["International Joint Conference on Natural Language Processing, pages 525–533, Nagoya, Japan, 14-18 October 2013."]},{"title":"Learning a Product of Experts with Elitist Lasso Mengqiu Wang Computer Science Department Stanford University Stanford, CA 94305, USA mengqiu@cs.stanford.edu Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305, USA manning@cs.stanford.edu Abstract","paragraphs":["Discriminative models such as logistic regression profit from the ability to incorporate arbitrary rich features; however, complex dependencies among overlapping features can often result in weight undertraining. One popular method that attempts to mitigate this problem is logarithmic opinion pools (LOP), which is a specialized form of product of experts model that automatically adjusts the weighting among experts. A major problem with LOP is that it requires significant amounts of domain expertise in designing effective experts. We propose a novel method that learns to induce experts — not just the weighting between them — through the use of a mixed l2l1 norm as previously seen in elitist lasso. Unlike its more popular sibling l1l2 norm (used in group lasso), which seeks feature sparsity at the group-level, l2l1 norm encourages sparsity within feature groups. We demonstrate how this property can be leveraged as a competition mechanism to induce groups of diverse experts, and introduce a new formulation of elitist lasso MaxEnt in the FOBOS optimization framework (Duchi and Singer, 2009). Results on Named Entity Recognition task suggest that this method gives consistent improvements over a standard logistic regression model, and is more effective than conventional induction schemes for experts."]},{"title":"1 Introduction","paragraphs":["Conditionally trained discriminative models like logistic regression (a.k.a., MaxEnt models) gain power from their ability to incorporate a large number of arbitrarily overlapping features. But such models also exhibit complex feature dependencies and therefore are susceptible to the problem of weight undertraining — the contributions of certain features are overlooked because of features they co-occur with (Sutton et al., 2007; Hinton et al., 2012).","One popular method that attempts to mitigate this problem is logarithmic opinion pools (LOP) (Heskes, 1998; Smith et al., 2005; Sutton et al., 2007). LOP works in a similar fashion to a product of experts model, in which a number of experts are individually assembled first, and then their predictions are combined multiplicatively to create an ensemble model. Experts are generated by first partitioning the feature space into subsets, then an independent model (expert) is trained on each subset of features (Sutton et al., 2007). Under the assumption that strongly correlated features are partitioned into separate subsets, this method effectively forces the models to learn how to make predictions with each subset of the features independently. It also helps in scenarios where some strong features stop other features from getting weight, a problem known as feature co-adaptation. The theoretical justification for favoring a product ensemble over an additive ensemble is that inference with a product of log-linear models is significantly easier than inference with a sum. Sutton et al. (2007) also suggests that product ensembles give better results than additive mixtures on sequence labeling tasks.","Smith et al. (2005) and Sutton et al. (2007) showed that the quality of experts and diversity among them have a direct impact on the final performance of the ensemble. Designing effective and diverse experts was left as an art, and requires 525 a great deal of domain expertise.","In this paper, we directly learn to induce diverse experts by using a mixed l2l1 norm known as elitist lasso in the context of generalized linear models (Kowalski and Torrésani, 2009). We design a novel competition mechanism to encourage experts to use non-overlapping feature sets, effectively learning a partition of the feature space. Efficient optimization of a maximum conditional likelihood objective with respect to l2l1 norm is non-trivial and has not been previously studied. We propose a novel formulation to incorporate l2l1 norm in the FOBOS optimization framework (Duchi and Singer, 2009). Experiments on Named Entity Recognition task suggest that our proposed method gives consistent improvements over a baseline MaxEnt model and conventional LOP experts. In particular, our method gives the most improvements in recognizing entity types for out-of-vocabulary words."]},{"title":"2 MaxEnt Model","paragraphs":["Given an input sequence x (e.g., words in a sentence), a MaxEnt model defines the conditional probability of an output variable y (e.g., named-entity tag of a word) as follows: P (y = vi|x) = exp ( K∑ k=1 θk(vi)fk(x) ) V∑ j=1 exp ( K∑ k=1 θk(vj)fk(x) ) where fk(x) is the kth input feature, K is the total number of input features, and θk(y) is the weight parameter associated with feature fk for output class vi. We denote the output classes of y to be {v1, v2, · · · , vV }, where V is the total number of output classes.","It is helpful to consider the connection between a MaxEnt model and a Linear Network model commonly seen in the neural network literature, by re-expressing this objective using a softmax activation function. The softmax function is defined as: softmax (q(y)) = exp (q(y))","∑ y′ exp (q(y′",")) in our case q(y) = K∑ k=1 θk(y)fk(x), and we have: P (y|x) = softmax","( K ∑ k=1 θk(y)fk(x) ) f1(x) f2(x) y = v1 y = v2 θ1(v1) θ2(v2) softmax Input Output Softmax MaxEnt f1(x) f2(x) y = v1 y = v2 θe1 1 (v1) θe1 1 (v2) θe2 2 (v1) θe2 2 (v2) softmax α1 α1 α2 α2 e1 e2 Input Expert Output Softmax LOP f1(x) f2(x) y = v1 y = v2 θe1 1 (v1) θe2 1 (v1) θe2 2 (v1) θe2 2 (v2) softmax α1 α1 α2 α2 e1 e2 Input Expert Output Softmax elitist LOP Figure 1: The top part shows a regular MaxEnt model with 2 features and 2 output classes. The middle part shows a LOP with two experts. α1 and α2 are the expert mixing coefficients to be learned in the LOP model. The bottom part shows an elitist LOP expert induction model with two experts. For each feature weight parameter θ, the superscript denotes which feature group it belongs to (e.g., θe1","belongs to expert e","1); the subscript denotes which input feature it belongs to (e.g., θ1 means it applies to input feature f1(x)); and the value in parentheses denotes which output class this feature is associated with (e.g., θ(v1) applies to output class y = v1). Features that belong to the same group are shown in the same color. 526 We visualize this using a neural network style diagram in the top part of Figure 1. In this diagram, the value at each node is computed as the sum of all incoming edge weights (parameters in the model) multiplied by the values of nodes on the originating end of edges. In this example, K = 2 and we have two input features (f1(x) and f2(x)); there are two possible value assignments for y (v1 and v2). Correspondingly, there are four weight parameters (θi(vj), for i = 1, 2, j = 1, 2) as shown by the directed edge going from “Input” layer nodes to “Output” layer nodes. We apply a softmax function to the “Output” layer to produce the probability assignment of each output class."]},{"title":"3 Weight Undertraining","paragraphs":["The problem of weight undertraining occurs in discriminatively trained classifiers when some strongly indicative features occur during training but not at test time. Because of the presence of such strong features during training, weaker features that occur at both training and test time do not receive enough weight, and thus impede the classifier from reaching its full potential at test time. This phenomenon is also known as feature co-adaptation (Hinton et al., 2012). Sutton et al. (2007) gave an excellent demonstration of this problem using a synthetic logistic regression model. In their example, the output function is the softmax of the linear sum of a set of weights x1 to xn, where each of the xi acts as a “weak” predictor. A “strong” feature which takes the form of x′","= ∑n","i=1 xi+N (where N is an added Gaussian noise) is then added to the classifier during training but removed at test time, which simulates a feature co-adaptation scenario. Simulation results show that classifier accuracy can drop by as much as 40% as a result of weight undertraining.","This problem is difficult to combat because we do not know a priori what set of features will be present at test time. However, a number of methods including feature bagging (Sutton et al., 2007), random feature dropout (Hinton et al., 2012), and logarithmic opinion pools (Heskes, 1998; Smith et al., 2005) have been introduced as ways to alleviate weight undertraining."]},{"title":"4 Logarithmic Opinion Pools","paragraphs":["The idea behind logarithmic opinion pools (LOP) is that instead of training one classifier, we can train a set of classifiers with non-overlapping or competing feature sets. At test time, we can combine these classifiers and average their results. By creating a diverse set of classifiers and training each of them separately, we can potentially re-duce the effect of weight undertraining. For example, when two strongly correlated features are partitioned into different classifiers, they do not overshadow each other. Unlike voting or feature bagging where an additive ensemble of experts is used, LOP derives a joint model by taking a product of experts.1","The conditional probability of a LOP ensemble can be expressed as: PLOP (y|x) =","∏n j=1 [Pj(y|x)]αj","∑ y′","∏n j=1 [Pj(y|x)]αj Pj(y|x) = softmax","( K ∑ k=1 θ ej k (y)fk(x) ) where n is the total number of experts, Pj(y|x) is the probability assignment of expert j, and αj is the mixing coefficient for this expert.","The experts are typically selected so that they capture different signals from the training data (e.g., via feature subsetting). Each expert is trained separately, and their model parameters θ ej k (y) are fixed during LOP combination.","Early work on LOPs either fixed the expert mixing coefficients to some uniformly assigned value (Hinton, 1999), or used some arbitrary handpicked values (Sutton et al., 2007). In both of these two cases, no new parameter values need to be learned, therefore no extra training is required.","Smith et al. (2005) proposed to learn the weights by maximizing log-likelihood of the ensemble product model, and showed that the learned weights work better than uniformly assigned values.","We can visualize the LOP again using a network diagram, as shown in the middle part of Figure 1. Two experts are shown in this diagram, each represented by a plate in the “Expert” layer. This example illustrates an expert generation scheme by partitioning the features into non-overlapping subsets – expert e1 has feature function f1(x) while expert e2 has feature function f2(x). The weights of the two experts are pre-trained and remain fixed. Training the LOP model involves only learning the α mixing parameters.","1","A product in the raw probability space is equivalent to a sum in the logarithmic space, and hence the name “logarithmic” opinion pools. 527"]},{"title":"5 Automatic Induction of Experts","paragraphs":["A major concern in adopting existing LOP methods is that there are no straightforward guidelines on how to create experts. Different feature partition schemes can result in dramatically different performance (Smith et al., 2005). A successfully designed LOP expert ensemble typically involves non-trivial engineering effort and a significant amount of domain expertise. Furthermore, the improvements shown in one application domain by a well-designed feature partition scheme do not necessarily carry over to other domains.","Therefore, it is our goal to investigate and search for effective methods to automatically induce LOP experts. We would like to find a set of experts that are diverse (i.e., having non-overlapping feature sets) and accurate. The main idea here is that we can train multiple copies of MaxEnt models together to optimize training likelihood, but at the same time we encourage the MaxEnt models to differ as much as possible in their choice of features to employ. To meet this end, we use a l2l1 mixed norm to regularize a LOP, and denote the new model as elitist LOP. 5.1 Elitist LOP Before we introduce the l2l1 norm, which has not been frequently used in NLP research, let us revisit a closely related but much better known sibling – the l1l2 norm used in group lasso.","For a set of model parameters θ, assume we have some feature grouping that partitions the features into G sub-groups. For simplicity, let us further assume that each feature group has the same number of features (M ). We denote the index of each feature by g (for “group”) and m (for “member”), and use θĝ to denote the feature group of index g.","The l1l2 mixed norm used in group lasso takes the following form: ∥θ∥1,2 = "," G ∑ g=1 ( M ∑ m=1 |θg,m|2 )1/2  = ∥(∥θ1̂∥2, · · · , ∥θ Ĝ∥2)∥1","This norm applies l2 regularization to each group of features, but enforces l1 regularization at the group level. Since l1 norm encourages sparse solutions, therefore the effect of the mixed norm is to sparsify features at the group level by eliminat-ing a whole group of features at a time.","It is easy to see how it relates to the l2l1 mixed norm, which is given as: ∥θ∥2,1 =   G ∑ g=1 ( M ∑ m=1 |θg,m| )2  1/2 = ∥(∥θ1̂∥1, · · · , ∥θ Ĝ∥1)∥2 Like l1l2, this is also a group-sensitive sparsity-inducing norm, but instead of encouraging sparsity across feature groups, it promotes sparsity within each feature group.","We illustrate how we apply l2l1 norm to induce diverse experts in the bottom diagram in Figure 1. Similar to the case of LOP, we create two “Expert” layer plates, one for each expert ( e1 and e2). But unlike traditional LOP, where each expert only gets a subset of the input features (e.g., f1(x) ↦→ e1 and f2(x) ↦→ e2), each input feature is fully connected to each expert plate (i.e., there are four edges with the same expert superscript from the “Input” layer going into each plate in the “Expert” layer, shown in different colors). We group every pair of feature weights that originate from the same input feature and end at the same output class into a separate feature group (e.g., θe1","1 (v1) and θe2","1 (v1) belongs to one group, while θe1","2 (v1) and θe2","2 (v1) belongs to another group, etc.). In total we have four feature groups, each shown in a different color.","When we apply l2l1 norm to this feature grouping while learning the parameter weights in the LOP model, the regularization will push most of the features that belong to the same group towards 0. But since we are optimizing a likelihood objective, if a particular input feature is indeed predictive, the model will also try to assign some weights to the weight parameters associated with this feature. These two opposite forces create a novel competition mechanism among features that belong to the same group.","This competition mechanism encourages the experts to use different sets of non-overlapping features, thus achieving the diversity effect we want. But because we are also optimizing the likelihood of the ensemble, each expert is encouraged to use parameters that are as predictive as possible. Learning all the experts together can also be seen as a form of feature sharing, as suggested by Ando and Zhang (2005) for multi-task learning. 528 5.2 FOBOS with l2l1 norm In our automatic expert induction model with l2l1 norm, the overall objective is given as:","L = argmin θ ( − log (","∏n j=1 [Pj(y|x)]αj","∑ y′","∏n j=1 [Pj(y|x)]αj ) +λ∥(∥θ1̂∥1, · · · , ∥θ Ĝ∥1)∥2 ) where λ is a parameter that controls the regularization strength.","The feature groups are given as:","θ1̂ = {θe1","1 (v1), θe2","1 (v1), · · · , θen","1 (v1)}","θ2̂ = {θe1","2 (v1), θe2","2 (v1), · · · , θen","2 (v1)}","· · ·","θ K̂ = {θe1 K (v1), θe2","K (v1), · · · , θen","K (v1)}","θ K̂+1 = {θe1","1 (v2), θe2","1 (v2), · · · , θen","1 (v2)}","θ K̂+2 = {θe1","2 (v2), θe2","2 (v2), · · · , θen","2 (v2)}","· · ·","θ Ĝ = {θe1 K (vV ), θe2","K (vV ), · · · , θen","K (vV )} There is a total of G = K × V feature groups, and each feature group has M = n features.","The key difference here from LOP is that the θ parameters are not pre-trained and fixed, but jointly learned with respect to l2l1 regularization. In the LOP case, learning the mixing coefficients αi (i = {1, . . . , n}) can be done by taking the gradients of αi with respect to the training objective and directly plugging it into a gradient-based optimization framework, such as the limited memory variable metric (LMVM) used by Smith et al. (2005) or Stochastic Gradient Descent. However, learning the θ parameters in our case is not as straightforward, since the objective is non-differentiable at many points due to the l2l1 norm. We cannot simply throw in an off-the-shelf gradient-based optimizer.","To alleviate the problem of non-differentiability, the recently proposed FOrward-Backward Splitting (FOBOS) optimization framework (Duchi and Singer, 2009) comes to mind. FOBOS is a (sub-)gradient-based framework that solves the optimization problem iteratively in two steps: in each iteration, we first take a sub-gradient step, and then take an analytical minimization step that incorporates the regularization term, which can often be solved with a closed form solution. For-mally, each iteration at timestamp t in FOBOS consists of the following two steps:","θt+ 1 2 = θt − ηtgt (1)","θt+1 = argmin θ { 1 2","∥θ − θt+ 1 2","∥2 + η tφ(θ) } (2) gt is the subgradient of −log(PLOP (y|x)) at θt. Step (1) is just a regular gradient-based step, where ηt is the step size and gt is the sub-gradient vector of parameter vector θ. Step (2) finds a new vector that stays close to the interim vector after step (1), but also has a low penalty score accord-ing to the regularization term φ(θ) (in our case, φ(θ) = ∥θ∥2,1). In the case of l1 and l2 regularization, each feature θi is independent in step (2), and thus can be solved separately.2","Duchi and Singer (2009) gave closed form so-lutions for solving the minimization problem in step (2) for l1, l2, and l1l2 norms, but no past research has demonstrated how to solve it for l2l1 norm. We leverage the results given by Kowalski and Torrésani (2009) for elitist lasso, and show that parameter θt+1,g,m (the mth","feature of group g at timestamp t + 1) can be solved analytically as follows:","θt+1,g,m = sign(θt+ 1 2 ,g,m) ( |θt+ 1","2 ,g,m| − τg )+ τg = λ′","1 + λ′ Mĝ(λ′",") ∥ ∥ ∥xt+ 1","2 ,g,1:Mĝ(λ′",") ∥ ∥ ∥ 1 sign(x) is the mathematical sign function. For x ∈ R, we have x+","= x if x ≥ 0 and x+","= 0 if x ≤ 0. λ′ = η","tλ is the regularization weight adjusted by the current step size.","Let θ̄ĝ denote a new vector that holds the positive absolute value of θĝ, sorted in descending order, i.e., θ̄g,1 ≥ θ̄g,2 ≥ · · · θ̄g,M . Mĝ(λ′",") is a positive integer given by the following definition:","θ̄g,Mĝ(λ′ )+1 ≤","Mĝ(λ′",")+1 ∑ m=1 ( θ̄g,m − θ̄g,Mĝ(λ′",")+1 ) and","θ̄g,Mĝ(λ′ ) > λ′ Mĝ(λ′",") ∑ m=1 ( θ̄g,m − θ̄g,Mĝ(λ′",") ) 2","This property turns out to be of great importance in realworld large scale scenarios, since it allows the optimization of high-dimensional feature vectors to be parallelized. 529 xt+ 1","2 ,g,1:Mĝ(λ′",") is then the subset of features from 1 to Mĝ(λ′",") – in descending order of their absolute value – in group g at timestamp t + 1","2 .","One problem with finding the exact solution of elitist lasso as illustrated above is that computing the value Mĝ(λ′",") involves sorting the parameter vector θ, which is a O(n log n) operation. When the number of features within each group is large, this could be prohibitively expensive. An approximate solution simply replaces Mĝ(λ′",") with the size of the group M . Kowalski and Torrésani (2009) found that approximated elitist lasso works nearly as well as the exact version, but is faster and easier to implement. We adopt this approximation in our experiments.","With this, we have described a general-purpose method for solving any convex optimization problem with l2l1 norm."]},{"title":"6 Experiments","paragraphs":["We evaluate the performance of our proposed method on the task of Named Entity Recognition (NER). The first dataset we evaluate on is the standard CoNLL-2003 English shared task benchmark dataset (Sang and Meulder, 2003), which is a collection of documents from Reuters newswire articles, annotated with four entity types: Person, Location, Organization, and Miscellaneous. We adopt the BIO2 annotation standard. Beginning and intermediate positions of an entity are marked with B- and I- tags, and non-entities with O tag. The training set contains 204K words (14K sentences), the development set contains 51K words (3.3K sentences), and the test set contains 46K words (3.5K sentences). The second dataset is the MUC6/7 set, which contains 255K words for training and 59K words for testing. The MUC data is annotated with 7 entity types. It is missing the Miscellaneous entity type, but includes 4 additional entity types that do not occur in CoNLL-2003: Date, Time, Money, and Percent.","We used a comprehensive set of features from Finkel et al. (2005) for training the MaxEnt model. A total number of 437905 features were generated on the CoNLL-2003 training dataset. The most important features are listed in Table 1. 6.1 Experimental Setup We tuned the regularization parameters in the l1, l2 and l1l2 norms on the development dataset via grid search. We used a tuning procedure similar to – The word, word shape (e.g., whether capitalized, numeric, etc.), and letter n-grams (up to 6gram) at current position – The word and word shape of the previous and next position – Previous word shape in conjunction with current word shape – Disjunctive word set of the previous and next 4 positions – Capitalization pattern in a 3 word window – The current word matched against a list of name titles (e.g., Mr., Mrs.) – Previous two words in conjunction with the word shape of the previous word Table 1: MaxEnt features. the one used in Turian et al. (2010) where we evaluate results on the development set after each optimization iteration, and terminate the procedure after not observing a performance increase on the development set in 25 continuous iterations. We found 150 to be a good λ value for l2, 0.1 for l1 and 0.1 for l2l1. Similarly, we tuned the number of experts for both LOP baselines and the elitist LOP induction scheme. All model parameters were initialized to a random value in [−0.1, 0.1].","Results are measured in precision (P), recall (R) and F1 score. Statistical significance is measured using the paired bootstrap resampling method (Efron and Tibshirani, 1993). We compare our results against a MaxEnt baseline model with both l1 and l2 regularization, as well as an automatic expert induction model with l1 norm.","Two commonly used LOP expert induction schemes were also compared. In the first scheme (LOP random), experts are constructed by randomly partitioning the features into n subsets, where n is the number of experts. Each expert therefore has 1","n th of the full feature set. The second scheme (LOP sequential) works in a similar way, but instead of random partition, we create feature subsets sequentially and preserve the relative order of the features. We also list results reported in Smith et al. (2005) on the same dataset for comparison. They experimented with three manually crafted expert induction schemes (Smith et al. 05 {simple;positional;label}) for LOP with a Conditional Random Field (CRF), which is a more powerful sequence model than our MaxEnt baseline. 6.2 Main Results Results on the CoNLL and MUC dataset are shown in Table 2. The proposed automatic expert induction scheme (row elitist LOP) gives consistent and statistically significant improvements over the MaxEnt baselines on both CoNLL and MUC test sets.","In particular, on the CoNLL test set, we ob-530","Dev Set Test Set Models # of experts Precision Recall F1 Precision Recall F1 CoNLL MaxEnt-l2 1 88.46 87.73 88.09 81.42 81.64 81.53 MaxEnt-l1 1 88.46 89.63 89.04 82.20 84.24 83.21† LOP random 2 88.75 90.79 89.76 82.73 85.84 84.25†‡ LOP sequential 2 88.68 90.79 89.72 83.03 86.03 84.50†‡ LOP-l1 5 88.76 90.41 89.58 82.90 85.94 84.40†‡ elitist LOP 3 89.65 91.08 90.36 83.96 86.67 85.29†‡ Smith et al. 05 simple 2 - - 90.26 - - 84.22 Smith et al. 05 positional 3 - - 90.35 - - 84.71 Smith et al. 05 label 5 - - 89.30 - - 83.27 MUC MaxEnt-l2 1 91.47 86.20 88.76 89.06 80.44 84.53 MaxEnt-l1 1 92.56 85.50 88.89 89.61 79.09 84.02 LOP random 2 93.77 84.95 89.14 91.05 78.89 84.54 LOP sequential 2 94.34 84.18 88.97 91.71 77.42 83.96 LOP-l1 5 93.25 86.09 89.53 90.70 79.57 84.78‡ elitist LOP 3 93.47 86.48 89.84 91.22 80.15 85.33†‡ Table 2: Results of NER on CoNLL and MUC dataset. † and ‡ indicate statistically significantly better F1 scores than the MaxEnt-l2 and MaxEnt-l1 baselines, respectively.","IV OOV Models Precision Recall F1 Precision Recall F1","CoNLL MaxEnt-l2 89.21 86.81 88.00 85.69 80.61 83.08 elitist LOP 90.25 89.85 90.05†","86.30 86.67 86.49†","MUC MaxEnt-l2 90.21 80.88 85.29 84.90 78.80 81.74 elitist LOP 92.27 80.02 85.71 87.49 80.62 83.91† Table 3: OOV and IV results breakdown. † indicates statistically significantly better F1 scores than the MaxEnt-l2 baseline. serve an improvement of 3.7% absolute F1 over Maxent-l2. The gains are particularly large in recall (by 5% absolute score), although there is also a 2.5% improvement in precision. The two conventional LOP induction schemes also show significant improvements over the MaxEnt baselines on this dataset, with the sequential feature partition scheme working slightly better than the random feature partition. However, elitist LOP out-performs the two LOP schemes by as much as 1% in absolute F1, and also gives better results than manually crafted experts for CRFs in Smith et al. (2005).","On the MUC test set, while elitist LOP still out-performs the MaxEnt baselines, the LOP schemes do not help or in some cases hurt performance. This suggests the lack of robustness of LOP which was discussed in Section 5. The automatically learned LOP experts are more robust on this data set, and gives a 0.8% improvement measured in absolute F1 score over the MaxEnt-l2 baseline.","The elitist LOP model is more expressive than MaxEnt since it has more parameters to be combined. To understand whether performance gain is obtained by the expressiveness or by the regularization of l2l1 norm, we compare against an elitist LOP model with just l1 regularization (LOP-l1). Results show that the l2l1 norm is a better regular-izer for avoiding overfitting with these models. We also see a significant gain inLOP-l1 over MaxEnt-l1, suggesting that the expressiveness of the model also helps. 6.3 Out-of-vocabulary vs. In-vocabulary To further understand how our method improves over the MaxEnt baseline, we break down the test results into two subsets: words that were seen in the training dataset (in-vocabulary, or IV), versus words that were not (out-of-vocabulary, or OOV). We removed the entity boundary tags (e.g. “I-ORG” becomes “ORG”) so that each word token is evaluated separately. Dividing IV and OOV subsets can be done by checking each word against a lexicon compiled from training dataset.","If our automatic expert induction LOP method does successfully mitigate the weight undertraining problem, we would expect to see an improvement in OOV recognition. The result of this analysis is shown in Table 3.","On the CoNLL dataset, our method gives significant improvements over MaxEnt in both OOV and IV category. In particular, improvement from 531 OOV subset (3.4% in F1) is larger than the improvement from IV subset (2% in F1). This matches that our hypothesis that our method mitigates the weight undertraining problem and thus gives a stronger boost in OOV recognition.","This effect is even more pronounced on the MUC dataset. The improvement in IV subset in this case is actually quite modest (0.5%), but we get a significant 2.2% improvement from the OOV subset."]},{"title":"7 Related Work","paragraphs":["Beyond the several aforementioned works that address the issue of weight undertraining, another recent work that looks at this problem is Wang and Manning (2013). They examined the objective function of dropout training prescribed by Hinton et al. (2012), and proposed an approximation of dropout by directly sampling from a Gaussian approximation. The resulting algorithm is orders of magnitude faster than the iterative algorithm given by Hinton et al. (2012). It will be interesting to compare our proposed method with this method in the future.","The use of group-sparsity norms in NLP research is relatively rare. Martins et al. (2011) proposed the use of structure-inducing norms, in particular l1l2 norm (group lasso), for learning the structure of classifiers. A key observation is that during testing, most of the runtime is consumed in the feature instantiation stage. Since l1l2 norm discards whole groups of features at a time, there are fewer feature templates that need to be instantiated, therefore it could give a significant runtime speedup.","Our use of l2l1 norm takes a different flavor in that we explore the internal structure of the model. There is, however, one recent paper that also makes use of l2l1 norm for a similar reason. Das and Smith (2012) employed l2l1 norm for learning sparse structure in a network formed by lexical predicates and semantic frames. Their results show that l2l1 norm yields much more compact models than l1 or l2 norms, with superior performance in learning to expand lexicons.","However, the optimization problem in Das and Smith (2012) was much simpler since all parameters can only take on positive values, and therefore they could directly use a gradient-descent method with specialized edge condition checks. In our work, we have shown a general-purpose method in the FOBOS framework for optimizing any convex function with l2l1 norm.","A related l1-l2 mixed norm is called elastic net (Zou and Hastie, 2005). It was proposed to over-come a problem of lasso (i.e., l1 regularization), which occurs when there is a group of correlated predictors, and lasso tends to pick one and ignore all the others. Elastic net takes the form of a sum of a l1 and a l2 norm. It was used in Lavergne et al. (2010) for learning large-scale Conditional Random Fields.","Another popular approach that also explores diversity in model predictions is system combination, which is related to bagging predictors (Breiman, 1996). For example, Sang et al. (2000) reported that by combing systems using different tagging representation and diverse classifier models (e.g., support vector machine, logistic regression, naive Bayes), they were able to achieve significantly higher accuracy than any individual classifier alone. This approach has also been applied extensively in Machine Translation (DeNero et al., 2010) and Speech Recognition (Mikolov et al., 2011)."]},{"title":"8 Conclusion","paragraphs":["Our results show that previous automatic expert definition methods used in logarithmic opinion pools lack robustness across different tasks and data sets. Instead of manually defining good (i.e., diverse) experts, we demonstrated an effective way to induce experts automatically, by using a sparsity-inducing mixed l1l2 norm inspired by elitist lasso. We proposed a novel formulation of the optimization problem with l2l1 norm in the FOBOS framework. Our method gives consistent and significant improvements over a MaxEnt baseline with fine-tunedl2 regularization on two NER datasets. The gains are most evident in recognizing entity types for out-of-vocabulary words."]},{"title":"Acknowledgments","paragraphs":["The authors would like to thank Rob Voigt and the three anonymous reviewers for their valuable comments and suggestions. We gratefully acknowl-edge the support of the DARPA Broad Operational Language Translation (BOLT) program through IBM. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA or the US government. 532"]},{"title":"References","paragraphs":["Rie K. Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853.","Leo Breiman. 1996. Bagging predictors. Machine Learning, 24:123–140.","Dipanjan Das and Noah A. Smith. 2012. Graph-based lexicon expansion with sparsity-inducing penalties. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).","John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).","John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899– 2934.","Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall, New York.","Jenny R. Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL).","Tom Heskes. 1998. Selecting weighting factors in logarithmic opinion pools. In Proceedings of Advances in Neural Information Processing Systems (NIPS) 10.","Geoffery Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by prevent-ing co-adaptation of feature detectors. CoRR, abs/1207.0580.","Geoffery Hinton. 1999. Products of experts. In Proceedings of the Ninth International Conference on Artificial Neural Networks (ICANN).","Matthieu Kowalski and Bruno Torrésani. 2009. Sparsity and persistence: mixed norms provide simple signal models with dependent coefficients. Signal, Image and Video Processing, 3(3):251–264.","Thomas Lavergne, Olivier Cappé, and Franco̧is Yvon. 2010. Practical very large scale CRFs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL).","André F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and Mário A. T. Figueiredo. 2011. Structured sparsity in structured prediction. In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP).","Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas Burget, and Jan Cernocky. 2011. Empirical evaluation and combination of advanced language modeling techniques. In Proceedings 12th Annual Conference of the International Speech Communication Association (INTERSPEECH).","Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: language-independent named entity recognition. In Proceedings of the 7th Conference on Natural language learning (CoNLL).","Erik F. Tjong Kim Sang, Walter Daelemans, Hervé Déjean, Rob Koeling, Yuval Krymolowski, Vasin Punyakanok, and Dan Roth. 2000. Applying system combination to base noun phrase identication. In Proceedings of the 18th International Conference on Computational Linguistics (COLING).","Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Logarithmic opinion pools for conditional random fields. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL).","Charles Sutton, Michael Sindelar, and Andrew McCallum. 2007. Reducing weight undertraining in structured discriminative learning. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT).","Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL).","Sida I. Wang and Christopher D. Manning. 2013. Fast dropout training. In Proceedings of the 30th International Conference on Machine Learning (ICML).","Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67:301–320. 533"]}]}