{"sections":[{"title":"","paragraphs":["International Joint Conference on Natural Language Processing, pages 1366–1374, Nagoya, Japan, 14-18 October 2013."]},{"title":"Linguistically Aware Coreference Evaluation Metrics Chen Chen and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {yzcchen,vince}@hlt.utdallas.edu Abstract","paragraphs":["Virtually all the commonly-used evaluation metrics for entity coreference resolution are linguistically agnostic, treating the mentions to be clustered as generic rather than linguistic objects. We argue that the performance of an entity coreference resolver cannot be accurately reflected when it is evaluated using linguistically agnostic metrics. Consequently, we propose a framework for incorporating linguistic awareness into commonly-used coreference evaluation metrics."]},{"title":"1 Introduction","paragraphs":["Coreference resolution is the task of determin-ing which mentions in a text or dialogue refer to the same real-world entity. Designing appropriate evaluation metrics for coreference resolution is an important and challenging task. Since there is no consensus on which existing coreference evaluation metric is the best, the organizers of the CoNLL-2011 and CoNLL-2012 shared tasks on unrestricted coreference (Pradhan et al., 2011, 2012) decided to take the average of the scores computed by three coreference evaluation metrics, MUC (Vilain et al., 1995), B3","(Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as the official score of a participating coreference resolver.","One weakness shared by virtually all existing coreference evaluation metrics is that they are linguistically agnostic, treating the mentions to be clustered as generic rather than linguistic objects. In other words, while MUC, B3",", and CEAF were designed for evaluating coreference resolvers, their linguistic agnosticity implies that they can be used to evaluate any clustering task, including those that are not linguistic in nature.1","1","This statement is also true for BLANC (Recasens and Hovy, 2011), a Rand Index-based coreference evaluation metric we will not focus on in this paper.","To understand why linguistic agnosticity is a potential weakness of existing scoring metrics, consider a document in which there are three coreferent mentions, Hillary Clinton, she, and she, appearing in this order in the document. Assume that two coreference resolvers, R1 and R2, are applied to these three mentions, where R1 only posits Hillary Clinton and she as coreferent, and R2 only posits the two occurrences of she as coreferent. Being linguistically agnostic, existing scoring metrics will assign the same score to both resolvers after seeing that both of them correctly assign two of the three objects to the same cluster. Intuitively, however, R1 should receive a higher score than R2: R1 has facilitated automated text understanding by successfully finding the referent of one of the pronouns, whereas from R2’s output we know nothing about the referent of the two pronouns. Failure to rank R1 higher than R2 implies that existing scoring metrics fail to adequately reflect the performance of a resolver.2","Our goal in this paper is to address the aforementioned weakness by proposing a framework for incorporating linguistic awareness into the most commonly-used coreference scoring metrics, including MUC, B3",", and CEAF. Rather than making different modifications to different metrics, one of the contributions of our work lies in the proposal of a unified framework that enables us to employ the same set of modifications to create linguistically aware versions of all these metrics."]},{"title":"2 Existing Evaluation Metrics","paragraphs":["In this section, we review four scoring metrics, MUC, B3",", and the two versions of CEAF, namely,","2","One may disagree that R1 should be ranked higher than R2 by arguing that successful identification of two coreferential pronouns is not necessarily easier than resolving an anaphoric pronoun to a non-pronominal antecedent. Our argument, however, is based on the view traditionally adopted in pronoun resolution research that resolving an anaphoric pronoun entails finding a non-pronominal antecedent for it. 1366 CEAFm and CEAFe. As F-score is always computed as the unweighted harmonic mean of recall and precision, we will only show how recall and precision are computed. Note that unlike previous discussion of these metrics, we present them in a way that reveals their common elements. 2.1 Notation and Terminology In the rest of this paper, we use the terms coreference chains and coreference clusters interchangeably. For a coreference chain C, we define |C| as the number of mentions in C. Key chains and system chains refer to gold coreference chains and system-generated coreference chains, respectively. In addition, K(d) and S(d) refer to the set of gold chains and the set of system-generated chains in document d, respectively. Specifically, K(d) = {Ki : i = 1, 2, · · · , |K(d)|}, S(d) = {Sj : j = 1, 2, · · · , |S(d)|}, where Ki is a chain in K(d) and Sj is a chain in S(d). |K(d)| and |S(d)| are the number of chains in K(d) and S(d), respectively. 2.2 MUC (Vilain et al., 1995) MUC is a link-based metric. Given a document d, recall is computed as the number of common links between the key chains and the system chains in d divided by the number of links in the key chains. Precision is computed as the number of common links divided by the number of links in the system chains. Below we show how to compute (1) the number of common links, (2) the number of key links, and (3) the number of system links.","To compute the number of common links, a partition P (Sj ) is created for each system chain Sj using the key chains. Specifically,","P (Sj) = {Ci j : i = 1, 2, · · · , |K(d)|} (1)","Each subset Ci","j in P (Sj ) is formed by intersecting Sj with Ki. Note that |Ci","j | = 0 if Sj and Ki have no mentions in common. Since there are |K(d)|∗|S(d)| subsets in total, the number of common links is c(K(d), S(d)) = |S(d)| ∑ j=1 |K(d)| ∑ i=1","wc(Ci j ),","where wc(Ci j) = {","0 if |Ci j| = 0;","|Ci j| − 1 if |Ci","j| > 0. (2)","Intuitively, wc(Ci j ) can be interpreted as the","“weight” of Ci","j. In MUC, the weight of a cluster is","defined as the minimum number of links needed","to create the cluster, so wc(Ci","j ) = |Ci","j| − 1 if |Ci j| > 0. The number of links in the key chains, K(d), is calculated as: k(K(d)) = |K(d)| ∑ i=1 wk(Ki), (3) where wk(Ki) = |Ki| − 1. The number of links in the system chains, s(S(d)), is calculated as: s(S(d)) = |S(d)| ∑ j=1 ws(Sj), (4) where ws(Sj ) = |Sj | − 1.","2.3 B3 (Bagga and Baldwin, 1998) One of MUC’s shortcoming is that it fails to reward successful identification of singleton clusters. To address this weakness, B3","first computes the recall and precision for each mention, and then averages these per-mention values to obtain the overall recall and precision.","Let mn be the nth mention in document d. Its recall, R(mn), and precision, P (mn), are computed as follows. Let Ki and Sj be the key chain and the system chain that contain mn, respectively, and let Ci","j be the set of mentions appearing in both Sj and Ki. R(mn) =","wc(Ci j ) wk(Ki) , P (mn) =","wc(Ci j ) ws(Sj ) , (5)","where wc(Ci j ) = |Ci","j|, wk(Ki) = |Ki|, and ws(Sj ) = |Sj|. 2.4 CEAF (Luo, 2005) While B3","addresses the shortcoming of MUC, Luo presents counter-intuitive results produced by B3",", which it attributes to the fact that B3","may use a key/system chain more than once when computing recall and precision. To ensure that each key/system chain will be used at most once in the scoring process, his CEAF scoring metric scores a coreference partition by finding an optimal one-to-one mapping (or alignment) between the chains in K(d) and those in S(d).","Since the mapping is one-to-one, not all key chains and system chains will be involved in it. Let 1367 Kmin(d) and Smin(d) be the set of key chains and the set of system chains involved in the alignment, respectively. The alignment can be represented as a one-to-one mapping function g, where g(Ki) = Sj, Ki ∈ Kmin(d) and Sj ∈ Smin(d). The score of g, Φ(g), is defined as Φ(g) = ∑ Ki∈Kmin(D) φ(Ki, g(Ki)), where φ is a function that computes the similarity between a gold chain and a system chain. The optimal alignment, g∗",", is the alignment whose Φ value is the largest among all possible alignments, and can be computed efficiently using the Kuhn-Munkres algorithm (Kuhn, 1955).","Given g∗",", the recall (R) and precision (P) of a system partition can be computed as follows: R =","Φ(g∗",")","∑|K(d))|","i=1 φ(Ki, Ki) , P = Φ(g∗",")","∑|S(d))| j=1 φ(Sj , Sj ) .","As we can see, at the core of CEAF is the similarity function φ. Luo defines two different φ functions, φ3 and φ4:","φ3(Ki, Sj) = |Ki ∩ Sj| = wc(Ci j ) (6) φ4(Ki, Sj) = 2|Ki ∩ Sj| |Ki| + |Sj| =","2 ∗ wc(Ci j )","wk(Ki) + ws(Sj)","(7) φ3 and φ4 result in mention-based CEAF (a.k.a.","CEAFm) and entity-based CEAF (a.k.a. CEAFe),","respectively. 2.5 Common functions Recall that the three weight functions, wc, wk, and ws, are involved in all the scoring metrics we have discussed so far. To summarize:","• wc(Ci j) is the weight of the common subset","between Ki and Sj. For MUC, its value is 0","if Ci j is empty and |Ci","j | − 1 otherwise; for B3",",","CEAFm and CEAFe, its value is |Ci","j|.","• wk(Ki) is the weight of key chain Ki. For MUC, its value is |Ki| − 1, while for B3",", CEAFm and CEAFe, its value is |Ki|.","• ws(Sj) is the weight of system chain Sj. For MUC, its value is |Sj | − 1, while for B3",", CEAFm and CEAFe, its value is |Sj|.","Next, we will show that simply by redefin-ing these three functions appropriately, we can create linguistically aware versions of MUC, B3",", CEAFm, and CEAFe.3","For convenience, we will refer to their linguistically aware counterparts as LMUC, LB3",", LCEAF m, and LCEAFe.4"]},{"title":"3 Incorporating Linguistic Awareness","paragraphs":["As mentioned in the introduction, one of the contributions of our work lies in identifying the three weight functions that are common to MUC, B3",", CEAFm, and CEAFe (see Section 2.5). To see why these weight functions are important, note that any interaction between a scoring metric and a coreference chain is mediated by one of these weight functions. In other words, if these weight functions are linguistically agnostic (i.e., they treat the mentions as generic rather than linguistic objects when assigning weights), the scoring metric that employs them will be linguistically agnostic. On the other hand, if these weight functions are linguistically aware, the scoring metric that employs them will be linguistically aware.","This observation makes it possible for us to design a unified framework for incorporating linguistic awareness into existing coreference scoring metrics. Specifically, rather than making different modifications to different scoring metrics to incorporate linguistic awareness, we can simply incorporate linguistic awareness into these three weight functions. So when they are being used in different scoring metrics, we can handily obtain the linguistically aware versions of these metrics.","In the rest of this section, we will suggest one way of implementing linguistic awareness. This is by no means the only way to implement linguistic awareness, but we believe that this is a good starting point, which hopefully will initiate further discussions in the coreference community. 3.1 Formalizing Linguistic Awareness Other than illustrating the notion of linguistic awareness via a simple example in the introduc-tion, we have thus far been vague about what ex-","3","Note that for a given scoring metric, wc(C) = wk(C) = ws(C) for any non-empty chain C. The reason why we define three weight functions as opposed to one is that they are defined differently in the linguistically aware scoring metrics, as we will see.","4","Our implementation of the linguistically aware evaluation metrics is available from http://www.hlt. utdallas.edu/ỹzcchen/coreference. 1368 actly it is. In this section, we will make this notion more concrete.","Recall that the goal of (co)reference resolution is to facilitate automated text understanding by finding the referent for each referring expression in a text. Hence, when resolving a mention, a resolver should be rewarded more if the selected antecedent allows the underlying entity to be in-ferred than if it doesn’t, because the former contributes more to understanding the corresponding text than the latter. Note that the more informative the selected antecedent is, the easier it will be for the reader to infer the underlying entity. Here, we adopt a simple notion of linguistic informativeness based on the mention type: a name is more informative than a nominal, which in turn is more informative than a pronoun.5","Hence, a coreference link involving a name should be given a higher weight than one that doesn’t, and a coreference link involving a nominal should be given a higher weight than one that involves only pronouns.","We implement this observation by assigning to each link el a weight of wl(el), where wl(el) is defined using the first rule applicable to el below: Rule 1: If el involves a name, wl(el) = wnam. Rule 2: If el involves a nominal, wl(el) = wnom. Rule 3: wl(el) = wpro.","There is a caveat, however. By assigning weights to coreference links rather than mentions, we will be unable to reward successful identification of singleton clusters, since they contain no links (and hence they carry no weights). To address this problem, we introduce a singleton weight wsing, which will be assigned to any chain that contains exactly one mention.","So far, we have introduced four weights, W = (wnam, wnom, wpro, wsing), which encode our (somewhat simplistic) notion of linguistic awareness. Below we show how these four weights are incorporated into the three weight functions, wc, wk, and and ws, to create their linguistically aware counterparts, wL","c , wL","k , and wL","s .","3.2 Defining wL c Recall that Ci","j represents the set of mentions common to key chain Ki and system chain Sj. To define the linguistically aware weight function wL c (Ci","j ), there are three cases to consider:","5","Different notions of linguistic informativeness might be appropriate for different natural language applications. In our framework, a different notion of linguistic informativeness can be implemented simply by altering the weight functions. Case 1: |Ci","j | ≥ 2 Recall that the linguistically agnostic wc function returns a weight of |Ci","j| − 1. This makes sense, because in a linguistically agnostic situation, all the links have the same weight, and hence the weight assigned to Ci","j will be the same regardless of which |Ci","j | − 1 links in Ci","j are chosen. However, the same is no longer true in a linguistically aware setting: since the links may not necessarily have the same weight, the weight assigned to Ci j depends on which |Ci","j | − 1 links are chosen. In this case, it makes sense for our linguistically aware wL","c function to find the |Ci","j | − 1 links that have the largest weights and assign to wL","c the sum of these weights, since they reflect how well a resolver managed to find informative antecedents for the mentions. Note that the sum of the |Ci","j | − 1 links that have the largest weights is equal the weight of the maximum spanning tree defined over the mentions in Ci","j.","Case 2: |Ci","j | = 0","In this case Ci","j is empty, meaning that Ki and Sj","do not have any mention in common. wL","c simply","returns a weight of 0 when applied to Ci j . Case 3: |Ci","j | = 1 In this case, Ki and Sj have one mention in common. The question, then, is: can we simply re-turn wsing, the weight associated with a singleton cluster? The answer is no: since wsing was created to reward successful identification of singleton clusters, a resolver should be rewarded by wsing only if it correctly identifies a singleton cluster. In other words, wL","c returns wsing if all of Ci","j , Ki and Sj contain exactly one mention (which implies that the singleton cluster Ci","j is correctly iden-","tified); otherwise, wL","c returns 0. The definition of wL","c is summarized as follows,","where E is the set of edges in the maximum span-","ning tree defined over the mentions in Ci","j . wL c (Ci","j ) =    ∑ el∈E","wl(el) if |Ci j| > 1; wsing if |Ci","j|, |Ki|, |Sj | = 1; 0 otherwise. (8)","3.3 Defining wL k Recall that wL","k aims to compute the weight of key chain Ki. Given the definition of wL","c , in order to ensure that the maximum recall is 1, it is natural to define wL","K as follows, where E is the set of edges 1369 appearing in the maximum spanning tree defined over the mentions in Ki. wL k (Ki) = { ∑ el∈E wl(el) if |Ki| > 1; wsing if |Ki| = 1. (9) 3.4 Defining wL","s Finally, we define wL","s , the function for computing the weight of system chain Sj. To better understand how we might want to define wL","s , recall that in MUC, B3",", and both versions of CEAF, precision and recall play a symmetric role. In other words, precision is computed by reversing the roles of the key partition K(d) and the system partition S(d) used to compute recall for document d. If we wanted precision and recall to also play a symmetric role in the linguistically aware versions of these scoring metrics, it would be natural to define wL","s in the same way as wL","k , where E is the set of edges appearing in the maximum spanning tree defined over the mentions in Sj. wL s (Sj) = { ∑ el∈E wl(el) if |Sj | > 1; wsing if |Sj | = 1. (10)","However, there is a reason why it is undesirable for us to define wL","s in this manner. Consider the special case in which a system partition S(d) contains only correct links, some of which are suboptimal.6","Although S(d) contains only correct links, the precision computed by any scoring metric that employs wL","s with the above definition will be less than one simply because it contains suboptimal links. In other words, if a scoring metric employs wL s with the above definition, it will penalize a resolver for choosing suboptimal links twice, once in recall and once in precision.","To avoid penalizing a resolver for the same mis-take twice, wL","s cannot be defined in the same way as wL","k .7","In particular, only spurious links (i.e., links between two non-coreferent mentions), not suboptimal links, should be counted as precision errors. To avoid this problem, recall that P (Sj ) is defined as a partition of system chain Sj created by intersecting Sj with all key chains in K(d).","P (Sj) = {Ci j : i = 1, 2, · · · , |K(d)|} 6 Suboptimal links are links that are correct but do not ap-","pear in a maximum spanning tree for any of its chains. 7 This implies that precision and recall will no longer play","a symmetric role in our linguistically aware scoring metrics.","Note that a link is spurious if it links a mention in Ci1","j with a mention in Ci2","j , where 1 ≤ i1 ̸= i2 ≤ K(d). Without loss of generality, assume that there are nej non-empty clusters in P (Sj ). Note that we need nej −1 spurious links in order to connect the nej non-empty clusters. To adequately reflect the damage created by these spurious links, among the different sets of nej−1 spurious links that connect the nej non-empty clusters in P (Sj), we choose the set where the sum of the weights of the links is the largest and count the edges in it as precision errors. We denote this set as Et(Sj ).","Now we are ready to define wL","s . There are two cases to consider. Case 1: |Sj | > 1 In this case, wL","s (Sj) is computed as follows: wL s (Sj ) = ∑ Ci j∈P (Sj) wL c (Ci","j ) + ∑ e∈Et(Sj) wl(e).","(11) Note that the second term corresponds to the precision errors discussed in the previous paragraph, whereas the first term corresponds to the sum of the values returned by wL","c when applied to each cluster in P (Sj). The first term guarantees that a resolver is penalized for precision errors because of spurious links, not suboptimal links. Case 2: |Sj | = 1 In this case, Sj only contains one mention. We set wL s (Sj ) to wsing."]},{"title":"4 Evaluation","paragraphs":["In this section, we design experiments to better understand our linguistically aware metrics (hence-forth LMetrics). Specifically, our evaluation is driven by two questions. First, given that the LMetrics are parameterized by a vector of four weights W , how do their behaviors change as we alter W ? Second, how do the LMetrics differ from the existing metrics (henceforth OMetrics)? 4.1 Experimental Setup We use as our running example the paragraph shown in Figure 1, which is adapted from the Bible domain of the English portion of the OntoNotes v5.0 corpus. There are 19 mentions in the paragraph, each of which is enclosed in parentheses and annotated as m","y","x, where y is the ID of the chain to which this mention belongs, and x is the mention ID. Figure 2 shows five system responses (a–e) for our running example along with the key chains. 1370","(Jesus)1","a came near (Jerusalem)2","d. Looking at (the city)2","e, (he)1","b began to cry for (it)2","f and said, (I)1","c wish (you)2","g knew what","would bring (you)2","h (peace)4","p. But it is hidden from (you)2","i (now)5","q. (A time)6","r is coming when ((your)2","j enemies)3","n will hold","(you)2","k in on (all sides)7","s. (They)3 o will destroy (you)2","l and (all (your)2","m people)8","t . Figure 1: A paragraph adapted from the Bible domain of the OntoNotes 5.0 corpus. For conciseness, a mention is denoted by its mention ID, and each connected sub-graph forms one coreference chain. Moreover, the type of a mention is denoted by its shape: a square denotes a NAME mention; a triangle denotes a NOMINAL mention, and a circle denotes a PRONOUN mention. Note that Syou, the set of coreferent “you” mentions consisting of {m2","g, m2","h, · · · , m2","m}, appears in all system responses. Figure 2: Key and system coreference chains.","Let us begin by describing the five system responses. Response (a) is produced by a simple and conservative resolver. Besides forming Syou, this resolver also correctly links m1","b with m1","c . Responses (b), (c) and (d) each improves upon response (a) by linking Syou to one of three preceding mentions, namely, one PRONOUN mention, one NOMINAL mention, and one NAME mention respectively. Response (e) is produced by an aggressive resolver that tries to resolve all the pro-","nouns to a non-pronominal antecedent, but unfor-","tunately, it wrongly connects Syou to m1","a, m1","b and m1","c .","Next, we investigate the two questions posed at the beginning of Section 4.1. To determine how the LMetrics behave when used in combination with different weight vectors W = (wnam, wnom, wpro, wsing), we experiment with:","W1 = (1.0, 1.0, 1.0, 10−20",");8","W2 = (1.0, 1.0, 1.0, 0.5);","W3 = (1.0, 1.0, 1.0, 1.0);","W4 = (1.0, 0.75, 0.5, 1.0);","W5 = (1.0, 0.5, 0.25, 1.0). Note that W1, W2, and W3 differ only with respect to wsing, so comparing the results obtained using these weight vectors will reveal the impact of wsing on the LMetrics. On the other hand, W4 and W5 differ with respect to the gap of the weights associated with the three types of mentions. Examining the LMetrics when they are used in combination with W4 and W5 will reveal the difference between having “relatively similar” weights versus having “relatively different” weights on the three mention types.","Figure 3 shows four graphs, one for each of the four LMetrics. Each graph contains six curves, five of which correspond to curves generated by using the aforementioned five weight vectors, and the remaining one corresponds to the OMetric curve that we include for comparison purposes. Each curve is plotted using five points that correspond to the five system responses. 4.2 Impact of wsing We first investigate the impact of wsing. We will determine how the LMetrics behave in response to W1, W2 and W3.","The first graph in Figure 3 shows the LMUC and MUC F-scores. As we can see, the scores of MUC and LMUC(W1) are almost the same. This is understandable: the uniform edge weights and a very small wsing in W1 imply that LMUC will 8","We set wsing to a very small value other than 0, because setting wsing to 0 may cause the denominator of the expressions in (5) and (7) to be 0. 1371 40 50 60 70 80 90 100 (a) (b) (c) (d) (e) F% System Response LMUC","MUC,W1 W2 W3 W4 W5 40 50 60 70 80 90 100 (a) (b) (c) (d) (e) F% System Response LB3","B3 W1,2,3 W4 W5 40 50 60 70 80 90 100 (a) (b) (c) (d) (e) F% System Response LCEAFm","CEAFm W1 W2 W3 W4 W5 40 50 60 70 80 90 100 (a) (b) (c) (d) (e) F% System Response LCEAFe CEAFe W1,2,3 W4 W5 Figure 3: Comparison of the LMetrics scores under different weight settings and the OMetrics scores. essentially ignore correct identification of single clusters and consider all errors to be equal, just like MUC. When we replace W1 with W2 and W3, the two weight vectors with a larger wsing value, and rescore the five responses, we see that the LMUC scores for responses (a), (b), (c) and (d) decrease. This is because LMUC uses wsing to penalize these four responses for identifying wrong singleton clusters. On the other hand, the LMUC score for response (e) is higher than the corresponding MUC score, because LMUC additionally rewards response (e) for correctly classifying all singleton clusters without introducing erroneous singleton clusters. The second graph in Figure 3 shows the LB3","and B3","F-scores. Here, we see that the scores","for LB3","(W","1), LB3 (W","2) and LB3 (W","3) are identical. These results suggest that the value of wsing does not affect the LB3","score, despite the fact that LB3","does take into account singleton clusters when scoring, a property that it inherits from B3",". The reason is that regardless of what wsing is, if a mention m is correctly classified as a singleton mention, both of R(m) and P (m) will be 1, other-wise, both will be 0 (see formula (5)). Note, how-ever, that there is a difference between LB3","and B3",": for an erroneously identified singleton cluster containing mention m, LB3","sets P (m) to 0 while B3","sets P (m) to 1. In other words, LB3","puts a higher penalty on precision given erroneous singleton clusters. This difference causes LB3","and B3 to evaluate responses (a) and (e) differently. Recall that responses (a) and (e) are quite different: response (e) correctly finds informative antecedents for m1","b , m1","c , m2","e, m2","f and m3","o, whereas response (a) contains many erroneous singleton clusters. Despite the large differences in these responses, B3 only gives 0.7% more points to response (e) than response (a). On the other hand, LB3","assigns a much lower score to response (a) owing to the numerous erroneous singleton clusters it contains.","The third graph of Figure 3 shows the LCEAFm and CEAFm F-scores. Since LCEAFm uses both singleton and non-singleton clusters when computing the optimal alignment, it should not be surprising that as we increase wsing, the singleton clusters will play a more important role in the LCEAFm score. Consider, for example, LCEAFm(W1). Since wsing = 0, LCEAFm(W1) ignores the correct identification of singleton clusters. From the graph, we see that LCEAFm(W1) gives a higher score to response (a) than response (e). This is understandable: response (a) is not penalized for the many erroneous singleton clusters it contains; on the other hand, response (e) is penalized for the erroneous coreference links it introduces. Now, consider LCEAF(W3), where wsing = 1. Here, response (e) is assigned a higher score by LCEAF(W3) than response (a): response (a) is heavily penalized because of the many erroneous clusters it contains.","The rightmost graph of Figure 3 shows the LCEAFe and CEAFe F-scores. Like LB3",", LCEAFe returns the same score when it is used in combination with W1, W2 and W3, because the φ4 similarity function returns 0 or 1 when the key cluster or the system cluster it is applied to is a singleton cluster, regardless of the value of wsing. In addition, we can see that LCEAFe penalizes erroneous singleton clusters more than CEAFe does 1372 ch- MUC LMUC B3","LB3","CEAFm LCEAFm CEAFe LCEAFe ains R P F R P F R P F R P F R P F R P F R P F R P F (a) 58.3 100 73.7 50.7 58.6 54.4 64.3 100 78.3 39.2 70.0 50.2 75.0 75.0 75.0 50.7 58.6 54.4 91.1 56.1 69.4 73.8 45.4 56.2 (b) 66.7 100 80.0 53.7 64.3 58.5 71.3 100 83.3 43.1 75.0 54.7 80.0 80.0 80.0 53.7 64.3 58.5 91.9 61.3 73.6 74.5 49.7 59.6 (c) 66.7 100 80.0 64.2 68.3 66.2 71.3 100 83.3 50.8 75.0 60.6 80.0 80.0 80.0 64.2 68.3 66.2 91.9 61.3 73.6 76.7 51.1 61.4 (d) 66.7 100 80.0 74.6 71.4 73.0 71.3 100 83.3 58.6 75.0 65.8 80.0 80.0 80.0 74.6 71.4 73.0 91.9 61.3 73.6 78.4 52.3 62.8 (e) 91.7 91.7 91.7 76.1 92.7 83.6 79.0 79.0 79.0 65.0 72.5 68.5 70.0 70.0 70.0 58.2 70.9 63.9 86.5 86.5 86.5 85.8 85.8 85.8 Table 1: Comparison of the LMetrics(W4) scores and the OMetrics scores. for the same reason that LB3","penalizes erroneous singleton clusters more than B3","does.","In sum, the value of wsing does not impact LB3 and LCEAFe. On the other hand, LMUC and LCEAFm pay more attention to singleton clusters as wsing increases. 4.3 Impact of wnam, wnom and wpro When we were analyzing the LMetrics in the previous subsection, by setting wnam, wnom, and wpro to the same value, we were not exploiting their capability to be linguistically aware. In this subsection, we investigate the impact of linguistic awareness using W4 and W5, which employ different values for the three weights.9","To better understand the differences in recall and precision scores for each of the five system responses, we show these scores as computed by the LMetrics when they are used in combination with W4.","First, consider response (a). As we can see from Figure 3 and the first row of Table 1, the OMetrics give decent scores to this output. Linguistically speaking, however, the system should be penalized more. The reason is that its output contributes little to understanding the document: in response (a), only the links between the PRONOUN mentions are established, and none of the PRONOUN or NOMINAL mentions is linked to a more informative mention that would enable the underlying entity to be inferred.","As expected, LMetrics(W4) and LMetrics(W5) assign much lower scores to response (a) than the OMetrics, owing to a relatively small value of wpro. Also, we see that the LMetrics(W5) scores are even lower than the LMetrics(W4) scores. This suggests that the smaller the values of wpro and wnom are, the more heavily a resolver will be penalized for its failure to link a mention to a more informative coreferent mention.","Next, consider responses (b), (c) and (d). As the 9","Like W3, we set wsing to 1 in W4 and W5, because this assignment makes CEAFm(W3) rank response (e) above response (a), which we think is reasonable. OMetrics ignore the type of mentions while scoring, they are unable to distinguish the differences among these three system responses: the OMetrics results in Figure 3 and their results in rows 2, 3 and 4 of Table 1 show that the scores for responses (b), (c) and (d) are identical. Linguistically speak-ing, however, they should not be. Response (d) contributes the most to document understanding, because the presence of NAME mention m2","d in its output enables one to infer the entity (Jerusalem) to which the mentions in Syou refer. In contrast, although response (b) correctly links Syou to PRONOUN mention m2","f , one cannot infer the entity to which the mentions in Syou refer. The contribution of response (c) is in-between, because via m2","e, we at least know that the mentions in Syou point to one city, although we do not know which city it is. Such differences in responses (b), (c) and (d) are captured by LMetrics(W4) and LMetrics(W5). Specifically, the LMetrics scores for response (d) are higher than those for response (c), which in turn are higher than those for response (b).","It is worth noting that the performance gaps between responses (b) and (c) and between responses (c) and (d) are larger under LMetrics(W5) than under LMetrics(W4). This is because wnom and wpro in W5 are comparatively smaller. These results enable us to conclude that as the difference in the three edge weights becomes larger, the performance gap between a less informative resolver and a more informative resolver according to the LMetrics widens."]},{"title":"5 Conclusion","paragraphs":["We addressed the problem of linguistic agnosticity in existing coreference evaluation metrics by proposing a framework that enables linguistic awareness to be incorporated into these metrics. While our experiments were performed on gold mentions, it is important to note that our linguistically aware metrics can be readily combined with, for example, Cai and Strube’s (2010) method, so that they can be applied to system mentions. 1373"]},{"title":"Acknowledgments","paragraphs":["We thank the three anonymous reviewers for their detailed and insightful comments on an earlier draft of the paper. This work was supported in part by NSF Grants IIS-1147644 and IIS-1219142. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of NSF."]},{"title":"References","paragraphs":["Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the LREC Workshop on Linguistic Coreference, page 563–566.","Jie Cai and Michael Strube. 2010. Evaluation metrics for end-to-end coreference resolution systems. In Proceedings of the 11th Annual SIGDIAL meeting on Discourse and Dialogue, pages 28–36.","Harold W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2:83–97.","Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 25–32.","Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 1– 27.","Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: Shared Task, pages 1– 40.","Marta Recasens and Eduard Hovy. 2011. BLANC: Implementing the Rand Index for coreference resolution. Natural Language Engineering, 17(4):485– 510.","Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the Sixth Message Understanding Conference, pages 45–52. 1374"]}]}