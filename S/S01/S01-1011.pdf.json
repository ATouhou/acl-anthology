{"sections":[{"title":"SENSEV AL-2: The Swedish Framework Dimitrios KOKKINAKIS","paragraphs":["Sprakdata, Goteborg University"]},{"title":"Jerker JARBORG","paragraphs":["Sprakdata, Gateborg University"]},{"title":"Yvonne CEDERHOLM","paragraphs":["Sprakdata, Goteborg University Box 200, SE-405 30 Goteborg, Sweden Dimitrios.Kokkinakis","@svenska.gu.se Box 200, SE-405 30 Goteborg, Sweden Jerker.Jaerborg@","svenska.gu.se Box 200, SE-405 30 Goteborg, Sweden Yvonne. Cederholm@","svenska.gu.se Abstract In this paper we describe the organisation and results of the SENSEVAL-2 exercise for Swedish. We present some of the experiences we gained by participating as developers and organisers in the exercise. We particularly focus on the choice of the lexical and corpus material, the annotation process, the scoring scheme, the motivations for choosing the lexical-sample branch of the exercise, the participating systems and the official results. Introduction Word sense ambiguity is a potential source for errors in human language technology applications, such as Machine Translation, and it is considered as the great open problem at the lexical level of Natural Language Processing (NLP). There are, however, several computer programs for automatically determining which sense of a word is being used in a given context, according to a variety of semantic, or defining dictionaries as demonstrated in the SENSEV AL-l exercise; (Kilgarriff and Palmer, 2000). The purpose of SENSEV AL is to be able to say which programs and methods perform better, which worse, which words, or varieties of language, present particular problems to which programs; when modifications improve performance of systems, and how much and what combinations of modifications are optimal. Specifically for Swedish, we would also like to investigate to what extent sense disambiguation can be accomplished and the potential resources available for the task. We would thus be creating a framework that can be shared both within the"]},{"title":"45","paragraphs":["exercise and for future evaluation exercises of similar kind, national and international. 1 Choice of Task Three tasks were identified for SENSEVAL-2, namely: the lexical-sample, the all-words and the 'in a system' tasks. In the lexical sample task, first, we sample the lexicon, then we find instances in context of the sample words and the evaluation . is carried out on the sampled instances. In the all-word task a system will be evaluated on its disambiguation performance on every word in the test collection. Finally, in the third type of task, a. word sense disambiguation (WSD) system is evaluated on how well it improves the performance of a NL system (MT, IR etc). The reasons we chose the lexical-sample task for Swedish are summarised below:","1. Cost-effectiveness of annotation: it is easier and quicker for the human annotators to sense-tag multiple occurrences of one word at a time, particularly when robust interactive means are utilized (Section 3);","2. The lexical-sample reduces the work of preparing training data since only a subset of the sense inventory is used;","3. More systems can/could (eventually) participate;","4. The all-words task requires access to a full dictionary, which is problematic from the copyright point of view, since industrial partners were also allowed to participate; and, as Kilgarriff and Palmer (2000) noted:","5. Provided that the sample is well chosen, the lexical sample strategy would be more informative about the current strengths and failings of sense disambiguation research than the all-words task. 2 Development Process In this section we will give a concise description of how the whole exercise (for Swedish) was set up, putting more emphasis on some of the main ingredients of the work, i.e. sampling, resources, annotation and scoring.","A number of likely participants were invited to express their interest and participate in the Swedish SENSEVAL (summer, 2000). A plan for selecting the evaluation material was agreed in Sprakdata, and human annotators were set on the task of generating the training and testing material. The material was released to the participants at the end of April 2001 and during the second week of June, 2001 the results were returned for scoring. The Swedish SENSEV AL material was divided into three parts and released in stages:"]},{"title":"• • •","paragraphs":["Trial data: freezing and showing the data formatting conventions (lexicon & corpus); Training data: the finalised sense inventory and portion of the 'gold standard'; Evaluation data: the rest of the 'gold standard', untagged. 2.1 Dictionary and Corpus At least three lexical resources were candidates for the Swedish lexicon-sample task. These were the Swedish versions of the WordNet (http://www.ling.lu.se/projects/Swordnet) and the Swedish SIMPLE (http://spraakdata.gu.se/simple/), as well as the Gothenburg Lexical Data Base/semantic Database (GLDB/SDB) (http://spraakdata.gu.se/lb/gldb.html). We chose the GLDB/SDB. The creation of a Swedish version of WordNet, a resource that is extensively used for the semantic annotation of texts in other languages, is under development and had (up to that point) limited coverage, while the SIMPLE lexicon, although available, has limited coverage (in principle it could be used and it is linked to the GLDB/SDB). However, a draWback of the Swedish SIMPLE is that very fine-grained sub senses are not adequately described (or not described at all) in the material. GLDB/SDB is a generic defining dictionary of 65,000 lemmas available and developed at our department and became the final choice for the lexical inventory. (see Allen, 1999[1981] for a description of the model utilized in the dictionary)."]},{"title":"46","paragraphs":["For the textual material we chose the Stockholm-Umea Corpus (SUC), Ejerhed et al. (1992). The particular corpus was chosen for three main reasons. It is available to the research community; it is considered the \"standard reference\" corpus for contemporary"]},{"title":"writte~","paragraphs":["Swedish; and, third, it is the corpus utilised in the SemTag project (next section). 2.2 Sampling There is no standard method for sampling the lexical data. However, certain features were considered. These were: frequency, polysemy, part-of-speech and distribution of senses. Words were chosen based not so much on intuition, but rather on their frequency and polysemy. Still, it was hard to find a balance between these two features since high frequency words tend to be monosemous in a corpus, while highly polysemous words tend to have few senses in a corpus. In the case that a word was frequent and polysemous we tried to provide more data (context), than for words that were less frequent. Part-of-speech information was consulted for the decision of choosing more nouns in the sample (highest portion in the GLDB/SDB), than verbs (less than nouns, but more than adjectives in the GLDB/SDB) and adjectives (which are fewer than nouns and verbs in GLDB/SDB). We chose a sample of words where the amount of senses was evenly distributed, i.e. lemmas (dictionary entries) with 2-7 lexemes (senses) and 1-23 cycles (subsenses). 2.3 SemTag Creating a sense-annotated reference corpus is a laborious task. Therefore, we developed the majority of the test and reference material within an ongoing project highly relevant for our mission, namely SemTag (Lexikalisk betydelse och anviindningsbetydelse - \"Lexical Sense and Sense in Context\", financed by the Swedish Council for Research in the Humanities and Social Sciences (HSFR)); see Jarborg (1999). In brief, the purpose of the project is to create a large sample of sense-annotated corpus (several hundreds of thousands of words), which can be used among other things for:"]},{"title":"•","paragraphs":["measuring the performance of automatic methods for WSD;","• testing, in practice and on a large scale, the validity of the lemma-lexeme model implemepteq in GLDB/SI)13;","• the improvement of lexicographic descriptions, and the production of (new and) more fine-grained senses in GLDB/SDB;","• the adjustment of the definitions in GLDB/SDB to better fit the textual use;","• describing new words, not covered by the content of the GLDB/SDB;","• producing material, adequate for training supervised methods to sense disambiguation. 2.4 Corpus/Sense Inventory Table 1 shows information on the sense inventory, the amount of corpus instances (training/testing) and the distribution of senses and sub-senses (Lexemes/Cycles) in the material for the twenty nouns (N), fifteen verbs (V) and the five adjectives (A). The total amount of training and testing corpus instances was: 8716/1525. The average polysemy in the sample is 3,517,6 for lexemes and cycles respectively. ! Corpus"]},{"title":"!","paragraphs":["Lexemesl ! Word"]},{"title":"I","paragraphs":["POS Instances"]},{"title":"i","paragraphs":["Cycles","I barn/1 N 656/115 I 316 betydelse/1 N 295/52 2/1 farg/1 N 110/19 4/ll konst/1 N 77/13 316 kraft/1 N 152127 4/11 kyrka/1 N 154/27 2/3 kiinsla/1 N 142/25 2/4 ledning/1 N 9l/16 4/1 makt/1 N 128/22 3/4 massa/1 N 93116 613 mening/1 N 168/29 4/1 natur/1 N 90/16 3/4 program/1 N 139/24 4110 rad/1 N 145/25 4/3 rum/1 N 223/39 317 scen/1 N 101117 417 tillfalle/1 N ll7/20 2/4 uppgift/1 N 174/30 2/3 vatten/1 N 285/50 2/3 amne/1 N 198/34 4/4 betyda/1"]},{"title":"v","paragraphs":["198/35 4/4 flytta/1"]},{"title":"v","paragraphs":["188/33 2/4 fylla/2"]},{"title":"v","paragraphs":["96117 4111 roija/1"]},{"title":"v","paragraphs":["~45/61 5/19 forklara/1"]},{"title":"v","paragraphs":["169/30 2/9 galla/1"]},{"title":"v","paragraphs":["843/148 4/6 handla/1"]},{"title":"v","paragraphs":["250/44 415"]},{"title":"47","paragraphs":["hora/1 i"]},{"title":"v","paragraphs":["523/92 5114 mrua/1"]},{"title":"v","paragraphs":["96/16 217 skjuta/1"]},{"title":"v","paragraphs":["79/14 6115 spela/1"]},{"title":"v","paragraphs":["267/47 6/23 vanta/1"]},{"title":"v","paragraphs":["248/43 3/15 viixa/1"]},{"title":"v","paragraphs":["203/36 2/9 oka/1"]},{"title":"v","paragraphs":["436177 2/2 oppna/1"]},{"title":"v","paragraphs":["147/25 4/16 bred/1 A 103/18 311 klar/1 A 307/54 4/11 naturlig/1 A 139/24 4/5 stark/1 A 352/62 5111 oppen A 189/33 7/21 Table 1. Data for the Swedish Lexical Sample 3 Annotation The annotation was carried out interactively using a concordance-based interface (developed in SemTag) and which interacts with the corpus and the dictionary; (see http:/ I svens ka. gu.se/ -svedk!S ENS EV AUi mages/semt ag.gif for a screenshot of this tool). Due to our limited financial resources only two professional lexicographers and a trained Phd student were involved in the tagging process, which was preferred to (untrained) students doing the annotation. High replicability between the human annotators was observed (>95%). The uncertain cases were not used in the training or testing material, while the provided dictionary descriptions for the 40 lemmas were revised (extended and/or modified) prior to their release. 4 Scoring Prior to SENSEV AL, evaluating WSD performance was based solely on the exact match criterion, which is not consider a \"fair\" metric, and has a lot of drawbacks (e.g. it does not account for the semantic distance between senses when assigning penalties for incorrect labels, and it does not offer a mechanism to offer partial credit; cf Resnik & Yarowsky (2000)) Instead, in SENSEVAL-2 three scoring policies are adopted:","1. Fine-grained: answers must match exactly","2. Coarse-grained: answers are mapped to coarse-grained senses and compared to the gold standard tags, also mapped to coarse grained ones (sense map is required; see below)","3. Mixed-grained: if a sense subsumption hierarchy is available, then the mixedgrained scoring gives some credit to choosing a more coarse-grained sense than the gold standard tag, but not full credit (also using a sense map; see below). A \"sense map\" containing a complete list of all sense-ids involved in the evaluation was provided in order to perform the two last types of scoring policies. Each line in the sense map included sense subsumption information and contained a list of the subsumer senses and branching factors. 5 Participants and Results Five groups showed interest in participating in the Swedish task (eight systems in total). Table 2 provides information for the participating systems, while their average performance is given in Table 3, the score in parenthesis concerns: Verbs/Noun/ Adjectives. All systems returned answers for all instances, thus precision equals recall, all used supervised methods and all systems scored lower on the adjectives and higher on the nouns.","Group (Systems) Uppsala Univ. (PWE,3) Linkoping Univ. (LIU, 1) Goteborg Univ. (Spnlkdata, 2) John Hopkins Univ. (JHU, 1) Maryland Univ. (UMD, 1) System JHU PWE-Vote Spnlkdata-ML PWE-Simple UMD LIU PWE-Disj Sprakdata Overlap Method","Contact","Person(s) TBL-tranade T. Lager, Pro log word N. Zinovjeva experts Multilevel L. Ahrenberg, decision list M. Merkel, approach M. Andersson Machine D. Kokkinakis learning & feature overlap --- D. Yarowsky Support vector P. Resnik, J. Stevens, machine C. Cabezas Table2. Participants","Results Fine-Grained"]},{"title":"I","paragraphs":["Mixed-Grained 70,1(63,4n6,9/51,8) . 74,7(70,9n9,8/59,5) 63,0(58,5/72,7/48,7) 68,6(65,9n5,0/57,9) 62,0(57 ,sm ,3/48,2) 68,2(66,1n4,9/54.4) 61 '1 (55,4173,2/43,5) 66,8(63,2n5,7/51 ,7) 61 '1(56,4ni ,4/45,5) 65,6(61 ,7173,6/54,3) 56,5(47 ,8n 1 ,6/40,8) 61 ,6(54,7 173,3/49,6) 54,0(46,3/67,7/38,4) 60,7(55,3ni,0/47,5) 46,0(36,6/57,8/43,1) ' 55,8(47,8/65,7/53,8) Table 3. Results. Overall Precision followed by precision for (Verb/Noun/Adective) instances"]},{"title":"48","paragraphs":["Conclusion The process of WSD is a complex, controversial matter, but relevant for a number of NLP applications. Our contribution to the exercise will eventually sharpen the focus of WSD in Sweden; the material developed in SENSEVAL-2 can be used as benchmark for other researchers that need to measure their system's WSD performance against a concrete reference point (although the dictionary is limited). We think that WSD opens up exciting opportunities for linguistic analysis, contributing with very important information for the assignment of lexical semantic knowledge to polysemous and homonymous content words. The existence of sense ambiguity (polysemy and homonymy) is one of the major problems affecting the usefulness of basic corpus exploration tools. In this respect, we regard WSD as a very important process when it is seen in the context of a wider and deeper NLP system. Acknowledgements We would like to thank the Swedish Council for Research in the Humanities and Social Sciences (HSFR) for providing financial support for the coordination of the task. References","Allen S. (1999[1981]). The Lernma-Lexeme Model of the Swedish Lexical Database. Empirical Semantics, 376-387. Rieger B. (ed). Bochum. (Reprinted in: Allen S. (1999). Modersmalet i Faderneslandet. Ett urval uppsatser under fyrtio ar av Sture Allen, 268-278. Meijerbergs Arkiv 25).","Ejerhed E., Kallgren G., Wennstedt G. and Astri:im M. (1992). The Linguistic Annotation of the Stockholm-Umea Corpus project. Technical Report No. 33, Univ. of Umea.","Jarborg J. (1999). Lexikon i konfrontation. Research Reports from the Department of Swedish, Spnikdata, GU- ISS-99-6. Available from: http://svenska.gu.se/-svedk/resrapp/konfront.pdf. (In Swedish).","Kilgarriff A. and Palmer M. (2000). Introduction to the Special Issue on SENSEV AL. Computer and the Humanities, 00:1-13, Kluwer Acad. Publishers.","Resnik P. and Yarowsky D. (2000). Distinguishing Systems and Distinguishing Senses: New Evaluation Methods for Word Sense Disambiguation. Natural Language Engineering, 5(2): 113-133, Cambridge."]}]}