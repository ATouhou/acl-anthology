{"sections":[{"title":"Japanese word sense disambiguation using the simple Bayes and support vector machine methods Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing Ma, and Hitoshi !sahara Communications Research Laboratory 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan Abstract","paragraphs":["We submitted four systems to the Japanese dictionary-based lexical-sample task of SENSEVAL-2. They were"]},{"title":"i)","paragraphs":["the support vector machine method ii) the simple Bayes method, iii) a method combining the two, and iv) a method combining two kinds of each. The combined methods obtained the best precision among the submitted systems. After the contest, we tuned the parameter used in the simple Bayes method, and it obtained higher preciSIOn. An explanation of these systems used in Japanese word sense disambiguation was provided. 1"]},{"title":"Introduction","paragraphs":["We participated in the Japanese dictionary based lexical-sample task of the SENSEVAL-2 contest. We used machine learning approaches and submitted four systems. After the con test, we tuned the parameter used in the simple Bayes method and carried out additional exper iments. In this paper, we explain the systems and their experimental results. 2"]},{"title":"Task Descriptions","paragraphs":["The test data included 10,000 instances for eval uation. The RWC corpus (Shirai et al., 2001) was given as the training data. It was made from 3000 articles published in the Mainichi Newspaper. The nouns, verbs, and adjectives (the total number of which was about 150,000) were assigned sense tags defined on the basis of the Iwanami dictionary. The purpose of this task was to estimate the sense of a word by us ing its context."]},{"title":"3 Methods","paragraphs":["Because the word sense assigned to each word is dependent on the word itself, estimations 135 were conducted using machine learning meth ods for each word. That is, we constructed as many learning machines as there were individ ual words.","We used the simple Bayes and support vec tor machine methods as the machine learning method. 1 In this section, we explain each of the machine learning methods and then explain the method combining several of them. 3.1 Simple Bayes Method This method estimates probability based on the Bayes theory. The category (i.e., the sense tag) with the highest probability is judged to be the desired one. This is a basic approach to the disambiguation of word sense. The probability of category a appearing in context b is defined as: p(alb) p(a) p(b) p(bla) (1) p(a)"]},{"title":"IT-","paragraphs":["p(b) . p(f;ia), (2)","' where context"]},{"title":"b","paragraphs":["is a set of features fj ( E"]},{"title":"F,","paragraphs":["1 ~ j ~ k) that is defined in advance. p(b) is the probability of context b, which is not calculated because it is a constant and is not dependent on category"]},{"title":"a. p(a)","paragraphs":["and jj(fila) are the prob abilities estimated by using the training data and indicate the probability of the occurrence of category a in the examples of the training data and the probability of feature"]},{"title":"fi","paragraphs":["occur ring, given category a, respectively. When we use the maximum likelihood estimation to cal culate"]},{"title":"p(fiia),","paragraphs":["which often has a value of 0 and is therefore difficult to estimate the desired cat egory, smoothing process is used. We used this","1","We made preliminary experiments using various methods: the simple Bayes, the decision list, the max imum entropy, and the support vector machine. The results showed that the simple Bayes and support vector machine methods were better than the other two (M u rata et al., 2001). We used these two methods in the contest."]},{"title":". . . . . .","paragraphs":["• •"]},{"title":". .","paragraphs":["•"]},{"title":". •","paragraphs":["0~ @"]},{"title":". • . • . .","paragraphs":["• •"]},{"title":".","paragraphs":["~"]},{"title":". .","paragraphs":["0 0 ~ •"]},{"title":"•","paragraphs":["~ •"]},{"title":". • . . • . .","paragraphs":["•"]},{"title":".. •","paragraphs":["0 o'. •"]},{"title":". .","paragraphs":["•,"]},{"title":".","paragraphs":["•"]},{"title":"·-","paragraphs":["Small Margin Large Margin Figure"]},{"title":"1:","paragraphs":["Maximizing the margin equation for smoothing: _(!·I ) _ freq(f;, a)+ E"]},{"title":"*","paragraphs":["freq(a)","p,a- ()' freq(a)"]},{"title":"+","paragraphs":["E"]},{"title":"*","paragraphs":["freq a (3) where freq(Ji, a) is the number of events that have the feature"]},{"title":"fi","paragraphs":["and whose category is a and freq(a) is the number of events whose category is a. E is a constant set by experimentation. In this study, we used 0.01 and 0.0001 as E. 2 3.2 Support Vector Machine Method In this method, data consisting of two categories is classified by using a hyperplane to divide a space. When the two categories are, for exam ple, positive and negative, enlarging the margin between the positive and negative examples in the training data (see Figure 13 ) reduces the possibility of incorrectly choosing categories in test data. The hyperplane that maximizes the margin is thus determined, and classification is carried out using that hyperplane. Although the basics of this method are the same as those described above, in the extended versions of the method, the region between the margins through the training data can include a small number of examples, and the linearity of the hyperplane can be changed to a non-linearity by using kernel functions. The classification in the extended versions is equivalent to the classi fication using the following function (Equation ( 4)), and the two categories can be classified on the basis of whether the value output by the function is positive or negative ( Cristianini and Shawe-Taylor, 2000; Kudoh, 2000):","2","In the SENSEVAL-2 contest, we used 0.01 as t:. After the contest, we tested several values (0.1 to 0.00000001) as E. We confirmed that E = 0.0001 produced the best results using 10-fold cross validation in the training data.","3","In the figure, the white and black circles indicate positive and negative examples, respectively. The solid line indicates the hyperplane that divides the space, and the broken lines indicate the planes that mark the mar gins. 136 f(x) 'gn ("]},{"title":"t","paragraphs":["\";y;J((x;, x)"]},{"title":"+ /,)","paragraphs":["(4) max;,y;=-tb;"]},{"title":"+","paragraphs":["mini,y;=tb; b 2 l","b; - I:>'iYiK(xj, x;), j=l where x is the context (a set of features) of an input example, Xi indicates the context of a training datum, Yi"]},{"title":"(i =","paragraphs":["1, ... ,l,yi E"]},{"title":"{1,-1})","paragraphs":["indicates its category, and the function sgn is sgn(x)"]},{"title":"=","paragraphs":["1 (x 2: 0), (5) -1 (otherwise). Each O:i ( i"]},{"title":"=","paragraphs":["1, 2 ... ) is fixed as the value of O:i that maximizes the value of L(a) in Equation (6) under the conditions set by Equations (7) and (8). l l L(a)"]},{"title":"2:: a;-~ 2::","paragraphs":["a;O:jYiYjK(x;, Xj) (6) i=l i,j=l 0 S: a; S: C (i = 1, ... ,l) l"]},{"title":"L","paragraphs":["a;y; = 0 i=l (7) (8) Function K is called a kernel function and var ious functions are used as kernel functions. We have used the following polynomial function ex clusively. K(x,y) =(x·y+l)d (9) C and d are constants set by experimentation. For all of the experiments reported in this pa per, C was fixed as 1 and d was fixed as 2.","A set of Xi that satisfies O:i"]},{"title":">","paragraphs":["0 is called a support vector (SVs) 4. The summation portion of Equation ( 4) was calculated using only the examples that were support vectors.","Support vector machine methods are capable of handling data consisting of two categories. In general, data consisting of more than two cate gories is handled by using the pair-wise method (Kudoh and Matsumoto, 2000).","In this method, for data consisting of N cat egories, pairs of two different categories (N (N-1)/2 pairs) are constructed. The better cate-","1rn Figure 1, the circles in the broken lines indicate support vectors. gory is determined by using a 2-category clas sifier (in this paper, a support vector machine5 was used as the 2-category classifier), and the correct category is finally determined by \"vot ing\" on the N(N-1)/2 pairs that result from analysis using the 2-category classifier.","The support vector machine method is, in fact, performed by combining the support vec tor machine and pair-wise methods described above.","3.3 Combined Method","Our combined method changed the used","machine-learning method for each word. The","used method for each word was the best one","for the word in the 10-fold cross validation6 on","the training data among the given methods for","combination. We used the following three kinds of combi","nations. • Combined method 1 a combination of the simple Bayes and support vector machine methods • Combined method 2 a combination of two kinds of the simple Bayes method and two kinds of the support vector machine method (Here, \"the two kinds\" indicate an instance where all features were used and where the syn tactic feature alone were not).7 • Combined method 3 a combination of two kinds of the simple Bayes method (Here, \"the two kinds\" indicate instance where E"]},{"title":"=","paragraphs":["0.0001 and another where E"]},{"title":"=","paragraphs":["0.01)."]},{"title":"4 Features (information used in classification)","paragraphs":["In this paper, the following are defined as fea tures.","• Features based on strings - strings in the analyzed morpheme - strings of 1 to 3-grams just before the an-","alyzed morpheme","5","We used Kudoh's TinySVM software (Kudoh, 2000) as the support vector machine.","6","In the 10-fold cross validation, we first divide the training data into ten parts. The answers of the in stances in each part are estimated by using the instances in the remaining nine parts as the training data. We then use all the results in the ten parts for evaluation.","7","We used a case where the syntactic feature alone was not used because it obtained a higher precision than when all the features had been used in our preliminary experiments.","- strings of 1 to 3-grams just after the ana lyzed morpheme","• Features based on the morphological in formation given by the RWC tags","- the part of speech (POS), the minor POS, and the more minor POS of the analyzed morpheme 8","- the previous morpheme, its 5-digit cate gory number, its 3-digit category number, its POS, its minor POS, and its more mi nor POS9 the next morpheme, its 5-digit category number, its 3-digit category number, its POS, its minor POS, and its more minor POS","• Features based on the morphological in formation given by JUMAN The corpus was analyzed using the Japanese morphological analyzer, JUMAN (Kurohashi and Nagao, 1998), and the results were used as features. the POS, the minor POS, and the more minor POS of the analyzed morpheme, which were determined from the results of JUMAN. the previous morpheme, its 5-digit cate gory number, its 3-digit category number, its POS, its minor POS, and its more mi nor POS","- the next morpheme, its 5-digit category number, its 3-digit category number, its POS, its minor POS, and its more minor POS","• Features based on syntactic information The corpus was analyzed using the Japanese syntactic analyzer KNP (Kurohashi, 1998), and the results were used as features.","- the bunsetsu, 10 including the analyzed morpheme information on whether or not","8","The POS, the minor POS, and the more minor POS of a morpheme are the items in the third, fourth, and fifth fields of the RWC corpus, respectively.","9","A Japanese thesaurus, the Bunrui Goi Hyou dictio nary (NLRI, 1964), was used to determine the category number of each morpheme. This thesaurus is of the 'is a' hierarchical type, in which each word has a category number, which is a 10-digit number that indicates seven levels of an 'is-a' hierarchy. The top five levels are ex pressed by the first five digits, the sixth level is expressed by the next two digits, and the final level is expressed by the final three digits.","10","Bunsetsu is a Japanese grammatical term. A bun setsu is similar to a phrase in English, but is a slightly smaller component. Eki-de \"at the station\" is a bun setsu, and sono, which corresponds to \"the\" or \"its,\" is also a bunsetsu. A bunsetsu is, roughly, a unit of items that refers to entities. 137 Table 1: Experimental results","Method Precision Baseline method 0.726 Support vector machine (CRL1) 0.783 Simple Bayes method, t = 0.01 (CRL2) 0.778 Simple Bayes method, t = 0.0001 0.790 Combined method 1 (CRL3) 0.786 Combined method 2 (CRL4) 0.786 Combined method 3 0.793 The best method in the contest 0.786 the bunsetsu was a noun phrase, the POS of the bunsetsu's particle, the minor POS of the particle, and the more minor POS of the particle the main word that the bunsetsu modifies, including the analyzed morpheme and its 5-digit category number, 3-digit category number, POS, minor POS, and more mi nor POS the main words of the modifiers of the bunsetsu including the analyzed mor pheme and their 5-digit category numbers, 3-digit category numbers, POSs, minor POSs, and more minor POSs (In this case, the information on the particle, such as ga or o, was used as well).","• Features of all words co-occurring in the same sentence The corpus was analyzed using the Japanese morphological analyzer JUMAN (Kurohashi and Nagao, 1998), and lists of the results were used as features. each morphology in the same sentence, its 5-digit category number, and its 3-digit category number","• Features of the UDC code in a document In the RWC corpus, each document has a uni versal decimal code (UDC), indicating its cat egory. the first digit, the first two-digits, and the first three-digits of the UDC in the docu ment"]},{"title":"5 Experiments","paragraphs":["We submitted the four systems ( CRL1 to CRL4), the support vector machine method, the simple Bayes method"]},{"title":"(E =","paragraphs":["0.01), Combined method 1, and Combined method 2. After the contest, we carried out the experiments using the simple Bayes"]},{"title":"(E =","paragraphs":["0.0001) and Combined method 3. Their experimental results are shown in Table 1. \"Baseline method\" selected the cate gory that most frequently occurred in the train ing data as the answer. \"The best method in the contest\" was the best among all the sys tems submitted to the contest, which was CRL4 (0.786483). The precisions shown in the table are the mixed-grained scores calculated by soft ware \"scorer2\", which was given by the com mittees of SENSEVAL-2. (In our systems, all the instances were attempted, so the recall rate was equal to its precision rate.)","We found the following items from the results.","• All the methods produced higher precision than","the baseline method.","• Among the four submitted systems (CRL1 to","CRL4), Combined method 2 was the best.","• The simple Bayes method using E = 0.0001","and Combined method 3 (the combination of","the two simple Bayes methods) obtained higher","precision. This indicates that the simple Bayes","method was effective."]},{"title":"6 Conclusion","paragraphs":["Our methods combining the simple Bayes and support vector machine methods obtained the best precision among all the submitted systems. After the contest, we tuned the parameter used in the simple Bayes method using the 10-fold cross validation in the training data, and it ob tained higher precision. The best method was the combination of the two simple Bayes, whose precision was 0.793."]},{"title":"References","paragraphs":["Nello Cristianini and John Shawe-Taylor. 2000. An Introduc tion to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.","Taku Kudoh and Yuji Matsumoto. 2000. Use of support vec tor learning for chunk identification. CoNLL-2000.","Taku Kudoh. 2000. TinySVM: Support Vector Machines. http://cl.aist-nara.ac.jp/ taku-ku// software/TinySVM/ index.html.","Sadao Kurohashi and Makoto Nagao, 1998. Japanese Mor phological Analysis System JUMAN version 3.5. Depart ment of Informatics, Kyoto University. (in Japanese).","Sadao Kurohashi, 1998. Japanese Dependency/Case Struc ture Analyzer KNP version 2.0b6. Department of Infor matics, Kyoto University. (in Japanese).","Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing Ma, and Hitoshi !sahara. 2001. Experiments on word sense disambiguation using several machine-learning meth ods. In IEICE-WGNLC2001-2. (in Japanese).","NLRI. 1964. Bunrui Goi Hyou. Shuuei Publishing.","Kiyoaki Shirai, Wakako Kashino, Minako Hashimoto, Takenobu Tokunaga, Eiichi Arita, Hitoshi !sahara, Shiho Ogino, Ryuichi Kobune, Hlronobu Takahashi, Katashi Nagao, Koiti Hasida, and Masaki Murata. 2001. Text database with word sense tags defined by Iwanami Japanese dictionary. Information Processing Society of Japan, WGNL 141-19. (in Japanese). 138"]}]}