{"sections":[{"title":"The ,Johns Hopkins SENSEVAL2 System Descriptions David Yarowsky, Silviu Cucerzan, Radu Florian, Charles Schafer and Richard Wicentowski {yarowsky,silviu,rflorian,cschafer,richardw }@cs.jhu.edu Department of Computer Science Johns Hopkins University Baltimore, Maryland, 21218, USA Abstract","paragraphs":["This article describes the Johns Hopkins Univer sity ( JHU) sense disambiguation systems that par ticipated in seven SENSEVAL2 tasks: four super vised lexical choice systems (Basque, English, Span ish, Swedish), one unsupervised lexical choice sys tem (Italian) and two supervised all-words systems (Czech, Estonian). The common core supervised system utilizes voting-based classifier combination over several diverse systems, including decision lists (Yarowsky, 2000), a cosine-based vector model and two Bayesian classifiers. The classifiers employed a rich set of features, including words, lemmas and part-of-speech informatino modeled in several syn tactic relationships (e.g. verb-object), bag-of-words context and local collocational n-grams. The all words systems relied heavily on morphological anal ysis in the two highly inflected languages. The un supervised Italian system was a hierarchical class model using the Italian WordNet."]},{"title":"1 The Feature Space","paragraphs":["The JHU SENSEVAL2 systems utilized a rich fea ture space based on raw words, lemmas and part of-speech (POS) tags in a variety of positional re lationships to the target word. These positions in clude traditional bag-of-word context, local bigram and trigram collocations and several syntactic re lationships based on predicate-argument structure (described in Section 1.2). Their use is illustrated on a sample English sentence for train in Figure 1. 1.1 Part-of-Speech Tagging and","Lemmatization Part-of-speech tagger availability varied across the languages included in this sense-disambiguation sys tem evaluation. Transformation-based taggers (Ngai and Florian, 2001) were trained on standard data for English (Penn Treebank), Swedish (SUC-1 cor pus) and Estonian (MultextEast corpus). For Czech, an available POS tagger (Hajic and Hladka, 1998), which includes lemmatization, was used. The re maining languages - Spanish, Italian and Basque - were tagged using an unsupervised tagger ( Cucerzan 163 \"Many mothers do not even try to toilet trmn their children until the age of 2 years or later ..."]},{"title":"\"","paragraphs":[".t<eature Word POS Lemma type Context"]},{"title":". . . . . . ...","paragraphs":["Context try VB tryjv Context to TO to/T Context toilet NN toilet/N Context train VBP train/v Context their DT their/D Context"]},{"title":". . . . . . ...","paragraphs":["Syntactic {predicate-argument) features Object children NNS child/N","Prep until IN until/I ObjPrep age NN age/N","Ngram collocational features -1 bigram toilet NN toilet/N +1 bigram their DT their/D -2/-1 trigram to toilet • TO-NN tojT toilet/N • -1/H trigram to • their TO-DT to/T • their/D","+1/+2 trigram their children DT-NN their/D child/N Figure 1: Example sentence and extracted features and Yarowsky, 2000). Lemmatization was per formed using a combination of supervised and un supervised methods (Yarowsky and Wicentowski, 2000), and using existing trie-based supervised mod els for English.","1.2 Syntactic Features","Extracted syntactic relationships in the feature","space depended on the keyword's part of speech: • for verb keywords - the head noun of the","verb's object, particle/preposition and object","of-preposition were extracted when available. • for noun keywords - the headword of any verb","object, subject-verb or noun-noun relationships","identified for the keyword. • for adjective keywords - the head noun modified","by the adjective (if identifiable).","These syntactic features were extracted using sim ple heuristic patterns and regular expressions over the parts-of-speech surrounding the keyword."]},{"title":"2 Supervised Lexical Choice Systems","paragraphs":["The supervised JHU systems utilize classifier com bination merging the results of five diverse learning models. 2.1 Core Algorithm Design The lexical choice task can be cast as a classifica tion task: training data is given in the form of a set of word-document pairs"]},{"title":"T","paragraphs":["="]},{"title":"[(w;,","paragraphs":["D;j), S;jJi,j (Sij being the sense associated with the document D;j of keyword wi), labeled with the corresponding gold standard class. The goal is to establish the clas sification of a set of unlabeled word-document pairs"]},{"title":"T'","paragraphs":["= { (wi, D~J·)}"]},{"title":".. ,","paragraphs":["not previously seen in the train- •J .","ing data. The training data"]},{"title":"T","paragraphs":["is used to estimate class probabilities and then the sense classification is made by choosing the class with the maximum a posteriori class probability:","S = argmaxP (s'ID) = argmaxP (S') · P (DIS') S' S'","The disambiguation models used in our exper iments are feature-based models. A feature is a boolean function defined as"]},{"title":"f","paragraphs":["w : F x 1J"]},{"title":"-+ {","paragraphs":["0, 1}, where F is the entire set of features and 1J is the document space. An overview of the exploited fea ture space was given in Section 1. 2.2 Vector-based Algorithms Our Bayesian and cosine-based models use a com mon vector representation, capturing both tra ditional bag-of-words features and the extended Ngram and predicate-argument features in a single data structure.","In these models, a vector is created for each doc ument in the collection:","D; = (D;J)j=l,IFI where F is the entire utilized feature space","Cij","Dij = NWJ where c;j is the the number i of times the feature fJ appears in document D;, Ni is the number of words in the document D; and Wj is the weight associated with the feature"]},{"title":"fi.","paragraphs":["To avoid confusion between the same word in mul tiple feature roles, feature values are marked with their positional type (e.g. children_ object, toilet_ L, and their R as distinct from children, toilet and their in u;marked bag-of-words context). The basic sense disambiguation algorithm pro ceeds as follows: 1. Vectors in the training data are assigned to","classes based on their classification; 2. For each vector in the test data, the a posteriori","class distribution is computed as P (SID)= Sim (D, Cs)"]},{"title":"2::","paragraphs":["Sim (D, Cs') S' 164 where Cs is the centroid corresponding to the sense S and Sim is the similarity measure used by the algorithm (cosine or Bayes).","3. The sample D is labeled with sense S if S"]},{"title":"=","paragraphs":["argmaxP (S'ID). S' 2.2.1 The Cosine-based Model In this model, traditional cosine similarity is used to compute similarity between a document D and a centroid"]},{"title":"C.","paragraphs":["The weight associated with a feature (Fj) is its inverse document frequency Wj =log!:;, where N is the total number of documents and Nj is the number of documents containing feature"]},{"title":"fJ.","paragraphs":["Function words and POS tags were excluced from the cosine vectors. 2.2.2 The Bayesian Models In the Bayes model, the Bayes similarity is computed as: and the following assumption of independence is made: P (D;ICs)"]},{"title":"= II","paragraphs":["P (!JIGs) /jED;","The probability distribution P (!jiGs) is obtained by smoothing the word relative frequencies in the cluster C s. Given the lack of independence between the word-based and lemma-based feature spaces, these are utilized in two separate Bayesian models with output combined in Section 2.5. 2.3 Decision Lists The decision list model we used in our system is a non-hierarchical variant of the method of inter polated decision lists described in Yarowsky (2000). For each feature fi a smoothed log of likelihood ratio (log P(fdSi) ) is computed for each sense Sj, with P(f;i~Si) . . . smoothing based on an empmcally estimated function of feature type and relative frequency. Can didate features are ordered by this smoothed ra tio (putting the best evidence first), and the re maining probabilities are computed via the interpo lation of the global and history-conditional proba bilities. By utilizing the single strongest-matching evidence in context, non-independent feature spaces combine readily without inflated confidence, and can be mapped to accurate and robust probability esti mates as shown in Figure 2. 2.4 Additional Details The English task differs slightly from the other lexical-choice tasks in that phrasal verbs are ex pljcitly marked in the training and test data. To make reasonable use of this information, when a phrasal verb is marked, only corresponding phrasal senses are considered; conversely when a phrasal ~ ~ 0.98 a o.96 ~ ~"]},{"title":"*","paragraphs":["0.94 0 ~ 0.92 ~ ] 0.9 ~ 0.88"]},{"title":"&","paragraphs":["0.86 ':-:---:-'-::-::--~-~~-~-~~-.....J 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995","Raw Confidence Score Figure 2: Mapping between raw confidence scores and classification accuracy for English decision lists verb is not marked, no phrasal senses are considered. Likewise, when a training or test sentence matches a compound noun in the observed sense inventory (e.g. art_gallery%1:06:00::) only the matching phrasal sense(s) are considered unless there is at least one non-phrasal sense tagged in the training data for that compound (indicating the potential for both compositional and non-compositional interpre tations). 2.5 Classifier Combination Several classifier combination approaches were inves tigated in the system development phase. They are outlined below, along with their cross-validated per formance on the English lexical-sample training data (in Table 1). In each case four individual classifiers were combined: the cosine model, two Bayes models (one based on words and one based on lemmas 1","), and the decision-list model.","The first two model combination approches sim ply averages the output of the participating clas sifiers over each candidate sense tag, in terms of P(SjiDi) and rank(SjiDi) respectively, with each classifier given an equal vote2","•","The remaining methods assign potentially vari able weights to the votes of different classi fiers. Interestingly, Equal Weighting of all four classifiers slightly outperforms classifier weighting proportional to each model's aggregate accuracy (Performance-Weighted voting), similar to the tech nique used for classifier combination in part-of speech tagging in van Halteren et al. (1998). Finally, it was observed that on sentences where decision lists have high model confidence their accuracy exceeds other classifiers. Thus the most effective approach, based on training-data cross validation, was found to be a very basic Thresholded Model Voting:","1 0n training-set cross-validation it was observed that the two systems were uncorrelated enough to make it useful to keep both of them.","2 Decision lists are not included because they only assign a probability to their selected classifier output but not to lower ranked candidates. 165 • If the decision_list_confidence;::: 0.985 (an em","pirically selected threshold) then return the out","put of the decision list; • Otherwise, each system votes for the sense that","is most likely under it and, another vote is ob","tained from the most probable class yielded by","linear interpolation of the 4 classifiers.","This simple top-performing approach was utilized in","the evaluation system, and is reasonably close to the","performance of an Oracle upper bound for classifier","combination (using the output of the single best clas","sifier on each test instance - unknowable in practice). Accuracy Model Averaging (excluding decision lists):","Weighted Model Voting (includes decision lists)' Equal-weighted Model Voting .667 .736 Performance-Weighted Voting .655 .724 Thresholded Model Voting .676 .746 Oracle Voting (Upper Bound) .734 .761 Table 1: Comparison of classifier combination meth ods on English (using 5-fold cross-validation)"]},{"title":"3 Supervised All-Words Systems","paragraphs":["3.1 Estonian All-words Task Because of the importance of morphological analy sis in a highly inflected language such as Estonian, a lemmatizer based on Yarowsky and Wicentowski (2000) was first applied to all words in the train ing data (and, at evaluation time, the test data). For each lemma, the P (sensejlemma) distribution was measured on the training data. For all lem mas exhibiting only one sense in the training data, this sense was returned. Likewise, if there was in sufficient data for word-specific training (the sum of the minority sense examples for the word in training data was below a threshold) the majority sense in training was returned for all instances of that lemma. In the remaining cases where a lemma had more than one sense in training, with sufficient minority exam ples to adequately be modeled, the generic JHU lex ical sample sense classifier was trained and applied. 3.2 Czech All-words Task Czech is another example of a highly inflected lan guage. A part-of-speech tagger and lemmatizer kindly provided by Jan Hajic of Charles Univer sity (Hajic and Hladk:a, 1998) was first applied to the data. Consistent with the spirit of evaluating sense disambiguation rather than morphology, the JHU system focused on those words where more than one sense was possible for a root word (e.g. the -1 and -2 suffixes in the Czech inventory). In these cases, the fine-grained output of the Czech lemmatizer was ignored (in both training and test) and a generic lexical-sample sense classifier was ap plied to the sense-distinction tags extracted from the lemmatized training data (see Section 2), using the classification models employed in Estonian. When ever insufficient numbers of minority tagged exam ples were available for training a word-specific clas sifier, the majority sense for the POS-level lemma was returned. Likewise, if only one possible sense tag was observed for any POS-levellemma analysis, then this unambiguous sense tag was returned."]},{"title":"4 Unsupervised Italian System","paragraphs":["The Italian task stands out from the group of lexical choice tasks because no labelled training was data provided for Italian; instead a subset of the Italian Wordnet was provided. To obtain a sense classifier for Italian, we employed an unsupervised method that used hierarchical class models of the Wordnet relationships among words (synonymy, hypernomy, etc) and a large unannotated corpus of Italian news paper data to obtain sense centroids.","First, every relationship type in the Italian Word net received an initial weight, based on a roughly es timated measure of the relative dissimilarity of two words in that relationship. For instance, the syn onymy relationship received a small weight (words are semantically \"close\"), while other relationships (has_ near_ synonym, causes, has_ hypemym) re ceived proportionately larger weights (words are more semantically distant). Starting from the senses Sofa target k, the wordnet relationships graph was explored, up to a given distance (two links away), creating \"clouds\" of similar words, Ms, together with a similarity3 to the original sense, S.","For each of the words win Ms, we extracted sen tences from the unannotated corpus that contained the word w, and then considered them as being ex amples of context for the sense S of target k, and as signed them to the centroid C"]},{"title":"s","paragraphs":["(the centroid of the sense S) with a weight corresponding to the similar ity between the word w and the sense S (computed using the wordnet graph). After all the documents were distributed, the test documents were also as signed to the most probable cluster, similar to the other lexical choice tasks.","The centroids were then allowed to adjust in a manner similar to k-means clustering. At each step, the centroids were recomputed, after which each document migrated to the closest cluster (i.e. argmaxs P (CsiD)), and the process was repeated. After the process converged, each test document was","3The weight on a path was computed as the sum of the weights on the path, and the similarity was computed as Sim ( w, S)"]},{"title":"=","paragraphs":["e-c(w,S) -large weights result in 0 similarity."]},{"title":"166","paragraphs":["Accuracy on Test Data Task Fine.:Grained Coarse-Grained Basque .757 .971 English .642 .713 Spanish .712 - Swedish .701 1.00 Italian .353 .423 Czech .935 - Estonian .666 - Table 2: Official JHU system performance assigned the label corresponding to the sense cen troid it converged into. This process is completely unsupervised, and the only structured resource that was used is the provided Italian Wordnet subset."]},{"title":"5 Results","paragraphs":["Table 2 lists the official performance of the JHU sys tems on unseen test data in the final SENSEVAL2 evaluation. Coarse-grained performance scores are based on a hierarchical sense clustering given by the task organizers in 4 of the languages. In the lexical sample tasks, these scores were obtained after cor rection of a simple bug in the merger of final system output as provided for in the SENSEVAL evaluation protocols.","As illustrated in the comparative performance ta bles elsewhere in this volume, the JHU systems are consistently very successful across all 7 languages and 3 major system types described here."]},{"title":"References","paragraphs":["S. Cucerzan and D. Yarowsky. 2000. Language inde pendent minimally supervised induction of lexical probabilities. In Proceedings of ACL-2000, pages 270-277, Hong Kong.","J. Hajic and Hladka. 1998. Tagging inflective lan guages: Prediction of morphological categories for a rich, structured tagset. In Proceedings of CO LING/ A CL-98, pages 483-490, Montreal.","G. Ngai and"]},{"title":"R.","paragraphs":["Florian. 2001. Transformation based learning in the fast lane. In Proceedings of NAACL-2001, pages 40-47, Pittsburgh.","H. van Halteren, J. Zavrel and W. Daelemans. 1998. Improving Data Driven Wordclass Tag ging by System Combination In Proceedings of COLING/ACL-1998, pages 491-497, Montreal.","D. Yarowsky and"]},{"title":"R.","paragraphs":["Wicehtowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of ACL-2000, pages 207-216, Hong Kong.","D. Yarowsky. 2000. Hierarchical decision lists for word sense disambiguation. Computers and the Humanities, 34(2):179-186."]}]}