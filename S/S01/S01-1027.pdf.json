{"sections":[{"title":"Using Domain Information for Word Sense Disambiguation Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo and Alfio Gliozzo ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica, I-38050 Trento, ITALY email: { magnini, strappa, pezzulo, gliozzo }@itc.it Abstract","paragraphs":["The major goal in ITC-irst's participation at SENSEVAL-2 was to test the role of domain in formation in word sense disambiguation. The underlying working hypothesis is that domain labels, such as MEDICINE, ARCHITECTURE and SPORT provide a natural way to establish se mantic relations among word senses, which can be profitably used during the disambiguation process. For each task in which we participated (i.e. English all words, English 'lexical sample' and Italian 'lexical sample') a different mix of knowledge based and statistical techniques were implemented."]},{"title":"1 Introduction","paragraphs":["Current investigation in Word Sense Disam biguation (WSD) at ITC-irst focuses on the role of domain information. The hypothesis is that domain labels (such as MEDICINE, ARCHITEC TURE and SPORT) provide a natural and pow erful way to establish semantic relations among word senses, which can be profitably used dur ing the disambiguation process. In particular, domains constitute a fundamental feature of text coherence, such that word senses occurring in a coherent portion of text tend to maximize domain similarity. The importance of domain information in WSD has been remarked in sev eral works, including (Gonzalo et al., 1998) and (Buitelaar and Sacaleanu, 2001). In (Magnini and Strapparava, 2000) we introduced \"Word Domain Disambiguation\" (WDD) as a variant of WSD where for each word in a text a domain label (among those allowed by the word) has to be chosen instead of a sense label. We also ar gued that WDD can be applied to disambigua tion tasks that do not require fine grained sense distinctions, such as information retrieval and content-based user modeling. For SENSEVAL-111 2 the goal was to evaluate the role of domain information in WSD: no other syntactic or se mantic information has been used (e.g. seman tic relations in WoRDNET) except domain la bels. Three systems have been implemented, in tegrating knowledge-based and statistical tech niques, for the three tasks we participated in, i.e. English 'all words', English 'lexical sample' and Italian 'lexical sample'. The main lexical resource for domains is \"WordNet Domains\", an extension of English Wordnet 1.6 (Fellbaum, 1998) developed at ITC-irst, where synsets have been annotated with domain information."]},{"title":"2 WordN et Domains","paragraphs":["The basic lexical resource we used in SENSEVAL-2 is \"WordNet Domains\", an extension of WoRDNET 1.6 where each synset has been an notated with at least one domain label, se lected from a set of about two hundred la bels hierarchically organized (see (Magnini and Cavaglia, 2000) for the annotation methodol ogy and for the evaluation of the resource). The information from the domains that we added is complementary to what is already in WoRDNET. First of all a domain may in clude synsets of different syntactic categories: for instance MEDICINE groups together senses from Nouns, such as doctor#i and hospi tal#i, and from Verbs such as operate#7. Sec ond, a domain may include senses from dif ferent WoRDNET sub-hierarchies (i.e. deriv ing from different \"unique beginners\" or from different \"lexicographer files\"). For example, SPORT contains senses such as athlete#i, de riving from life...forrn#i, garne_equiprnent#i from physicaLobj ect#i, sport#i from act#2, and playing...field#i from location#i. Fi nally, domains may group senses of the same word into homogeneous clusters, with the side effect of reducing word polysemy in WoRD NET. Table 1 shows an example. The word \"bank\" has ten different senses in WoRDNET 1.6: three of them (i.e. sense 1, 3 and 6) can be grouped under the EcoNOMY domain, while sense 2 and 7 both belong to GEOGRAPHY and GEOLOGY, causing the reduction of the polysemy from 10 to 7 senses. For the purposes of SENSEVAL-2 we have considered 41 disjoint labels which al low a good level of abstraction without loosing relevant information (i.e. in the experiments we have used SPORT in place of VOLLEY or BAs KETBALL, which are subsumed by SPORT).","Sens Synset €1 Gloss Domains Semcor","occurr.","#1 depository finan- EcoNOMY 20 cial institution, bank, banking concern, banking company (a financial institu-tion ... )","#2 bank (sloping GEOGRAPHY, 14 land ... ) GEOLOGY","#3 bank (a supply or EcoNOMY"]},{"title":"-","paragraphs":["stock held In reserve ... )","#4 bank, bank ARCHITECTURE - building (a build- EcoNOMY ing ... )","#5 bank (an arrange- FACTOTUM 1 ment of similar objects ... )","#6 savmgs bank, EcoNOMY - coin bank, money box, bank (a container ... )","#7 bank (a long GEOGRAPHY, 2 ridge or pile ... ) GEOLOGY","#8 bank (the funds EcoNOMY, held by a gam- PLAY bling house ... )","#9 bank, cant, cam- ARCHITECTURE - ber (a slope in the turn of a road ... )","#10 bank (a flight rna- TRANSPORT - neuver ... ) Table 1: WORDNET senses, domains C!.,nd occur rences in Semcor for the word \"bank\"","Two mapping procedures have been imple mented for SENSEVAL-2 in order to use do main information. For the English tasks a map ping from WORDNET 1.6 to the WORDNET 1. 7 pre-release made available to participants; 112 for the Italian task a mapping from WoRD NET 1.6 to WoRDNET 1.5, because the inter lingual index of EuroWordNet (Vossen, 1998) is in that version. The mapping to WoRDNET 1. 7 is based on a set of heuristics (e.g. corre spondences between synonyms, glosses and hy pernyms) which discover corresponding synset pairs. Then, an inheritance algorithm is ap plied to WoRDNET 1.7 in order to fill unas signed synsets with domain labels. As far as the Italian wordnet is concerned the same pro cedure used for the WoRD NET 1.7 mapping has been applied to WoRDNET 1.5, resulting in the annotation of the Interlingual Index. Then the equivalence links (we excluded eq_hyperonym and eq_hyphonym) from the ILl to the Italian synsets were used to bring the domain informa tion to Italian words.","There was no time for a complete evaluation of the quality of the mapping procedures. 3 Algorithms The starting point in the algorithm design was the previous work in word domain disambigua tion reported in (Magnini and Strapparava, 2000). One drawback of that approach is that, for rather long texts, it does not consider do main variations. To overcome this problem we have introduced contexts within which domains are calculated. A second direction of work has been the acquisition of domain information from annotated texts (i.e. Semcor and the training data). The following sections presents details of the disambiguation procedures implemented for SENSEVAL-2. 3.1 Linguistic Processing XML files made available by the task organizers have been processed with an XML parser. As for lemmatization and part-of-speech tagging the Tree Tagger, developed at the University of Stuttgart (Schmid, 1994) has been used, both for English and Italian. The WordNet mor phological analyser has also been used in order to resolve ambiguities and lemmatization mis takes. After this process texts are represented as vectors of triples: word lemma, WoRDNET part of speech and position in the text. 3.2 Scoring Domains for a Lemma 'Fhe basic procedure in domain driven disam biguation is a function that, given a lemma L, associates a score to each domain defined for that lemma in Wordnet Domains. Such a score is the relative frequency of the domain in L, computed on the basis of the occurrences of the synsets of L in Semcor. Semcor occurrences for synsets with multiple dqmain annqtations are repeated for each domain (e.g. if a synset has 2 occurrences and 2 labels it is counted as having 4 occurrences), while synsets with 0 occurrences are counted as 0.5. As an example, consider the lemma \"bank\" in Table 1. According to our scoring method, it has 57 total occurrences in Semcor. The GEOLOGY domain collects contri butions from senses 2 and 7, for a total of 16 occurrences in Semcor, which corresponds to a frequency .28 (i.e."]},{"title":"fq[Dceology](bank) =","paragraphs":["0.28). 3.3 Domain Vectors The data structure that collects domain infor mation is called a"]},{"title":"Domain Vector","paragraphs":["(DV). Intu itively a DV represents the domains that are relevant for a certain lemma (or word sense) in a certain context. We have considered three kinds of DV's: a DV for a lemma L within a context C"]},{"title":"(DV[),","paragraphs":["for the case of test data; a DV for a synset Sofa lemma . .L within a context C ("]},{"title":"DVf),","paragraphs":["for the case of training data; and a DV for a synset Sofa lemma Lin WoRDNET"]},{"title":"(DVs),","paragraphs":["which is used when no training data are available. DV for a lemma in context"]},{"title":"(DV[).","paragraphs":["Given a set of domains D 1 ... Dn, a DV for a lemma L in a position I< within a text represents the rele vance of those domains for that lemma, i.e. each component"]},{"title":"DVL[i]","paragraphs":["gives the degree of relevance of the domain"]},{"title":"Di","paragraphs":["for the lemma"]},{"title":"L.","paragraphs":["Given a con text of"]},{"title":"±C","paragraphs":["words before and after the lemma"]},{"title":"L","paragraphs":["in the position I<, each component of the do main vector is defined with the following for mula: +C"]},{"title":"DV[[i] = L Fq[Di](Lk) *gauss","paragraphs":["k=-C where"]},{"title":"gauss","paragraphs":["is the normal distribution cen tered on the position I<. In the current al gorithms C is set to 50 because our experi ments with Semcor showed that the precision decreases below that thresold.","Intuitively, the above formula takes into ac count the contribution of the lemmas in the con text C to the sense of the target lemma L. In 113 addition a DV actually selects a set of relevant domains rather than just one domain. DV for a synset in context ("]},{"title":"DVf)","paragraphs":["In case a training corpus is available where lemmas are annotated with the correct sense, Domain Vec tors are computed with the formula above. In stead of considering a lemma in a position I< within a text, we have a sense for that lemma (i.e. a synset)."]},{"title":"DVf","paragraphs":["represents a \"typical\" vec tor for a sense Sofa lemma L. DV for a synset without context"]},{"title":"(DVs)","paragraphs":["When a training corpus is not available (as for the 'all words' task), a simpler way to build a DV for a certain synset is to compute it with respect to WordNet Domains. Given a synset S in WordNet Domains, the domain vector"]},{"title":"DVs","paragraphs":["is a vector that has 1 's in the position of its domain(s) and O's otherwise. A more accurate DV could be obtained by considering contextual information such as the synset gloss. 3.4 Comparing Domain Vectors To disambiguate a lemma L (i.e. the target lemma) in a text, first its"]},{"title":"DV[","paragraphs":["is computed. The next step consists of comparing the DV of the target lemma L with the domain vec tors for each sense of L derived either from the training set, when available, or from WordNet Domains, when training data are not available. The sense vector"]},{"title":"DVs","paragraphs":["which maximizes the sim ilarity is selected as the appropriate sense of L in that text. The similarity between two DV's is calculated with the standard scalar product:"]},{"title":"DV1 · DV2","paragraphs":["="]},{"title":"I::i DV1[i] * DV1[i]. 4 Results and Discussion","paragraphs":["Table 2 presents the results, in terms of pre cision and recall, obtained at the SENSEVAL-2 initiative for the three tasks in which we partic ipated."]},{"title":"I","paragraphs":["Task Precision"]},{"title":"I","paragraphs":["Recall"]},{"title":"I","paragraphs":["English All Words (fine g.) .748 .357 English All Words (coarse g.) .748 .357 English Lexical Sample (fine g.) .665 .249 English Lexical Sample (coarse g.) .720 .269 Italian Lexical Sample (fine g.) .375 .371 Table 2: Final results of ITC-irst systems at SENSEVAL-2 i 4.1 English 'All Words' The 'all words' task seems to benefit from the domain approach. One reason for this is that texts are enough long to provide an accurate context (as mentioned in section 3.3, we used a window of 100 content words around the tar get word) within which domains are coherent. The rather low degree of recall reflects the fact that few words in a text carry relevant domain information. Most of the words actually be have such as a \"factotum\" (see (Magnini and Cavaglia, 2000) for a preliminary discussion on this problem) that can equally occur in almost every domain. Some words lie outside the do main approach and their senses could be cap tured with the integration of local (e.g. syntac tic) information. 4.2 English 'Lexical Sample' From the point of view of domain driven dis ambiguation, the 'lexical sample' task was in herently more difficult than the 'all words' task for two reasons. First the context provided for disambiguation was generally shorter than the 100 words we used to build a semantic vector. Second, the high number of \"factotum\" words to be disambiguated resulted in a recall even lower (i.e. about 0.24) than for the 'all words' task. The improvement of performance from the fine grained to the coarse grained evalua tion seems to confirm that, at least to some de gree, domain clustering corresponds to the sense grouping created by the task organizers. 4.3 Italian 'Lexical Sample' The low results obtained for the Italian 'lexical sample' task may have several causes. First of all, the absence of a training set and the ab sence of any tagged text for Italian forced us to use a similarity function (see 3.4) trained to an English corpus. This was possible be cause we maintained the mappings between the English and the Italian wordnets. However, these multiple mappings (i.e. from WoRD NET1.6 to WORDNET1.5 and then to the Ital ian synsets through the equivalence links) are another source of possible errors, especially con cerning the domain information associated with Italian synsets."]},{"title":"5 Conclusions","paragraphs":["We have described an approach to word sense disambiguation based on domain information. The underlying assumption is that domains con stitute a fundamental feature of text coherence. As a consequence, word senses occurring in a co herent portion of text tend to maximize domain similarity. Three systems have been imple mented, integrating knowledge-based and sta tistical techniques, for the three task we partic ipated in. As for lexical resources, the systems make use of WordNet Domains, an extension of English Wordnet 1.6, where synsets have been annotated with domain information. The dis ambiguation algorithm is based on domain vec tors that collect contextual information with re spect to the target word. At this moment only domain information is used in our system. A promising research direction is the use of local information (e.g. syntax) to capture word be haviors that lie outside the domain approach."]},{"title":"References","paragraphs":["P. Buitelaar and B. Sacaleanu. 2001. Ranking and selecting synsets by domain relevance. In Proc. of NAACL Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, Pittsburgh, PA, June.","C. Fellbaum. 1998. WordNet. An Electronic Lexical Database. The MIT Press.","J. Gonzalo, F. Verdejio, C. Peters, and N. Cal zolari. 1998. Applying eurowordnet to cross language text retrieval. Computers and Hu manities, 32(2-3):185~207.","B. Magnini and G. Cavaglia. 2000. Integrat ing subject field codes into WordNet. In Pro ceedings of LREC-2000, Second International Conference on Language Resources and Eval uation, Athens, Greece, June.","B. Magnini and C. Strapparava. 2000. Exper iments in word domain disambiguation for parallel texts. In Proc. of SIGLEX Workshop on Word Senses and Multi-linguality, Hong Kong, October.","H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Meth ods in Language Processing.",", P. Vossen. 1998. Special issue on eurowordnet. ·computers and Humanities, 32. 114"]}]}