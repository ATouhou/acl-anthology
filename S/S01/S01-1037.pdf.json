{"sections":[{"title":"WASP-Bench: a Lexicographic Tool Supporting Word Sense Disambiguation David Tugwell & Adam Kilgarriff ITRI, University of Brighton Lewes Road, Brighton BN2 4GJ, UK","paragraphs":["{David.Tugwell,Adam.Kilgarriff}©itri.bton.ac.uk"]},{"title":"Abstract","paragraphs":["We present WASP-Bench: a novel approach to Word Sense Disambiguation, also providing a semi-automatic environment for a lexicographer to compose dictionary entries based on corpus evidence. For WSD, involving lexicographers tackles the twin obstacles to high accuracy: paucity of training data and insufficiently ex plicit dictionaries. For lexicographers, the com putational environment fills the need for a cor pus workbench which supports WSD. Results under simulated lexicographic use on the En glish lexical-sample task show precision compa rable with supervised systems1, without using the laboriously-prepared training data."]},{"title":"1 Introduction","paragraphs":["WASP-Bench2 is a web-based tool support ing both corpus-based lexicography and Word Sense Disambiguation. The central premise be hind the initiative is that deciding what the senses for a word are, and developing a WSD program for it, should be tightly coupled. In the course of the corpus analysis, the lexicographer explores the textual clues that indicate a word is being used in one sense or another; given an appropriate computational environment, these clues can be gathered and used to seed a boot strapping WSD program.","This strategy clearly requires human input for each word to be disambiguated, which may raise","1","It should be noted that the lower figure for recall reflects solely the fact that not all words were attempted due to time constraints.","2","The system has been developed under EP-SRC project M54971. A demo is available at http:jjwasps.itri.bton.ac.uk. The second author was also a co-ordinator for the SENSEVAL-2 evaluation exercise-to limit any conflict of interest only the first author was in volved applying the system to the SENSEVAL-2 task and had no prior knowledge of the format of the task. ' 151 the objection that the lexicon is far too large for any word-by-word work to be viable. How ever, the amount of human interaction needed is far less than that involved in preparing train ing data3 and lexicographers are already in the position of having to inspect every word in the vocabulary. If they use a interactive tool such as the WASP-Bench to help them in this, then total coverage becomes a feasible proposition."]},{"title":"2 WASP-Bench Methodology","paragraphs":["The workbench is implemented in perl and uses cgi-scripts and a browser for user interaction. 2.1 Grammatical relations database The central resource is a collection of all gram matical relations holding between words in the corpus. The corpus currently used in WASP Bench is the British National Corpus4 (BNC): . Using finite-state techniques operating over part-of-speech tags, we process the whole cor pus finding quintuples of the form: {Rei, Wl,"]},{"title":"W2,","paragraphs":["Prep, Position}, where Rei is a relation, Wl is the lemma of the word for which Rei holds,"]},{"title":"W2","paragraphs":["is the lemma of the other open-class word involved, Prep is the preposition or parti cle involved and Position is the position of Wl in the corpus. Relations may have null values for"]},{"title":"W2","paragraphs":["and Prep. The database contains 70 million quintuples.","The current inventory of relations is shown in Table 1. All inverse relations, ie. subject-of etc, found by taking W2 as the head word in stead of Wl are explicitly represented, to give a total of twenty-six distinct relations. These pro vide a flexible resource to be used as the basis of the computations of the workbench. Keeping 3 See results section for details. 4 100 million words of contemporary British English.","see http://info.ox.ac.uk/bnc"]},{"title":"I","paragraphs":["relation example bare-noun the angle of bankT possessive my bank1 plural the banks1 passive was seen1 reflexive see1","herself ing-comp love1 eating fish finite-comp know1 he came inf-comp decision 1 to eat fish wh-comp know1","why he came subject the bank1 refusedT object climb1 the bank1 adj-comp grow1 certain2 noun- modifier merchant2","bank1 modifier a big2 bank:l and-or banks1 and mounds2 predicate banks1","are barriers:r particle grow1 upP Prep+gerund tired1","ofP eating fish"]},{"title":"I","paragraphs":["PP-comp/mod j banks1 ofP the river:.! Table 1: Grammatical Relations the position numbers of examples allows us to find associations between relations and to dis play examples. 2.2 Word Sketches The user enters the word and using the gram matical relations database, the system com poses a word sketch for this word. This is a page of data such as Table 2, which shows, for the word in question (W1), ordered lists of high-salience grammatical relations, relation W2 pairs, and relation-W2-Prep triples for the word.","The number of patterns shown is set by the user, but will typically be over 200. These are listed for each relation in order of salience, with the count of corpus instances. The instances can be instantly retrieved and shown in a con cordance window. Producing a word sketch for a medium-to-high frequency word takes in the order of ten seconds.","Salience is calculated as the product of Mu tual Information I (Church and Hanks, 1989) and log frequency. I for a word W1 in a gram matical relation Rel5 with a second word W2 is calculated as: 5 {Grammatical-relation, preposition} pairs","treated as atomic relations in calculating MI. are 152","I(Wl Rel W2) = log(II*,Rel,*llxttWl,Rel,W21) ' ' \\IWl,Re!,*llxii*,Re!,W21 The notation here is adopted from (Lin, 1998) (who also spells out the derivation from the definition of"]},{"title":"I).","paragraphs":["IIW1, Rel, W2ll denotes the frequency count of the triple {W1, Rel, W2}6 in the grammatical relations database. Where W1, Rel or W2 is the wild card"]},{"title":"(*),","paragraphs":["the fre quency is of all the dependency triples that match the remainder of the pattern.","The word sketches are presented to the user as a list of relations, with items in each list or dered according to salience. Our experience of working lexicographers' use of Mutual Informa tion or log-likelihood lists shows that, for lex icographic purposes, these over-emphasise low frequency items, and that multiplying by log frequency is an appropriate adjustment. 2.3 Matching patterns with senses The next task is to enter a preliminary list of senses for the word, possibly in the form of some arbitrary mnemonics: for example, MONEY, CLOUD and RIVER for three senses of bank. 7 This inventory may be drawn from the user's knowledge, from a perusal of the word sketch, or from a pre-existing dictionary entry.","As Table 2 shows, and in keeping with \"one sense per collocation\" (Yarowsky, 1993) in most cases, high-salience patterns or clues indicate just one of the word's senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unam biguous clues. The number of relations marked will depend on the time available, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the user to discover fresh, unconsid ered senses usages of the word.","The pattern-sense associations are then sub mitted to the next stage: automatic disam biguation. 2.4 The Disambiguation Algorithm The workbench currently uses Yarowsky's de cision list approach to WSD (Yarowsky, 1995). This is a bootstrapping algorithm that, given 6 0r, strictly, of the quintuple {Wl, Rel - part -","1, W2, Rel- part- 2, ANY}. • 7 W:ASP-Bench can also be used for Machine Transla","tion lexicography, where arbitrary mnemonics would be","replaced by target language translations."]},{"title":"I","paragraphs":["subj-of num sal"]},{"title":"I","paragraphs":["obj-of num sal"]},{"title":"I","paragraphs":["modifier num sal"]},{"title":"I","paragraphs":["n-mod num sal"]},{"title":"I","paragraphs":["lend 95 21.2 burst 27 16.4 central 755 25.5 merchant 213 29.4 issue 60 11.8 rob 31 15.3 Swiss 87 18.7 clearing 127 27.0 charge 29 9.5 overflow 7 10.2 commercial 231 18.6 river 217 25.4 operate 45 8.9 line 13 8.4 grassy 42 18.5 creditor 52 22.8 modifies pp inv-PP and-or holiday 404 32.6 of England 988 37.5 governor of 108 26.2 society 287 24.6 account 503 32.0 of Scotland 242 26.9 balance at 25 20.2 bank 107 17.7 loan 108 27.5 of river 111 22.1 borrow from 42 19.1 institution 82 16.0 lending 68 26.1 of Thames 41 20.1 account with 30 18.4 Lloyds 11 14.1 Table 2: Extract of word sketch for bank some initial seeding, iteratively divides the corpus examples into the different senses. Yarowsky notes that the most effective ini tial seeding option he considered was labelling salient corpus collocates with different senses. The user's first interaction with the workbench is just this.","At the user-input stage, only clues involving grammatical relations are used. At the WSD al gorithm stage, some \"bag-of-words\" and n-gram clues are also considered. Any content word (lemmatised) occurring within a k-word window of the nodeword is a bag-of-words clue.8"]},{"title":"N","paragraphs":["gram clues capture local context which may not be covered by any grammatical relation. The n-gram clues are all bigrams and trigrams in cluding the nodeword. N-grams and context word clues frequently duplicate the grammati cal relations already found, but the merit of the decision list approach is that probabilities are not combined, so such dependencies are not a problem. 2.5 Sense Profiles The output of the algorithm is both a sense dis ambiguated corpus, and a decision list. The de cision list can be viewed as a lexical entry or as a WSD program. It will contain {Rel, W2} pairs (as in the original word sketch), bag-of words words, and n-grams. The components of the decision list which assign to a particular sense can be displayed as \"sense profiles\", in a manner comparable to the original word sketch. They will contain new clues, not originally seen in the word sketch and may point to new senses 8 The user can set the value of k. The default is cur","rently 30. or usages needing addition to the lexical entry. Users can then re-run the WSD algorithm, it erating until they are satisfied with the sense inventory, and with the accuracy of the disam biguation performed."]},{"title":"3 Evaluating the workbench","paragraphs":["3.1 Lexicographic evaluation For the last two years, a set of 6000 word sketches has been used in a large dictio nary project (Rundell, 2002), with a team of thirty professional lexicographers covering ev ery medium-to-high frequency noun, verb and adjective of English. The feedback received is that they are hugely useful, and transform the way the lexicographer uses the corpus. They radically reduce the amount of time the lex icographers need to spend reading individual instances, and give the dictionary improved claims to completeness, as common patterns are far less likely to be missed. 3.2 Results for senseval-2 Performance as a WSD system was evaluated on the SENSEVAL-2 English lexical sample exercise.","The words to be tested were divided between the first author and one paid volunteer, who had no previous experience of WASP-Bench. We carried out the procedure as above, with the difference that instead of having to establish a sense inventory, the inventory was already given as that of WordNet. After assigning sufficient clues to cover the various senses, these assign ments were submitted as seeds to the disam biguation algorithm. Using the example sen tences from the BNC this gave us a decision list of clues, which could then be used to disam biguate the test sentences. 153","The marking of senses took anywhere from 3 to 35 minutes, depending upon the subtlety of the sense divisions to be made. The average time was around 15 minutes per word. A sub stantial part of this was taken up by reading and understanding the dictionary entry even before patterns were marked. Crucially we made no use of the training data,9 although this would certainly have been of use as a reference in clar ifying the sense distinctions to be made. U nfor tunately, due to severe time constraints, it only proved possible to carry out analysis for the 29 nouns and 15 adjectives in the lexical sample, and there was no time to carry out the analysis of the verbs. 10","Results on the task were 66.1% for coarse grained precision and 58.1% for fine-grained. 11 This was significantly higher than other systems which did not use the training data (the best scores being 51.8% for coarse-grained and 40.2% for fine-grained precision), demonstrating that the relatively small amount of human interac tion is very beneficial. Indeed, the system's per formance was similar to the majority of systems which had used the training data. 3.2.1 Significant problems The most pervasive problem was the difficulty of getting a clear conception of the sense dis tinctions made in the inventory, here WordNet. Without this, assigning putative senses to clues could be an exasperating and painful task.","To illustrate, for the adjective simple there were no less than 13 sense distinctions to be made, the first two of which were particularly hard to distinguish:","1. simple (vs. complex) - (not complex or complicated or involved): a simple problem","2. elementary, simple, uncomplicated, un problematic - (not involved or compli cated): an elementary problem in statistics","9","In fact, we had to download the data to find out the words to be tested, but made no other use of it.","10","Also no results were returned for the noun day, as processing the 93,000+ examples in the BNC led to an processing delay that could not be fixed in time.","11","Due to the limited number of words attempted the figures for recall were 36.3% and 31.9%. It should be understood that there was no precision/recall tradeoff here-the system returned an answer for all sentences in the words it covered. 154","U nsurprisingly, the system fared particularly badly here with 37.9% precision, while inter annotator agreement was also low at 67.8%. 3.2.2 Previous results We previously measured the performance of the system on the dataset from the SENSEVAL-1 ex ercise (Kilgarriff and Palmer, 2000) under sim ilar conditions of use. Results for the WASP Bench here were significantly higher at 74.9% precision which was very close to the best super vised system (within 1%). This was undoubt edly due to the clearer sense distinctions and greater number of examples to be found in the sense inventory used for this task in SENSEVAL-1, which made it possible to assign senses to clues with more confidence."]},{"title":"4 Summary","paragraphs":["The results for the WASP-Bench show that high-quality disambiguation can be achieved with much less human interaction than is needed for preparing a training corpus. Further more, this interaction can be motivated since it has been shown to be of proven benefit for the users of the system: lexicographers. Establish ing this synergy may prove to be of great Im portance for both camps. References","Kenneth Church and Patrick Hanks. 1989. Word association norms, mutual information and lexicography. In A CL Proceedings, 27th Annual Meeting, pages 76-83, Vancouver.","Adam Kilgarriff and Martha Palmer. 2000. Introduction, Special Issue on SENSEVAL: Evaluating Word Sense Disambiguation Pro grams. Computers and the Humanities, 34(1-2):1-13.","Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In COLING ACL, pages 768-774, Montreal.","Michael Rundell. 2002. Macmillan English Dic tionary for Advanced Learners. Macmillan.","David Yarowsky. 1993. One sense per colloca tion. In Proc. ARPA Human Language Tech nology Workshop, Princeton.","David Yarowsky. 1995. Unsupervised word sense disambiguation rivalling supervised methods. In ACL 95, pages 189-196, MIT."]}]}