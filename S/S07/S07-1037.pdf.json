{"sections":[{"title":"","paragraphs":["Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 177–182, Prague, June 2007. c⃝2007 Association for Computational Linguistics"]},{"title":"I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense Disambiguation, and English Word Sense Disambiguation Zheng-Yu Niu, Dong-Hong Ji Institute for Infocomm Research 21 Heng Mui Keng Terrace 119613 Singapore niu zy@hotmail.com dhji@i2r.a-star.edu.sg Chew-Lim Tan Department of Computer Science National University of Singapore 3 Science Drive 2 117543 Singapore tancl@comp.nus.edu.sg Abstract","paragraphs":["This paper describes the implementation of our three systems at SemEval-2007, for task 2 (word sense discrimination), task 5 (Chinese word sense disambiguation), and the rst subtask in task 17 (English word sense disambiguation). For task 2, we applied a cluster validation method to estimate the number of senses of a target word in untagged data, and then grouped the instances of this target word into the estimated number of clusters. For both task 5 and task 17, We used the label propagation algorithm as the classier for sense disambiguation. Our system at task 2 achieved 63.9% F-score under unsupervised evaluation, and 71.9% supervised recall with supervised evaluation. For task 5, our system obtained 71.2% micro-average precision and 74.7% macro-average precision. For the lexical sample subtask for task 17, our system achieved 86.4% coarse-grained precision and recall."]},{"title":"1 Introduction","paragraphs":["SemEval-2007 launches totally 18 tasks for evaluation exercise, covering word sense disambiguation, word sense discrimination, semantic role labeling, and sense disambiguation for information retrieval, and other topics in NLP. We participated three tasks in SemEval-2007, which are task 2 (Evaluating Word Sense Induction and Discrimination Systems), task 5 (Multilingual Chinese-English Lexical Sample Task) and the rst subtask at task 17 (English Lexical Sample, English Semantic Role Labeling and English All-Words Tasks).","The goal for SemEval-2007 task 2 (Evaluating Word Sense Induction and Discrimination Systems)(Agirre and Soroa, 2007) is to automatically discriminate the senses of English target words by the use of only untagged data. Here we address this word sense discrimination problem by (1) estimat-ing the number of word senses of a target word in untagged data using a stability criterion, and then (2) grouping the instances of this target word into the estimated number of clusters according to the similarity of contexts of the instances. No sense-tagged data is used to help the clustering process.","The goal of task 5 (Chinese Word Sense Disambiguation) is to create a framework for the evaluation of word sense disambiguation in Chinese-English machine translation systems. Each participates of this task will be provided with sense tagged training data and untagged test data for 40 Chinese polysemous words. The sense tags for the ambiguous Chinese target words are given in the form of their English translations. Here we used a semi-supervised classication algorithm (label propagation algorithm) (Niu, et al., 2005) to address this Chinese word sense disambiguation problem.","The lexical sample subtask of task 17 (English Word Sense Disambiguation) provides sense-tagged training data and untagged test data for 35 nouns and 65 verbs. This data includes, for each target word: OntoNotes sense tags (these are groupings of Word-Net senses that are more coarse-grained than tradi-177 tional WN entries), as well as the sense inventory for these lemmas. Here we used only the training data supplied in this subtask for sense disambiguation in test set. The label propagation algorithm (Niu, et al., 2005) was used to perform sense disambiguation by the use of both training data and test data.","This paper will be organized as follows. First, we will provide the feature set used for task 2, task 5 and task 17 in section 2. Secondly, we will present the word sense discrimination method used for task 2 in section 3. Then, we will give the label propagation algorithm for task 5 and task 17 in section 4. Section 5 will provide the description of data sets at task 2, task 5 and task 17. Then, we will present the experimental results of our systems at the three tasks in section 6. Finally we will give a conclusion of our work in section 7."]},{"title":"2 Feature Set","paragraphs":["In task 2, task 5 and task 17, we used three types of features to capture contextual information: part-of-speech of neighboring words (no more than three-word distance) with position information, unordered single words in topical context (all the contextual sentences), and local collocations (including 11 collocations). The feature set used here is as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations."]},{"title":"3 The Word Sense Discrimination Method for Task 2","paragraphs":["Word sense discrimination is to automatically discriminate the senses of target words by the use of only untagged data. So we can employ clustering algorithms to address this problem. Another problem is that there is no sense inventories for target words. So the clustering algorithms should have the ability to automatically estimate the sense number of a target word.","Here we used the sequential Information Bottleneck algorithm (sIB) (Slonim, et al., 2002) to estimate cluster structure, which measures the similarity of contexts of instances of target words according to the similarity of their contextual feature conditional distribution. But sIB requires the number of clusters as input. So we used a cluster validation method to automatically estimate the sense number of a tar-Table 1: Sense number estimation procedure for word sense discrimination. 1 Set lower bound Kmin and upper bound Kmax","for sense number k; 2 Set k = Kmin; 3 Conduct the cluster validation process","presented in Table 2 to evaluate the merit of k; 4 Record k and the value of Mk; 5 Set k = k + 1. If k ≤ Kmax, go to step 3,","otherwise go to step 6; 6 Choose the value k̂ that maximizes Mk,","where k̂ is the estimated sense number. get word before clustering analysis. Cluster validation (or stability based approach)is a commonly used method to the problem of model order identication (or cluster number estimation) (Lange, et al., 2002; Levine and Domany, 2001). The assumption of this method is that if the model order is identical with the true value, then the cluster structure estimated from the data is stable against resampling, otherwise, it is more likely to be the artifact of sampled data. 3.1 The Sense Number Estimation Procedure Table 1 presents the sense number estimation procedure. Kmin was set as 2, and Kmax was set as 5 in our system. The evaluation function Mk (described in Table 2) is relevant with the sense number k. q is set as 20 here. Clustering solution which is stable against resampling will give rise to a local optimum of Mk, which indicates the true value of sense number. In the cluster validation procedure, we used the sIB algorithm to perform clustering analysis (described in section 3.2).","The function M (Cμ",", C) in Table 2 is given by (Levine and Domany, 2001):","M(Cμ , C) =","∑","i,j 1{Cμ i,j = Ci,j = 1, di ∈ Dμ",", dj ∈ Dμ","}","∑","i,j 1{Ci,j = 1, di ∈ Dμ",", dj ∈ Dμ","} ,","(1)","where Dμ","is a subset with size α|D| sampled from","full data set D, C and C μ","are |D| × |D| connectivity","matrixes based on clustering solutions computed on","D and Dμ","respectively, and 0 ≤ α ≤ 1. The con-","nectivity matrix C is dened as: Ci,j = 1 if di and","dj belong to the same cluster, otherwise Ci,j = 0.","Cμ is calculated in the same way. α is set as 0.90 in","this paper. 178","Table 2: The cluster validation method for evalua-","tion of values of sense number k. Function: Cluster Validation(k, D, q) Input: cluster number k, data set D, and sampling frequency q; Output: the score of the merit of k;","1 Perform clustering analysis using sIB on data set D with k as input;","2 Construct connectivity matrix Ck based on above clustering solution on D;","3 Use a random predictor ρk to assign uniformly drawn labels to instances in D;","4 Construct connectivity matrix Cρk using above clustering solution on D;","5 For μ = 1 to q do","5.1 Randomly sample a subset (Dμ",") with size α|D| from D, 0 ≤ α ≤ 1;","5.2 Perform clustering analysis using sIB on (Dμ",") with k as input;","5.3 Construct connectivity matrix C","μ","k using above clustering solution on (Dμ",");","5.4 Use ρk to assign uniformly drawn labels to instances in (Dμ",");","5.5 Construct connectivity matrix C μ","ρk using above clustering solution on (Dμ","); Endfor","6 Evaluate the merit of k using following objective function: Mk = 1","q","∑ μ M (C μ k , Ck) − 1","q","∑","μ M (Cμ","ρk , Cρk ),","where M (Cμ",", C) is given by equation (1);","7 Return Mk;","M (Cμ",", C) measures the proportion of document pairs in each cluster computed on D that are also as-signed into the same cluster by clustering solution on Dμ",". Clearly, 0 ≤ M ≤ 1. Intuitively, if cluster number k is identical with the true value, then clustering results on different subsets generated by sampling should be similar with that on full data set, which gives rise to a local optimum of M (C μ",", C). In our algorithm, we normalize M (C","μ","F,k, CF,k) using the equation in step 6 of Table 2, which makes our objective function different from the gure of merit (equation ( 1)) proposed in (Levine and Domany, 2001). The reason to normalize M (C μ F,k, CF,k) is that M (C μ F,k, CF,k) tends to decrease when increasing the value of k. Therefore for avoiding the bias that smaller value of k is to be selected as cluster number, we use the cluster validity of a random predictor to normalize M (C μ F,k, CF,k). 3.2 The sIB Clustering Algorithm Here we used the sIB algorithm (Slonim, et al., 2002) to estimate cluster structure, which measures the similarity of contexts of instances according to the similarity of their feature conditional distribu-tion. sIB is a simplied hard variant of information bottleneck method (Tishby, et al., 1999).","Let d represent a document, and w represent a feature word, d ∈ D, w ∈ F . Given the joint distribution p(d, w), the document clustering problem is formulated as looking for a compact representation T for D, which preserves as much information as possible about F . T is the document clustering solution. For solving this optimization problem, sIB algorithm was proposed in (Slonim, et al., 2002), which found a local maximum of I(T, F ) by: given an initial partition T , iteratively drawing a d ∈ D out of its cluster t(d), t ∈ T , and merging it into tnew","such that tnew","= argmax","t∈T d(d, t). d(d, t) is","the change of I(T, F ) due to merging d into cluster","tnew",", which is given by d(d, t) = (p(d) + p(t))J S(p(w|d), p(w|t)). (2) J S(p, q) is the Jensen-Shannon divergence, which is dened as J S(p, q) = πpDKL(p∥p) + πqDKL(q∥p), (3) DKL(p∥p) = ∑ y plog p p , (4) DKL(q∥p) = ∑ y qlog q p , (5) {p, q} ≡ {p(w|d), p(w|t)}, (6) {πp, πq} ≡ {","p(d) p(d) + p(t) ,","p(t) p(d) + p(t) }, (7) p = πpp(w|d) + πqp(w|t). (8) 179"]},{"title":"4 The Label Propagation Algorithm for Task 5 and Task 17","paragraphs":["In the label propagation algorithm (LP) (Zhu and Ghahramani, 2002), label information of any vertex in a graph is propagated to nearby vertices through weighted edges until a global stable stage is achieved. Larger edge weights allow labels to travel through easier. Thus the closer the examples, more likely they have similar labels (the global consistency assumption).","In label propagation process, the soft label of each initial labeled example is clamped in each iteration to replenish label sources from these labeled data. Thus the labeled data act like sources to push out labels through unlabeled data. With this push from labeled examples, the class boundaries will be pushed through edges with large weights and settle in gaps along edges with small weights. If the data structure ts the classication goal, then LP algorithm can use these unlabeled data to help learning classication plane.","Let Y 0","∈ N n×c","represent initial soft labels at-tached to vertices, where Y 0","ij = 1 if yi is sj and 0","otherwise. Let Y 0","L be the top l rows of Y 0","and Y 0","U","be the remaining u rows. Y 0","L is consistent with the","labeling in labeled data, and the initialization of Y 0","U can be arbitrary.","Optimally we expect that the value of Wij across different classes is as small as possible and the value of Wij within same class is as large as possible. This will make label propagation to stay within same class. In later experiments, we set σ as the average distance between labeled examples from different classes.","Dene n × n probability transition matrix Tij = P (j → i) =","Wij","∑n","k=1 Wkj , where Tij is the probability","to jump from example xj to example xi.","Compute the row-normalized matrix T by T ij =","Tij/ ∑n","k=1 Tik. This normalization is to maintain","the class probability interpretation of Y .","Then LP algorithm is dened as follows:","1. Initially set t=0, where t is iteration index;","2. Propagate the label by Y t+1","= T Y t",";","3. Clamp labeled data by replacing the top l row","of Y t+1","with Y 0","L . Repeat from step 2 until Y t","converges; 4. Assign xh(l + 1 ≤ h ≤ n) with a label sĵ,","where ĵ = argmaxjYhj. This algorithm has been shown to converge to","a unique solution, which is ̂YU = limt→∞ Y t","U =","(I − T uu)−1 T ulY 0","L (Zhu and Ghahramani, 2002).","We can see that this solution can be obtained with-","out iteration and the initialization of Y 0","U is not important, since Y 0","U does not affect the estimation of ̂YU . I is u × u identity matrix. T uu and T","ul are acquired by splitting matrix T after the l-th row and the l-th column into 4 sub-matrices.","For task 5 and 17, we constructed connected graphs as follows: two instances u, v will be connected by an edge if u is among v’s k nearest neighbors, or if v is among u’s k nearest neighbors as measured by cosine or JS distance measure. k is set 10 in our system implementation."]},{"title":"5 Data Sets of Task 2, Task 5 and Task 17","paragraphs":["The test data for task 2 includes totally 27132 untagged instances for 100 ambiguous English words. There is no training data for task 2.","There are 40 ambiguous Chinese words in task 5. The training data for this task consists of 2686 instances, while the test data includes 935 instances.","There are 100 ambiguous English words in the rst subtask of task 17. The training data for this task consists of 22281 instances, while the test data includes 4851 instances."]},{"title":"6 Experimental Results of Our Systems at Task 2, Task 5 and Task 17","paragraphs":["Table 3: The best/worst/average F-score of all the","systems at task 2 and the F-score of our system at","task 2 for all target words, nouns and verbs with un-","supervised evaluation.","All words Nouns Verbs","Best 78.7% 80.8% 76.3%","Worst 56.1% 65.8% 45.1% Average 65.4% 69.0% 61.4% Our system 63.9% 68.0% 59.3%","Table 3 lists the best/worst/average F-score of all the systems at task 2 and the F-score of our system at task 2 for all target words, nouns and verbs with 180","Table 4: The best/worst/average supervised recall of","all the systems at task 2 and the supervised recall of","our system at task 2 for all target words, nouns and","verbs with supervised evaluation.","All words Nouns Verbs","Best 81.6% 86.8% 75.7%","Worst 78.5% 81.4% 75.2% Average 79.6% 83.0% 75.7% Our system 81.6% 86.8% 75.7%","Table 5: The best/worst/average micro-average pre-","cision and macro-average precision of all the sys-","tems at task 5 and the micro-average precision and","macro-average precision of our system at task 5.","Micro-average Macro-average","Best 71.7% 74.9%","Worst 33.7% 39.6% Average 58.5% 62.7% Our system 71.2% 74.7% unsupervised evaluation. Our system obtained the fourth place among six systems with unsupervised evaluation. Table 4 shows the best/worst/average supervised recall of all the systems at task 2 and the supervised recall of our system at task 2 for all target words, nouns and verbs with supervised evaluation. Our system is ranked as the rst among six systems with supervised evaluation. Table 7 lists the estimated sense numbers by our system for all the words at task 2. The average of all the estimated sense numbers is 3.1, while the average of all the ground-truth sense numbers is 3.6 if we consider the sense inventories provided in task 17 as the answer. It seems that our estimated sense numbers are close to the ground-truth ones.","Table 5 provides the best/worst/average micro-average precision and macro-average precision of all the systems at task 5 and the micro-average precision and macro-average precision of our system at task 5. Our system obtained the second place among six systems for task 5.","Table 6 shows the best/worst/average coarse-grained score (precision) of all the systems the lexical sample subtask of task 17 and the coarse-grained score (precision) of our system at the lexical sample Table 6: The best/worst/average coarse-grained score (precision) of all the systems at the lexical sample subtask of task 17 and the coarse-grained score (precision) of our system at the lexical sample subtask of task 17.","Coarse-grained score (precision)","Best 88.7%","Worst 52.1%","Average 70.0% Our system 86.4% subtask of task 17. The attempted rate of all the systems is 100%. So the precision value is equal to the recall value for all the systems. Here we listed only the precision for the 13 systems at this subtask. Our system is ranked as the third one among 13 systems."]},{"title":"7 Conclusion","paragraphs":["In this paper, we described the implementation of our I2R systems that participated in task 2, task 5, and task 17 at SemEval-2007. Our systems achieved 63.9% F-score and 81.6% supervised recall for task 2, 71.2% micro-average precision and 74.7% macro-average precision for task 5, and 86.4% coarse-grained precision and recall for the lexical sample subtask of task 17. The performance of our system is very good under supervised evaluation. It may be explained by that our system has the ability to nd some minor senses so that it can outperforms the baseline system that always uses the most frequent sense as the answer."]},{"title":"References","paragraphs":["Agirre E. , & Soroa A. 2007. SemEval-2007 Task 2: Evaluating Word Sense Induction and Discrimination Systems. Proceedings of SemEval-2007, Association for Computational Linguistics.","Lange, T., Braun, M., Roth, V., & Buhmann, J. M. 2002. Stability-Based Model Selection. Advances in Neural Information Processing Systems 15.","Lee, Y.K., & Ng, H.T. 2002. An Empirical Evalua-tion of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation. Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, (pp. 41-48). 181","Levine, E., & Domany, E. 2001. Resampling Method for Unsupervised Estimation of Cluster Validity. Neural Computation, Vol. 13, 25732593.","Niu, Z.Y., Ji, D.H., & Tan, C.L. 2005. Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning. Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.","Slonim, N., Friedman, N., & Tishby, N. 2002. Unsupervised Document Classication Using Sequential Information Maximization. Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.","Tishby, N., Pereira, F., & Bialek, W. (1999) The Information Bottleneck Method. Proc. of the 37th Allerton Conference on Communication, Control and Comput-ing.","Zhu, X. & Ghahramani, Z.. 2002. Learning from Labeled and Unlabeled Data with Label Propagation. CMU CALD tech report CMU-CALD-02-107.","Table 7: The estimated sense numbers by our system","for all the words at task 2. explain 2 move 3 position 3 express 4 buy 2 begin 2 hope 3 prepare 3 feel 5 policy 2 hold 2 attempt 2 work 5 recall 3 people 4 nd 2 system 2 join 2 bill 2 build 2 hour 5 base 3 value 4 management 2 job 5 turn 4 rush 2 kill 2 ask 2 area 5 approve 4 affect 4 capital 4 keep 5 purchase 2 improve 2 propose 2 do 2 see 3 drug 5 president 3 come 5 power 3 disclose 4 effect 2 avoid 3 part 5 plant 2 exchange 4 share 2 state 2 carrier 2 care 5 complete 2 promise 3 maintain 3 estimate 2 development 4 rate 2 space 5 say 2 raise 3 remove 5 future 3 grant 4 network 3","remember 3 announce 5 cause 2 start 3 point 5 order 2 occur 4 defense 5 authority 3 set 3 regard 2 chance 2 go 3 produce 2 allow 4 negotiate 2 describe 2 enjoy 4 prove 3 exist 4 claim 4 replace 3 x 2 examine 3 end 5 lead 3 receive 3 source 2 complain 3 report 2 need 2 believe 2 condition 2 contribute 3 182"]}]}