{"sections":[{"title":"","paragraphs":["Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 138–141, Uppsala, Sweden, 15-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"OWNS: Cross-lingual Word Sense Disambiguation Using Weighted Overlap Counts and Wordnet Based Similarity Measures Lipta Mahapatra Meera Mohan Dharmsinh Desai University Nadiad, India","paragraphs":["lipta.mahapatra89@gmail.com mu.mohan@gmail.com"]},{"title":"Mitesh M. Khapra Pushpak Bhattacharyya Indian Institute of Technology Bombay Powai, Mumbai 400076,India","paragraphs":["miteshk@cse.iitb.ac.in pb@cse.iitb.ac.in"]},{"title":"Abstract","paragraphs":["We report here our work on English French Cross-lingual Word Sense Disambiguation where the task is to find the best French translation for a target English word depending on the context in which it is used. Our approach relies on identifying the nearest neighbors of the test sentence from the training data using a pairwise similarity measure. The proposed measure finds the affinity between two sentences by calculating a weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures. Once the nearest neighbors have been identified, the best translation is found by taking a majority vote over the French translations of the nearest neighbors."]},{"title":"1 Introduction","paragraphs":["Cross Language Word Sense Disambiguation (CL-WSD) is the problem of finding the correct target language translation of a word given the context in which it appears in the source language. In many cases a full disambiguation may not be necessary as it is common for different meanings of a word to have the same translation. This is especially true in cases where the sense distinction is very fine and two or more senses of a word are closely related. For example, the two senses of the word letter, namely, “formal document’ and “written/printed message” have the same French translation “lettre”. The problem is thus reduced to distinguishing between the coarser senses of a word and ignoring the finer sense distinctions which is known to be a common cause of errors in conventional WSD. CL-WSD can thus be seen as a slightly relaxed version of the conventional WSD problem. However, CL-WSD has its own set of challenges as described below.","The translations learnt from a parallel corpus may contain a lot of errors. Such errors are hard to avoid due to the inherent noise associated with statistical alignment models. This problem can be overcome if good bilingual dictionaries are available between the source and target language. EuroWordNet1","can be used to construct such a bilingual dictionary between English and French but it is not freely available. Instead, in this work, we use a noisy statistical dictionary learnt from the Europarl parallel corpus (Koehn, 2005) which is freely downloadable.","Another challenge arises in the form of match-ing the lexical choice of a native speaker. For example, the word coach (as in, vehicle) may get translated differently as autocar, autobus or bus even when it appears in very similar contexts. Such decisions depend on the native speaker’s in-tuition and are very difficult for a machine to replicate due to their inconsistent usage in a parallel training corpus.","The above challenges are indeed hard to overcome, especially in an unsupervised setting, as evidenced by the lower accuracies reported by all systems participating in the SEMEVAL Shared Task on Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010). Our system ranked second in the English French task (in the out-of-five evaluation). Even though its average performance was lower than the baseline by 3% it performed better than the baseline for 12 out of the 20 target nouns.","Our approach identifies the top-five translations of a word by taking a majority vote over the translations appearing in the nearest neighbors of the test sentence as found in the training data. We use a pairwise similarity measure which finds the affinity between two sentences by calculating a","1","http://www.illc.uva.nl/EuroWordNet 138 weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures.","The remainder of this paper is organized as follows. In section 2 we describe related work on WSD. In section 3 we describe our approach. In Section 4 we present the results followed by conclusion in section 5."]},{"title":"2 Related Work","paragraphs":["Knowledge based approaches to WSD such as Lesk’s algorithm (Lesk, 1986), Walker’s algorithm (Walker and Amsler, 1986), Conceptual Density (Agirre and Rigau, 1996) and Random Walk Algorithm (Mihalcea, 2005) are fundamentally overlap based algorithms which suffer from data sparsity. While these approaches do well in cases where there is a surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is a semantic match between two occurrences of the target word even though there is no surface match between them. The main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., train is-a vehicle).","On the other hand, WSD approaches which use Wordnet based semantic similarity measures (Patwardhan et al., 2003) account for such semantic generalizations and can be used in conjunc-tion with overlap based approaches. We there-fore propose a scoring function which combines the strength of overlap based approaches – frequently co-occurring words indeed provide strong clues – with semantic generalizations using Wordnet based similarity measures. The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. Once the nearest neighbors have been identified, the best translation is found by taking a majority vote over the translations of these nearest neighbors."]},{"title":"3 Our approach","paragraphs":["In this section we explain our approach for Cross Language Word Sense Disambiguation. The main emphasis is on disambiguation i.e. finding English sentences from the training data which are closely related to the test sentence. 3.1 Motivating Examples To explain our approach we start with two motivating examples. First, consider the following occurrences of the word coach:","• S1:...carriage of passengers by coach and bus...","• S2:...occasional services by coach and bus and the transit operations... • S3:...the Gloucester coach saw the game...","In the first two cases, the word coach appears in the sense of a vehicle and in both the cases the word bus appears in the context. Hence, the surface similarity (i.e., word-overlap count) of S1 and S2 would be higher than that of S1 and S3 and S2 and S3. This highlights the strength of overlap based approaches – frequently co-occurring words can provide strong clues for identifying similar usage patterns of a word.","Next, consider the following two occurrences of the word coach: • S1:...I boarded the last coach of the train...","• S2:...I alighted from the first coach of the bus...","Here, the surface similarity (i.e., word-overlap count) of S1 and S2 is zero even though in both the cases the word coach appears in the sense of vehicle. This problem can be overcome by using a suitable Wordnet based similarity measure which can uncover the hidden semantic similarity between these two sentences by identifying that {bus, train} and {boarded, alighted} are closely related words. 3.2 Scoring function Based on the above motivating examples, we propose a scoring function for calculating the similarity between two sentences containing the target word. Let S1 be the test sentence containing m words and let S2 be a training sentence containing n words. Further, let w1i be the i-th word of S1 and let w2j be the j-th word of S2. The similarity between S1 and S2 is then given by, Sim(S1, S2) = λ ∗ Overlap(S1, S2)","+ (1 − λ) ∗ Semantic Sim(S1, S2) (1) where, 139 Overlap(S1, S2) =","1 m + n m ∑ i=1 n ∑ j=1 f req(w1i) ∗ 1{w1i=w2j} and, Semantic Sim(S1, S2) = 1 m m ∑ i=1 Best Sim(w1i, S2) where,","Best Sim(w1i, S2) = max w2j∈S2 lch(w1i, w2j ) We used the lch measure (Leacock and Chodorow, 1998) for calculating semantic similarity of two words. The semantic similarity between S1 and S2 is then calculated by simply summing over the maximum semantic similarity of each constituent word of S1 over all words of S2. Also note that the overlap count is weighted according to the frequency of the overlapping words. This frequency is calculated from all the sentences in the training data containing the target word. The rational behind using a frequency-weighted sum is that more frequently appearing co-occurring words are better indicators of the sense of the target word (of course, stop words and function words are not considered). For example, the word bus appeared very frequently with coach in the training data and was a strong indicator of the vehicle sense of coach. The values of Overlap(S1, S2) and Semantic Sim(S1, S2) are appropriately normalized before summing them in Equation (1). To prevent the semantic similarity measure from in-troducing noise by over-generalizing we chose a very high value of λ. This effectively ensured that the Semantic Sim(S1, S2) term in Equation (1) became active only when the Overlap(S1, S2) measure suffered data sparsity. In other words, we placed a higher bet on Overlap(S1, S2) than on Semantic Sim(S1, S2) as we found the former to be more reliable. 3.3 Finding translations of the target word We used GIZA++2","(Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) to get word level alignments for the sentences in the English-French 2 http://sourceforge.net/projects/giza/ portion of the Europarl corpus. Under this alignment, each word in the source sentence is aligned to zero or more words in the corresponding target sentence. Once the nearest neighbors for a test sentence are identified using the similarity score described earlier, we use the word alignment models to find the French translation of the target word in the top-k nearest training sentences. These translations are then ranked according to the number of times they appear in these top-k nearest neighbors. The top-5 most frequent translations are then returned as the output."]},{"title":"4 Results","paragraphs":["We report results on the English-French Cross-Lingual Word Sense Disambiguation task. The test data contained 50 instances for 20 polysemous nouns, namely, coach, education, execution, figure, job, letter, match, mission, mood, paper, post, pot, range, rest, ring, scene, side, soil, strain and test. We first extracted the sentences containing these words from the English-French portion of the Europarl corpus. These sentences served as the training data to be compared with each test sentence for identifying the nearest neighbors. The appropriate translations for the target word in the test sentence were then identified using the approach outlined in section 3.2 and 3.3. For the best evaluation we submitted two runs: one containing only the top-1 translation and another containing top-2 translations. For the oof evaluation we submitted one run containing the top-5 translations. The system was evaluated using Precision and Recall measures as described in the task paper (Lefever and Hoste, 2010). In the oof evaluation our system gave the second best performance among all the participants. However, the average precision was 3% lower than the baseline calculated by simply identifying the five most frequent translations of a word according to GIZA++ word alignments. A detailed analysis showed that in the oof evaluation we did better than the baseline for 12 out of the 20 nouns and in the best evaluation we did better than the baseline for 5 out of the 20 nouns. Table 1 summarizes the performance of our system in the best evaluation and Table 2 gives the detailed performance of our system in the oof evaluation. In both the evaluations our system provided a translation for every word in the test data and hence the precision was same as recall in all cases. We refer to our system as OWNS (Overlap 140 and WordNet Similarity). System Precision Recall OWNS 16.05 16.05 Baseline 20.71 20.71 Table 1: Performance of our system in best evaluation Word OWNS Baseline","(Precision) (Precision) coach 45.11 39.04 education 82.15 80.4 execution 59.22 39.63 figure 30.56 35.67 job 43.93 40.98 letter 46.01 42.34 match 31.01 15.73 mission 55.33 97.19 mood 35.22 64.81 paper 48.93 40.95 post 36.65 41.76 pot 26.8 65.23 range 16.28 17.02 rest 39.89 38.72 ring 39.74 33.74 scene 33.89 38.7 side 37.85 36.58 soil 67.79 59.9 strain 21.13 30.02 test 64.65 61.31 Average 43.11 45.99 Table 2: Performance of our system in oof evaluation"]},{"title":"5 Conclusion","paragraphs":["We described our system for English French Cross-Lingual Word Sense Disambiguation which calculates the affinity between two sentences by combining the weighted word overlap counts with semantic similarity measures. This similarity score is used to find the nearest neighbors of the test sentence from the training data. Once the nearest neighbors have been identified, the best translation is found by taking a majority vote over the translations of these nearest neighbors. Our system gave the second best performance in the oof evaluation among all the systems that participated in the English French Cross-Lingual Word Sense Disambiguation task. Even though the average performance of our system was less than the baseline by around 3%, it outperformed the baseline system for 12 out of the 20 nouns."]},{"title":"References","paragraphs":["Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In In Proceedings of the 16th International Conference on Computational Linguistics (COLING).","Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19:263–311.","P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In In Proceedings of the MT Summit.","C. Leacock and M. Chodorow, 1998. Combining local context and WordNet similarity for word sense identification, pages 305–332. In C. Fellbaum (Ed.), MIT Press.","Els Lefever and Veronique Hoste. 2010. Semeval-2010 task 3: Cross-lingual word sense disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Association for Computational Linguistics.","Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In In Proceedings of the 5th annual international conference on Systems documentation.","Rada Mihalcea. 2005. Large vocabulary unsupervised word sense disambiguation with graph-based algorithms for sequence data labeling. In In Proceedings of the Joint Human Language Technology and Empirical Methods in Natural Language Processing Conference (HLT/EMNLP), pages 411–418.","Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 40–47.","Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.","Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In In pro-ceedings of the Fourth International Conference on Intelligent Text Processing and Computation Linguistics (CICLing.","D. Walker and R. Amsler. 1986. The use of machine readable dictionaries in sublanguage analysis. In In Analyzing Language in Restricted Domains, Grishman and Kittredge (eds), LEA Press, pages 69–83. 141"]}]}