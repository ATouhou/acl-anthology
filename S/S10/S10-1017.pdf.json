{"sections":[{"title":"","paragraphs":["Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 88–91, Uppsala, Sweden, 15-16 July 2010. c⃝2010 Association for Computational Linguistics"]},{"title":"RelaxCor: A Global Relaxation Labeling Approach to Coreference Resolution Emili Sapena, Lluı́s Padró and Jordi Turmo TALP Research Center Universitat Politècnica de Catalunya Barcelona, Spain {esapena, padro, turmo}@lsi.upc.edu Abstract","paragraphs":["This paper describes the participation of RelaxCor in the Semeval-2010 task number 1: “Coreference Resolution in Multiple Languages“. RelaxCor is a constraint-based graph partitioning approach to coreference resolution solved by relaxation labeling. The approach combines the strengths of groupwise classifiers and chain formation methods in one global method."]},{"title":"1 Introduction","paragraphs":["The Semeval-2010 task is concerned with intra-document coreference resolution for six different languages: Catalan, Dutch, English, German, Italian and Spanish. The core of the task is to identify which noun phrases (NPs) in a text refer to the same discourse entity (Recasens et al., 2010).","RelaxCor (Sapena et al., 2010) is a graph representation of the problem solved by a relaxation labeling process, reducing coreference resolution to a graph partitioning problem given a set of constraints. In this manner, decisions are taken considering the whole set of mentions, ensuring consistency and avoiding that classification decisions are independently taken.","The paper is organized as follows. Section 2 describes RelaxCor, the system used in the Semeval task. Next, Section 3 describes the tuning needed by the system to adapt it to different languages and other task issues. The same section also analyzes the obtained results. Finally, Section 4 concludes the paper."]},{"title":"2 System Description","paragraphs":["This section briefly describes RelaxCor. First, the graph representation is presented. Next, there is an explanation of the methodology used to learn constraints and train the system. Finally, the algorithm used for resolution is described. 2.1 Problem Representation Let G = G(V, E) be an undirected graph where V is a set of vertices and E a set of edges. Let m = (m1, ..., mn) be the set of mentions of a document with n mentions to resolve. Each mention mi in the document is represented as a vertex vi ∈ V in the graph. An edge eij ∈ E is added to the graph for pairs of vertices (vi, vj) representing the possibility that both mentions corefer.","Let C be our set of constraints. Given a pair of mentions (mi, mj), a subset of constraints Cij ⊆ C restrict the compatibility of both mentions. Cij is used to compute the weight value of the edge connecting vi and vj. Let wij ∈ W be the weight of the edge eij: wij = ∑ k∈Cij λkfk(mi, mj) (1) where fk(·) is a function that evaluates the constraint k and λk is the weight associated to the constraint. Note that λk and wij can be negative.","In our approach, each vertex (vi) in the graph is a variable (vi) for the algorithm. Let Li be the number of different values (labels) that are possible for vi. The possible labels of each variable are the partitions that the vertex can be assigned. A vertex with index i can be in the first i partitions (i.e. Li = i). 88 Distance and position: DIST: Distance between mi and mj in sentences: number DIST MEN: Distance between mi and mj in mentions: number APPOSITIVE: One mention is in apposition with the other: y,n I/J IN QUOTES: mi/j is in quotes or inside a NP or a sentence in quotes: y,n I/J FIRST: mi/j is the first mention in the sentence: y,n Lexical: I/J DEF NP: mi/j is a definitive NP: y,n I/J DEM NP: mi/j is a demonstrative NP: y,n I/J INDEF NP: mi/j is an indefinite NP: y,n STR MATCH: String matching of mi and mj: y,n PRO STR: Both are pronouns and their strings match: y,n PN STR: Both are proper names and their strings match: y,n NONPRO STR: String matching like in Soon et al. (2001) and mentions are not pronouns: y,n HEAD MATCH: String matching of NP heads: y,n Morphological: NUMBER: The number of both mentions match: y,n,u GENDER: The gender of both mentions match: y,n,u AGREEMENT: Gender and number of both mentions match: y,n,u I/J THIRD PERSON: mi/j is 3rd person: y,n PROPER NAME: Both mentions are proper names: y,n,u I/J PERSON: mi/j is a person (pronoun or proper name in a list): y,n ANIMACY: Animacy of both mentions match (persons, objects): y,n I/J REFLEXIVE: mi/j is a reflexive pronoun: y,n I/J TYPE: mi/j is a pronoun (p), entity (e) or nominal (n) Syntactic: NESTED: One mention is included in the other: y,n MAXIMALNP: Both mentions have the same NP parent or they are nested: y,n I/J MAXIMALNP: mi/j is not included in any other mention: y,n I/J EMBEDDED: mi/j is a noun and is not a maximal NP: y,n BINDING: Conditions B and C of binding theory: y,n Semantic: SEMCLASS: Semantic class of both mentions match: y,n,u (the same as (Soon et al., 2001)) ALIAS: One mention is an alias of the other: y,n,u (only entities, else unknown) I/J SRL ARG: Semantic role of mi/j: N,0,1,2,3,4,M,L SRL SAMEVERB: Both mentions have a semantic role for the same verb: y,n Figure 1: Feature functions used. 2.2 Training Process Each pair of mentions (mi, mj) in a training document is evaluated by the set of feature functions shown in Figure 1. The values returned by these functions form a positive example when the pair of mentions corefer, and a negative one otherwise. Three specialized models are constructed depend-ing on the type of anaphor mention (mj) of the pair: pronoun, named entity or nominal.","A decision tree is generated for each specialized model and a set of rules is extracted with C4.5 rule-learning algorithm (Quinlan, 1993). These rules are our set of constraints. The C4.5rules algorithm generates a set of rules for each path from the learned tree. It then checks if the rules can be generalized by dropping conditions.","Given the training corpus, the weight of a constraint Ck is related with the number of examples where the constraint applies ACk and how many of them corefer CCk . We define λk as the weight of constraint Ck calculated as follows: λk = CC k AC k − 0.5 2.3 Resolution Algorithm Relaxation labeling (Relax) is a generic name for a family of iterative algorithms which perform function optimization, based on local information (Hummel and Zucker, 1987). The algorithm solves our weighted constraint satisfaction problem dealing with the edge weights. In this manner, each vertex is assigned to a partition satisfying as many constraints as possible. To do that, the algorithm assigns a probability for each possible label of each variable. Let H = (h1",", h2",", . . . , hn",") be the weighted labeling to optimize, where each hi is a vector containing the probability distribution of vi, that is: hi","= (hi","1, hi","2, . . . , hi","Li). Given that the resolution process is iterative, the probability for label l of variable vi at time step t is hi","l(t), or","simply hi l when the time step is not relevant.","Initialize: H := H0,","Main loop: repeat For each variable vi","For each possible label l for vi","Sil =","∑","j∈A(v","i) wij × hj","l End for For each possible label l for vi hi","l(t + 1) = hi l(t)×(1+S","il)","∑L i k=1 hi","k(t)×(1+S","ik)","End for End for Until no more significant changes Figure 2: Relaxation labeling algorithm","The support for a pair variable-label (Sil) expresses how compatible is the assignment of label l to variable vi taking into account the labels of adjacent variables and the edge weights. The support is defined as the sum of the edge weights that relate variable vi with each adjacent variable vj multiplied by the weight for the same label l of variable vj: Sil =","∑","j∈A(vi) wij × hj","l where wij is the edge weight obtained in Equation 1 and vertex vi has |A(vi)| adjacent vertices. In our version of the algorithm for coreference resolution A(vi) is the list of adjacent vertices of vi but only considering the ones with an index k < i.","The aim of the algorithm is to find a weighted labeling such that global consistency is maximized. Maximizing global consistency is defined 89","Figure 3: Representation of Relax. The vertices represent-","ing mentions are connected by weighted edges eij. Each ver-","tex has a vector hi","of probabilities to belong to different par-","titions. The figure showsh2",", h3","and h4",". as maximizing the average support for each variable. The final partitioning is directly obtained from the weighted labeling H assigning to each variable the label with maximum probability.","The pseudo-code of the relaxation algorithm can be found in Figure 2. The process updates the weights of the labels in each step until convergence, i.e. when no more significant changes are done in an iteration. Finally, the assigned label for a variable is the one with the highest weight. Figure 3 shows an example of the process."]},{"title":"3 Semeval task participation","paragraphs":["RelaxCor have participated in the Semeval task for English, Catalan and Spanish. The system does not detect the mentions of the text by itself. Thus, the participation has been restricted to the gold-standard evaluation, which includes the manual annotated information and also provides the mention boundaries.","All the knowledge required by the feature functions (Figure 1) is obtained from the annota-tions of the corpora and no external resources have been used, with the exception of WordNet (Miller, 1995) for English. In this case, the system has been run two times for English: English-open, using WordNet, and English-closed, without WordNet. 3.1 Language and format adaptation The whole methodology of RelaxCor including the resolution algorithm and the training process is totally independent of the language of the document. The only parts that need few adjustments are the preprocess and the set of feature functions. In most cases, the modifications in the feature functions are just for the different format of the data for different languages rather than for specific language issues. Moreover, given that the task includes many information about the mentions of the documents such as part of speech, syntactic dependency, head and semantic role, no preprocess has been needed.","One of the problems we have found adapting the system to the task corpora was the large amount of available data. As described in Section 2.2, the training process generates a feature vector for each pair of mentions into a document for all the documents of the training data set. However, the great number of training documents and their length overwhelmed the software that learns the constraints. In order to reduce the amount of pair examples, we run a clustering process to reduce the number of negative examples using the positive examples as the centroids. Note that negative examples are near 94% of the training examples, and many of them are repeated. For each positive example (a corefering pair of mentions), only the negative examples with distance less than a threshold d are included in the final training data. The distance is computed as the number of different values inside the feature vector. After some experiments over development data, the value of d was assigned to 3. Thus, the negative examples were discarded when they have more than three features different than any positive example.","Our results for the development data set are shown in Table 1. 3.2 Results analysis Results of RelaxCor for the test data set are shown in Table 2. One of the characteristics of the system is that the resolution process always takes into account the whole set of mentions and avoids any possible pair-linkage contradiction as well as forces transitivity. Therefore, the system favors the precision, which results on high scores with metrics CEAF and B3",". However, the system is penalized with the metrics based on pair-linkage, specially with MUC. Although RelaxCor has the highest precision scores even for MUC, the recall is low enough to finally obtain low scores for F1.","Regarding the test scores of the task comparing with the other participants (Recasens et al., 2010), RelaxCor obtains the best performances for Cata-90 - CEAF MUC B3 language R P F1 R P F1 R P F1 ca 69.7 69.7 69.7 27.4 77.9 40.6 67.9 96.1 79.6 es 70.8 70.8 70.8 30.3 76.2 43.4 68.9 95.0 79.8 en-closed 74.8 74.8 74.8 21.4 67.8 32.6 74.1 96.0 83.7 en-open 75.0 75.0 75.0 22.0 66.6 33.0 74.2 95.9 83.7 Table 1: Results on the development data set - CEAF MUC B3","BLANC","language R P F1 R P F1 R P F1 R P Blanc","Information: closed Annotation: gold ca 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7 es 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6 en 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3","Information: open Annotation: gold en 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7 Table 2: Results of the task lan (CEAF and B3","), English (closed: CEAF and B3","; open: B3",") and Spanish (B3","). Moreover, RelaxCor is the most precise system for all the metrics in all the languages except for CEAF in English-open and Spanish. This confirms the robustness of the results of RelaxCor but also remarks that more knowledge or more information is needed to increase the recall of the system without loosing this precision","The incorporation of WordNet to the English run is the only difference between English-open and English-closed. The scores are slightly higher when using WordNet but not significant. Analyz-ing the MUC scores, note that the recall is improved, while precision decreases a little which corresponds with the information and the noise that WordNet typically provides.","The results for the test and development are very similar as expected, except the Spanish (es) ones. The recall considerably falls from development to test. It is clearly shown in the MUC recall and also is indirectly affecting on the other scores."]},{"title":"4 Conclusion","paragraphs":["The participation of RelaxCor to the Semeval coreference resolution task has been useful to evaluate the system in multiple languages using data never seen before. Many published systems typically use the same data sets (ACE and MUC) and it is easy to unintentionally adapt the system to the corpora and not just to the problem. This kind of tasks favor comparisons between systems with the same framework and initial conditions.","The results obtained confirm the robustness of the RelaxCor, and the performance is considerably good in the state of the art. The system avoids contradictions in the results which causes a high precision. However, more knowledge is needed about the mentions in order to increase the recall without loosing that precision. A further error analysis is needed, but one of the main problem is the lack of semantic information and world knowledge specially for the nominal mentions – the mentions that are NPs but not including named entities neither pronouns–."]},{"title":"Acknowledgments","paragraphs":["The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP7/2007-2013) under Grant Agreement number 247762 (FAUST), and from the Spanish Science and Innovation Ministry, via the KNOW2 project (TIN2009-14715-C04-04)."]},{"title":"References","paragraphs":["R. A. Hummel and S. W. Zucker. 1987. On the foundations of relaxation labeling processes. pages 585–605. G.A. Miller. 1995. WordNet: a lexical database for English.","J.R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.","M. Recasens, L. Màrquez, E. Sapena, M.A. Martı́, M. Taulé, V. Hoste, M. Poesio, and Y. Versley. 2010. SemEval-2010 Task 1: Coreference resolution in multiple languages. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Uppsala, Sweden.","E. Sapena, L. Padró, and J. Turmo. 2010. A Global Relaxation Labeling Approach to Coreference Resolution. Submitted.","W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A Machine Learning Approach to Coreference Resolution of Noun Phrases. Computational Linguistics, 27(4):521–544. 91"]}]}