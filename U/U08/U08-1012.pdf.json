{"sections":[{"title":"Comparing the value of Latent Semantic Analysis on two English-to-Indonesian lexical mapping tasks   Eliza Margaretha Ruli Manurung","paragraphs":["Faculty of Computer Science Faculty of Computer Science","University of Indonesia University of Indonesia Depok, Indonesia Depok, Indonesia elm40@ui.edu maruli@cs.ui.ac.id"," "]},{"title":"Abstract","paragraphs":["This paper describes an experiment that attempts to automatically map English words and concepts, derived from the Princeton WordNet, to their Indonesian analogues appearing in a widely-used Indonesian dictionary, using Latent Semantic Analysis (LSA). A bilingual semantic model is derived from an English-Indonesian parallel corpus. Given a particular word or concept, the semantic model is then used to identify its neighbours in a high-dimensional semantic space. Results from various experiments indicate that for bilingual word mapping, LSA is consistently outperformed by the basic vector space model, i.e. where the full-rank word-document matrix is applied. We speculate that this is due to the fact that the ‘smoothing’ effect LSA has on the word-document matrix, whilst very useful for revealing implicit semantic patterns, blurs the cooccurrence information that is necessary for establishing word translations."]},{"title":"1 Overview","paragraphs":["An ongoing project at the Information Retrieval Lab, Faculty of Computer Science, University of Indonesia, concerns the development of an Indonesian WordNet1",". To that end, one major task concerns the mapping of two monolingual dictionaries at two different levels: bilingual word mapping, which seeks to find translations of a lexical entry from one language to another, and bilingual concept mapping, which defines equivalence classes 1 http://bahasa.cs.ui.ac.id/iwn over concepts defined in two language resources. In other words, we try to automatically construct two variants of a bilingual dictionary between two languages, i.e. one with sense disambiguated entries and one without.","In this paper we present an extension to LSA into a bilingual context that is similar to (Rehder et al., 1997; Clodfelder, 2003; Deng and Gao, 2007), and then apply it to the two mapping tasks described above, specifically towards the lexical resources of Princeton WordNet (Fellbaum, 1998), an English semantic lexicon, and the Kamus Besar Bahasa Indonesia (KBBI)2",", considered by many to be the official dictionary of the Indonesian language.","We first provide formal definitions of our two tasks of bilingual word and concept mapping (Section 2) before discussing how these tasks can be automated using LSA (Section 3). We then present our experiment design and results (Sections 4 and 5) followed by an analysis and discussion of the results in Section 6."]},{"title":"2 Task Definitions","paragraphs":["As mentioned above, our work concerns the mapping of two monolingual dictionaries. In our work, we refer to these resources as WordNets due to the fact that we view them as semantic lexicons that index entries based on meaning. However, we do not consider other semantic relations typically associated with a WordNet such as hypernymy, hyponymy, etc. For our purposes, a WordNet can be 2 The KBBI is the copyright of the Language Centre, Indonesian Ministry of National Education. formally defined as a 4-tuple , , χ, ω as follows:","• A concept is a semantic entity, which represents a distinct, specific meaning. Each concept is associated with a gloss, which is a textual description of its meaning. For example, we could define two concepts, and , where the former is associated with the gloss “a financial institution that accepts deposits and channels the money into lending activities” and the latter with “sloping land (especially the slope beside a body of water”.","• A word is an orthographic entity, which represents a word in a particular language (in the case of Princeton WordNet, English). For example, we could define two words, and , where the former represents the orthographic string bank and the latter represents spoon.","• A word may convey several different concepts. The function : returns all concepts conveyed by a particular word. Thus, , where , returns , the set of all concepts that can be conveyed by w. Using the examples above, ,.","• Conversely, a concept may be conveyed by several words. The function : returns all words that can convey a particular concept. Thus, , where , returns , the set of all words that convey c. Using the examples above, .","We can define different WordNets for different","languages, e.g.","",",",",",",","and","","",", ,",",",". We also introduce the nota-","tion","to denote word in","and","to denote","concept in",". For the sake of our discussion, we","will assume","to be an English WordNet, and","","to be an Indonesian WordNet.","If we make the assumption that concepts are","language independent,","and","should theoreti-","cally share the same set of universal concepts, .","In practice, however, we may have two WordNets","with different conceptual representations, hence","the distinction between","and",". We introduce","the relation","","to denote the explicit","mapping of equivalent concepts in","and",".","We now describe two tasks that can be per-","formed between","and",", namely bilingual con-","cept mapping and bilingual word mapping.","The task of bilingual concept mapping is essen-","tially the establishment of the concept equivalence","relation E. For example, given the example con-","cepts in Table 1, bilingual concept mapping seeks","to establish",",",",",",",","," , ,",","," .  Concept Word Gloss Example  w"," an instance or single occasion for some event “this time he succeeded”","c w"," a suitable moment “it is time to go”","c w"," a reading of a point in time as given by a clock (a word","signifying the frequency of an event) “do you know what time it is?” c w"," kata untuk menyatakan kekerapan tindakan (a word signifying a particular instance of an ongoing series of events) “dalam satu minggu ini, dia sudah empat kali datang ke rumahku” (this past week, she has come to my house four times)  w","","kata untuk menyatakan salah satu waktu terjadinya peristiwa yg merupakan bagian dari rangkaian peristiwa yg pernah dan masih akan terus terjadi (a word signifying a particular instance of an ongoing series of events) “untuk kali ini ia kena batunya” (this time he suffered for his actions) ","","","saat yg tertentu untuk melakukan sesuatu (a specific time to","be doing something) “waktu makan” (eating time) ",""," saat tertentu, pada arloji jarumnya yg pendek menunjuk angka tertentu dan jarum panjang menunjuk angka 12 (the point in time when the short hand of a clock points to a certain hour and the long hand points to 12) “ia bangun jam lima pagi” (she woke up at five o’clock) ","","","sebuah sungai yang kecil (a small river) “air di kali itu sangat keruh” (the water in","that small river is very murky)","Table 1. Sample Concepts in and"," The task of bilingual word mapping is to find,","given word","",", the set of all its plausible","translations in",", regardless of the concepts being","conveyed. We can also view this task as computing","the union of the set of all words in","that convey","the set of all concepts conveyed by",". Formally,","we compute the set w","","","","","where","",", and","","",".","For example, in Princeton WordNet, given"," (i.e. the English orthographic form time),","","","returns more than 15 different concepts,","among others",",",",","(see Table 1).","In Indonesian, assuming the relation E as de-","fined above, the set of words that convey",", i.e.",""," , includes","(as in “kali ini dia berhasil”","= “this time she succeeded”).","On the other hand,","","may include","","(as in “ini waktunya untuk pergi” = “it is time to","go”) and","(as in “sekarang saatnya menjual","saham” = “now is the time to sell shares”), and","lastly,","","may include","(as in “apa anda","tahu jam berapa sekarang?” = “do you know what","time it is now?”).","Thus, the bilingual word mapping task seeks to","compute, for the English word",", the set of","Indonesian words",",",",",",",",....","Note that each of these Indonesian words may","convey different concepts, e.g.","","may in-","clude in Table 1."]},{"title":"3 Automatic mapping using Latent Semantic Analysis","paragraphs":["Latent semantic analysis, or simply LSA, is a method to discern underlying semantic information from a given corpus of text, and to subsequently represent the contextual meaning of the words in the corpus as a vector in a high-dimensional semantic space (Landauer et al., 1998). As such, LSA is a powerful method for word sense disambiguation.","The mathematical foundation of LSA is provided by the Singular Value Decomposition, or SVD. Initially, a corpus is represented as an word-passage matrix M, where cell , represents the occurrence of the -th word in the -th passage. Thus, each row of represents a word and each column represents a passage. The SVD is then applied to , decomposing it such that",", where is an matrix of left singular vectors,","is an matrix of right singular vectors, and is an matrix containing the singular values of .","Crucially, this decomposition factors using an orthonormal basis that produces an optimal reduced rank approximation matrix (Kalman, 1996). By reducing dimensions of the matrix irrelevant information and noise are removed. The optimal rank reduction yields useful induction of implicit relations. However, finding the optimal level of rank reduction is an empirical issue.","LSA can be applied to exploit a parallel corpus to automatically perform bilingual word and concept mapping. We define a parallel corpus P as a set of pairs ,, where is a document written in the language of",", and is its translation in the language of",".","Intuitively, we would expect that if two words ","and","consistently occur in documents that are translations of each other, but not in other documents, that they would at the very least be semantically related, and possibly even be translations of each other. For instance, imagine a parallel corpus consisting of news articles written in English and Indonesian: in English articles where the word Japan occurs, we would expect the word Jepang to occur in the corresponding Indonesian articles.","This intuition can be represented in a word-document matrix as follows: let be a word-document matrix of English documents and English words, and be a word-document matrix of Indonesian documents and Indonesian words. The documents are arranged such that, for 1 , the English document represented by column of and the Indonesian document represented by column of form a pair of translations. Since they are translations, we can view them as occupying exactly the same point in semantic space, and could just as easily view column of both matrices as representing the union, or concatenation, of the two articles.","Consequently, we can construct the bilingual word-document matrix     which is an matrix where cell , contains the number of occurrences of word in article . Row i forms the semantic vector of, for , an English word, and for , an Indonesian word. Conversely, column forms a vector representing the English and Indonesian words appearing in translations of document .","This approach is similar to that of (Rehder et al., 1997; Clodfelder, 2003; Deng and Gao, 2007). The SVD process is the same, while the usage is different. For example, (Rehder et al., 1997) employ SVD for cross language information retrieval. On the other hand, we use it to accomplish word and concept mappings.","LSA can be applied to this bilingual word-document matrix. Computing the SVD of this matrix and reducing the rank should unearth implicit patterns of semantic concepts. The vectors representing English and Indonesian words that are closely related should have high similarity; word translations more so.","To approximate the bilingual word mapping task, we compare the similarity between the semantic vectors representing words in","and",". Specifically, for the first rows in which represent words in",", we compute their similarity to each of the last rows which represent words in",". Given a large enough corpus, we would expect all words in","and","to be represented by rows in .","To approximate the bilingual concept mapping task, we compare the similarity between the semantic vectors representing concepts in","and",". These vectors can be approximated by first constructing a set of textual context representing a concept . For example, we can include the words in together with the words from its gloss and example sentences. The semantic vector of a concept is then a weighted average of the semantic vectors of the words contained within this context set, i.e. rows in . Again, given a large enough corpus, we would expect enough of these context words to be represented by rows in M to form an adequate semantic vector for the concept ."]},{"title":"4 Experiments 4.1 Existing Resources","paragraphs":["For the English lexicon, we used the most current version of WordNet (Fellbaum, 1998), version 3.03",". For each of the 117659 distinct synsets, we only use the following data: the set of words be-3 More specifically, the SQL version available from http://wnsqlbuilder.sourceforge.net longing to the synset, the gloss, and example sentences, if any. The union of these resources yields a set 169583 unique words.","For the Indonesian lexicon, we used an electronic version of the KBBI developed at the University of Indonesia. For each of the 85521 distinct word sense definitions, we use the following data: the list of sublemmas, i.e. inflected forms, along with gloss and example sentences, if any. The union of these resources yields a set of 87171 unique words.","Our main parallel corpus consists of 3273 English and Indonesian article pairs taken from the ANTARA news agency. This collection was developed by Mirna Adriani and Monica Lestari Paramita at the Information Retrieval Lab, University of Indonesia4",".","A bilingual English-Indonesia dictionary was constructed using various online resources, includ-ing a handcrafted dictionary by Hantarto Widjaja5",", kamus.net, and Transtool v6.1, a commercial translation system. In total, this dictionary maps 37678 unique English words to 60564 unique Indonesian words. 4.2 Bilingual Word Mapping Our experiment with bilingual word mapping was set up as follows: firstly, we define a collection of article pairs derived from the ANTARA collection, and from it we set up a bilingual word-document matrix (see Section 3). The LSA process is subsequently applied on this matrix, i.e. we first compute the SVD of this matrix, and then use it to compute the optimal -rank approximation. Finally, based on this approximation, for a randomly chosen set of vectors representing English words, we compute the nearest vectors representing the most similar Indonesian words. This is conventionally computed using the cosine of the angle between two vectors.","Within this general framework, there are several variables that we experiment with, as follows:","• Collection size. Three subsets of the parallel corpus were randomly created: P100 contains 100 article pairs, P500 contains 500 article pairs, and P1000 contains 1000 article pairs. Each subsequent subset wholly contains the previous subsets, i.e. P100 ⊂ P500 ⊂ P1000. 4 publication forthcoming 5 http://hantarto.definitionroadsafety.org","• Rank reduction. For each collection, we applied LSA with different degrees of rank approximation, namely 10%, 25%, and 50% the number of dimensions of the original collection. Thus, for P100 we compute the 10, 25, and 50-rank approximations, for P500 we compute the 50, 125, and 250-rank approximations, and for P1000 we compute the 100, 250, and 500-rank approximations.","• Removal of stopwords. Stopwords are words that appear numerously in a text, thus are as-sumed as insignificant to represent the specific context of the text. It is a common technique used to improve performance of information retrieval systems. It is applied in preprocessing the collections, i.e. removing all instances of the stopwords in the collections before apply-ing LSA.","• Weighting. Two weighting schemes, namely TF-IDF and Log-Entropy, were applied to a word-document matrix separately.","• Mapping Selection. For computing the precision and recall values, we experimented with the number of mapping results to consider: the top 1, 10, 50, and 100 mappings based on similarity were taken. film filmnya sutradara garapan perfilman penayangan kontroversial koboi irasional frase 0.814 0.698 0.684 0.581 0.554 0.544 0.526 0.482 0.482 0.482","pembebanan kijang halmahera alumina terjadwal viskositas tabel royalti reklamasi penyimpan 0.973 0.973 0.973 0.973 0.973 0.973 0.973 0.973 0.973 0.973 (a) (b)","Table 2. The Most 10 Similar Indonesian Words for the English Words (a) Film and (b) Billion","As an example, Table 2 presents the results of mapping","and",", i.e. the two English words film and billion, respectively to their Indonesian translations, using the P1000 training collection with 500-rank approximation. No weighting was applied. The former shows a successful mapping, while the latter shows an unsuccessful one. Bilingual LSA correctly maps","to its translation,",", despite the fact that they are treated as separate elements, i.e. their shared orthography is completely coincidental. Additionally, the other Indonesian words it suggests are semantically related, e.g. sutradara (director), garapan (creation), penayangan (screening), etc. On the other hand, the suggested word mappings for","are in-correct, and the correct translation, milyar, is miss-ing. We suspect this may be due to several factors. Firstly, billion does not by itself invoke a particular semantic frame, and thus its semantic vector might not suggest a specific conceptual domain. Secondly, billion can sometimes be translated numerically instead of lexically. Lastly, this failure may also be due to the lack of data: the collection is simply too small to provide useful statistics that represent semantic context. Similar LSA approaches are commonly trained on collections of text numbering in the tens of thousands of articles.","Note as well that the absolute vector cosine values do not accurately reflect the correctness of the word translations. To properly assess the results of this experiment, evaluation against a gold standard is necessary. This is achieved by comparing its precision and recall against the Indonesian words returned by the bilingual dictionary, i.e. how isomorphic is the set of LSA-derived word mappings with a human-authored set of word mappings?","We provide a baseline as comparison, which computes the nearness between English and Indonesian words on the original word-document occurrence frequency matrix. Other approaches are possible, e.g. mutual information (Sari, 2007).","Table 3(a)-(e) shows the different aspects of our experiment results by averaging the other variables. Table 3(a) confirms our intuition that as the collection size increases, the precision and recall values also increase. Table 3(b) presents the effects of rank approximation. It shows that the higher the rank approximation percentage, the better the mapping results. Note that a rank approximation of 100% is equal to the FREQ baseline of simply using the full-rank word-document matrix for computing vector space nearness. Table 3(c) suggests that stopwords seem to help LSA to yield the correct mappings. It is believed that stopwords are not bounded by semantic domains, thus do not carry any semantic bias. However, on account of the small size of the collection, in coincidence, stopwords, which consistently appear in a specific domain, may carry some semantic information about the domain. Table 3(d) compares the mapping results in terms of weighting usage. It suggests that weighting can improve the mappings. Additionally, Log-Entropy weighting yields the highest results. Table 3(e) shows the comparison of mapping selections. As the number of translation pairs selected increases, the precision value decreases. On the other hand, as the number of translation pairs selected increases, the possibility to find more pairs matching the pairs in bilingual dictionary increases. Thus, the recall value increases as well.","(a)","Rank Approximation P R 10% 0.0680 0.1727 25% 0.0845 0.2070 50% 0.0967 0.2226 100% 0.1009 0.2285 (b) (c) Weighting Usage","FREQ LSA","P R P R","No Weighting 0.1009 0.2285 0.0757 0.1948","Log-Entropy 0.1347 0.2753 0.1041 0.2274 TF-IDF 0.1013 0.2319 0.0694 0.1802 (d) (e) Table 3. Results of bilingual word mapping comparing (a) collection size, (b) rank approximation, (c) removal of stopwords, (d) weighting schemes, and (e)","mapping selection","Most interestingly, however, is the fact that the FREQ baseline, which uses the basic vector space model, consistently outperforms LSA. 4.3 Bilingual Concept Mapping Using the same resources from the previous experiment, we ran an experiment to perform bilingual concept mapping by replacing the vectors to be compared with semantic vectors for concepts (see Section 3). For concept","",", i.e. a WordNet synset, we constructed a set of textual context as the union of , the set of words in the gloss of ",", and the set of words in the example sentences associated with",". To represent our intuition that the words in played more of an important role in defining the semantic vector than the words in the gloss and example, we applied a weight of 60%, 30%, and 10% to the three components, respectively. Similarly, a semantic vector representing a concept","",", i.e. an Indonesian word sense in the KBBI, was constructed from a textual context set composed of the sublemma, the definition, and the example of the word sense, using the same weightings. We only average word vectors if they appear in the collection (depending on the experimental variables used).","We formulated an experiment which closely resembles the word sense disambiguation problem: given a WordNet synset, the task is to select the most appropriate Indonesian sense from a subset of senses that have been selected based on their words appearing in our bilingual dictionary. These specific senses are called suggestions. Thus, instead of comparing the vector representing communication with every single Indonesian sense in the KBBI, in this task we only compare it against suggestions with a limited range of sublemmas, e.g. komunikasi, perhubungan, hubungan, etc.","This setup is thus identical to that of an ongoing experiment here to manually map WordNet synsets to KBBI senses. Consequently, this facilitates as-sessment of the results by computing the level of agreement between the LSA-based mappings with human annotations.","To illustrate, Table 4(a) and 4(b) presents a successful and unsuccessful example of mapping a WordNet synset. For each example we show the synset ID and the ideal textual context set, i.e. the set of words that convey the synset, its gloss and example sentences. We then show the actual textual context set with the notation {{X}, {Y}, {Z}}, where X, Y , and Z are the subset of words that appear in the training collection. We then show the Indonesian word sense deemed to be most similar. For each sense we show the vector similarity score, the KBBI ID and its ideal textual context set, i.e. the sublemma, its definition and example sen-Collection Size","FREQ LSA","P R P R 0.0668 0.1840 0.0346 0.1053 0.1301 0.2761 0.0974 0.2368 0.1467 0.2857 0.1172 0.2603 Stopwords","FREQ LSA","P R P R","Contained 0.1108 0.2465 0.0840 0.2051","Removed 0.1138 0.2440 0.0822 0.1964 Mapping Selection","FREQ LSA","P R P R Top 1 0.3758 0.1588 0.2380 0.0987 Top 10 0.0567 0.2263 0.0434 0.1733 Top 50 0.0163 0.2911 0.0133 0.2338 Top 100 0.0094 0.3183 0.0081 0.2732 tences. We then show the actual textual context set with the same notation as above."," WordNet Synset ID: 100319939, Words: chase, following, pursual, pursuit, Gloss: the act of pursuing in an effort to overtake or capture, Example: the culprit started to run and the cop took off in pursuit, Textual context set: {{following, chase}, {the, effort, of, to, or, capture, in, act, pursuing, an}, {the, off, took, to, run, in, culprit, started, and}} KBBI ID: k39607 - Similarity: 0.804, Sublemma: mengejar, Definition: berlari untuk menyusul menangkap dsb memburu, Example: ia berusaha mengejar dan menangkap saya, Textual context set: {{mengejar}, {memburu, berlari, menangkap, untuk, menyusul},{berusaha, dan, ia, mengejar, saya, menangkap}} (a) WordNet synset ID: 201277784, Words: crease, furrow, wrinkle Gloss: make wrinkled or creased, Example: furrow one’s brow, Textual context set: {{}, {or, make}, {s, one}} KBBI ID: k02421 - Similarity: 0.69, Sublemma: alur, Definition: jalinan peristiwa dl karya sastra untuk mencapai efek tertentu pautannya dapat diwujudkan oleh hubungan temporal atau waktu dan oleh hubungan kausal atau sebab-akibat, Example: (none), Textual context set: {{alur}, {oleh, dan, atau, jalinan, peristiwa, diwujudkan, efek, dapat, karya, hubungan, waktu, mencapai, untuk, tertentu}, {}} (b)","Table 4. Example of (a) Successful and (b) Unsuccessful Concept Mappings","In the first example, the textual context sets from both the WordNet synset and the KBBI senses are fairly large, and provide sufficient context for LSA to choose the correct KBBI sense. However, in the second example, the textual context set for the synset is very small, due to the words not appearing in the training collection. Furthermore, it does not contain any of the words that truly convey the concept. As a result, LSA is unable to identify the correct KBBI sense.","","For this experiment, we used the P1000 training collection. The results are presented in Table 5. As a baseline, we select three random suggested Indonesian word senses as a mapping for an English word sense. The reported random baseline in Table 5 is an average of 10 separate runs. Another baseline was computed by comparing English common-based concepts to their suggestion based on a full rank word-document matrix. Top 3 Indonesian concepts with the highest similarity values are designated as the mapping results. Subsequently, we compute the Fleiss kappa (Fleiss, 1971) of this result together with the human judgements.","The average level of agreement between the LSA mappings 10% and the human judges (0.2713) is not as high as between the human judges themselves (0.4831). Nevertheless, in general it is better than the random baseline (0.2380) and frequency baseline (0.2132), which suggests that LSA is indeed managing to capture some measure of bilingual semantic information implicit within the parallel corpus.","Furthermore, LSA mappings with 10% rank approximation yields higher levels of agreement than LSA with other rank approximations. It is contradictory with the word mapping results where LSA with bigger rank approximations yields higher results (Section 4.2)."]},{"title":"5 Discussion","paragraphs":["Previous works have shown LSA to contribute positive gains to similar tasks such as Cross Language Information Retrieval (Rehder et al., 1997). However, the bilingual word mapping results presented in Section 4.3 show the basic vector space model consistently outperforming LSA at that particular task, despite our initial intuition that LSA should actually improve precision and recall.","We speculate that the task of bilingual word mapping may be even harder for LSA than that of Judges Synsets Fleiss Kappa Values Judges only Judges + RNDM3 Judges + FREQ Top","3 Judges + LSA 10% Top3 Judges + LSA 25% Top3","Judges +","LSA 50%","Top3","≥ 2 144 0.4269 0.1318 0.1667 0.1544 0.1606 0.1620","≥ 3 24 0.4651 0.2197 0.2282 0.2334 0.2239 0.2185","≥ 4 8 0.5765 0.3103 0.2282 0.3615 0.3329 0.3329","≥ 5 4 0.4639 0.2900 0.2297 0.3359 0.3359 0.3359","Average 0.4831 0.2380 0.2132 0.2713 0.2633 0.2623 Table 5. Results of Concept Mapping bilingual concept mapping due to its finer alignment granularity. While concept mapping attempts to map a concept conveyed by a group of semantically related words, word mapping attempts to map a word with a specific meaning to its translation in another language.","In theory, LSA employs rank reduction to remove noise and to reveal underlying information contained in a corpus. LSA has a ‘smoothing’ effect on the matrix, which is useful to discover general patterns, e.g. clustering documents by semantic domain. Our experiment results, however, generally shows the frequency baseline, which employs the full rank word-document matrix, outperforming LSA.","We speculate that the rank reduction perhaps blurs some crucial details necessary for word mapping. The frequency baseline seems to encode more cooccurrence than LSA. It compares word vectors between English and Indonesian that contain pure frequency of word occurrence in each document. On the other hand, LSA encodes more semantic relatedness. It compares English and Indonesian word vectors containing estimates of word frequency in documents according to the context meaning. Since the purpose of bilingual word mapping is to obtain proper translations for an English word, it may be better explained as an issue of cooccurrence rather than semantic related-ness. That is, the higher the rate of cooccurrence between an English and an Indonesian word, the likelier they are to be translations of each other.","LSA may yield better results in the case of finding words with similar semantic domains. Thus, the LSA mapping results should be better assessed using a resource listing semantically related terms, rather than using a bilingual dictionary listing translation pairs. A bilingual dictionary demands more specific constraints than semantic related-ness, as it specifies that the mapping results should be the translations of an English word.","Furthermore, polysemous terms may become another problem for LSA. By rank approximation, LSA estimates the occurrence frequency of a word in a particular document. Since polysemy of English terms and Indonesian terms can be quite different, the estimations for words which are mutual translations can be different. For instance, kali and waktu are Indonesian translations for the English word time. However, kali is also the Indonesian translation for the English word river. Suppose kali and time appear frequently in documents about multiplication, but kali and river appear rarely in documents about river. Then, waktu and time appear frequently in documents about time. As a result, LSA may estimate kali with greater frequency in documents about multiplication and time, but with lower frequency in documents about river. The word vectors between kali and river may not be similar. Thus, in bilingual word mapping, LSA may not suggest kali as the proper translation for river. Although polysemous words can also be a problem for the frequency baseline, it merely uses raw word frequency vectors, the problem does not affect other word vectors. LSA, on the other hand, exacerbates this problem by tak-ing it into account in estimating other word frequencies."]},{"title":"6 Summary","paragraphs":["We have presented a model of computing bilingual word and concept mappings between two semantic lexicons, in our case Princeton WordNet and the KBBI, using an extension to LSA that exploits implicit semantic information contained within a parallel corpus.","The results, whilst far from conclusive, indicate that that for bilingual word mapping, LSA is consistently outperformed by the basic vector space model, i.e. where the full-rank word-document matrix is applied, whereas for bilingual concept mapping LSA seems to slightly improve results. We speculate that this is due to the fact that LSA, whilst very useful for revealing implicit semantic patterns, blurs the cooccurrence information that is necessary for establishing word translations.","We suggest that, particularly for bilingual word mapping, a finer granularity of alignment, e.g. at the sentential level, may increase accuracy (Deng and Gao, 2007)."]},{"title":"Acknowledgment","paragraphs":["The work presented in this paper is supported by an RUUI (Riset Unggulan Universitas Indonesia) 2007 research grant from DRPM UI (Direktorat Riset dan Pengabdian Masyarakat Universitas Indonesia). We would also like to thank Franky for help in software implementation and Desmond Darma Putra for help in computing the Fleiss kappa values in Section 4.3."]},{"title":"References","paragraphs":["Eneko Agirre and Philip Edmonds, editors. 2007. Word Sense Disambiguation: Algorithms and Applications. Springer.","Katri A. Clodfelder. 2003. An LSA Implementation Against Parallel Texts in French and English. In Proceedings of the HLT-NAACL Workshop: Build-ing and Using Parallel Texts: Data Driven Machine Translation and Beyond, 111–114.","Yonggang Deng and Yuqing Gao. June 2007. Guiding statistical word alignment models with prior knowledge. In Proceedings of the 45th","Annual Meeting of the Association of Computational Linguistics, pages 1–8, Prague, Czech Republic. Association for Computational Linguistics.","Christiane Fellbaum, editor. May 1998. WordNet: An Electronic Lexical Database. MIT Press.","Joseph L. Fleiss. 1971.Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.","Dan Kalman. 1996. A singularly valuable decomposition: The svd of a matrix. The College Mathematics Journal, 27(1):2–23.","Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25:259–284.","Bob Rehder, Michael L. Littman, Susan T. Dumais, and Thomas K. Landauer. 1997. Automatic 3-language cross-language information retrieval with latent semantic indexing. In Proceedings of the Sixth Text Retrieval Conference (TREC-6), pages 233–239.","Syandra Sari. 2007. Perolehan informasi lintas bahasa indonesia-inggris berdasarkan korpus paralel dengan menggunakan metoda mutual information dan metoda similarity thesaurus. Master’s thesis, Faculty of Computer Science, University of Indonesia, Call number: T-0617."]}]}