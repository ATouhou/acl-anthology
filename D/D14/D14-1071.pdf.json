{"sections":[{"title":"","paragraphs":["Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650, October 25-29, 2014, Doha, Qatar. c⃝2014 Association for Computational Linguistics"]},{"title":"Joint Relational Embeddings for Knowledge-based Question AnsweringMin-Chul Yang","paragraphs":["†"]},{"title":"Nan Duan","paragraphs":["‡"]},{"title":"Ming Zhou","paragraphs":["‡"]},{"title":"Hae-Chang Rim","paragraphs":["† †"]},{"title":"Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea","paragraphs":["‡"]},{"title":"Microsoft Research Asia, Beijing, Chinamcyang@nlp.korea.ac.kr{nanduan, mingzhou}@microsoft.comrim@nlp.korea.ac.krAbstract","paragraphs":["Transforming a natural language (NL) question into a corresponding logical form (LF) is central to the knowledge-based question answering (KB-QA) task. Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates, this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets."]},{"title":"1 Introduction","paragraphs":["Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with the logical predicates, so their types should be consistent with the predicates as well. However, most NER components used in existing KB-QA systems are independent from the NLE-to-predicate mapping procedure.","We present a novel embedding-based KB-QA method that takes all the aforementioned limitations into account, and maps NLE-to-entity and NLE-to-predicate simultaneously using simple vector operations for structured query construction. First, low-dimensional embeddings of n-grams, entity types, and predicates are jointly learned from an existing knowledge base and from entries <entitysubj, NL relation phrase, entityobj> that are mined from NL texts labeled as KB-properties with weak supervision. Each such entry corresponds to an NL expression of a triple <entitysubj, predicate, entityobj> in the KB. These embeddings are used to measure the semantic associations between lexical phrases and two properties of the KB, entity type and logical predicate. Next, given an NL-question, all possible structured queries as candidate LFs are generated and then they are ranked by the similarity between the embeddings of observed features (n-grams) in the NL-question and the embeddings of logical features in the structured queries. Last, answers are retrieved from the KB using the selected LFs.","The contributions of this work are two-fold: (1) as a smoothing technique, the low-dimensional embeddings can alleviate the coverage issues of lexical triggers; (2) our joint approach integrates entity span selection and predicate mapping tasks for KB-QA. For this we built independent entity embeddings as the additional component, solving the entity disambiguation problem."]},{"title":"2 Related Work","paragraphs":["Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007) heavily rely on the <sentence, semantic an-645 notation> pairs for lexical trigger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers.","Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains.","Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NL-questions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion.","Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1","(Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only low-dimensional embeddings of n-grams, entity types, and predicates learned from texts and KB."]},{"title":"3 Setup3.1 Relational Components for KB-QA","paragraphs":["Our method learns semantic mappings between NLEs and the KB2","based on the paired relationships of the following three components: C denotes a set of bag-of-words (or n-grams) as context features (c) for NLEs that are the lexical representations of a logical predicate (p) in KB; T denotes a set of entity types (t) in KB and each type can be used as the abstract expression of a subject entity","1","http://www.freebase.com","2","For this paper, we used a large scale knowledge base that contains 2.3B entities, 5.5K predicates, and 18B assertions. A 16-machine cluster was used to host and serve the whole data. (s) that occurs in the input question; P denotes a set of logical predicates (p) in KB, each of which is the canonical form of different NLEs sharing an identical meaning (bag-of-words; c).","Based on the components defined above, the paired relationships are described as follows: T - P can investigate the relationship between subject entity and logical predicate, as object entity is always missing in KB-QA; C-T can scrutinize subject entity’s attributes for the entity span selection such as its positional information and relevant entity types to the given context, which may solve the entity disambiguation problem in KB-QA; C-P can leverage the semantic overlap between question contexts (n-gram features) and logical predicates, which is important for mapping NL-questions to their corresponding predicates. 3.2 NLE-KB Pair Extraction This section describes how we extract the semantic associated pairs of NLE-entries and KB-triples to learn the relational embeddings (Section 4.1). <Relation Mention, Predicate> Pair (MP) Each relation mention denotes a lexical phrase of an existing KB-predicate. Following information extraction methods, such as PATTY (Nakashole et al., 2012), we extracted the <relation mention, logical predicate> pairs from English WIKIPEDIA3",", which is closely connected to our KB, as follows: Given a KB-triple <entitysubj, logical predicate, entityobj>, we extracted NLE-entries <entitysubj, relation mention, entityobj> where relation mention is the shortest path between entitysubj and entityobj in the dependency tree of sentences. The assumption is that any relation mention (m) in the NLE-entry containing such entity pairs that occurred in the KB-triple is likely to express the predicate (p) of that triple.","With obtaining high-quality MP pairs, we kept only relation mentions that were highly associated with a predicate measured by the scoring function: S(m, p) = PMI(em; ep) + PMI(um; up) (1) where ex is the set of total pairs of both-side entities of entry x (m or p) and ux is the set of unique (distinct) pairs of both-side entities of entry x. In this case, the both-side entities in-dicate entitysubj and entityobj. For a frequency-based probability, PMI(x; y) = log P (x,y)","P (x)P (y) 3 http://en.wikipedia.org/ 646 (Church and Hanks, 1990) can be re-written as PMI(x; y) = log |x ⋂","y|·C","|x|·|y| , where C denotes the total number of items shown in the corpus. The function is partially derived from the support score (Gerber and Ngonga Ngomo, 2011), but we focus on the correlation of shared entity pairs between relation mentions and predicates using the PMI computation. <Question Pattern, Predicate> Pair (QP) Since WIKIPEDIA articles have no information to leverage interrogative features which highly depend on the object entity (answer), it is difficult to distinguish some questions that are composed of only different 5W1H words, e.g., {When|Where} was Barack Obama born? Hence, we used the method of collecting question patterns with human labeled predicates that are restricted by the set of predicates used in MP (Bao et al., 2014)."]},{"title":"4 Embedding-based KB-QA","paragraphs":["Our task is as follows. First, our model learns the semantic associations of C-T , C-P, and T -P (Section 3.1) based on NLE-KB pairs (Section 3.2), and then predicts the semantic-related KB-query which can directly find the answer to a given NL-question.","For our feature space, given an NLE-KB pair, the NLE (relation mention in MP or question pattern in QP) is decomposed into n-gram features: C = {c | c is a segment of NLE}, and the KB-properties are represented by entity type t of entitysubj and predicate p. Then we can obtain a training triplet w = [C, t, p]. Each feature (c ∈ C, t ∈ T , p ∈ P) is encoded in the distributed representation which is n-dimensional embedding vectors (En","): ∀x, x encode","⇒ E(x) ∈ En",".","All n-gram features (C) for an NLE are merged into one embedding vector to help speed up the learning process: E(C) = ∑","c∈C E(c)/|C|. This feature representation is inspired by previous work in embedding-based relation extraction (Weston et al., 2013), but differs in the following ways: (1) entity information is represented on a separate embedding, but its positional information remains as symbol ⟨entity⟩; (2) when the vectors are combined, we use the average of each index to normalize features.","For our joint relational approach, we focus on the set of paired relationships R = {C-t, C-p, t-p} that can be semantically leveraged. Formally, these features are embedded into the same latent space (En",") and their semantic similarities can be computed by a dot product operation:","Sim(a, b) = Sim(rab) = E(a)⊺ E(b) (2) where rab denotes a paired relationship a-b (or (a, b)) in the above set R. We believe that our joint relational learning can smooth the surface (lexical) features for semantic parsing using the aligned entity and predicate. 4.1 Joint Relational Embedding Learning Our ranking-based relational learning is based on a ranking loss (Weston et al., 2010) that supports the idea that the similarity scores of observed pairs in the training set (positive instances) should be larger than those of any other pairs (negative in-stances):","∀i, ∀y′ ̸= yi, Sim(xi, yi) > 1+Sim(xi, y′",") (3) More precisely, for each triplet wi = [Ci, ti, pi] obtained from an NLE-KB pair, the relationships Ri = {Ci-ti, Ci-pi, ti-pi} are trained under the soft ranking criterion, which conducts Stochastic Gradient Descent (SGD). We thus aim to minimize the following: ∀i, ∀y′","̸= yi, max(0, 1−Sim(xi, yi)+Sim(xi, y′","))","(4) Our learning strategy is as follows. First, we initialize embedding space En","by randomly giving mean 0 and standard deviation 1/n to each vector. Then for each training triplet wi, we select the negative pairs against positive pairs (Ci-ti, Ci-pi, and ti-pi) in the triplet. Last, we make a stochastic gradient step to minimize Equation 4 and update En","at each step. 4.2 KB-QA using Embedding Models Our goal for KB-QA is to translate a given NL-question to a KB-query with the form <subject entity, predicate, ?>, where ? denotes the answer entity we are looking for. The decoding process consists of two stages. The first stage involves generating all possible KB-queries (Kq",") for an NL-question q. We first extract n-gram features (Cq",") from the NL-question q. Then for a KB-query kq",", we find all available entity types (tq",") of the identified subject entities (sq",") using the dictionary-based entity detection on the NL-question q (all of spans can be candidate entities), and assign all items of predicate set (P) as the candidate predicates (pq","). Like the training triplets, 647 q where is the city of david? k̂(q) [The City of David, contained by, ?] Cq n-grams of “where is ⟨entity⟩ ?” tq location pq contained by Table 1: The corresponding KB-query k̂(q) for a NL-question q and its decoding triplet wq",".","we also represent the above features as the triplet","form wq i = [Cq","i , tq","i , pq","i ] which is directly linked to","a KB-query kq","i = [sq","i , pq","i , ?]. The second stage","involves ranking candidate KB-queries based on","the similarity scores between the following paired","relationships from the triplet wq","i : Rq","i = {Cq","i -tq","i ,","Cq","i -pq","i , tq i -pq","i }. Unlike in the training step, the sim-","ilarities of Cq","i -tq","i and Cq","i -pq","i are computed by sum-","mation of all pairwise elements (each context em-","bedding E(c), not E(C), with each paired E(t) or","E(p)) for a more precise measurement. Since sim-","ilarites of Rq","are calculated on different scales, we","normalize each value using Z-score (Z(x) = x−μ","σ )","(Kreyszig, 1979). The final score is measured by:","Simq2k(q, kq ) = ∑","r∈Rq Z(Sim(r)) (5) Then, given any NL-question q, we can predict the corresponding KB-query k̂(q):","k̂(q) = arg max k∈Kq Simq2k(q, k) (6) Last, we can retrieve an answer from the KB using a structured query k̂(q). Table 1 shows an example of our decoding process. Multi-related Question Some questions include two-subject entities, both of which are crucial to understanding the question. For the question who plays gandalf in the lord of the rings? Gandalf (character) and The Lord Of The Rings (film) are explicit entities that should be joined to a pair of the two entities (implicit entity). More precisely, the two entities can be combined into one concatenated entity (character-in-film) using our manual rule, which compares the possible pairs of entity types in the question with the list of pre-defined entity type pairs that can be merged into a concatenated entity. Our solution enables a multi-related question to be transformed to a single-related question which can be directly translated to a KB-query. Then, the two entity # Entries Accuracy MP pairs 291,585 89% QP pairs 4,764 98% Table 2: Statistics of NLE-KB pairs mentions are replaced with the symbol ⟨entity⟩ (who play ⟨entity⟩ in ⟨entity⟩ ?). We regard the result of this transformation as one of the candidate KB-queries in the decoding step."]},{"title":"5 ExperimentsExperimental Setting","paragraphs":["We first performed preprocessing, including lowercase transformation, lemmatization and tokenization, on NLE-KB pairs and evaluation data. We used 71,310 n-grams (uni-, bi-, tri-), 990 entity types, and 660 predicates as relational components shown in Section 3.1. The sum of these three numbers (72,960) equals the size of the embeddings we are going to learn. In Table 2, we evaluated the quality of NLE-KB pairs (MP and QP) described in Section 3.2. We can see that the quality of QP pairs is good, mainly due to human efforts. Also, we obtained MP pairs that have an acceptable quality using threshold 3.0 for Equation 1, which leverages the redundancy information in the large-scale data (WIKIPEDIA). For our embedding learning, we set the embedding dimension n to 100, the learning rate (λ) for SGD to 0.0001, and the it-eration number to 30. To make the decoding procedure computable, we kept only the popular KB-entity in the dictionary to map different entity mentions into a KB-entity.","We used two publicly released data sets for QA evaluations: Free917 (Cai and Yates, 2013) in-cludes the annotated lambda calculus forms for each question, and covers 81 domains and 635 Freebase relations; WebQ. (Berant et al., 2013) provides 5,810 question-answer pairs that are built by collecting common questions from Web-query logs and by manually labeling answers. We used the previous three approaches (Cai and Yates, 2013; Berant et al., 2013; Bao et al., 2014) as our baselines. Experimental Results Table 3 reports the over-all performances of our proposed KB-QA method on the two evaluation data sets and compares them with those of the three baselines. Note that we did not re-implement the baseline systems, but just borrowed the evaluation results reported in their 648 Methods Free917 WebQ. Cai and Yates (2013) 59.00% N/A Berant et al. (2013) 62.00% 31.40% Bao et al. (2014) N/A 37.50% Our method 71.38% 41.34% Table 3: Accuracy on the evaluation data Methods Free917 WebQ. Our method 71.38% 41.34% w/o T -P 70.65% 40.55% w/o C-T 67.03% 38.44% w/o C-P 31.16% 19.24% Table 4: Ablation of the relationship types papers. Although the KB used by our system is much larger than FREEBASE, we still think that the experimental results are directly comparable because we disallow all the entities that are not in-cluded in FREEBASE.","Table 3 shows that our method outperforms the baselines on both Free917 and WebQ. data sets. We think that using the low-dimensional embeddings of n-grams rather than the lexical triggers greatly improves the coverage issue. Unlike the previous methods which perform entity disambiguation and predicate prediction separately, our method jointly performs these two tasks. More precisely, we consider the relationships C-T and C-P simultaneously to rank candidate KB-queries. In Table 1, the most independent NER in KB-QA systems may detect David as the subject entity, but our joint approach can predict the appropriate subject entity The City of David by leveraging not only the relationships with other components but also other relationships at once. The syntax-based (grammar formalism) approaches such as Combinatory Categorial Grammar (CCG) may experience errors if a question has grammatical errors. However, our bag-of-words model-based approach can handle any question as long as the question contains keywords that can help in understanding it.","Table 4 shows the contributions of the relationships (R) between relational components C, T , and P. For each row, we remove the similarity from each of the relationship types described in Section 3.1. We can see that the C-P relationship plays a crucial role in translating NL-questions to KB-queries, while the other two relationships are slightly helpful. Result Analysis Since the majority of questions in WebQ. tend to be more natural and diverse, our method cannot find the correct answers to many questions. The errors can be caused by any of the following reasons. First, some NLEs cannot be easily linked to existing KB-predicates, mak-ing it difficult to find the answer entity. Second, some entities can be mentioned in several different ways, e.g., nickname (shaq→Shaquille O’neal) and family name (hitler→Adolf Hitler). Third, in terms of KB coverage issues, we cannot detect the entities that are unpopular. Last, feature representation for a question can fail when the question consists of rare n-grams.","The two training sets shown in Section 3.2 are complementary: QP pairs provide more opportunities for us to learn the semantic associations between interrogative words and predicates. Such resources are especially important for understanding NL-questions, as most of them start with such 5W1H words; on the other hand, MP pairs enrich the semantic associations between context information (n-gram features) and predicates."]},{"title":"6 Conclusion","paragraphs":["In this paper, we propose a novel method that transforms NL-questions into their corresponding logical forms using joint relational embeddings. We also built a simple and robust KB-QA system based on only the learned embeddings. Such embeddings learn the semantic associations between natural language statements and KB-properties from NLE-KB pairs that are automatically extracted from English WIKIPEDIA using KB-triples with weak supervision. Then, we generate all possible structured queries derived from latent logical features of the given NL-question, and rank them based on the similarity scores between those relational attributes. The experimental results show that our method outperforms the latest three KB-QA baseline systems. For our future work, we will build concept-level context embeddings by leveraging latent meanings of NLEs rather than their surface n-grams with the aligned logical features on KB. Acknowledgement This research was supported by the Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2012M3C4A7033344). 649"]},{"title":"References","paragraphs":["Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao. 2014. Knowledge-based question answering as machine translation. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967–976. Association for Computational Linguistics. Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1415–1425. Association for Computational Linguistics.","Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October. Association for Computational Linguistics.","Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In In Proceedings of 15th International Conference on Artificial Intelligence and Statistics.","Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL), pages 423–433. The Association for Computer Linguistics.","Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22–29, March.","Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In Association for Computational Linguistics (ACL), pages 1608–1618. The Association for Computer Linguistics.","Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011. Bootstrapping the linked data web. In 1st Workshop on Web Scale Knowledge Extraction @ ISWC 2011.","E. Kreyszig. 1979. Advanced Engineering Mathematics. Wiley.","Jayant Krishnamurthy and Tom M. Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 754–765, Stroudsburg, PA, USA. Association for Computational Linguistics.","Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556, Seattle, Washington, USA, October. Association for Computational Linguistics.","Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 590–599, Stroudsburg, PA, USA. Association for Computational Linguistics.","RaymondJ. Mooney. 2007. Learning for semantic parsing. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 4394 of Lecture Notes in Computer Science, pages 311–324. Springer Berlin Heidelberg.","Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: A taxonomy of relational patterns with semantic types. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1135–1145, Stroudsburg, PA, USA. Association for Computational Linguistics.","Jason Weston, Samy Bengio, and Nicolas Usunier. 2010. Large scale image annotation: Learning to rank with joint word-image embeddings. Machine Learning, 81(1):21–35, October.","Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1366–1371, Seattle, Washington, USA, October. Association for Computational Linguistics.","Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 956–966, Baltimore, Maryland, June. Association for Computational Linguistics.","John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, AAAI’96, pages 1050–1055. AAAI Press. Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI, pages 658–666. AUAI Press. 650"]}]}
