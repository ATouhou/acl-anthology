{"sections":[{"title":"","paragraphs":["Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917–927, October 25-29, 2014, Doha, Qatar. c⃝2014 Association for Computational Linguistics"]},{"title":"A Polynomial-Time Dynamic Oraclefor Non-Projective Dependency ParsingCarlos Gómez-Rodrı́guezDepartamento deComputaciónUniversidade da Coruña, Spaincgomezr@udc.es Francesco SartorioDepartment ofInformation EngineeringUniversity of Padua, Italysartorio@dei.unipd.it Giorgio SattaDepartment ofInformation EngineeringUniversity of Padua, Italysatta@dei.unipd.itAbstract","paragraphs":["The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets."]},{"title":"1 Introduction","paragraphs":["Greedy transition-based parsers for dependency grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These meth-ods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history.","Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these meth-ods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beam-search, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency.","The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, and is later used for decoding. Traditionally, so-called static oracles have been exploited in training, where a static oracle is defined only for configurations that have been reached by computations with no mistake, and it returns a single canonical transition among those that are optimal.","Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transition-based parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser.","Naı̈ve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several projective dependency parsers. To our knowledge, no polynomial-time algorithm has been published for transition-based parsers based on non-projective dependency grammars.","In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006). This restriction was further investigated by Kuhlmann and Nivre (2010) and Cohen et al. (2011). We provide an implementation for a dynamic oracle for this parser running in polynomial time.","We experimentally compare the parser trained with the dynamic oracle to a baseline obtained by training with a static oracle. Significant accuracy improvements are achieved on many languages when using our dynamic oracle. To our knowledge, these are the first experimental results on non-projective parsing based on a dynamic oracle. 917"]},{"title":"2 Preliminary Definitions","paragraphs":["Transition-based dependency parsing was originally introduced by Yamada and Matsumoto (2003) and Nivre (2003). In this section we briefly summarize the notation we use for this framework and introduce the notion of dynamic oracle. 2.1 Transition-Based Dependency Parsing We represent an input sentence as a string w = w0 · · · wn, n ≥ 1, where each wi with i ̸= 0 is a lexical symbol and w0 is a special symbol called root. Set Vw = {i | 0 ≤ i ≤ n} denotes the symbol occurrences in w. For i, j ∈ Vw with i ̸= j, we write i → j to denote a grammatical dependency of some unspecified type betweenwi and wj, where wi is the head and wj is the dependent.","A dependency tree t for w is a directed tree with node set Vw and with root node 0. An arc of t is a pair (i, j), encoding a dependency i → j; we will often use the latter notation to denote arcs.","A transition-based dependency parser typically uses a stack data structure to process the input string from left to right, in a way very similar to the classical push-down automaton for context-free languages (Hopcroft et al., 2006). Each stack element is a node from Vw, representing the root of a dependency tree spanning some portion of the input w, and no internal state is used. At each step the parser applies some transition that updates the stack and/or consumes one symbol from the input. Transitions may also construct new dependencies, which are added to the current configuration of the parser.","We represent the stack as an ordered sequence σ = [hd, . . . , h1], d ≥ 0, of nodes hi ∈ Vw, with the topmost element placed at the right. When d = 0, we have the empty stack σ = []. We use the vertical bar to denote the append operator for σ, and write σ = σ′","|h1 to indicate that h1 is the topmost element of σ.","The portion of the input string still to be processed by the parser is called the buffer. We represent the buffer as an ordered sequence β = [i, . . . , n] of nodes from Vw, with i the first element of the buffer. We denote the empty buffer as β = []. Again, we use the vertical bar to denote the append operator, and write β = i|β′","to indicate that i is the first symbol occurrence ofβ; consequently, we have β′","= [i + 1, . . . , n].","In a transition-based parser, the parsing process is defined through the technical notions of configuration and transition. A configuration of the parser relative to w is a triple c = (σ, β, A), where σ and β are a stack and a buffer, respect-ively, and A is the set of arcs that have been built so far. A transition is a partial function mapping the set of parser configurations into itself. Each transition-based parser is defined by means of some finite inventory of transitions. We will later introduce the specific inventory of transitions for the parser that we investigate in this paper. We use the symbol ⊢ to denote the binary relation formed by the union of all transitions of a parser.","With the notions of configuration and transition in place, we can define a computation of the parser on w as a sequence c0, c1, . . . , cm, m ≥ 0, of configurations relative tow, under the condition that ci−1 ⊢ ci for each i with 1 ≤ i ≤ m. We use the reflexive and transitive closure of ⊢, written ⊢∗",", to represent computations. 2.2 Configuration Loss and Dynamic Oracles A transition-based dependency parser is a non-deterministic device, meaning that a given configuration can be mapped into several configurations by the available transitions. However, in several implementations the parser is associated with a discriminative model that, on the basis of some features of the current configuration, always chooses a single transition. In other words, the model is used to run the parser as a pseudo-deterministic device. The training of the discriminative model relies on a component called the parsing oracle, which maps parser configurations to “optimal” transitions with respect to some reference dependency tree, which we call the gold tree.","Traditionally, so-called static oracles have been used which return a single, canonical transition and they do so only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. In recent work, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) have introduced dynamic oracles, which return the set of all transitions that are optimal with respect to a gold tree, and are well-defined and correct for every configuration that is reachable by the parser. These authors have shown that the accuracy of transition-based dependency parsers can be substantially improved if dynamic oracles are used in place of static ones. In what follows, we provide a mathematical definition of dynamic oracles, following Goldberg et al. (2014). 918 (σ, k|β, A) ⊢sh (σ|k, β, A) (σ|i|j, β, A) ⊢la (σ|j, β, A ∪ {j → i}) (σ|i|j, β, A) ⊢ra (σ|i, β, A ∪ {i → j}) (σ|i|j|k, β, A) ⊢la2 (σ|j|k, β, A ∪ {k → i}) (σ|i|j|k, β, A) ⊢ra2 (σ|i|j, β, A ∪ {i → k}) Figure 1: Transitions of the non-projective parser.","Let t1 and t2 be dependency trees for w, with arc sets A1 and A2, respectively. The loss of t1 with respect to t2 is defined as L(t1, t2) = |A1 \\ A2| . (1) Note that L(t1, t2) = L(t2, t1), since |A1| = |A2|. Furthermore L(t1, t2) = 0 if and only if t1 and t2 are the same tree.","Let c be a configuration of a transition-based parser relative to w. Let also D(c) be the set of all dependency trees that can be obtained in a computation of the form c ⊢∗","cf , where cf is a final configuration, that is, a configuration that has constructed a dependency tree for w. We extend the loss function in (1) to configurations by letting","L(c, t2) = min t1∈D(c) L(t1, t2) . (2)","Let tG be the gold tree for w. Quantity L(c, tG) can be used to define a dynamic oracle as follows. For any transition ⊢τ in the finite inventory of our parser, we use the functional notation τ (c) = c′","in place of c ⊢τ c′",". We then let oracle(c, tG) = {τ | L(τ (c), tG) − L(c, tG) = 0} . (3) In words, (3) provides the set of transitions that do not increase the loss of c; we call these transitions optimal for c.","A naı̈ve way of implementing (3) would be to explicitly compute the set D(c) in (2), which has exponential size. More interestingly, the implementation of dynamic oracles proposed by the above cited authors all run in polynomial time. These oracles are all defined for projective parsing. In this paper, we present a polynomial-time oracle for a non-projective parser."]},{"title":"3 Non-Projective Dependency Parsing","paragraphs":["In this section we introduce a parser for non-projective dependency grammars that is derived from the transition-based parser originally presented by Attardi (2006), and was further investigated by Kuhlmann and Nivre (2010) and Cohen et al. (2011). Our definitions follow the framework introduced in Section 2.1.","We start with some additional notation. Let t be a dependency tree for w and let k be a node of t. Consider the complete subtree t′","of t rooted at k, that is, the subtree of t induced by k and all of the descendants of k in t. The span of t′","is the sub-sequence of tokens in w represented by the nodes of t′",". Node k has gap-degree 0 if the span of t′ forms a (contiguous) substring of w. A dependency tree is called projective if all of its nodes have gap-degree 0; a dependency tree which is not projective is called non-projective.","Given w as input, the parser starts with the initial configuration ([], [0, . . . , n], ∅), consisting of an empty stack, a buffer with all the nodes representing the symbol occurrences in w, and an empty set of constructed dependencies (arcs). The parser stops when it reaches a final configuration of the form ([0], [], A), consisting of a stack with only the root node and of an empty buffer; in any such configuration, set A always implicitly defines a valid dependency tree (rooted in node 0).","The core of the parser consists of an inventory of five transitions, defined in Figure 1. Each transition is specified using the free variables σ, β, A, i, j and k. As an example, the schema (σ|i|j, β, A) ⊢la (σ|j, β, A ∪ {j → i}) means that if a configurationc matches the antecedent, then a new configuration is obtained by instantiating the variables in the consequent accordingly.","The transition ⊢sh, called shift, reads a new token from the input sentence by removing it from the buffer and pushing it into the stack. Each of the other transitions, collectively called reduce transitions, has the effect of building a dependency between two nodes in the stack, and then removing the dependent node from the stack. The removal of the dependent ensures that the output dependency tree is built in a bottom-up order, collecting all of the dependents of each node i before linking i to its head.","The transition ⊢la, called left-arc, creates a left-ward arc where the topmost stack node is the head and the second topmost node is the dependent, and removes the latter from the stack. The transition ⊢ra, called right-arc, is defined symmetrically, so that the topmost stack node is at-919"]},{"title":"σ σ σh h hh","paragraphs":["1 1 2 3 minimum stack length at c c1"]},{"title":"c c c","paragraphs":["0 1 stack length"]},{"title":"...","paragraphs":["... m m Figure 2: General form of the computations associated with an item [h1, h2, h3]. tached as a dependent of the second topmost node. The combination of the shift, left-arc and right-arc transitions provides complete coverage of projective dependency trees, but no support for nonprojectivity, and corresponds to the so-called arc-standard parser introduced by Nivre (2004).","Support for non-projective dependencies is achieved by adding the transitions ⊢la2 and ⊢ra2, which are variants of the left-arc and right-arc transitions, respectively. These new transitions create dependencies involving the first and the third topmost nodes in the stack. The creation of dependencies between non-adjacent stack nodes might produce crossing arcs and is the key to the construction of non-projective trees.","Recall that transitions are partial functions, meaning that they might be undefined for some configurations. Specifically, the shift transition is only defined for configurations with a non-empty buffer. Similarly, the left-arc and right-arc transitions can only be applied if the length of the stack is at least 2, while the transitions ⊢la2 and ⊢ra2 require at least 3 nodes in the stack.","Transitions ⊢la2 and ⊢ra2 were originally introduced by Attardi (2006) together with other, more complex transitions. The parser we define here is therefore more restrictive than Attardi (2006), meaning that it does not cover all the non-projective trees that can be processed by the original parser. However, the restricted parser has recently attracted some research interest, as it covers the vast majority of non-projective constructions appearing in standard treebanks (Attardi, 2006; Kuhlmann and Nivre, 2010), while keeping simplicity and interesting properties like being compatible with polynomial-time dynamic programming (Cohen et al., 2011)."]},{"title":"4 Representation of Computations","paragraphs":["Our oracle algorithm exploits a dynamic programming technique which, given an input string, combines certain pieces of a computation of the parser from Section 3 to obtain larger pieces. In order to efficiently encode pieces of computations, we borrow a representation proposed by Cohen et al. (2011), which is introduced in this section.","Let w = a0 · · · an and Vw be specified as in Section 2, and let w′","be some substring of w. (The specification of w′","is not of our concern in this section.) Let also h1, h2, h3 ∈ Vw. We are interested in computations of the parser processing the substring w′","and having the form c0, c1, . . . , cm, m ≥ 1, that satisfy both of the following conditions, exemplified in Figure 2.","• For some sequence of nodes σ with |σ| ≥ 0, the stack associated with c0 has the form σ|h1 and the stack associated with cm has the form σ|h2|h3.","• For each intermediate configuration ci, 1 ≤ i ≤ m − 1, the stack associated with ci has the form σσi, where σi is a sequence of nodes with |σi| ≥ 2.","An important property of the above definition needs to be discussed here, which is at the heart of the polynomial-time algorithm in the next section. If in c0, c1, . . . , cm we replace σ with a different sequence σ′",", we obtain a valid computation for w′ constructing exactly the same dependencies as the original computation. To see this, let ci−1 ⊢τi ci for each i with 1 ≤ i ≤ m. Then ⊢τ1 must be a shift, otherwise |σ1| ≥ 2 would be violated. Consider now a transition ⊢τi with 2 ≤ i ≤ m that builds some dependency. From |σi| ≥ 2 we derive |σi−1| ≥ 3. We can easily check from Figure 1 that none of the nodes in σ can be involved in the constructed dependency.","Intuitively, the above property asserts that the sequence of transitions ⊢τ1, ⊢τ2, . . . , ⊢τm can be applied to parse substring w′","independently of the context σ. This suggests that we can group into an equivalence class all the computations satisfy-ing the conditions above, for different values of σ. We indicate such class by means of the tuple [h1, h2h3], called item. It is easy to see that each item represents an exponential number of computations. In the next section we will show how we can process items with the purpose of obtaining an efficient computation for dynamic oracles. 920"]},{"title":"5 Dynamic Oracle Algorithm","paragraphs":["Our algorithm takes as input a gold tree tG for string w and a parser configuration c = (σ, β, A) relative to w, specified as in Section 2. We assume that tG can be parsed by the non-projective parser of Section 3 starting from the initial configuration. 5.1 Basic Idea The algorithm consists of two separate stages, in-formally discussed in what follows. In the first stage we identify some tree fragments of tG that can be constructed by the parser after reaching configurationc, in a way that does not depend on the content of σ. This means that these fragments can be precomputed by looking only into β. Furthermore, since these fragments are subtrees of tG, their computation has no effect on the overall loss of a computation on w.","For each fragment t with the above properties, we replace all the nodes in β that are also nodes of t with the root node of t itself. The result of the first stage is therefore a new node sequence shorter than β, which we call the reduced buffer βR.","In the second stage of the algorithm we use a variant of the tabular method developed by Cohen et al. (2011), which was originally designed to simulate all computations of the parser in Section 3 on an input string w. We run the above method on the concatenation of the stack and the reduced buffer, with some additional constraints that restrict the search space in two respects. First, we visit only those computations of the parser that step through configuration c. Second, we reach only those dependency trees that contain all the tree fragments precomputed in the first stage. We can show that such search space always contains at least one dependency tree with the desired loss, which we then retrieve performing a Viterbi search. 5.2 Preprocessing of the Buffer Let t be a complete subtree of tG, having root node k in β. Consider the following two conditions, defined ont.","• Bottom-up completeness: No arc i → j in t is such that i is a node in β, i ̸= k, and j is a node in σ.","• Zero gap-degree: The nodes of t that are in β form a (contiguous) substring of w. We claim that if t satisfies the above conditions, then we can safely reduce the nodes of t appearing in β, replacing them with node k. We only report here an informal discussion of this claim, and omit a formal proof.","As a first remark, recall that our parser implements a purely bottom-up strategy. This means that after a tree has been constructed, all of its nodes but the root are removed from the parser configuration. Then the Bottom-up completeness condition guarantees that if we remove from β all nodes of t but k, the nodes of t that are in σ can still be processed in a way that does not affect the loss, since their parent must be either k or a node that is neither in β nor in σ. Note that the nodes of t that are neither in β nor in σ are irrelevant to the precomputation of t from β, since these nodes have already been attached and are no longer available to the parser.","As a second remark, the Zero gap-degree condition guarantees that the span of t over the nodes of β is not interleaved by nodes that do not belong to t. This is also an important requirement for the precomputation of t from β, since a tree fragment having a discontinuous span over β might not be constructable independently of σ. More specific-ally, parsing such fragment implies dealing with the nodes in the discontinuities, and this might require transitions involving nodes from σ.","We can now use the sufficient condition above to compute βR. We process β from left to right. For each node k, we can easily test the Bottom-up completeness condition and the Zero gap-degree condition for the complete subtree t of tG rooted at k, and perform the reduction if both conditions are satisfied. Note that in this process a node k resulting from the reduction of t might in turn be removed from β if, at some later point, we reduce a supertree of t. 5.3 Computation of the Loss We describe here our dynamic programming algorithm for the computation of the loss of an input configurationc. We start with some additional notation. Let γ = σβR be the concatenation of σ and βR, which we treat as a string of nodes. For integers i with 0 ≤ i ≤ |γ| − 1, we write γ[i] to denote the (i + 1)-th node of γ. Let also l = |σ|. Symbol l is used to mark the boundary between the stack and the reduced buffer in γ, thus γ[i] with i < l is a node of σ, while γ[i] with i ≥ l is a node of βR.","Algorithm 1 computes the loss of c by processing the sequence γ in a way quite similar to the 921 standard nested loop implementation of the CKY parser for context-free grammars (Hopcroft et al., 2006). The algorithm uses a two-dimensional array T whose indexes range from 0 to |γ| = l + |βR|, and only the cells T [i, j] with i < j are filled.","We view each T [i, j] as an association list whose keys are items [h1, h2h3], defined in the context of the substring γ[i] · · · γ[j − 1] of γ; see Section 4. The value stored at T [i, j]([h1, h2h3]) is the minimum loss contribution due to the computations represented by [h1, h2h3]. For technical reasons, we assume that our parser starts with a symbol $ ̸∈ Vw in the stack, denoting the bottom of the stack.","We initialize the table by populating the cells of the form T [i, i + 1] with information about the trivial computations consisting of a single ⊢sh transition that shifts the node γ[i] into the stack. These computations are known to have zero loss contribution, because a ⊢sh transition does not create any arcs. In the case where the node γ[i] be-longs to σ, i.e., i < l, we assign loss contribution 0 to the entry T [i, i + 1]([γ[i − 1], γ[i − 1]γ[i]]) (line 3 of Algorithm 1), because γ[i] is shifted with γ[i − 1] at the top of the stack. On the other hand, if γ[i] is in β, i.e., i ≥ l, we assign loss contribution 0 to several entries in T [i, i + 1] (line 6) because, at the time γ[i] is shifted, the content of the stack depends on the transitions executed before that point.","After the above initialization, we consider pairs of contiguous substrings γ[i] · · · γ[k − 1] and γ[k] · · · γ[j − 1] of γ. At each inner iteration of the nested loops of lines 7-11 we update cell T [i, j] based on the content of the cells T [i, k] and T [k, j]. We do this through the procedure PROCESSCELL(T , i, k, j), which considers all pairs of keys [h1, h2h3] in T [i, k] and [h3, h4h5] in T [k, j]. Note that we require the index h3 to match between both items, meaning that their computations can be concatenated. In this way, for each reduce transition τ in our parser, we compute the loss contribution for a new piece of computation defined by concatenating a computation with minimum loss contribution in the first item and a computation with minimum loss contribution in the second item, followed by the transition τ . The fact that the new piece of computation can be represented by an item is exemplified in Figure 3 for the case τ = ⊢ra2."]},{"title":"σh","paragraphs":["1"]},{"title":"c","paragraphs":["0"]},{"title":"σhh","paragraphs":["23"]},{"title":"c σhh","paragraphs":["23"]},{"title":"c σhh","paragraphs":["2"]},{"title":"ch","paragraphs":["5 4"]},{"title":"σhh","paragraphs":["45"]},{"title":"c","paragraphs":["+1 la : create arc h h and remove h from stack2 5 2 2 [h , h h ]1 2 3 la 2"]},{"title":"˫σh","paragraphs":["1"]},{"title":"c","paragraphs":["0"]},{"title":"σhh","paragraphs":["45"]},{"title":"c","paragraphs":["+1"]},{"title":"... ...+ +","paragraphs":["[h , h h ]3 4 5 [h , h h ]1 4 5"]},{"title":"...","paragraphs":["→ m m r r r Figure 3: Concatenation of two computations/items and transition ⊢ra2, resulting in a new computation/item.","The computed loss contribution is used to update the entry in T [i, j] corresponding to the item associated with the new computation. Observe how the loss contribution provided by the arc created by τ is computed by the δG function at lines 17, 20, 23 and 26, which is defined as:","δG(i → j) = { 0, if i → j is in tG; 1, otherwise. (4)","We remark that the nature of our problem allows us to apply several shortcuts and optimiza-tions that would not be possible in a setting where we actually needed to parse the string γ. First, the range of variable i in the loop in line 8 starts at max{0, l − d}, rather than at 0, because we do not need to combine pairs of items originating from nodes in σ below the topmost node, as the items resulting from such combinations correspond to computations that do not contain our input configuration c. Second, when we have set values for i such that i+2 < l, we can omit calling PROCESSCELL for values of the parameter k ranging from i+2 to l−1, as those calls would use as their input one of the items described above, which are not of interest. Finally, when processing substrings that are entirely in βR (i ≥ l) we can restrict the transitions that we explore to those that generate arcs that either are in the gold tree tG, or have a parent node which is not present in γ (see conditions in 922 Algorithm 1 Computation of the loss function 1: T [0, 1]([$, $0]) ← 0 ▷ shift node 0 on top of empty stack symbol $ 2: for i ← 1 to l − 1 do 3: T [i, i + 1]([γ[i − 1], γ[i − 1]γ[i]]) ← 0 ▷ shift node γ[i] with γ[i − 1] on top of the stack 4: for i ← l to |γ| do 5: for h ← 0 to i − 1 do 6: T [i, i + 1]([γ[h], γ[h]γ[i]]) ← 0 ▷ shift node γ[i] with γ[h] on top of the stack 7: for d ← 2 to |γ| do ▷ consider substrings of length d 8: for i ← max{0, l − d} to |γ| − d do ▷ i = beginning of substring 9: j ← i + d ▷ j − 1 = end of substring 10: PROCESSCELL(T , i, i + 1, j) ▷ We omit the range k = i + 2 to max{i + 2, l} − 1 11: for k ← max{i + 2, l} to j do ▷ factorization of substring at k 12: PROCESSCELL(T , i, k, j)","13: return T [0, |γ|]([$, $0]) + ∑ i∈[0,l−1] Lc(σ[i], tG) 14: procedure PROCESSCELL(T , i, k, j) 15: for each key [h1, h2h3]) defined inT [i, k] do 16: for each key [h3, h4h5]) defined inT [k, j] do ▷ h3 must match between the two entries 17: lossla ← T [i, k]([h1, h2h3]) + T [k, j]([h3, h4h5]) + δG(h5 → h4) 18: if (i < l) ∨ δG(h5 → h4) = 0 ∨ (h5 ̸∈ γ) then 19: T [i, j]([h1, h2h5]) ← min{lossla, T [i, j]([h1, h2h5])} ▷ cell update ⊢la 20: lossra ← T [i, k]([h1, h2h3]) + T [k, j]([h3, h4h5]) + δG(h4 → h5) 21: if (i < l) ∨ δG(h4 → h5) = 0 ∨ (h4 ̸∈ γ) then 22: T [i, j]([h1, h2h4]) ← min{lossra, T [i, j]([h1, h2h4])} ▷ cell update ⊢ra 23:","lossla 2 ← T [i, k]([h1, h2h3]) + T [k, j]([h3, h4h5]) + δG(h5 → h2) 24: if (i < l) ∨ δG(h5 → h2) = 0 ∨ (h5 ̸∈ γ) then 25:","T [i, j]([h1, h4h5]) ← min{lossla 2, T [i, j]([h1, h4h5])}","▷ cell update ⊢la 2 26:","lossra 2 ← T [i, k]([h1, h2h3]) + T [k, j]([h3, h4h5]) + δG(h2 → h5) 27: if (i < l) ∨ δG(h2 → h5) = 0 ∨ (h2 ̸∈ γ) then 28:","T [i, j]([h1, h2h4]) ← min{lossra 2, T [i, j]([h1, h2h4])}","▷ cell update ⊢ra 2 lines 18, 21, 24, 27), because we know that incorrectly attaching a buffer node as a dependent of an-other buffer node, when the correct head is available, can never be an optimal decision in terms of loss.","Once we have filled the table T , the loss for the input configurationc can be obtained from the value of the entry T [0, |γ|]([$, $0]), representing the minimum loss contribution among computations that reach the input configurationc and parse the whole input string. To obtain the total loss, we add to this value the loss contribution accumulated by the dependency trees with root in the stack σ of c. This is represented in Algorithm 1 as∑","i∈[0,l−1] Lc(σ[i], tG), where Lc(σ[i], tG) is the count of the descendants of σ[i] (the (i + 1)-th element of σ) that had been assigned the wrong head by the parser with respect to tG. 5.4 Sample Run Consider the Czech sentence and the gold dependency tree tG shown in Figure 4(a). Given the configuration c = (σ, β, A) where σ = [0, 1, 3, 4], β = [5, . . . , 13] and A = {3 → 2}, we trace the two stages of the algorithm. Preprocessing of the buffer The complete subtree rooted at node 7 satisfies the Bottom-up completeness and the Zero gap-degree conditions in Section 5.2, so the nodes 5, . . . , 12 in β can be replaced with the root 7. Note that all the nodes in the span 5, . . . , 12 have all their (gold) dependents in that span, with the exception of the root 7, with its dependent node 1 still in the stack. No other reduction is possible, and we have βR = [7, 13]. The corresponding fragment of tG is represented in Figure 4(b). Computation of the loss Let γ = σβR. Algorithm 1 builds the two-dimensional array T in Figure 4(c). Each cell T [i, j] contains an association list, whose (key:value) pairs map items to their loss contribution. Figure 4(c) only shows the pairs involved in the minimum-loss computation.","Lines 1-6 of Algorithm 1 initialize the cells in the diagonal, T [0, 1], . . . , T [5, 6]. The boundary between stack and buffer is l = 4, thus cells T [0, 1], T [1, 2], and T [2, 3] contain only one element, while T [3, 4], T [4, 5] and T [5, 6] contain as many as the previous elements in γ, although not all of them are shown in the figure.","Lines 7-12 fill the superdiagonals until T [0, 6] is reached. The cells T [0, 2], T [0, 3] and T [1, 3] 923","-Root- V běžném provozu však telefonnı́ linky nermajı́ takivou kvalitu jako v laboratoři . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 (a) Non-projective dependency tree from the Prague Dependency Treebank.","-Root- V provozu však nermajı́ . 0 1 3 4 7 13","σ βR (b) Fragment of dependency tree in (a) after buffer reduction. i j 1 2 3 4 5 6 0 [$,$ 0]:0 ∅ ∅ ... [$,$ 0]:1 [$,$ 0]:1 1 [0,0 1]:0 ∅ ... [0,0 4]:1 ... 2 [1,1 3]:0 [1,1 4]:1 [1,4 7]:1 ... 3 [3,3 4]:0 [3,4 7]:1 ... 4 [4,4 7]:0 ... 5 [0,0 13]:0 (c) Relevant portion of T computed by Algorithm 1, with the loss of c in the yellow entry. Figure 4: Example of loss computation given the sentence in (a) and considering a configurationc with σ = [0, 1, 3, 4] and β = [5, . . . , 13]. are left empty because l = 4. Once T [0, 6] is calculated, it contains only the entry with key [$, $, 0], with the associated value 1 representing the minimum number of wrong arcs that the parsing algorithm has to build to reach a final configuration from c. Then, Line 13 retrieves the loss of the configuration, computed as the sum of T [0, 6]([$, $, 0]) with the term Lc, representing the erroneous arcs made before reaching c.","Note that in our example the loss of c is 1, even though Lc = 0, meaning that there are no wrong arcs in A. Indeed, given c, there is no single computation that builds all the remaining arcs in tG. This is reflected in T , where the path to reach the item with minimum loss has to go through either T [3, 5] or T [2, 4], which implies building the erroneous arc (w7 → w3) or (w4 → w3), respectively."]},{"title":"6 Computational Analysis","paragraphs":["The first stage of our algorithm can be easily implemented in time O(|β| |tG|), where |tG| is the number of nodes in tG, which is equal to the length n of the input string.","For the worst-case complexity of the second stage (Algorithm 1), note that the number of cell updates made by calling PROCESSCELL(T , i, k, j) with k < l is O(|σ|3","|γ|2","|βR|). This is because these updates can only be caused by procedure calls on line 10 (as those on line 12 always set k ≥ l) and therefore the index k always equals i + 1, while h2 must equal h1 because the item [h1, h2h3] is one of the initial items created on line 3. The variables i, h1 and h3 must index nodes on the stack σ as they are bounded by k, while j ranges over βR and h4 and h5 can refer to nodes either on σ or on βR.","On the other hand, the number of cell updates triggered by calls to PROCESSCELL such that k ≥ l is O(|γ|4","|βR|4","), as they happen for four indices referring to nodes of βR (k, j, h4, h5) and four indices that can range over σ or βR (i, h1, h2, h3).","Putting everything together, we conclude that the overall complexity of our algorithm is O(|β| |tG| + |σ|3","|γ|2","|βR| + |γ|4","|βR|4",").","In practice, quantities |σ|, |βR| and |γ| are significantly smaller thann, providing reasonable training times as we will see in Section 7. For instance, when measured on the Czech treebank, the average value of |σ| is 7.2, with a maximum of 87. Even more interesting, the average value of |βR| is 2.6, with a maximum of 23. Comparing this to the average and maximum values of |β|, 11 and 192, respectively, we see that the buffer reduction is crucial in reducing training time.","Note that, when expressed as a function of n, our dynamic oracle has a worst-case time complexity of O(n8","). This is also the time complexity of the dynamic programming algorithm of Cohen et al. (2011) we started with, simulating all computations of our parser. In contrast, the dynamic oracle of Goldberg et al. (2014) for the projective case achieves a time complexity of O(n3",") from the dynamic programming parser by Kuhlmann et al. (2011) running in time O(n5","). 924","The reason why we do not achieve any asymptotic improvement is that some helpful properties that hold with projective trees are no longer satisfied in the non-projective case. In the projective (arc-standard) case, subtrees that are in the buffer can be completely reduced. As a consequence, each oracle step always combines an inferred entry in the table with either a node from the stack or a node from the reduced buffer, asymptotically reducing the time complexity. However, in the non-projective (Attardi) case, subtrees in the buffer can not always be completely reduced, for the reasons mentioned in the second-to-last paragraph of Section 5.2. As a consequence, the oracle needs to make cell updates in a more general way, which includes linking pairs of elements in the reduced buffer or pairs of inferred entries in the table.","-Root- John was not as good for the job as Kate . 0 1 2 3 4 5 6 7 8 9 10 11 Figure 5: Non-projective dependency tree adapted from the Penn Treebank.","An example of why this is needed is provided by the gold tree in Figure 5. Assume a configuration c = (σ, β, A) where σ = [0, 1, 2, 3, 4], β = [5, . . . , 11], and A = ∅. It is easy to see that the loss of c is greater than zero, since the gold tree is not reachable from c: parsing the subtree rooted at node 5 requires shifting 6 into the stack, and this makes it impossible to build the arcs 2 → 5 and 2 → 6. However, if we reduced the subtree in the buffer with root 5, we would incorrectly obtain a loss of 0, as the resulting tree is parsable if we start with ⊢sh followed by ⊢la and ⊢ra2. Note that there is no way of knowing whether it is safe to reduce the subtree rooted at 5 without using non-local information. For example, the arc 2 → 6 is crucial here: if 6 depended on 5 or 4 instead, the loss would be zero. These complications are not found in the projective case, allowing for the mentioned asymptotic improvement."]},{"title":"7 Experimental Evaluation","paragraphs":["For comparability with previous work on dynamic oracles, we follow the experimental settings reported by Goldberg et al. (2014) for their arc-standard dynamic oracle. In particular, we use the same training algorithm, features, and root node position. However, we train the model for 20 itera-static dynamic UAS LAS UAS LAS Arabic 80.90 71.56 82.23 72.63 Basque 75.96 66.74 74.32 65.59 Catalan 90.55 85.20 89.94 84.96 Chinese 84.72 79.93 85.34 81.00 Czech 79.83 72.69 82.08 74.44 English 85.52 84.46 87.38 86.40 Greek 79.84 72.26 81.55 74.14 Hungarian 78.13 68.90 76.27 68.14 Italian 83.08 78.94 84.43 80.45 Turkish 79.57 69.44 79.41 70.32 Bulgarian 89.46 85.99 89.32 85.92 Danish 85.58 81.25 86.03 81.59 Dutch 79.05 75.69 80.13 77.22 German 88.34 86.48 88.86 86.94 Japanese 93.06 91.64 93.56 92.18 Portuguese 84.80 81.38 85.36 82.10 Slovene 76.33 68.43 78.20 70.22 Spanish 79.88 76.84 80.25 77.45 Swedish 87.26 82.77 87.24 82.49 PTB 89.55 87.18 90.47 88.18 Table 1: Unlabelled Attachment Score (UAS) and Labelled Attachment Score (LAS) using a static and a dynamic oracle. Evaluation on CoNLL 2007 (first block) and CoNLL 2006 (second block) datasets is carried out including punctuation, evaluation on the Penn Treebank excludes it. tions rather than 15, as the increased search space and spurious ambiguity of Attardi’s non-projective parser implies that more iterations are required to converge to a stable model. A more detailed description of the experimental settings follows. 7.1 Experimental Setup Training We train a global linear model using the averaged perceptron algorithm and a labelled version of the parser described in Section 3. We perform on-line training using the oracle defined in Section 5: at each parsing step, the model’s weights are updated if the predicted transition results into an increase in configuration loss, but the process continues by following the predicted transition independently of the loss increase.","As our baseline we train the model using the static oracle defined by (Cohen et al., 2012). This oracle follows a canonical computation that creates arcs as soon as possible, and prioritizes the ⊢la transition over the ⊢la2 transition in situations 925 where both create a gold arc. The static oracle is not able to deal with configurations that can-not reach the gold dependency tree, so we constrain the training algorithm to follow the zero-loss transition provided by the oracle.","While this version of Attardi’s parser has been shown to cover the vast majority of non-projective sentences in several treebanks (Attardi, 2006; Cohen et al., 2012), there still are some sentences which are not parsable. These sentences are skipped during training, but not during test and evaluation of the model. Datasets We evaluate the parser performance over CoNLL 2006 and CoNLL 2007 datasets. If a language is present in both datasets, we use the latest version. We also include results over the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006). For the CoNLL datasets we use the provided part-of-speech tags and the standard training/test partition; for the PTB we use automatically assigned tags, we train on sections 2-21 and test on section 23. 7.2 Results and Analysis In Table 1 we report the unlabelled (UAS) and labelled (LAS) attachment scores for the static and the dynamic oracles. Each figure is an average over the accuracy provided by 5 models trained with the same setup but using a different random seed. The seed is only used to shuffle the sentences in random order during each iteration of training.","Our results are consistent with the results reported by Goldberg and Nivre (2013) and Goldberg et al. (2014). For most of the datasets, we obtain a relevant improvement in both UAS and LAS. For Dutch, Czech and German, we achieve an error reduction of 5.2%, 11.2% and 4.5%, respectively. Exceptions to this general trend are Swedish and Bulgarian, where the accuracy differences are negligible, and the Basque, Catalan and Hungarian datasets, where the performance actually decreases.","If instead of testing on the standard test sets we use 10-fold cross-validation and average the resulting accuracies, we obtain improvements for all languages in Table 1 but Basque and Hungarian. More specifically, measured (UAS, LAS) pairs for Swedish are (86.85, 82.17) with dynamic oracle against (86.6, 81.93) with static oracle; for Bulgarian (88.42, 83.91) against (88.20, 83.55); and for Catalan (88.33, 83.64) against (88.06, 83.13). This suggests that the negligible or unfavourable results in Table 1 for these languages are due to statistical variability given the small size of the test sets.","As for Basque, we measure (75.54, 67.58) against (76.77, 68.20); similarly, for Hungarian we measure (75.66, 67.66) against (77.22, 68.42). Unfortunately, we have no explanation for these performance decreases, in terms of the typology of the non-projective patterns found in these two datasets. Note that Goldberg et al. (2014) also observed a performance decrease on the Basque dataset in the projective case, although not on Hungarian.","The parsing times measured in our experiments for the static and the dynamic oracles are the same, since the oracle algorithm is only used during the training stage. Thus the reported improvements in parsing accuracy come at no extra cost for parsing time. In the training stage, the extra processing needed to compute the loss and to explore paths that do not lead to a gold tree made training about 4 times slower, on average, for the dynamic oracle model. This confirms that our oracle algorithm is fast enough to be of practical interest, in spite of its relatively high worst-case asymptotic complexity."]},{"title":"8 Conclusions","paragraphs":["We have presented what, to our knowledge, are the first experimental results for a transition-based non-projective parser trained with a dynamic oracle. We have also shown significant accuracy improvements on many languages over a static oracle baseline.","The general picture that emerges from our approach is that dynamic programming algorithms originally conceived for the simulation of transition-based parsers can effectively be used in the development of polynomial-time algorithms for dynamic oracles."]},{"title":"Acknowledgments","paragraphs":["The first author has been partially funded by Ministerio de Economı́a y Competitividad/FEDER (Grant TIN2010-18552-C03-02) and by Xunta de Galicia (Grant CN2012/008). The third author has been partially supported by MIUR under project PRIN No. 2010LYA9RH 006. 926"]},{"title":"References","paragraphs":["Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL), pages 166– 170, New York, USA. Jinho D. Choi and Andrew McCallum. 2013. Transition-based dependency parsing with selectional branching. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1052– 1062, Sofia, Bulgaria, August. Association for Computational Linguistics.","Shay B. Cohen, Carlos Gómez-Rodrı́guez, and Giorgio Satta. 2011. Exact inference for generative probabilistic non-projective dependency parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234– 1245, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.","Shay B. Cohen, Carlos Gómez-Rodrı́guez, and Giorgio Satta. 2012. Elimination of spurious ambiguity in transition-based dependency parsing. CoRR, abs/1206.6735.","Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), volume 6, pages 449–454.","Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. In Proc. of the 24th","COLING, Mumbai, India.","Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. Transactions of the association for Computational Linguistics, 1.","Yoav Goldberg, Francesco Sartorio, and Giorgio Satta. 2014. A tabular method for dynamic oracles in transition-based parsing. Transactions of the Association for Computational Linguistics, 2(April):119– 130.","John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2006. Introduction to Automata Theory, Languages, and Computation (3rd Edition). Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.","Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July.","Marco Kuhlmann and Joakim Nivre. 2010. Transition-based techniques for non-projective dependency parsing. Northern European Journal of Language Technology, 2(1):1–19.","Marco Kuhlmann, Carlos Gómez-Rodrı́guez, and Giorgio Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 673–682, Portland, Oregon, USA, June. Association for Computational Linguistics.","Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.","Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT), pages 149–160, Nancy, France.","Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Workshop on Incremental Parsing: Bringing Engineering and Cognition To-gether, pages 50–57, Barcelona, Spain.","Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 195–206.","Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA, June. Association for Computational Linguistics. 927"]}]}
