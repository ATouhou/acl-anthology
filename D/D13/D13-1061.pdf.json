{"sections":[{"title":"","paragraphs":["Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647–657, Seattle, Washington, USA, 18-21 October 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Deep Learning for Chinese Word Segmentation and POS TaggingXiaoqing ZhengFudan University220 Handan RoadShanghai, 200433, Chinazhengxq@fudan.edu.cn Hanyang ChenFudan University220 Handan RoadShanghai, 200433, Chinachenhy12345@gmail.com Tianyu XuFudan University220 Handan RoadShanghai, 200433, Chinaxty213@gmail.comAbstract","paragraphs":["This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-the-art performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented."]},{"title":"1 Introduction","paragraphs":["Word segmentation has been a long-standing challenge for the Chinese NLP community. It has received steady attention over the past two decades. Previous studies show that joint solutions usually lead to the improvement in accuracy over pipelined systems by exploiting POS information to help word segmentation and avoiding error propagation. However, traditional joint approaches usually involve a great number of features, which arises four limitations. First, the size of the result models is too large for practical use due to the storage and computing constraints of certain real-world applications. Second, the number of parameters is so large that the trained model is apt to overfit on training corpus. Third, a longer training time is required. Last but not the least, the decoding by dynamic programming technique might be intractable since a large search space is faced by the decoder.","The choice of features, therefore, is a critical success factor for these systems. Most of the state-of-the-art systems address their tasks by applying linear statistical models to the features carefully optimized for the tasks. This approach is effective because researchers can incorporate a large body of linguistic knowledge into the models. However, the approach does not scale well when it is used to perform more complex joint tasks, for example, the task of joint word segmentation, POS tagging, parsing, and semantic role labeling. A challenge for such a joint model is the large combined search space, which makes engineering effective task-specific features and structured learning of parameters very hard. In-stead, we use multilayer neural networks to discover the useful features from the input sentences.","There are two main contributions in this paper. (1) We describe a perceptron-style algorithm for training the neural networks, which not only speeds up the training of the networks with negligible loss in performance, but also can be implemented more easily; (2) We show that the tasks of Chinese word segmentation and POS tagging can be effectively performed by the deep learning. Our networks achieved close to state-of-the-art performance by transferring the unsupervised internal representations of Chinese characters into the supervised models.","Section 2 presents the general architecture of neural networks, and our perceptron-style training algorithm for tagging. Section 3 describes how to 647 leverage large unlabeled data to obtain more useful character embeddings, and reports the experimental results of our systems. Section 4 presents a brief overview of related work. The conclusions are given in section 5."]},{"title":"2 The Neural Network Architecture","paragraphs":["Chinese word segmentation and part-of-speech tagging tasks can be formulated as assigning labels to characters of an input sentence. The performance of the traditional tagging approaches is heavily dependent on the choice of features, for example, conditional random fields (CRFs), often with a set of feature templates. For that reason, much of the effort in designing such systems goes into the feature engineering, which is important but labor-intensive, mainly based on human ingenuity and linguistic in-tuition.","In order to make learning algorithms less dependent on the feature engineering, we chose to use a variant of the neural network architecture first proposed by (Bengio et al., 2003) for probabilistic language model, and reintroduced later by (Collobert et al., 2011) for multiple NLP tasks. The network takes the input sentence and discovers multiple levels of feature extraction from the inputs, with higher levels representing more abstract aspects of the inputs. The architecture is shown in Figure 1. The first layer extracts features for each Chinese character. The next layer extracts features from a window of characters. The following layers are classical neural network layers. The output of the network is a graph over which tag inference is achieved with a Viterbi algorithm. 2.1 Mapping Characters into Feature Vectors The characters are fed into the network as indices that are used by a lookup operation to transform characters into their feature vectors. We consider a fixed-sized character dictionary D1",". The vector representations are stored in a character embedding matrix M ∈ Rd×|D|",", where d is the dimensionality of the vector space (a hyper-parameter to be chosen) and |D| is the size of the dictionary. 1","Unless otherwise specified, the character dictionary is extracted from the training set. Unknown characters are mapped to a special symbol that is not used elsewhere. . . ."]},{"title":"蹲 墙在 角1 \"A dog sits in the corner\"2345d − 1d ......6 狗 ... ... ... ...TextInput WindowFeaturesLookup Table Concatenate ......Number of Hidden UnitsLinear b2× +SigmoidW 3 b3× +g( ) Number of tagsW 2LinearTag InferenceBESI f(t|1) f(t|2) f(t|n)f(t|n−1)f(t|i) AijNumber of Hidden Units","paragraphs":["Figure 1: The neural network architecture.","Formally, assume we are given a Chinese sentence c[1:n] that is a sequence of n characters ci, 1 ≤ i ≤ n. For each character ci ∈ D that has an associated index ki into the column of the embedding matrix, an d-dimensional feature vector representation is retrieved by the lookup table layer ZD(·) ∈ Rd",": ZD(ci) = Meki (1) where we use a binary vector eki ∈ R|D|×1","which is zero in all positions except at the ki-th index. The lookup operation can be seen as a simple projection layer. The feature vector of each character, starting from a random initialization, can be automatically trained by back propagation to be relevant to the task of interest.","In practice, it is common that one might want to provide other additional features that is thought to be helpful for the task. For example, for the name entity recognition task, one could provide a feature which says if a character is in a list of the common Chinese surnames or not. Another common practice is to introduce some statistics-based measures, such 648 as boundary entropy (Jin and Tanaka-Ishii, 2006) and accessor variety (Feng et al., 2004), which are commonly used in unsupervised CWS models. We associate a lookup table to each additional feature, and the character feature vector becomes the concatenation of the outputs of all these lookup tables. 2.2 Tag scoring A neural network can be considered as a function fθ(·) with parameters θ. Any feed-forward neural network with L layers can be seen as a composition of functions f l","θ(·) defined for each layer l:","fθ(·) = f L","θ (f L−1","θ (. . . f 1","θ (·) . . .)) (2)","For each character in a sentence, a score is produced for every tag by applying several layers of the neural network over the feature vectors produced by the lookup table layer. We use a window approach to handle the sequences of variable sentence length. The window approach assumes that the tag of a character depends mainly on its neighboring characters. More precisely, given an input sentence c[1:n], we consider all successive windows of size w (a hyper-parameter), siding over the sentence, from character c1 to cn. At position ci, the character feature window produced by the first lookup table layer can be written as: f 1 θ (ci) = ","","","","","","","ZD(ci−w/2) ... ZD(ci) ...","ZD(ci+w/2)","","","","","",""," (3) The characters with indices exceeding the sentence boundaries are mapped to one of two special symbols, namely “start” and “stop” symbols.","The fixed-sized vector f 1","θ is fed to two standard Linear Layers that successively perform affine transformations over f 1","θ , interleaved with some nonlinearity function g(·), to extract highly non-linear features. Given a set of tags T for the task of interest, the network outputs a vector of size |T | for each character at position i, interpreted as a score for each tag in T and each character ci in the sentence:","fθ(ci) = f 3","θ (g(f 2","θ (f 1","θ (ci))))","= W 3","g(W 2 f 1 θ (ci) + b2",") + b3 (4)","where the matrices W 2","∈ RH×(wd)",", b2","∈ RH",",","W 3","∈ R|T |×H","and b3","∈ R|T |","are the parameters to","be trained. The hyper-parameter H is usually called","the number of hidden units. As non-linear function,","we chose a sigmoidal function2",":","g(x) = 1/(1 + e−x ) (5) 2.3 Tag Inference There are strong dependencies between character tags in a sentence for the tasks like word segmentation and POS tagging. The tags are organized in chunks, and it is impossible for some tags to follow other tags. We introduce a transition score Aij for jumping from i ∈ T to j ∈ T tags in successive characters, and an initial scores A0i for starting from the i-th tag for taking into account the sentence structure. We want the valid paths of tags to be encouraged, while discouraging all other paths.","Given an input sentence c[1:n], the network outputs the matrix of scores fθ(c[1:n]). We use a nota-tion fθ(t|i) to indicate the score output by the network with parameters θ, for the sentence c[1:n] and for the t-th tag, at the i-th character. The score of a sentence c[1:n] along a path of tags t[1:n] is then given by the sum of transition and network scores: s(c[1:n], t[1:n], θ) = n ∑ i=1 (Ati−1ti + fθ(ti|i)) (6) Given a sentence c[1:n], we can find the best tag path t∗ [1:n] by maximizing the sentence score:","t∗","[1:n] = arg max ∀t′ [1:n] s(c[1:n], t′","[1:n], θ) (7) The Viterbi algorithm can be used for this inference. Now we are prepared to show how to train the parameters of the network in an end-to-end fashion. 2.4 Training","The training problem is to determine all the para-","meters of the network θ = (M, W 2 , b2",", W 3",", b3",", A)","from training data. The network generally is trained 2","In our experiments, the sigmoidal function performs slightly better than the “hard” version of the hyperbolic tangent used by (Collobert, 2011). 649 by maximizing a likelihood over all the sentences in the training set R with respect to θ: θ ↦→ ∑ ∀(c,t)∈R log p(t|c, θ) (8) where c represents a sentence and its associated features, and t denotes the corresponding tag sequence. We drop the subscript [1 : n] from now for nota-tion simplification. The probability p(·) is calculated from the outputs of the neural network. We will present in the following section how to interpret neural network outputs as probabilities.","Maximizing the log-likelihood (8) with the gradient ascent algorithm3","is achieved by iteratively selecting a example (c, t) and applying the following gradient update rule:","θ ← θ + λ ∂ log p(t|c, θ) ∂θ (9) where λ is the learning rate (a hyper-parameter). The gradient in (9) can be computed by a classical back propagation: the differentiation chain rule is applied through the network, until the character embedding layer. 2.4.1 Sentence-Level Log-Likelihood","The score of a sentence (6) is interpreted as a conditional tag path probability by taking it to the exponential (making the score positive) and normalizing it over all possible tag paths (summing to 1 over all paths). Taking the log, the conditional probability of the true path t is given by4",":","log p(t|c, θ) = s(c, t, θ) − log ∑","∀t′ exp{s(c, t′",", θ)}","(10)","3","We did not use the stochastic gradient ascent algorithm (Bottou, 1991) to train the network as (Collobert et al., 2011). The gradient ascent algorithm was used instead for fairly comparing our algorithm with the sentence-level maximum-likelihood method (see Section 2.4.1). The gradient ascent algorithm requires a loop over all the examples to compute the gradient of the cost function, which will not cause a problem since all the training sets used in this article are finite.","4","The cost functions are differentiable everywhere thanks to the differentiability of sigmoidal function chosen as nonlinearity instead of a “hard” version of the hyperbolic tangent. For details about gradient computations, see Appendix A of (Collobert et al., 2011).","The number of terms in (10) grows exponentially with the length of the input sentence. Although one can compute it in linear time with the Viterbi algorithm, it is quite computationally expensive to compute the conditional probability of the true path, and its derivatives with respect to fθ(t|i) and Aij. The gradients with respect to the trainable parameters other than fθ(t|i) and Aij can all be computed using the derivatives with respect to fθ(t|i) by applying the differentiation chain rule. We will see in the next section our training algorithm that has the advantage of being much cheaper to compute the gradients. 2.5 A New Training Method The log-likelihood (10) can be seen as the difference between the forward score constrained over the valid path and the sum of the scores of all possible paths. While this training criterion is used, the neural networks are trained by maximizing the likelihood of training data. In fact, a CRF maximizes the same log-likelihood (Lafferty et al., 2001) by using a linear model in stead of a nonlinear neural network.","As an alternative to maximum-likelihood method, we propose the following training algorithm inspired by the work of (Collins, 2002). Given a training example (c, t), the network outputs the matrix of scores fθ(c) under the current parameter settings. The highest scoring sequence of tags for the input sentence c then can be found using the Viterbi algorithm: this tagged sequence is denoted by t′",". For every character ci where ti ̸= t′","i, we simply set","∂Lθ(t, t′","|c)","∂fθ(ti|i) ++, ∂Lθ(t, t′","|c)","∂fθ(t′","i|i) −− (11)","and for every transition where ti−1 ̸= t′","i−1 or ti ̸= t′","i,","we set","∂Lθ(t, t′","|c)","∂Ati−1ti ++, ∂Lθ(t, t′","|c) ∂At′","i−1t′","i −− (12) where “++” (which increases a value by one) and “−−” (which decreases a value by one) are two unary operators, and Lθ(t, t′","|c) is a new function which we now want to maximize over all the training pairs (c, t). The function Lθ(t, t′","|c) can be viewed as the difference between the score of the correct path and that of the incorrect one (which is the highest scoring sequence produced by the network under the current parameters θ). 650","As an example, say the correct tag sequence of the sentence ‘A dog sits in the corner’ is  /S /S /S /B /E and under the current parameter settings the highest scoring tag sequence is  /S /B /E /B /E Then the derivatives with respect to fθ(S| ), and fθ(S| ) will be set to 1, that with respect to ASB, ABE, AEB, fθ(B| ), and fθ(E| ) to −1, and that with respect to ASS to 2 respectively5",". Intuitively these assignments have the effect of updating the parameter values in a way that increases the score of the correct tag sequence and decreases the score of the incorrect one output by the network with the current parameter settings. If the tag sequence produced by the network is correct, no changes are made to the values of parameters.","Inputs:","R: a training set.","N: a specified maximum number of iterations.","E: a desired tagging precision.","Initialization: set the initial parameters of the network with","small random values.","Output: the trained parameters θ̃","Algorithm:","do for each example (c, t) ∈ R","get the matrix fθ(c) by the neural network under the","current parameters θ","find the highest scoring sequence of tags t′","for c with","fθ(c) and Aij by using the Viterbi algorithm","if (t ̸= t′",")","compute the gradients with respect to fθ(c) and","Aij as (11) and (12)","compute the gradients with respect to the weights","from output layer to character embedding layer","update the parameters of the network by (9)","until the desired precision E achieved or maximum num-","ber of iterations N reached","return θ̃ Figure 2: The training algorithm for tagging.","We propose the training algorithm in Figure 2. Note that the perceptron algorithm of (Collins, 2002) was designed for discriminatively training an 5 The derivatives with respect to ASB will be set to 0, be-","cause it is increased first and decreased afterwards. HMM-style tagger, while our algorithm is used to calculate the “direction” in which the parameters are updated (i.e. the gradient of the function we want to maximize). Due to space limitations, we do not give convergence theorems justifying the training algorithm in this paper. Intuitively it can be achieved by combining the theorems of convergence for the perceptron applied to tagging problem from (Collins, 2002) with the convergence results of backpropaga-tion algorithm from (Rumelhart et al., 1986)."]},{"title":"3 Experiments","paragraphs":["We conducted three sets of experiments. The goal of the first one is to test several variants for each training algorithm on the development set, to gain some understanding of how the choice of hyper-parameters impacts upon the performance. We applied the network both with the sentence-level log-likelihood (SLL) and our perceptron-style training algorithm (PSA) to the two Chinese NLP problems: word segmentation, and joint CWS and POS tagging. We ran this set of experiments on the part of Chinese Treebank 4 (CTB-4) 6",". Ninety percent of the sentences (1529) were randomly chosen for training and the rest (168) were used as development set.","The second set of experiments was run on the Chinese Treebank (CTB) data sets from Bakeoff-3 (Levow, 2006), which contains a training and a test corpus for supervised word segmentation and POS tagging tasks. The results were obtained without using any extra knowledge (i.e. the closed test), and are comparable with other models in the literature.","In the third experiment, we study to see how well large unlabeled texts can be used to enhance the supervised learning. Following (Collobert et al., 2011), we first use large unlabeled data set to obtain character embeddings carrying more syntactic and semantic information, and then use these improved embeddings to initialize the character lookup tables of the networks instead of previous random values. Our corpus is the Sina news7","that contains about 325MB data. 6 The data set was sections 1–43, 144–169, and 900–931 of","the treebank, containing 78,023 characters, 45,135 words and","1,697 sentences. These files are double-annotated and can be","regarded as golden standard files. 7 Available at http://www.sina.com.cn/ 651","We implemented two versions of the network: one for the sentence-level log-likelihood and one for our perceptron-style training algorithm. Both are written in Java language. All experiments were run on a computer equipped with an Intel Core i3 processor working at 2.13GHz, with 2GB RAM, running Linux and Java Development Kit 1.6. The standard F-score was used to evaluate the performance of both word segmentation and joint word segmentation and POS tagging tasks. F-score is the harmonic mean of precision p and recall r, which is defined as 2pr/(p + r). 90 80 70","60 1 3 5 7 9 Window Size F - s c o r e 100 Fword: SEG (PSA)Fword: SEG (SLL) Foov: SEG (PSA)Foov: SEG (SSL) Fword: JWP (PSA)Fword: JWP (SLL) Foov: JWP (PSA)Foov: JWP (SSL) Fpos: JWP (PSA)Fpos: JWP (SSL) Figure 3: Average F-score versus window size. 3.1 Tagging Schemes The network will output the scores for all the possible tags for the task of interest. For word segmentation, each character will be assigned one of four possible boundary tags: “B” for a character located at the beginning of a word, “I” for that inside of a word, “E” for that at the end of a word, and “S” for a character that is a word by itself.","Following Ng and Lou (2004) we perform joint word segmentation and POS tagging task in a labeling fashion by expanding boundary labels to include POS tags. For instance, we describe verb phases using four different tags. Tag “S VP” is used to mark a verb phase containing a single character. Other tags “B VP”, “I VP”, and “E VP” are used to 90 80 70 60 100 300 500 700 900 100 F - s c o r e Number of Hidden Units Fword: SEG (PSA)Fword: SEG (SLL) Foov: SEG (PSA)Foov: SEG (SSL) Fword: JWP (PSA)Fword: JWP (SLL) Foov: JWP (PSA)Foov: JWP (SSL) Fpos: JWP (PSA)Fpos: JWP (SSL) Figure 4: Average F-score versus number of hidden units. mark the first, in-between and last characters of the verb phrase. In fact, we used the “IOBES” tagging scheme, and tag “O” is not applicable to Chinese word segmentation and POS tagging tasks. 3.2 The Choice of Hyper-parameters We tuned the hyper-parameters by trying only a few different networks. We report in Figure 3 the F-scores on the development set versus window size for word segmentation (SEG) and joint word segmentation and POS tagging (JWP) tasks with the sentence-level log-likelihood (SLL) and our perceptron-style training algorithm (PSA), and report in Figure 4 the F-scores on the same data set versus number of hidden units. The average F-scores were obtained over 5 runs with different random initialization for each setting of the network. The F-scores of the word segmentation, out-of-vocabulary, and POS tagging are denoted by Fword, Foov and Fpos respectively.","Generally, the number of hidden units has a limited impact on the performance if it is large enough, which is consistent with the findings of (Collobert et al., 2011) for English. It can be seen from Figure 3 that the performance drops smoothly when the window size is larger than 3. In particularly, the F-score of out-of-vocabulary identification decreases relatively fast beyond window size 5, which shows 652 that the size of window (and the number of parameters) is too large that the trained network has over-fitted on training data. An explanation for this result is that most Chinese words are less than 3 characters, and the neighboring characters outside of the window (size 5) become “noise” when we perform word segmentation.","The hyper-parameters of the network used in all the following experiments are shown in Table 1. Although the top performance was obtained by the network with window size 3, we chose the architecture with window size 5 because a larger training corpus will be used in the following experiments, and the sparseness problem would be alleviated. Further-more, in order to obtain character embeddings by using large unlabeled data, we prefer to “observe” a character within a slightly larger window to better discover its syntactic and semantic information. Hyper-parameter Value Window size 5 Number of hidden units 300 Character feature dimension 50 Learning rate 0.02 Table 1: Hyper-parameters of the network.","We report in Table 2 the F-score of the first five iterations on the development set for word segmentation with SLL and PSA. The data in the fourth and fifth rows of the table shows the convergence of PSA. The difference of the F-scores between the networks with SLL and PSA can be negligible after the number of iteration is greater than 5. In our implementation, for each iteration the training time is reduced at least 10% by using PSA, compared with SLL. The training time can be reduced further for more complex tasks like POS tagging and semantic role labeling in which a larger tag set is used. Iteration 1 2 3 4 5 SSL Fword 49.89 69.56 88.91 90.19 91.24 Foov 15.92 27.54 54.37 55.89 59.74 Time (s) 209 398 586 737 886 PSA Fword 49.04 68.54 87.79 89.07 91.19 Foov 13.61 25.79 52.30 55.15 60.49 Time (s) 184 343 497 610 754 Table 2: Word segmentation results with SLL and PSA for the first five iterations.","Many exponential sums (∑","i exp(xi)) are required for training the networks with SLL, and in most cases the values of exponential sums will exceed the range of double-precision floating-point arithmetic defined in popular programming languages. These sums need to be estimated by an-alytic number theory. In comparison, a lot of the computation-intensive exponential sums are avoided in our training algorithm, which not only speed up the training of the networks but also make it easier to be implemented. 3.3 Closed Test on the SIGHAN Bakeoff We trained the networks with PSA on the Chinese Treebank (CTB) data set from Bakeoff-3 for both SEG and JWP tasks. The results are reported in Table 3. The hyper-parameters of our networks are reported in Table 1. Although results show that our networks with PSA are behind the state-of-the-art systems, the networks perform comparatively well, considering we did not use any extra information. Many other systems used some extra heuristics or resources to improve their performance. For example, a key parameter in the system of (Wang et al., 2006) was optimized in advance by using an external segmented corpus, and a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009).","It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing.","Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be close in the embedding space. If we knew that ‘dog’ and ‘cat’ were similar semantically, and similarly for ‘sit’ and ‘lie’, we could generalize from ‘A dog sits in the corner’ to ‘A cat sits in the corner’, and to ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character embeddings by using large unlabeled data in the next section. 653 Approach Fword Roov Fpos SEG (Zhao et al., 2006) 93.30 70.70 − (Wang et al., 2006) 93.00 68.30 − (Zhu et al., 2006) 92.70 63.40 − (Zhang et al., 2006) 92.60 61.70 − (Feng et al., 2006) 91.70 68.00 − PSA 92.59 64.24 − PSA + LM 94.57 70.12 − JWP (Ng and Lou, 2004) 95.20 − − (Zhang and Clark, 2008) 95.90 − 91.34 (Jiang et al., 2008) 97.30 − 92.50 (Kruengkrai et al., 2009) 96.11 − 90.85 PSA 93.83 68.21 90.79 PSA + LM 95.23 72.38 91.82 Table 3: Comparison of the F-scores on the Penn Chinese Treebank 3.4 Combined Approach We used the corpus of Sina news to obtain character embeddings carrying more semantic and syntactic information by training a language model that evaluates the acceptability of a piece of text. This language model is again the neural network, and we also use PSA to train the language model. Following (Collobert et al., 2011), we minimize the following criterion with respect to the parameters θ: θ ↦→ ∑","∀h∈H ∑","∀c′","∈D max{0, 1 − fθ(c|h) + fθ(c′","|h)}","(13) where the score fθ(c|h) is the output of the network with parameters θ for a character c at the center of a window h, D is the dictionary of characters, H is the set of all possible text windows (i.e. character sequences) from the training data, and c′","|h denotes the window obtained by replacing the central character of the window h by the character c′",".","We used a dictionary consisting of the characters extracted from all the data sets in Bakeoff-3, which contains about eight thousand characters. The total unsupervised training time was about two weeks. Our combined approach works as initializing the lookup tables of the supervised networks with the character embeddings obtained by unsupervised learning, and then performing supervised training on CTB-3. The lookup tables will not be modified at the supervised training stage.","We reported the results in Table 3, in which our combined approach is indicated by “PSA + LM”. It can be seen from Table 3 that this approach results in a performance boost for both SEG and JWP tasks. The POS tagging F-score of our approach was comparable to but still less than the model of (Jiang et al., 2008). They achieved the best score by first separately training multiple word-, character-, and POS n-gram based models, and then integrat-ing them by cascading method. In comparison, our networks achieve the performance by automatically discovering useful features by itself and avoiding the task-specific engineering.","Table 4 compares the decoding speeds on the test data from CTB-3 for our system and for two CRFs-based word segmentation systems. Regardless of the differences in implementation, the neural networks clearly run considerably faster than the systems based on the CRFs model. They also require much more memory than our neural networks. System Number of parameters Time (s) (Tsai et al., 2006) 3.1 × 106 1669 (Zhao et al., 2006) 3.8 × 106 2382 Neural network 4.7 × 105 138 Table 4: Comparison of computational cost."]},{"title":"4 Related Work","paragraphs":["Word segmentation has been pursued with considerable efforts in the Chinese NLP community, and statistical approaches are clearly dominant in the last decade. A popular statistical approach is the character-based tagging solution that treats word segmentation as a sequence tagging problem, assigning labels to the characters indicating whether a character locates at the beginning of, inside, or at the end of a word. The character-based tagging solution was first proposed in (Xue, 2003). This work caused quite a number of character position tagging based CWS studies because known and unknown words can be treated in the same way. Peng, Feng and Mc-Callum (2004) first introduced a linear-chain CRFs model to the character tagging based word segmentation. Zhang and Clark (2007) proposed a word-based CWS approach using a discriminative perceptron learning algorithm, which allows word-level information to be added as features.","Recent years have seen a rise of joint word segmentation and POS tagging approach that improves 654 the accuracies of both tasks and does not suffer from the error propagation. Ng and Lou (2004) perform such joint task in a labeling fashion by expanding boundary labels to include POS tags. Zhang and Clark (2008) proposed a linear model for the same joint task, which overcomed the disadvantage of (Ng and Lou, 2004), in which it was unable to incorporate “whole word + POS tag” features. Sun (2011) described a sub-word model using stacked learning technique for the joint task, which explored the complementary strength of different character- and word-based segmenters with different views.","The majority of the state-of-the-art systems address their tasks by applying linear statistical models to ad-hoc features. The researchers first chose task-specific features which are then fed to a classification algorithm. The selected features may vary greatly because they are usually chosen in a empirical process, mainly based first on linguistic intuition, and then trial and error. It seems reasonable to assume that the number and effectiveness of features constitutes a major factor in the performance of the various systems, and might even more important than the particular statistical models they used. In comparison, we try to avoid task-specific feature engineering, and use the neural network to learn several layers of feature extraction from the inputs. To the best of our knowledge, this study is among the first ones to perform Chinese word segmentation and POS tagging by deep learning.","It was reported that supervised and unsupervised approaches can be integrated to improve on the over-all performance of word segmentation by combining the strengths of both. Zhao and Kit (2011) explored the feasibility of enhancing supervised segmentation by informing the supervised learner of goodness scores obtained from large unlabeled corpus. Sun and Xu (2011) investigated how to improve on the accuracy of supervised word segmentation by leveraging the statistics-based features derived from large unlabeled in-domain corpus and the document to be segmented. The basic idea of these integration solutions is to incorporate a set of statistics-based measures into a CRFs model after these measures are derived from unlabeled data and discretized into feature values. In comparison, we use large unlabeled data to obtain the character embeddings with more syntactic and semantic information.","Several works have investigated how to use deep learning for NLP applications (Bengio et al., 2003; Collobert et al., 2011; Collobert, 2011; Socher et al., 2011). In most cases, words are fed to the neural networks as inputs, and the lookup tables map each word to a vector representation. Our network is different in that the inputs to the network are characters, more raw units than words. In many Asian languages, such as Chinese and Japanese, they are written without using whitespace to delimit words. For these languages, the character becomes a more natural form of input. Furthermore, a perceptron-style algorithm for tagging is proposed for training the networks."]},{"title":"5 Conclusion","paragraphs":["We have described a perceptron-style algorithm for training the neural networks, which is much easier to be implemented, and has speed advantage over the maximum-likelihood scheme, while the loss in performance is negligible. The neural networks trained with PSA have been applied to Chinese word segmentation and POS tagging tasks, and the networks achieved close to state-of-the-art performance by using the character representations learned from large unlabeled corpus.","Although we focus on the question of how far we can go for Chinese word segmentation and POS tagging without using the extra task-specific features in this study, there are at least three ways to further improve the performance of the networks, which are worthy to be explored in the future: (1) introduce specific linguistic features (e.g. gazetteer features) that are helpful for the tasks; (2) incorporate some common techniques, such as cascading, voting, and ensemble; and (3) use the special network architecture tailored for the tasks of interest."]},{"title":"Acknowledgments","paragraphs":["The authors would like to thank the anonymous reviewers for their valuable comments. The work was supported by a grant from the National Natural Science Foundation of China (No. 60903078), a grant from Shanghai Leading Academic Discipline Project (No. B114), a grant from Shanghai Municipal Natural Science Foundation (No. 13ZR1403800), and a grant from FDUROP. 655"]},{"title":"References","paragraphs":["Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic langugage model. Journal of Machine Learning Research, 3: 1137–1155.","Léon Bottou. 1991. Stochastic gradient learning in neural networks. In Proceedings of the Neuro-Nı̂mes.","Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’02).","Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS’11).","Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12: 2493– 2537.","Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin Zheng. 2004. Accessor variety criteria for Chinese word extraction. Computational Linguistics, 30(1): 75–93.","Yuanyong Feng, Sun Le, and Yuanhua Lv. 2006. Chinese word segmentation and name entity recognition based on conditional random fields models. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).","Wenbin Jiang, Liang Huang, Qun Liu, Yajuan Lu. 2008. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL’08).","Zhihui Jin, and Kumiko Tanaka-Ishii. 2006. Unsupervised segmentation of Chinese text by use of branch-ing entropy. In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (COLING/ACL’06).","Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL’09).","John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine learning (ICML’01). Gina A. Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).","Hwee T. Ng and Jin K. Lou. 2004. Chinese part-of-speech tagging: one-at-atime or all-at-once? word-based or character-based? In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’04).","Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04).","David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning internal representations by backpropagating errors. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, 1: 318–362. MIT Press.","Richard Socher, Cliff C-Y. Lin, Andrew Y. Ng, and Christopher D. Manning 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In Proceedings of the International Conference on Machine learning (ICML’11).","Weiwei Sun. 2011. A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11).","Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’11).","Richard T.-H. Tsai, Hsieh C. Hung, Chenglung Sung, Hongjie Dai, and Wenlian Hsu. 2006. On closed task of Chinese word segmentation: An improved CRF model coupled with character clustering and automatically generated template matching. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).","Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and Xihong Wu. 2006. Chinese word segmentation with maximum entropy and n-gram language model. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).","Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1): 29–48.","Min Zhang, GuoDong Zhou, LingPeng Yang, and DongHong Ji. 2006. Chinese word segmentation and named entity recognition based on a contextdependent mutual information independence Model. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06). 656","Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-base perceptron algorithm. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07).","Yue Zhang and Stephen Clark. 2008. Joint word segmentation and pos tagging using a single perceptron. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL’08).","Hai Zhao, Chang N. Huang, and Mu Li. 2006. An improved Chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).","Hai Zhao and Chunyu Kit. 2011. Integrating unsupervised and supervised word segmentation: The role of goodness measures. Information Sciences, 181(1): 163–183.","Muhua Zhu, Yilin Wang, Zhenxing Wang, Huizhen Wang, and Jingbo Zhu. 2006. Designing special post-processing rules for SVM-based Chinese word segmentation. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06). 657"]}]}
