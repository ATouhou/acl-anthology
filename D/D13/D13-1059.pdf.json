{"sections":[{"title":"","paragraphs":["Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 624–635, Seattle, Washington, USA, 18-21 October 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Unsupervised Spectral Learning of WCFG as Low-rank Matrix CompletionRaphaël Bailly","paragraphs":["†"]},{"title":"Xavier Carreras","paragraphs":["†"]},{"title":"Franco M. Luque","paragraphs":["‡"]},{"title":"Ariadna Quattoni","paragraphs":["† †"]},{"title":"Universitat Politècnica de CatalunyaBarcelona, 08034","paragraphs":["rbailly,carreras,aquattoni@lsi.upc.edu‡"]},{"title":"Universidad Nacional de Córdoba and CONICETX500HUA Córdoba, Argentina","paragraphs":["francolq@famaf.unc.edu.ar"]},{"title":"Abstract","paragraphs":["We derive a spectral method for unsupervised learning of Weighted Context Free Grammars. We frame WCFG induction as finding a Hankel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions. The proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the Hankel matrix."]},{"title":"1 Introduction","paragraphs":["Weighted Context Free Grammars (WCFG) define an important class of languages. Their expressivity makes them good candidates for modeling a wide range of natural language phenomena. This expressivity comes at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved.","Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques.","Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization.","In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence.","In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking tool to reason about distributions over Σ∗",", the question of whether they can be used for unsupervised learning of WCFG seems natural. Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach.","In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging. Recently, Hsu et al. (2012) showed that the problem of recovering the joint distribution over PCFG derivations and their yields is not identifiable. 624 Although, for some simple grammar subclasses (e.g. independent left and right children), identification in the weaker sense (over the yields of the grammar) implies strong identification (e.g. over joint distribution of yields and derivations). In their paper, they propose a spectral algorithm based on a generaliza-tion of the method of moments for these restricted subclasses.","Thus one open direction for spectral research consists on defining subclasses of context free languages that can be learned (in the strong sense) from observations of yields. Yet, an alternative research direction is to consider learnability in the weaker sense. In this paper we take the second road, and focus on the problem of approximating the distribution over yields generated by a WCFG.","Our main contribution is to present a spectral algorithm for unsupervised learning of WCFG. Following ideas from Balle et al. (2012), the algorithm is framed as a convex optimization where we search for a low-rank matrix satisfying two types of constraints: (1) Constraints derived from observable statistics over yields; and (2) Constraints derived from certain recurrence relations satisfied by a WCFG. Our derivations of the learning algorithm illustrate the main ingredients behind the spectral approach to learning functions over Σ∗","which are: (1) to exploit the recurrence relations satisfied by the target family of functions and (2) provide algebraic formulations of these relations.","We alert the reader that although we are able to frame the problem as a convex optimization, the number of variables involved is quite large and prohibits a practical implementation of the method on a realistic scenario. The experiments we present should be regarded as examples designed to illustrate the behavior of the method. More research is needed to make the optimization more efficient, and we are optimistic that such improvements can be achieved by exploiting problem-specific properties of the optimization. Regardless of this, ours is a novel way of framing the grammatical inference problem.","The rest of the paper is organized as follows. Section 2 gives preliminaries on WCFG and the type of functions we will learn. Section 3 establishes that spectral methods can learn a WCFG from a Hankel matrix containing statistics about context-free cuts. Section 4 presents the unsupervised algorithm, where we formulate grammar induction as a low-rank optimization. Section 5 presents experiments, and finally we conclude the paper. Notation Let Σ be an alphabet. We use σ to denote an arbitrary symbol in Σ. The set of all finite strings over Σ is denoted by Σ⋆",", where we write λ for the empty string. We also use the set Σ+","= Σ⋆","\\ {λ}.","We use bold letters to represent column vectors v and matrices M . We use In to denote the n-dimensional identity matrix. We use M +","to denote the Moore-Penrose pseudoinverse of some matrix M . M ⊗ M ′","is the Kronecker product between matrices M ∈ Rm×n","and M ′","∈ Rp×q","resulting in a matrix in Rmp×nq",". The rest of notation will be given as needed."]},{"title":"2 Weighted Context Free Grammars","paragraphs":["In this section we define Weighted Context Free Grammars (WCFG). We start with a classic defini-tion and then describe an algebraic form of WCFG that will be used throughout the paper. We also describe the fundamental recursions in WCFG. 2.1 WCFG in Classic Form A WCFG over Σ is a tuple Ḡ = ⟨V, R, T, w⋆, wT , wR⟩ where","• V is the set of non-terminal symbols. We assume that V = {1, . . . , n} for some natural number n, and that V ∩ Σ = ∅.","• R is a set of binary rules of the form i → j k where i, j, k ∈ V .","• T is a set of unary rules of the form i → σ where i ∈ V and σ ∈ Σ.","• w⋆ : V → R, with w⋆(i) being the weight of starting a derivation with non-terminal i.","• wT : V × Σ → R, with wT (i → σ) being the weight of rule rewriting i into σ.","• wR : V × V × V → R, with wR(i → j k) being the weight of rewriting i into j k. A WCFG Ḡ computes a function g Ḡ : Σ+","→ R","defined as","g Ḡ(x) = ∑ i∈V w⋆(i) β̄ Ḡ(i ⋆","=⇒ x) , (1) 625 where we define theinside function β̄ Ḡ : V × Σ+","→ R recursively:","β̄ Ḡ(i ⋆ =⇒ σ) = wT (i → σ) (2)","β̄ Ḡ(i ⋆","=⇒ x) = ∑","j,k∈V x1,x2∈Σ+ s.t. x=x1x2 wR(i → j k) (3)","β̄ Ḡ(j ⋆ =⇒ x1) β̄ Ḡ(k ⋆","=⇒ x2) , where in the second case we assume |x| > 1. The inside function β̄ Ḡ(i ⋆","=⇒ x) exploits the fundamental inside recursion in WCFG (Baker, 1979; Lari and Young, 1990). We will find useful to define theout-side function ᾱ Ḡ : Σ⋆","× V × Σ⋆","→ R defined recursively as: ᾱ Ḡ(λ; i; λ) = w⋆(i) (4)","ᾱ Ḡ(x; i; y) = ∑","j,k∈V x1∈Σ⋆",",x2∈Σ+ s.t. x=x1x2 wR(j → k i)· (5)","ᾱ Ḡ(x1; j; y) · β̄ Ḡ(k ⋆ =⇒ x2)","+ ∑ j,k∈V","y1∈Σ+",",y2∈Σ⋆","s.t. y=y1y2 wR(j → i k)·","ᾱ Ḡ(x; j; y2) · β̄ Ḡ(k ⋆","=⇒ y1) ,","where in the second case we assume that either","x ̸= λ or y ̸= λ.","For x, z ∈ Σ⋆","and y ∈ Σ+","we have that","∑","i∈V ᾱ Ḡ(x; i; z) · β̄ Ḡ(i ⋆ =⇒ y) (6) is the weight that the grammar Ḡ assigns to a string xyz that has a cut or bracketing around y. Technically, it corresponds to the sum of the weights of all derivations that have a constituent spanning y. In particular we have that","g Ḡ(x) = ∑ i ᾱ Ḡ(λ; i; λ) · β̄ Ḡ(i ⋆","=⇒ x) .","If x is a string of length m, and x[t:t′","] is the substring","of x from positions t to t′",", it also happens that","g Ḡ(x) = ∑ i ᾱ Ḡ(x[1:t−1]; i; x[t+1:m])· β̄ Ḡ(i ⋆","=⇒ x[t])) for any t between 1 and m.","In this paper we will make frequent use of inside and outside quantities. Notationally, for outsides the semi-colon between two strings, i.e. x; z, will simbolize a cut where we can insert an inside string y.","Finally, we note that Probabilistic Context Free Grammars (PCFG) are a special case of WCFG where: w⋆(i) is the probability to start a derivation with non-terminal i; wR(i → j k) is the conditional probability of rewriting nonterminal i into j and k; wT (i → σ) is the probability of rewriting i into symbol σ; ∑","i w⋆(i) = 1; and for each i ∈ V ,∑","j,k wR(i → j k) + ∑","σ wT (i → σ) = 1. Under these conditions the function g Ḡ is a probability distibution over Σ+",". 2.2 WCFG in Algebraic Form We now define a WCFG in algebraic form. A Weighted Context Free Grammar (WCFG) over Σ with n states is a tuple G = ⟨α⋆, {βσ}, A⟩ with:","• An initial vector α⋆ ∈ Rn",".","• Terminal vectors βσ ∈ Rn","for σ ∈ Σ.","• A bilinear operator A ∈ Rn×n2",". A WCFG G computes a function gG : Σ⋆","→ R","defined as","gG(x) = α⊤ ⋆ βG(x) (7)","where the inside function βG : Σ+ → Rn","is βG(σ) = βσ (8) βG(x) = ∑ x1,x2∈Σ+ x=x1x2 A(βG(x1) ⊗ βG(x2)) (9) We will define the outside function αG : Σ⋆","×","Σ⋆ → Rn","as: αG(λ; λ) = α⋆ (10)","αG(x; z)⊤ = ∑","x1∈Σ⋆ ,x2∈Σ+ x=x1x2αG(x1; z)⊤","A(βG(x2) ⊗ In) + ∑","z1∈Σ+",",z2∈Σ⋆ z=z1z2αG(x; z2)⊤","A(In ⊗ βG(z1)) (11)","For x, z ∈ Σ⋆ and y ∈ Σ+","we have that","αG(x; z)⊤ βG(y) (12) is the weight that the grammar assigns to the string xyz with a cut around y. In particular, gG(x) = αG(λ; λ)⊤","βG(x). 626","Let us make clear that a WCFG is the same device in classic or algebraic forms. If Ḡ = ⟨V, R, T, w⋆, wT , wR⟩ and G = ⟨α⋆, {βσ}, A⟩, the mapping is: w⋆(i) = α⋆(i) (13) wT (i → σ) = βσ[i] (14) wR(i → j k) = A[i, j, k] (15)","β̄ Ḡ(i ⋆ =⇒ x) = βG(x)[i] (16) ᾱ Ḡ(x; i; z) = αG(x; z)[i] (17) See Section A.1 for a proof of Eq. 16 and 17."]},{"title":"3 WCFG and Hankel Matrices","paragraphs":["In this section we describe Hankel matrices for WCFG. These matrices explicitly capture inside-outside recursions employed by WCFG functions, and are key to a derivation of a spectral learning algorithm that learns a grammar G using statistics of a training sample.","Let us define some sets. We say that I1","= Σ+ is the set of inside strings. The set of composed inside strings I2","is the set of elements (x, x′","), where x, x′","∈ Σ+",". Intuitively (x, x′",") represents two adjacent spans with an operation, i.e., it keeps the trace of the operation that composes x with x′","and yields xx′",". We will use the set I = I1","∪ I2",".","The set of outside contexts O is the set containing elements ⟨x; z⟩, where x, z ∈ Σ⋆",". Intuitively, ⟨x; z⟩ represents a context where we can insert an inside element y in between x and z, yielding xyz.","Consider a function f : O × I → R. The Hankel matrix of f is the bi-infinite matrix Hf ∈ RO×I such that Hf (o, i) = f (o, i).","In practice we will work with finite sub-blocks of Hf . To this end we will employ the notion of basis B = (P, S), where {⟨λ, λ⟩} ⊆ P ⊆ O is a set of outside contexts and Σ ⊆ S ⊆ I1","is a set of inside strings. We will use p = |P| and s = |S|. Furthermore, we define the inside completion of S as the set S†","= {(x, x′",") | x, x′","∈ S}. Note that S†","⊆ I2",". We say that B†","= (P, S†",") is the inside completion of B.","The sub-block of Hf defined by B is the p × s matrix HB ∈ RP×S","with HB(o, i) = Hf (o, i) = f (o, i). In addition to HB, we are interested in these additional finite vectors and matrices:","• h⋆ ∈ RS","is the s-dimensional vector with coordinates h⋆(x) = f (⟨λ, λ⟩, x).","• hσ ∈ RP","is the p-dimensional vector with coordinates hσ(o) = f (o, σ). • HA ∈","RP×S† with HA(o, (x1, x2)) = f (o, (x1, x2)). 3.1 Hankel Factorizations If f is computed by a WCFG G, then Hf has rank n factorization. To see this, consider the following matrices. First a matrix S ∈ Rn×I1","of inside vectors for all strings, with column x taking value Sx = βG(x). Then a matrix P ∈ RO×n","of outside vectors for all contexts, with row ⟨x; z⟩ taking value P ⟨x;z⟩ = αG(x; z). It is easy to see that Hf = P S, since Hf (⟨x; z⟩, y) = P ⟨x;z⟩Sy = αG(x; z)⊤","βG(y). Therefore Hf has rank n.","The same happens for sub-blocks. If HB is the sub-block associated with basis B = (P, S), then the sub-blocks P B ∈ RP×n","and SB ∈ Rn×S","of P and S also accomplish that HB = P BSB. It also happens that h⊤ ⋆ = α⊤","⋆ SB (18) hσ = P Bβσ (19) HA = P BA(SB ⊗ SB) . (20)","We say that a basis B is complete for f if rank(HB) = rank(Hf ). The following is a key result for spectral methods. Lemma 1. Let B = (P, S) be a complete basis of dimension n for a function f and let HB ∈ RP×S be the Hankel sub-block of f for B. Let h⋆, hσ and HA be the additional matrices for B. If HB = P S is a rank n factorization, then the WCFG G = ⟨α⋆, {βσ}, A⟩ with","α⊤ ⋆ = h⊤","⋆ S+ (21)","βσ = P + hσ (22)","A = P + HA(S ⊗ S)+ (23) computes f . See proof in Section A.2. 627 3.2 Supervised Spectral Learning of WCFG The spectral learning method directly exploits Lemma 1. In a nutshell, the spectral method is:","1. Choose a complete basis B = (P, S) and a dimension n.","2. Use training data to compute estimates of the necessary Hankel matrices: HB, h⋆, hσ, HA.","3. Compute the SVD of HB, HB = U ΛV ⊤ .","4. Create a truncated rank n factorization of HB as P nSn, having P n = U nΛn and Sn = V ⊤","n , where we only consider the top n singular values/vectors of Λ, U , V . 5. Use Lemma 1 to compute G, using P n and Sn.","Because of Lemma 1, if B is complete and we have access to the true HB, h⋆, hσ, HA of a WCFG target function g∗",", then the algorithm will compute a G that exactly computes g∗",". In practice, we only have access to empirical estimates of the Hankel matrices. In this case, there exist PAC-style sample complexity bounds that state that gG will be a close approximation to g∗","(Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010).","The parameters of the algorithm are the basis and the dimension of the grammar n. One typically employs some validation strategy using held-out data. Empirically, the performance of these methods has been shown to be good, and similar to that of EM (Luque et al., 2012; Cohen et al., 2013). It is also important to mention that in the case that the target g∗","is a probability distribution, the function gG will be close to g∗",", but it will only define a distributionin the limit: in practice it will not sum to one, and for some inputs it might return negative values. This is a practical difficulty of spectral methods, for example to apply evaluation metrics like perplexity which are only defined for distributions."]},{"title":"4 Unsupervised Learning of WCFG","paragraphs":["In the previous section we have exposed that if we have access to estimates of a Hankel matrix of a WCFG G, we can recover G. However, the statistics in the Hankel require access to strings that have information about context-free cuts. We will assume that we only have access to statistics about plain strings of a distribution, i.e. p(x), which we call observations. In this scenario, one natural idea is to search for a Hankel matrix that agrees with the observations. The method we present in this section frames this problem as a low-rank matrix optimization problem. We first characterize the space of solutions to our problem, i.e. Hankel matrices associated with WCFG that agree with observable statistics. Then we present the method. 4.1 Characterization of a WCFG Hankel","In this section we describe valid WCFG Hankel ma-","trices using linear constraints. We first describe an inside-outside basis that is","an extension of the one in the previous section. In-","side elements are the same, namely I = I1","∪ I2",",","where I1","are strings (x) and I2","are composed","strings (x, x′","). The set of outside contexts O1","is","the set containing elements ⟨x; z⟩, defined as be-","fore. The set of composed outside contexts has el-","ements ⟨x, x′","; z⟩, and ⟨x; z′",", z⟩, where x, z ∈ Σ⋆","and x′",", z′","∈ Σ+",". These outside contexts keep an","operation open in one of the sides. For example, if","we consider ⟨x; z′",", z⟩ and insert a string y, we obtain","x(y, z′",")z, where we use (y, z′",") to explicitly denote a","composed inside string. We will use O = O1","∪ O2",". In this section, we will assume that I and O are","finite and closed. By closed, we mean that: • (x) ∈ I ⇒ (x1, x2) ∈ I for x = x1x2 • (x1, x2) ∈ I ⇒ x1 ∈ I, x2 ∈ I • ⟨x; z⟩ ∈ O ⇒ ⟨x1, x2; z⟩ ∈ O for x = x1x2 • ⟨x; z⟩ ∈ O ⇒ ⟨x; z1, z2⟩ ∈ O for z = z1z2 • ⟨x1, x2; z⟩ ∈ O ⇒ (x2) ∈ I • ⟨x; z1, z2⟩ ∈ O ⇒ (z1) ∈ I","We will consider a Hankel matrix H ∈ RO×I",". Some entries of this matrix will correspond to observable quantities. Specifically, for any stringx ∈ I1","for which we know p(x) we can define the following observable constraint: p(x) = H(⟨λ; λ⟩, (x)) (24)","The rest of entries of H correspond to a string with an inside-outside cut, and these are not observable. Our method will infer the values of these entries. The following constraints will ensure that the matrix H is a well defined Hankel matrix for WCFG: 628 • Hankel constraints: ∀ ⟨x; z⟩ ∈ O, (y1, y2) ∈ I H(⟨x; z⟩, (y1, y2)) = H(⟨x, y1; z⟩, (y2)) = H(⟨x; y2, z⟩, (y1)) (25) • Inside constraints: ∀ o ∈ O, (x) ∈ I","H(o, (x)) = ∑ x=x1x2 H(o, (x1, x2)) (26) • Outside constraints: ∀ ⟨x; z⟩ ∈ O, i ∈ I H(⟨x; z⟩, i) = ∑ x=x1x2 H(⟨x1, x2; z⟩, i) + ∑ z=z1z2 H(⟨x; z1, z2⟩, i) (27)","Constraint (25) states that composition operations that result in the same structure should have the same value. Constraints (26) and (27) ensure that the values in the Hankel follow the inside-outside recursions that define the computations of a WCFG function. The following lemma formalizes this concept. Let Hε be the sub-block of H restricted to O1","× I1",", i.e. without compositions. Lemma 2. If H satisfies constraints (25),(26) and (27), and if rank(H) = rank(Hε) then there exists a WCFG that generates Hε. See proof in Section A.3. 4.2 Convex Optimization We now present the core optimization program behind our method. Let vec(H) be a vector in R|O|·|I| corresponding to all coefficients of H in column vector form. Let O be a matrix such that O · vec(H) = z represents the observation constraints. For example, if i-th row of O corresponds to the Hankel coefficientH(⟨λ; λ⟩, (x)) then z(i) = p(x). Let K be a matrix such that K · vec(H) = 0 represents the constraints (25), (26) and (27).","The optimization problem is:","minimize H rank(H) subject to ∥O · vec(H) − z∥2 ≤ μ K · vec(H) = 0 ∥H∥2 ≤ 1. (28) Intuitively, we look for H that agrees with the observable statistics and satisfies the inside-outside constraints. μ is a parameter of the method that controls the degree of error in fitting the observablesz. The ∥H∥2 ≤ 1 is satisfied by any Hankel matrix derived from a true distribution, and is used to avoid incoherent solutions.","The above optimization problem, however, is computationally hard because of the rank objective. We employ a common relaxation of the rank objective, based on the nuclear norm as in (Balle et al., 2012). The optimization is:","minimize H ∥H∥∗ subject to ∥O · vec(H) − z∥2 ≤ μ K · vec(H) = 0 ∥H∥2 ≤ 1. (29)","To optimize (29) we employ a projected gradient strategy, similar to the FISTA scheme proposed by Beck and Teboulle (2009). The method alternates between separate projections for the observable constraints, the l2 norm, the inside-outside constraints, and the nuclear norm. Of these, the latter two are the most expensive.","Elsewhere, we develop theoretical properties of the optimization (28) applied to finite-state transductions (Bailly et al., 2013). One can prove that there is theoretical identifiability of the rank and the parameters of an FST distribution, using a rank minimiza-tion formulation. However, this problem is NP-hard, and it remains open whether there exists a polynomial method with identifiability results. These results should generalize to WCFG."]},{"title":"5 Experiments","paragraphs":["In this section we describe some experiments with the learning algorithms for WCFG. Our goal is to verify that the algorithms can learn some basic context-free languages, and to study the possibility of using them on real data. 5.1 Synthetic Experiments We performed experiments on synthetic data, obtained by choosing a PCFG with random parameters (∈ [0, 1]), with a normalization step in order to get a probability distribution. We built the Hankel matrix from the inside basis {(x)}x∈Σ and outside basis 629 1e-06 1e-05 0.0001 0.001 0.01 0.1 1 100 1000 10000 100000 1e+06 KL divergence Sample size","Unsupervised Spectral Supervised Spectral Unsupervised EM","Supervised EM Figure 1: KL divergence for spectral and EM methods, unsupervised and supervised, for different sizes of learning sample, on log-log scales. Results are averages over 50 random target PCFG with 2 states and 2 symbols. {⟨λ; λ⟩} ∪ {⟨x; λ⟩, ⟨λ; x⟩}x∈Σ. The composed insides for the operator matrix are thus {(x, y)}x,y∈Σ. The matrix in the optimizer has the following structure H =      (y) · · · (y, z) ⟨λ; λ⟩ (λ; y; λ) · · · (λ; y, z; λ) ⟨x; λ⟩ (x; y; λ) · · · (x; y, z; λ) ⟨λ; x⟩ (λ; y; x) · · · (λ; y, z; x) ... · · · · · · · · ·      The constraints we use are:","K ={H((x; y; λ)) = H((λ; x; y))}x,y∈Σ∪ {H((λ; x; y)) = H((λ; x, y; λ))}x,y∈Σ∪ {H((x; y; λ)) = H((λ; x, y; λ))}x,y∈Σ and O ={H((λ; x; λ)) = pS(x)}x∈Σ ∪","{H((λ; x; y)) = pS(xy)}x,y∈Σ ∪ {H((x; y, z; λ)) + H((λ; x, y; z)) = pS(xyz)}x,y,z∈Σ We use pS to denote the empirical distribution. Those are simplified versions of the Hankel, inside, outside and observation constraints. The set O is built from the following remarks: (1) (xy) = (x, y) and (2) (xyz) = (xy, z) + (x, yz). The method uses statistics for sequences up to length 3.","The algorithm we use for the unsupervised spectral method is a simplified version: we use alternatively a hard projection on the constraints (by 1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 KL divergence Sample size Unsupervised Spectral Supervised Spectral Figure 2: KL divergence for unsupervised and supervised spectral methods, for different sizes of learning sample, on log-log scales. Results are averages over 50 random target PCFG with 3 states and 6 symbols. projecting iteratively on each constraint), and a thresholding-shrinkage operation for the target dimension. We use the same trick as FISTA for the update. We finally use the regular spectral method on this matrix to get our model.","We compare this method with an unsupervised EM, and also with supervised versions of spectral method and EM. We compare the accuracy of the different models in terms of KL-divergence for sequences up to length 10. We run 50 optimization steps for the unsupervised spectral method, and 200 iterations for the EM methods. Figure 1 shows the results, corresponding the the geometric mean over 50 experiments on random targets of 2 symbols and 2 states.","For sample size greater than 105",", the unsupervised spectral method seems to provide better solutions than both EM and supervised EM. The solution, in terms of KL-divergence, is comparable to the one obtained with the supervised spectral method. The computation time of unsupervised spectral method is almost constant w.r.t. the sample size, around 1.67s, while computation time of unsupervised EM (resp. supervised EM) is 6.103","s (resp. 2.104","s) for sample size 106",".","Figure 2 presents learnings curve for random targets with 3 states and 6 symbols. One can see that, for big sample sizes (109","), the unsupervised spectral method is losing accuracy compared to the supervised method. This is due to a lack of information, 630 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 0 0.01 0.02 0.03 0.04 0.05 L1 distance Basis factor","Spectral WFA Unsupervised Spectral Supervised Spectral Figure 3: Learning errors for different models in terms of the size of the basis. and could be overcome by considering a greater basis (e.g. inside sequences up to length 2 or 3). 5.2 Dyck Languages We now present experiments using the following PCFG: S → S S (0.2) | a S b (0.4) | a b (0.4) This PCFG generates a probabilistic version of the well-known Dyck language or balanced parenthesis language, an archetypical context-free language.","We do experiments with the following models and algorithms:","• WFA: a Weighted Finite Automata learned using spectral methods as described in (Luque et al., 2012). Parameters: number of states and size of basis.","• Supervised Spectral: a WCFG learned from structured strings using the algorithm of section 3.2. We choose as basis the most frequent insides and outsides observed in the training data. The size of the basis is determined by a parameter f called the basis factor, that determines the proportion of total insides and outsides that will be in the basis.","• Unsupervised Spectral: a WCFG learned from strings using the algorithm of Section 4. The basis is like in the supervised case, but since context-free cuts in the strings are not observed, basis size of H obs. i/o ctr. 1 × 11 39 × 159 34 162 6 × 14 1,163 × 764 146 6,360 12 × 18 4,462 × 2,239 322 25,374 18 × 22 9,124 × 4,149 479 52,524 24 × 26 15,755 × 6,858 657 89,718 30 × 29 19,801 × 8,545 769 112,374 36 × 34 27,989 × 11,682 916 156,690 42 × 37 3,638 × 15,026 1,035 200,346 48 × 41 45,192 × 18,235 1,157 244,398 54 × 45 53,741 × 21,196 1,281 284,466 60 × 48 60,844 × 23,890 1,382 318,354 Table 1: Problem sizes for the WSJ10 training corpus. basis / n 5 10 15 20 1 × 11 1.265 10−3 6 × 14 7.06 10−4 6.92 10−4 12 × 18 7.30 10−4 6.28 10−4 6.01 10−4 18 × 22 7.31 10−4 6.29 10−4 5.84 10−4 5.59 10−4 24 × 26 7.35 10−4 6.39 10−4 5.88 10−4 5.31 10−4 30 × 29 7.34 10−4 6.41 10−4 5.86 10−4 5.30 10−4 Table 2: Experiments with the unsupervised spectral method on the WSJ10 corpus. Results are in terms of expected L1 on the training set, for different basis and numbers of states. all possible inside and outsides of the sample (i.e. all possible substrings and contexts) are considered.","We generate a training set by sampling 4,000 strings from the target PCFG and counting the relative frequency of each. For the supervised model, we generate strings paired with their context-free derivation. To measure the quality of the learned models, we use the L1 distance to the target distribution over a fixed set of stringsΣ≤n",", for n = 7.1","Figure 3 shows the results for the different models and for different basis sizes (in terms of the basis factor f ). Here we can clearly see that the WCFG models, even the unsupervised one, outperform the WFA in reproducing the target distribution. 5.3 Natural Language Experiments Now we present some preliminar tests using natural language data. For these tests, we used the WSJ10 subset of the Penn Treebank, as Klein and Manning (2002). This dataset consists of the sentences of length ≤ 10 after filtering punctuation and currency. We removed lexical items and mapped the POS tags","1","Given two functions f1 and f2 over strings, the L1 distance is the sum of the absolute difference over all strings in a set: ∑","x |f1(x) − f2(x)|. 631 to the Universal Part-of-Speech Tagset (Petrov et al., 2012), reducing the alphabet to a set of 11 symbols.","Table 1 shows the size of the problem for different basis sizes. As described in the previous sub-section for the unsupervised case, we obtain the basis by taking the most frequent observed substrings and contexts. We then compute all yields that can be generated with this basis, and close the basis to include all possible insides and outsides with operations completions, such that we create a Hankel as described in Section 4.1. Table 1 shows, for each base, the size of H we induce, the number of observable constraints (i.e. sentences we train from), and the number of inside-outside constraints.","With the current implementation of the optimizer we were only able to run the unsupervised learning for small basis sizes. Table 2 shows the expected L1 on training data. For a fixed basis, as we increase the number of states we see that the error decreases, showing that the method is inducing a Hankel matrix that explains the observable statistics."]},{"title":"6 Conclusions","paragraphs":["We have presented a novel approach for unsupervised learning of WCFG. Our method combines ingredients of spectral learning with low-rank convex optimization methods.","Our method optimizes over a matrix that, even if it grows polynomially with respect to the size of training, results in a large problem. To scale the method to learn languages of the complexity of natural languages we would need to identify optimization algorithms specially suited for this problem."]},{"title":"A ProofsA.1 Proof of Inside-Outside Eq. 16 and 17","paragraphs":["For the inside function, the base case is trivial. By induction:","βG(x)[i] = ∑ x=x1x2 A(βG(x1) ⊗ βG(x2))[i] =∑ j,k∈V x=x1x2 A[i, j, k] · βG(x1)[j] · βG(x2)[k] =∑ j,k∈V x=x1x2 wR(i → j k) · β̄ Ḡ(j ⋆","=⇒ x1) · β̄ Ḡ(k ⋆","=⇒ x2)","= β̄ Ḡ(i ⋆ =⇒ x)","For the outside function, let ei be an n-dimensional vector with coordinate i to 1 and the rest to 0. We reformulate the mapping as:","αG(x; z)⊤ ei = ᾱ Ḡ(x; i; z) (30)","The base case is trivial by definitions. We use the","property of Kronecker products that (v ⊗ In)v′","=","(v ⊗ v′",") and (In ⊗ v)v′ = (v′","⊗ v) for v, v′","∈ Rn",".","We first look at one of the terms ofαG(x; z)⊤","ei:","αG(x1; z)⊤ A(βG(x2) ⊗ In)ei","= αG(x1; z)⊤ A(βG(x2) ⊗ ei)","= ∑ j,k∈V (αG(x1; z)⊤","ej) · A[j, k, i] · βG(x2)[k]","= ∑ j,k∈V ᾱ Ḡ(x1; j; z) · wR(j → k i) · β̄ Ḡ(k ⋆","=⇒ x2) Applying the distributive property in αG(x; z)⊤","ei it is easy to see that all terms are mapped to the corresponding term in ᾱ Ḡ(x; i; z). A.2 Proof of Lemma 1","Let G′","= ⟨α′ ⋆, {β′","σ}, A′","⟩ be a WCFG for f that in-","duces a rank factorization H = P ′","S′",". We first show","that there exists an invertible matrix M that changes","the basis of the operators of G into those of G′",".","Define M = S′","S+","and note that P +","P ′","S′","S+","=","P +","HS+","= I implies that M is invertible with","M −1","= P + P ′",". We now check that the operators","of G correspond to the operators of G′","under this","change of basis. First we see that","A = P + HA(S ⊗ S)+","= P +","P ′ A′ (S′","⊗ S′",")(S ⊗ S)+","= M −1","A′","(S′ S+","⊗ S′","S+",")","= M −1 A′","(M ⊗ M ) .","Now, since h⋆ = α′⊤","⋆ S′","and hσ = P ′","β′","σ , it follows","that α⊤","⋆ = α′","⋆⊤ M and βσ = M −1","β′","σ.","Finally we check that G and G′","compute the","same function, namely f (o, i) = αG(o)⊤","βG(i) =","αG′(o)⊤","βG′(i). We first see that βG(x) = 632","M −1 βG′(x):","βG(σ) = βσ = M −1 β′ σ (31)","βG(x) = ∑ x=x1x2 A(βG(x1) ⊗ βG(x2)) (32)","= ∑","x=x1x2 M −1","A′","(M ⊗ M )(βG(x1) ⊗ βG(x2))","= M −1 ∑ x=x1x2 A′","(M βG(x1) ⊗ M βG(x2))","= M −1 ∑ x=x1x2 A′","(βG′(x1) ⊗ βG′(x2)) It can also be shown that αG(x; z)⊤ =","αG′(x; z)⊤ M . One must see that in any term:","αG(x1; z)⊤ A(βG(x2) ⊗ In) (33)","= αG(x1; z)⊤","M −1","A′","(M ⊗ M )(βG(x2) ⊗ In)","= αG′(x1; z)⊤ A′","(M βG(x2) ⊗ M In)","= αG′(x1; z)⊤ A′","(βG′(x2) ⊗ In)M and the relation follows. Finally:","αG(x; z)⊤ βG(y) (34)","= αG′(x; z)⊤ M M −1","βG′(y)","= αG′(x; z)⊤ βG′(y) A.3 Proof of Lemma 2 We will use the following sub-blocks of H:","• Hε is the sub-block restricted to O1","× I1",", i.e.","without compositions.","• HA is the sub-block restricted to O1","× I2",", i.e.","inside compositions.","• H′ A is the sub-block restricted to O2","× I1",", i.e.","outside compositions.","• h⊤ ⋆ ∈ RI1","is the row of Hε for ⟨λ; λ⟩.","• h(x) ∈ RO1","is the column of Hε for (x).","• h(x1,x2) ∈ RO1","is the column of HA for (x1, x2).","• h′ ⟨x;z⟩ ∈ RI1","is the row of hε for ⟨x; z⟩.","• h′ ⟨x1,x2;z⟩ and h′","⟨x;z1,z2⟩ be the rows in RI1","of","h′ A for ⟨x1, x2; z⟩ and ⟨x; z1, z2⟩). One supposes that rank(Hε) = rank(H). We defineG as","α⊤","⋆ = h⊤","⋆ H+","ε , βa = h(a), A = HA(H+","ε ⊗ H+","ε ) Lemma 3. One has that βG(x) = h(x), and βG(x1, x2) = h(x1,x2). Proof. By induction. For sequences of size 1, one has βG(x) = βx = h(x). For the recursive case, let e(x) be a vector in RI1","with 1 in the coordinate of (x) in Hε. Let e(x,y) be a vector in RI2","with 1 in the coordinate of (x, y) in HA. For βG(x, y), one has H+","ε βG(x) = e(x), and H+","ε βG(y) = e(y), thus H+","ε βG(x) ⊗ H+","ε βG(y) = e(x,y) and HA(H+","ε βG(x) ⊗ H+","ε βG(y)) = h(x,y). Finally, one has that βG(x) = ∑","x=x1x2 βG(x1, x2) =∑ x=x1x2 h(x1,x2) = h(x1x2x3) by the equation (26). One has a symmetric result for outside vectors. We defineG′","as","α⊤","⋆ = h⊤ ⋆ , βa = H+","ε h(a), A = H+","ε HA Lemma 4. One has that αG′(⟨x; z⟩)⊤ = h′ ⟨x;z⟩, αG′(⟨x1, x2; z⟩)⊤ = h′ ⟨x1,x2;z⟩ and","αG′(⟨x; z1, z2⟩)⊤ = h′","⟨x;z1,z2⟩. Proof. (Sketch) Equation (31) is used in the same way than (27) before. Equation (25) is used to ensure a link between H′","A and HA.","Let g be the mapping computed by G and","G′",". One has that g(o, i) = αG′(o)⊤","βG′(i) =","αG(o)⊤","βG(i) = αG′(o)⊤","H+","ε βG(i) = Hε(o, i). Acknowledgements We are grateful to Borja Balle and the anonymous reviewers for providing us with helpful comments. This work was supported by a Google Research Award, and by projects XLike (FP7-288342), ERA-Net CHISTERA VISEN, TACARDI (TIN2012-38523-C02-02), BASMATI (TIN2011-27479-C04-03), SGR-GPLN (2009-SGR-1082) and SGR-LARCA (2009-SGR-1428). Xavier Carreras was supported by the Ramón y Cajal program of the Spanish Government (RYC-2008-02223). Franco M. Luque was supported by the National University of Córdoba and by a Postdoctoral fellowship of CONICET, Argentinian Ministry of Science, Technology and Productive Innovation. 633"]},{"title":"References","paragraphs":["Pieter Adriaans, Marten Trautwein, and Marco Vervoort. 2000. Towards high speed grammar induction on large text corpora. In SOFSEM 2000: Theory and Practice of Informatics, pages 173–186. Springer.","Raphaël Bailly, Franois Denis, and Liva Ralaivola. 2009. Grammatical inference as a principal component analysis problem. In Léon Bottou and Michael Littman, editors, Proceedings of the 26th International Conference on Machine Learning, pages 33–40, Montreal, June. Omnipress.","Raphaël Bailly, Amaury Habrard, and Franco̧is Denis. 2010. A spectral approach for probabilistic grammatical inference on trees. In Proceedings of the 21st International Conference Algorithmic Learning Theory, Lecture Notes in Computer Science, pages 74–88. Springer.","Raphaël Bailly, Xavier Carreras, and Ariadna Quattoni. 2013. Unsupervised spectral learning of finite-state transducers. In Advances in Neural Information Processing Systems 26.","James K. Baker. 1979. Trainable grammars for speech recognition. In D. H. Klatt and J. J. Wolf, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, pages 547–550.","Borja Balle, Ariadna Quattoni, and Xavier Carreras. 2011. A spectral learning algorithm for finite state transducers. In Proceedings of ECML PKDD, pages 156–171.","Borja Balle, Ariadna Quattoni, and Xavier Carreras. 2012. Local loss optimization in operator models: A new insight into spectral learning. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-2012), ICML ’12, pages 1879–1886, New York, NY, USA, July. Omnipress.","Amir Beck and Marc Teboulle. 2009. A fast iterative shrinkage-thresholding algorithm for linear in-verse problems. SIAM J. Img. Sci., 2(1):183–202, March.","Stanley F Chen. 1995. Bayesian grammar induction for language modeling. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 228–235. Association for Computational Linguistics.","Alexander Clark. 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. In Proceedings of the 2001 workshop on Computational Natural Language Learning-Volume 7, page 13. Association for Computational Linguistics.","Alexander Clark. 2007. Learning deterministic context free grammars: The omphalos competition. Machine Learning, 66(1):93–110.","Shay B. Cohen, David M. Blei, and Noah A. Smith. 2010. Variational inference for adaptor grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 564– 572, Los Angeles, California, June. Association for Computational Linguistics.","Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2012. Spectral learning of latent-variable pcfgs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 223–231, Jeju Island, Korea, July. Association for Computational Linguistics.","Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2013. Experiments with spectral learning of latent-variable pcfgs. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 148–157, Atlanta, Georgia, June. Association for Computational Linguistics.","Matthew Gormley and Jason Eisner. 2013. Nonconvex global optimization for latent-variable models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), Sofia, Bulgaria, August. 11 pages.","Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A spectral algorithm for learning hidden markov models. In Proceedings of the Annual Conference on Computational Learning Theory (COLT).","Daniel Hsu, Sham Kakade, and Percy Liang. 2012. Identifiability and unmixing of latent parse trees. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1520–1528.","Dan Klein and Christopher D Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 128–135. Association for Computational Linguistics.","Kenichi Kurihara and Taisuke Sato. 2006. Variational bayesian grammar induction for natural language. In Grammatical Inference: Algorithms and Applications, pages 84–96. Springer.","Karim Lari and Steve J Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer speech & language, 4(1):35–56.","Percy Liang, Slav Petrov, Michael I Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical dirichlet processes. In EMNLP-CoNLL, pages 688– 697. 634","Franco M. Luque, Ariadna Quattoni, Borja Balle, and Xavier Carreras. 2012. Spectral learning for nondeterministic dependency parsing. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, Avignon, France, April. Association for Computational Linguistics.","Fernando Pereira and Yves Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 128– 135, Newark, Delaware, USA, June. Association for Computational Linguistics.","Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of LREC, May.","Kewei Tu and Vasant Honavar. 2008. Unsupervised learning of probabilistic context-free grammar using iterative biclustering. In Grammatical Inference: Algorithms and Applications, pages 224–237. Springer.","Menno Van Zaanen. 2000. Abl: Alignment-based learning. In Proceedings of the 18th conference on Computational linguistics-Volume 2, pages 961–967. Association for Computational Linguistics. 635"]}]}
