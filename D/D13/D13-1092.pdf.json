{"sections":[{"title":"","paragraphs":["Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 903–907, Seattle, Washington, USA, 18-21 October 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Implicit Feature Detection via a Constrained Topic Model and SVMWei Wang","paragraphs":["∗"]},{"title":", Hua Xu","paragraphs":["∗"]},{"title":"and Xiaoqiu Huang","paragraphs":["† ∗"]},{"title":"State Key Laboratory of Intelligent Technology and Systems,Tsinghua National Laboratory for Information Science and Technology,Department of Computer Science and Technology, Tsinghua University,Beijing 100084, China","paragraphs":["†"]},{"title":"Beijing University of Posts and Telecommunications, Beijing 100876, Chinaww880412@gmail.com, xuhua@tsinghua.edu.cn, alexalexhxqhxq@gmail.comAbstract","paragraphs":["Implicit feature detection, also known as implicit feature identification, is an essential as-pect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better."]},{"title":"1 Introduction","paragraphs":["Feature-specific opinion mining has been well defined by Ding and Liu(2008). Example 1 is a cell phone review in which two features are mentioned. Example 1 This cell phone is fashion in appearance, and it is also very cheap. If a feature appears in a review directly, it is called an explicit feature. If a feature is only implied, it is called an implicit feature. In Example 1, appearance is an explicit feature while price is an implicit feature, which is implied by cheap. Furthermore, an explicit sentence is defined as a sentence containing at least one explicit feature, and an implicit sentence is the sentence only containing implicit features. Thus, the first sentence is an explicit sentence, while the second is an implicit one.","This paper proposes an approach for implicit feature detection based on SVM and Topic Model(TM). The Topic Model, which incorporated into constraints based on the pre-defined product feature, is established to extract the training attributes for SVM. In the end, several SVM classifiers are constructed to train the selected attributes and utilized to detect the implicit features."]},{"title":"2 Related Work","paragraphs":["The definition of implicit feature comes from Liu et al. (2005)’s work. Su et al. (2006) used Pointwise Mutual Information (PMI) based semantic association analysis to identify implicit features, but no quantitative experimental results were provided. Hai et al. (2011) used co-occurrence association rule mining to identify implicit features. However, they only dealt with opinion words and neglected the facts. Therefore, in this paper, both the opinions and facts will be taken into account.","Blei et al. (2003) proposed the original LDA using EM estimation. Griffiths and Steyvers (2004) applied Gibbs sampling to estimate LDA’s parameters. Since the inception of these works, many variations have been proposed. For example, LDA has previously been used to construct attributes for classification; it often acts to reduce data dimension(Blei and Jordan, 2003; Fei-Fei and Perona, 2005; Quel-has et al., 2005). Here, we modify LDA and adopt it to select the training attributes for SVM."]},{"title":"3 Model Design3.1 Introduction to LDA","paragraphs":["We briefly introduce LDA, following the notation of Griffiths(Griffiths and Steyvers, 2004). Given D 903 documents expressed over W unique words and T topics, LDA outputs the document-topic distribution θ and topic-word distribution φ, both of which can be obtained with Gibbs Sampling. For this scheme, the core process is the topic updating for each word in each document according to Equation 1. P (zi = j|z−i, w, α, β) = ( n(wi) −i,j + β","∑W","w′ n(w′",")","−i,j + W β )( n(di) −i,j + α","∑T j n(di)","−i,j + T α ) (1) where zi = j represents the assignment of the ith word in a document to topic j, z−i represents all the topic assignments excluding the ith","word. n(w′",")","j is the number of instances of word w′","assigned to topic j and n(di)","j is the number of words from document di assigned to topic j, the −i notation signifies that the counts are taken omitting the value of zi. Furthermore, α and β are hyper-parameters for the document-topic and topic-word Dirichlet distributions, respectively. After N iterations of Gibbs sampling for all words in all documents, the distribution θ and φ are finally estimated using Equations 2 and 3. φ(wi) j = n(wi) j + β","∑W","w′ n(w′",")","j + W β (2) θ(di) j = n(di) j + α","∑T j n(di)","j + T α (3) 3.2 Framework Algorithm 1 summarizes the main steps. When a specific product and the reviews are provided, the explicit sentences and corresponding features are extracted(Line 1) by word segmentation, part-of-speech(POS) tagging and synonyms feature cluster-ing. Then the prior knowledge are drawn from the explicit sentences automatically and integrated into the constrained topic model((Line 3 - Line 5). The word clusters are chosen as the training attributes(Line 6). Finally, several SVM classifiers are generated and applied to detect implicit features(Line 7 - Line 12). Algorithm 1 Implicit Feature Detection 1: ES ← extract explicit sentence set 2: N ES ← non-explicit sentence set 3: CS ← constraint set from ES 4: CP K ← correlation prior knowledge from ES 5: ET M ← ConstrainedTopicModel(T ,ES,CS,CP K) 6: T A ← select training attributes from ET M 7: for each fi in feature clusters do 8: T Di ← GenerateTrainingData(T Ai,ES) 9: Ci ← BuildClassificationModelBySVM(T Di) 10: P Ri ← positive result of Classify(Ci,N ES) 11: the feature of sentence in P Ri ← fi 12: end for 3.3 Prior Knowledge Extraction and Incorporation It is obvious that the pre-existing knowledge can as-sist to produce better and more significant clusters. In our work, we use a constrained topic model to select attributes for each product features. Each topic is first pre-defined a product feature. Then two types of prior knowledge, which are derived from the pre-defined product features, are extracted automatically and incorporated: must-link/cannot-link and correlation prior knowledge. 3.3.1 Must-link and Cannot-link","Must-link: It specifies that two data instances must be in the same cluster. Here is the must-link from an observation: as ”cheap” to ”price”, some words must be associated with a feature. In order to mine these words, we compute the co-occurrence degree by frequency*PMI(f,w), whose formula is as following: Pf&w ∗ log2 Pf&w","Pf Pw , where P is the probability of subscript occurrence in explicit sentences, f is the feature, w is the word, and f &w means the co-occurrence of f and w. A higher value of frequency*PMI signifies that w often indicates f . For a feature fi, the top five words and fi constitute must-links. For example, the co-occurrence of ”price” and ”cheap” is very high, then the must-link between ”price” and ”cheap” can be identified.","Cannot-link: It specifies that two data instances cannot be in the same cluster. If a word and a feature never co-occur in our corpus, we assume them to form a cannot-link. For example, the word lowcost has never co-occurred with the product feature screen, so they constitute a cannot-link in our cor-904 pus.","In this paper, the pre-defined process, must-link, and cannot-link are derived from Andrzejewski and Zhu (2009)’s work, all must-links and cannot-links are incorporated our constrained topic model. We multiply an indicator function δ(wi, zj), which represents a hard constraint, to the Equation 1 as the final probability for topic updating (see Equation 4). P (zi = j|z−i, w, α, β) = δ(wi, zj)( n(wi) −i,j + β","∑W","w′ n(w′",")","−i,j + W β )( n(di) −i,j + α","∑T j n(di)","−i,j + T α )","(4)","As illustrated by Equations 1 and 4, δ(wi, zj), which represents intervention or help from pre-existing knowledge of must-links and cannot-links, plays a key role in this study. In the topic updating for each word in each document, we assume that the current word is wi and its linked feature topic set is Z(wi)",", then for the current topic zj, δ(wi, zj) is calculated as follows:","1. If wi is constrained by must-links and the","linked feature belongs to Z(wi)",", δ(wi, zj|zj ∈","Z(wi)",") = 1 and δ(wi, zj|zj /∈ Z(wi)",") = 0.","2. If wi is constrained by cannot-links and the","linked feature belongs to Z(wi)",", δ(wi, zj|zj ∈","Z(wi)",") = 0 and δ(wi, zj|zj /∈ Z(wi)",") = 1. 3. In other cases, δ(wi, zj|j = 1, . . . , T ) = 1. 3.3.2 Correlation Prior Knowledge","In view of the explicit product feature of each topic, the association of the word and the feature to topic-word distribution should be taken into accoun-t. Therefore, Equation 2 is revised as the following: φ(wi) j =","(1 + Cwi,j)(n(wi)","j ) + β","∑W","w′ (1 + Cw′ ,j)(n(w′",")","j ) + W β (5) where Cw′",",j reflects the correlation of w′","with the topic j, which is centered on the product feature fzj . The basic idea is to determine the association of w′ and fzj , if they have the high relevance, Cw′",",j should be set as a positive number. Otherwise, if we can determine w′","and fzj are irrelevant, Cw′",",j should be set as a positive number. In this paper, we attempt to using PMI or dependency relation to judge the relevance. For word w′","and feature fzj :","1. Dependency relation judgement: If w′","as parent node in the syntax tree mainly co-occurs with fzj , Cw′",",j will be set positive. If w′","mainly co-occurs with several features including fzj , Cw′",",j will be set negative. Otherwise, Cw′",",j will be set 0.","2. PMI judgement: If w′","mainly co-occurs with","fzj and P M I(w′",", fzj ) is greater than the giv-","en value, Cw′",",j will be set positive. Otherwise,","Cw′",",j will be set negative. 3.4 Attribute Selection Some words, such as ”good”, can modify several product features and should be removed. In the result of run once, if a word appears in the topics which relates to different features, it is defined as a conflicting word. If a term is thought to describe several features or indicate no features, it is defined as a noise word .","When each topic has been pre-allocated, we run the explicit topic model 100 times. If a word turns into a conflicting word Tcw times(Tcw is set to 20), we assume that it is a noise word. Then the noise word collection is obtained and applied to filter the explicit sentences. Actually, here 100 is just an estimated number. And for Tcw, when it is between 15 and 25, the result is same, and when it exceeds 25, the result does not change a lot. The most important part to filter noise words is the correlation computation. So the experiment can work well with only estimated parameters.","Next, By integrating pre-existing knowledge, the explicit topic model, which runs Titer times, sever-s as attribute selection for SVM. In every result for each topic cluster, we remove the least four probable of word groups and merge the results by the pre-defined product feature. For a feature, if a word appears in its topic words more than Titer ∗ tratio times, it is selected as one of the training attributes for the feature. In the end, if an attribute associates with different features, it is deleted. 905","0","1","0","2","0","3","0 4 0","5","0","6","0","7","0","8","0","9","0","1","0","0","A","t","t","r","i","b","u","t","e","","F a c t o r  N u","m","b","e","r  C h i S q u a","r","e  G a i n R a t i","o  I n f o G a i n (a) SVM based on traditional attribute selection method 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 t r a t i o ","T","M","","T","M","+ m u s t","","T","M","+ c a n n o t","","T","M","+ m u s t + c a n n o t","","T","M","+ s y n t a c t i c","","T","M","+ m u s t + c a n n o t +","s","y","n","t","a","c","t","i","c","","T","M","+ P M I","","T","M","+ m u s t + c a n n o t +","P","M","I","","T","M","+ c o r r e l a t i o n  k n","o","w","l","e","d","g","e","(","P","M","I","+","s","y","n","t","a","c","t","i","c",")","","T","M","+ m u s t + c a n n o t +","c","o","r","r","e","l","a","t","i","o","n","","k","n","o","w","l","e","d","g","e (b) our constrained topic model by different tratio (Titer = 20) 0 1 0 2 0 3 0 4 0 5 0 T  i t e r ","T","M","","T","M + m u s t","","T","M + c a n n o t","","T","M + m u s t + c a n n o t","","T","M + s y n t a c t i c","","T","M + m u s t + c a n n o t +","s","y","n","t","a","c","t","i","c","","T","M + P M I","","T","M + m u s t + c a n n o t +","P","M","I","","T","M + c o r r e l a t i o n  k n o","w","l","e","d","g","e","(","P","M","I","+","s","y","n","t","a","c","t","i","c",")","","T","M + m u s t + c a n n o t +","c","o","r","r","e","l","a","t","i","o","n","","k","n","o","w","l","e","d","g","e (c) our constrained topic model by different Titer (tratio = 0.1) Figure 1: Performance of different cases 3.5 Implicit Feature Detection via SVM After completing attribute selection, vector space model(VSM) is applied to the selected attributes on the explicit sentences. For each feature fi, a SVM classifier Ci is adopted. In train-set, the positive cases are the explicit sentences of fi, and the negative cases are the other explicit sentences. For a non-explicit sentence, if the classification result of Ci is positive, it is an implicit sentence which implies fi."]},{"title":"4 Evaluation of Experimental Results4.1 Data Sets","paragraphs":["There has no standard data set yet, we crawled the experiment data, which included reviews about a cellphone, from a famous Chinese shopping website1",". The data contains 14218 sentences. The feature of each sentence was manually annotated by two research assistants. A handful of sentences which were annotated inconsistently were deleted. Table 1 depicts the data set which is evaluated. Other features were ignored because of their rare appearance.","Here are some explanations: (1)The sentences containing several explicit features were not added to the train-set. (2) A tiny number of sentences contain both explicit and implicit features, and they can only be regarded as explicit sentences. (3) The training set contains 3140 explicit sentences, the test set contains 7043 non-explicit sentences and more than 5500 sentences have no feature. (4) According to the ratio among the explicit sentences(6:1:2:3:1:2), it is reasonable that the most suitable number of topics should be 14. For example, the ratio of the prod-1 http://www.360buy.com/ Table 1: Experiment data Features Explicit Implicit Total screen 1165 244 1409 quality 199 83 282 battery 456 205 661 price 627 561 1188 appearance 224 167 391 software 469 129 598 uct feature screen is 6, so we can assign the feature to topic 0,1,2,3,4,5. In our experiment, the performance of algorithm 1 is evaluated using F-measure. (5) Although the size of dataset is limited, out proposed is based on the constraint-based topic model, which has been widely used in different NLP field-s. So, our approach can generalize well in different datasets. Of course, more high quality data will be collected to do the experiment in the future. 4.2 Experimental Results Figure 1a depicts the performance of using traditional attribute selection methods on SVM. Using χ2","test on SVM can achieve the best performance, which is about 66.7%. In our constrained topic model, we use different Titer and tratio. We conducted experiments by incorporating different types prior knowledge. From Figure 1b and 1c, we conclude that: (1)All these methods perform much better than the traditional feature selection methods, the improvements are more than 6%. (2)The reason for the little improvement of must-links is that the topic clusters have already obtained these linked word-906 s. (3)All the pre-existing knowledge performs best and shows 3% improvement over non prior knowledge. (4)Different types of prior knowledge have different impact on the stabilities of different parameters. (5)As we have expected, by combing all prior knowledge, the best performance can reach 77.78%. Furthermore, as tratio or Titer changes, our constrained topic model incorporating all prior knowledge look like very stable."]},{"title":"5 Conclusions","paragraphs":["In this paper, we adopt a constrained topic model incorporating prior knowledge to select attribute for SVM classifiers to detect implicit features. Experiments show this method outperforms the attribute feature selection methods and detect implicit features better."]},{"title":"6 Acknowledgments","paragraphs":["This work is supported by National Natural Science Foundation of China (Grant No: 61175110) and National Basic Research Program of China (973 Program, Grant No: 2012CB316305)."]},{"title":"References","paragraphs":["David Andrzejewski and Xiaojin Zhu. 2009. Laten-t dirichlet allocation with topic-in-set knowledge. In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pages 43–48. Association for Computational Linguistics.","D.M. Blei and M.I. Jordan. 2003. Modeling annotated data. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 127–134. ACM.","D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Laten-t dirichlet allocation. the Journal of machine Learning research, 3:993–1022.","Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the international conference on Web search and web data mining, WSDM ’08, pages 231– 240, New York, NY, USA. ACM.","L. Fei-Fei and P. Perona. 2005. A bayesian hierarchical model for learning natural scene categories. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 2, pages 524–531. IEEE.","T.L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.","Z. Hai, K. Chang, and J. Kim. 2011. Implicit feature identification via co-occurrence association rule mining. Computational Linguistics and Intelligent Text Processing, pages 393–404.","B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Proceedings of the 14th international conference on World Wide Web, pages 342–351. ACM.","P. Quelhas, F. Monay, J.M. Odobez, D. Gatica-Perez, T. Tuytelaars, and L. Van Gool. 2005. Modeling scenes with local descriptors and latent aspects. In Computer Vision, 2005. ICCV 2005. Tenth IEEE In-ternational Conference on, volume 1, pages 883–890. IEEE.","Q. Su, K. Xiang, H. Wang, B. Sun, and S. Yu. 2006. Using pointwise mutual information to identify implicit features in customer reviews. Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead, pages 22–30. 907"]}]}
