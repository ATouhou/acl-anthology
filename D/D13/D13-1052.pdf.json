{"sections":[{"title":"","paragraphs":["Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 545–555, Seattle, Washington, USA, 18-21 October 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Flexible and Efficient Hypergraph Interactions for Joint Hierarchical andForest-to-String Decoding","paragraphs":["∗"]},{"title":"Martin Čmejrek†‡†IBM Prague Research LabV Parku 2294/4Prague, Czech Republic, 148 00martin.cmejrek@us.ibm.com Haitao Mi‡ and Bowen Zhou‡‡IBM T. J. Watson Research Center1101 Kitchawan RdYorktown Heights, NY 10598{hmi,zhou}@us.ibm.comAbstract","paragraphs":["Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (Ter-B leu)/2 points. We also provide a detailed experimental and qualitative analysis of the results."]},{"title":"1 Introduction","paragraphs":["Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits.","Numerous efforts that have been proposed in this field recently can be broadly divided into two cat-","∗","M. Č and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations, which only explore a small portion of the entire search space of each system. This issue is well addressed in joint decoding (Liu et al., 2009), or online system combination, showing comparable improvements to the offline combination methods. Rather than finding consensus translations from the outputs of individual systems, joint decoding works with different grammars at the decoding time. Although limited to individual systems sharing the same search paradigm (e.g. left-to-right or bottom-up), joint decoding offers many potential advatages: search through a larger space, better efficiency, features designed once for all subsystems, potential cross-system features, online sharing of partial hypotheses, and many others.","Different approaches have different strengths in general–hiero rules are believed to provide reliable lexical coverage, while tree-to-string rules are good at non-local reorderings. Di fferent contexts present different challenges–noun phrases usually follow the adjacency principle, while verb phrases require more challenging reorderings. In this work, we study different schemes of interaction between translation models, reflecting their specific strengths at different (syntactic) contexts. We make five new contributions: First, we propose a framework for joint decoding by means of flexible combination of translation hypergraphs, allowing for detailed con-545 trol of interactions between the different systems using soft constraints (Section 3). Second, we study three interaction schemes– special cases of joint decoding: generalization, specification, and interchange (Section 3.3). Third, instead of using a tree-to-string system, we use a much stronger forest-to-string system with fuzzy match of nonterminal categories (Section 2.1). Fourth, we train strong systems on a large-scale data set, and test all methods on four test sets. Experimental results (Section 6) show that our new approach brings improvement of up to 0.9 points in terms of (Ter − Bleu)/2 over the best single system. Fifth, we conduct a comprehensive experimental analysis, and find that joint decoding actually prefers tree-to-string rules in both shorter and longer spans. (Section 6.3).","The paper is organized as follows: We briefly review the individual models in Section 2, describe the method of joint decoding using three alternative interaction schemes in Section 3, describe the features controlling the interactions and fuzzy match in Section 4, review the related work in Section 5, and finally, describe our experiments and give detailed discussion of the results in Section 6."]},{"title":"2 Individual Models","paragraphs":["Our individual models are two state-of-the-art systems: a hiero model (Chiang, 2005), and a forest-to-string model (Mi et al., 2008; Mi and Huang, 2008).","We will use the following example from Chinese to English to explain both individual and joint decoding algorithms throughout this paper. SS tǎolùnSSSSSSSS hùiSSSSS zěnmeyàng discussion/NN SSS will/VV how/VV S discuss/VV SS meeting/NN There are several possible meanings based on the different POS tagging sequences: 1: NN VV VV: How is the discussion going? 2: VV NN VV: Discuss about the meeting. 3: NN NN VV: How was the discussion meeting? 4: VV VV VV: Discuss what will happen. id rule r1 VV(tǎolùn) → discuss r2 NP(tǎolùn) → the discussion r3 NP(hùi) → the meeting r4 VP(zěnmeyàng) → how r′ 4 VP(zěnmeyàng) → about r5 IP(x1:NP x2:VP) → x2 x1 r6 IP(x1:VV x2:IP) → x1 x2 r7 IP(x1:NP VP(VV(hùi) x2:VP)) → x2 is x1 going r11 X(x1:X zěnmeyàng) → how was x1 r12 X(zěnmeyàng) → what r13 X(tǎolùn hùi) → the discussion meeting r14 X(hùi x1:X) → x1 will happen r15 S(x1:S x2:X) → x1 x2 Table 1: Translation rules. Tree-to-string ( r1–r7), hiero (r11–r14), vanilla glue (r15). IP x1:NP VP VV hùi x2:VP → x2 is x1 going Figure 1: Tree-to-string rule r7.","Table 1 shows translation rules that can generate all four translations. We will use those rules in the following sections. 2.1 Forest-to-string Forest-to-string translation (Mi et al., 2008) is a linguistic syntax-based system, which significantly improves the translation quality of the tree-to-string model (Liu et al., 2006; Huang et al., 2006) by using a packed parse forest as the input instead of a single parse tree.","Figure 1 shows a tree-to-string translation rule (Huang et al., 2006), which is a tuple ⟨lhs(r), rhs(r), ψ(r)⟩, where lhs(r) is the source-side tree fragment, whose internal nodes are labeled by nonterminal symbols (like NP and VP), and whose frontier nodes are labeled by source-language words (like “hùi”) or variables from a set X = {x1, x2, . . .}; rhs(r) is the target-side string expressed in target-language words (like “going”) and variables; and ψ(r) is a mapping from X to nonterminals. Each 546 (a) IP0, 3 VV0, 1 tǎolùn NP0, 1 IP1, 3 NP1, 2 hùi VV1, 2 VP1, 3 VP2, 3 zěnmeyàng Rt ⇒ (b) IP0, 3 X0, 2 VV0, 1 tǎolùn NP0, 1 IP1, 3 NP1, 2 hùi VV1, 2 X1, 3 VP1, 3 VP2, 3 zěnmeyàng X0, 3 e5 e6 e7 ⇓ Rh ⇓","(b′ ) IP0, 3 X0, 2 VV0, 1 tǎolùn NP0, 1 IP1, 3 NP1, 2 hùi VV1, 2 X1, 3 VP1, 3 VP2, 3 zěnmeyàng X2, 3 X0, 3 e11 e14 ⇒ (c) IP0, 3 X0, 2 VV0, 1 tǎolùn NP0, 1 IP1, 3 NP1, 2 hùi VV1, 2 X1, 3 VP1, 3 VP2, 3 zěnmeyàng X2, 3 X0, 3 Figure 2: Parse and translation hypergraphs. (a) The parse forest of the example sentence. Solid hyperedges denote the 1-best parse. (b) The corresponding translation forest Ft","after applying the tree-to-string translation rule set Rt",". Target lexical content is not shown. Each translation hyperedge (e.g. e7) has the same index as the corresponding rule (r7). Gray nodes (e.g. VP1,3) became inaccessible due to the insufficient rule coverage. (b′",") The translation forest Fh after applying the hierarchical rule set Rh","to the input sentence. (c) The combined translation forest Hm","obtained by superimposing b and b′",". The nodes within each solid box share the same span. See Figure 3 for an example of the internal structure of a box. The forest-to-string system ca n produce the translation 1 (dashed derivation: r2, r4 and r7) and 2 (solid derivation: r1, r3, r′","4, r5, and r6). Hierarchical rules generate the translation 3 (r11 and r13). The translation 4 is available by using joint decoding at X1, 3 → IP1, 3 with the derivation: r1, r6, r12, and r14. variable xi ∈ X occurs exactly once in lhs(r) and exactly once in rhs(r). Take the rule r7 in Figure 1 for example, we have: lhs(r7) = IP(x1:NP VP(VV(hùi) x2:VP)), rhs(r7) = x2 is x1 going, ψ(r7) = {x1 ↦→ NP, x2 ↦→ VP}.","Typically, a forest-to-string system performs translation in two steps (shown in Figure 2): parsing and decoding. In the parsing step, we convert the source language input into a parse forest (a). In the decoding step, we first convert the parse forest into a translation forest Ft","in (b) by using the fast pattern-matching technique (Zhang et al., 2009). For example, we pattern-match the rule r7 rooted at IP0, 3, in such a way that x1 spans NP0, 1 and x2 spans VP2, 3, and add a translation hyperedge e7 in (b). Then the decoder searches for the best derivation on the translation forest and outputs the target string. 2.2 Hiero Hiero (hierarchical phrase-based) model (Chiang, 2005) acquires rules of synchronous context-free grammars (SCFGs) from word-aligned parallel data, and uses plain sequences of words as the input, with-out any syntactic information. 547 FN","IP′ 1, 3 IP1, 3 BBBBSN","X′ 1, 3 X1, 3 EEEE scheme interaction edges in supernode Generalization","IP′ 1, 3","X′ 1, 3 IP1, 3 X1, 3 Specification","IP′ 1, 3","X′ 1, 3 IP1, 3 X1, 3 Interchange","IP′ 1, 3","X′ 1, 3 IP1, 3 X1, 3 Figure 3: Three interaction schemes for joint decoding. Details of the interaction supernode for span (1, 3) shown in Figure 2 (c). Soft constraints control the transitions.","SCFG can be formalized as a set of tuples ⟨lhs(r), rhs(r), φ(r)⟩, where lhs(r) is the source-side one-level CFG, whose root is X or S, and whose frontier nodes are labeled by source-language words (like “hùi”) or variables from a set X = {x1, x2, . . .}; rhs(r) is the target-side string expressed in target-language words (like “going”) and variables; and φ(r) is a mapping from X to nonterminals. Table 1 shows examples of hiero rules r11–r15.","Although different on source side, hiero decoding can be formalized equally as forest-to-string decoding: First, pattern-match the input sentence into a translation forest Fh",". For example, since the rule r11 matches “zěnmeyàng” such that x1 spans the first two words, add a hyperedge e11 in Figure 2 (b′","). Then search for the best derivation over the translation forest."]},{"title":"3 Joint Decoding","paragraphs":["The goal of joint decoding is to let different MT models collaborate within the framework of a single decoder. This can be done by combining translation hypergraphs of the different models at the decoding time, so that online sharing of partial hypotheses overcomes weaknesses and boosts strengths of the systems combined.","As both forest-to-string and hiero produce translation forests that share the same hypergraph structure, we first formalize the hypergraph, then we introduce an algorithm to combine different hypergraphs, and finally we describe three joint decoding schemes over the merged hypergraph. 3.1 Hypergraphs More formally, a hypergraph H is a pair ⟨V, E⟩, where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 . . . wl, each node v ∈ V is in the form of Yi, j, where Y is a nonterminal in the context-free grammar 1","and i, j, 0 ≤ i < j ≤ l, are string positions in the sentence w1:l, which denote the recognition of nonterminal Y spanning the substring from positions i through j (that is, wi+1 . . . w j). Each hyperedge e ∈ E is a tuple ⟨tails(e), head(e), target(e)⟩, where head(e) ∈ V is the consequent node in the deductive step, tails(e) ∈ V∗","is the list of antecedent nodes, and target(e) is a list of rhs(r) for rules r such that each rule r has the same lhs(r) pattern-matched at the node head(e). For example, the hyperedge e7 in Figure 2 (b) is e7 = ⟨(NP0, 1, VP2, 3), IP0, 3, (x2 is x1 going)⟩, where we can infer the mapping to be {x1 ↦→ NP0, 1, x2 ↦→ VP2, 3 }.","We also denote BS(v) to be the set of incoming hyperedges of node v, which represent the different ways of deriving v. For example, BS(IP0, 3) is a set of e7 and e6.","There is also a distinguished root node TOP in each hypergraph, denoting the goal item in translation, which is simply TOP0, l. 3.2 Combining Hypergraphs","We enable interaction between translation hyper-","graphs, such as hiero Fh","= ⟨Vh",", Eh","⟩ and forest-to-","string Ft","= ⟨Vt",", Et","⟩, on nodes covering the same","span (e.g. IP1, 3 and X1, 3 in Figure 2 (c) grouped in","a box). We call such groups interaction supernodes","and show a detailed example of a supernode for span","(1, 3) in Figure 3. The combination runs in four steps: 1 In this paper, nonterminal labels X and S denote hiero","derivations, other labels are tree-to-string labels. 548","1. For each node v = Yi, j, v ∈ Vh","∪ Vt",", we create","a new interaction node v′","= Y′","i, j with empty","BS (v′ ). For example, we create two nodes,","IP′","1, 3 and X′","1, 3, at the top of Figure 3.","2. For each hyperedge e ∈ BS(v), v ∈ Vt","∪ Vh",",","we replace each v in tails(e) with v′",". For exam-","ple, e7 becomes ⟨(NP′ 0, 1, VP′","2, 3), IP0, 3, (x2 is","x1 going)⟩.","3. All the nodes and hyperedges form the merged hypergraph Fm",", such as in Figure 2 (c).","4. Insert interaction hyperedges connecting nodes within each interaction supernode to make Fm connected again. In the following subsection we present details of interactions and introduce three alternative schemes. 3.3 Three Schemes of Joint Decoding Interaction hyperedges within each supernode allow the decoder either to stay within the same system (e.g. in hiero using X1, 3 → X′","1, 3 in Figure 3), or to switch to the other (e.g. to forest-to-string using X 1, 3 → IP′","1, 3).","For example, translation 4 can be produced as follows: The source string “zěnmeyàng” is translated by the phrase rule r12. The hiero hyperedge e14 combines it with the translation of “hùi”, reaching the hiero node X1, 3. Using the interaction edge X1, 3 → IP′","1, 3 will switch into the tree-to-string model, so that the translation can be completed with the tree-to-string edge e6 that connects it with a partial tree-to string translation of “t ǎolùn” done by r1.","In order to achieve more precise control over the interaction between tree-to-string and hiero derivations, we propose the following three basic interaction schemes: generalization, specification, interchange. The schemes control the interaction between hiero and tree-to-string models by means of soft constraints. Some schemes may even restrict certain types of transitions. The schemes are depicted in Figure 3 and their details are discussed in the following three subsections. 3.3.1 Specification","The specification decoding scheme reflects the in-tuition of using hiero rules to translate shorter spans and tree-to-string rules to reorder higher-level sentence structures. In other words, the scheme allows one-way switching from the hiero general nonterminal into the more specific nonterminal of a tree-to-string rule. Transitions in reverse directions are not allowed. This is achieved by inserting specification interaction hyperedges e leading from hiero nodes Xi, j or Si, j into all tree-to-string interaction nodes Y′","i, j within the same supernode. 3.3.2 Generalization","In some translation domains, hiero outperforms tree-to-string systems, as was shown in experiments in Section 6. While local hiero or tree-to-string reorderings perform well, long distance reorderings proposed by tree-to-string may be too risky (e.g. due to parsing errors), so that monotone concatenation of long sequences2","is the more reliable strategy. The generalization decoding scheme, complementary to the specification, is motivated by the idea of incorporating reliable tree-to-string translations for some sequences into a strong hiero translation system. This is achieved by inserting generalization interaction hyperedges e leading from tree-to-string nodes Y i, j nodes into general hiero interaction nodes X′","i, j and S′","i, j within the same supernode. 3.3.3 Interchange","The interchange decoding scheme is a union of the two previous approaches. Any derivation can freely combine hiero and tree-to-string productions. Both specification and generalization interaction hyperedges are inserted leading from all hiero and tree-to-string nodes X i, j, Si, j, and Yi, j into all interaction nodes X′","i, j, S′","i, j, and Y′","i, j. 3.4 Fuzzy match The translation rule set cannot usually cover all hyperedges in the parse forest, thus some nodes become inaccessible in the translation forest (e.g. VP1, 3 in Figure 2). However, in the parse forest, as opposed to a 1-best tree, we can find other nodes spanning the same sequence wi: j (e.g. node IP1, 3). In order to re-enable inaccessible nodes and to in-crease the variability of the translation forest, we allow reaching them from the other tree-to-string 2 Monotone glue is the only possibility for very long spans","exceeding the hiero maxParse treshold. 549 nodes within the same interaction node. This can be achieved by adding fuzzy hyperedges between every tree-to-string state Yi, j and a differently labeled tree-to-string interaction state Z′","i, j. For example, in the span (0,1), we have a fuzzy hyperedge VV0, 1 → NP′","0, 1.","While interaction hyperedges combine different translation models, fuzzy hyperedges combine different derivations within the same (tree-to-string) model."]},{"title":"4 Interaction Features","paragraphs":["Our baseline systems use the log-linear framework to estimate the probability P(D) of a derivation D from features φi and their weights λi as P(D) ∝ exp ( ∑","i λiφi)",". Similarly as Chiang et al. (2009), our systems use tens of dense (e.g. language models, translation probabilities) and thousands of sparse (e.g. lexical, fertility) features.","The features related to the joint decoding experiments are the costs for specification, generalization, interchange, and the fuzzy match. Let Lt","be the set of the labels used by the source language parser and Lh","= {S,X} be the labels used by hiero.","The generalization feature φY→Z = |{e; e ∈ D, ∃i, j tails(e) = {Yi, j} (1)","∧head(e) = Z′ i, j}|","is the total number of generalization hyperedges in","D going from tree-to-string states Y ∈ Lt","to hiero","states Z′","∈ Lh",". The specification feature φZ→Y = |{e; e ∈ D, ∃i, j tails(e) = {Zi, j} (2)","∧head(e) = Y′ i, j}| is the total number of specification hyperedges in D going from hiero states Z ∈ Lh","to tree-to-string states Y′ ∈ Lt",". The interchange feature is implemented by enabling the generalization and specification features at the same time for both tuning and testing. The fuzzy match feature φU→W = |{e; e ∈ D, ∃i, j tails(e) = {Ui, j} (3)","∧head(e) = W′ i, j}| is the total number of fuzzy match hyperedges in D going from tree-to-tree states U ∈ Lt","to tree-to-string states W′","∈ Lt",". 3","We use MIRA to obtain weights for the new features by tuning on the development set. The number of new parameters to tune can be estimated as |Lh","| × |Lt","| for generalization and specification, and 2 × |Lh","| × |Lt","| for interchange. For the fuzzy match of tree-to-string nonterminals we have |Lt","| × |Lt","| parameters organized as a sparse matrix, since we only consider combinations on nonterminal labels that cooccur in the data.4"]},{"title":"5 Related Work","paragraphs":["From the previous explorations of online translation model combination, we see the work of Liu et al. (2009) proposing an unconstrained combination of hiero and tree-to-string models as a special configuration of our framework, and we also replicate it.","Denero et al. (2010) combine translation models even with different search paradigms. Their approach is different, since their component systems do not interact at decoding time, instead, each of them provides its weighted translation forest first, the forests are then combined to infer a new combination model."]},{"title":"6 Experiment","paragraphs":["In this section we describe the setup, present results, and analyze the experiments. Finally, we propose future directions of research.","3","Here we allow U = W, which can be viewed in such a way that exact match is a special case of fuzzy match.","4","We also carried out an alternative experiment with only three fuzzy match features estimated from the training data parse forest by Naı̈ve Bayes by observing all spans in the training data, accumulating counts Cs(U) and Cs(U, W) of nonterminals (or pairs of nonterminals) heading the same span s. The first two features (one for each direction) are based on conditional probabilities:","φ(U|W) = − log ( ∑ s∈spans Cs (U, W) ∑ s∈spans Cs(W) )",". (4) The third feature is based on joint probability: φ(U,W) = − log ( ∑ s∈spans Cs(U, W)","∑ s∈spans,A,B∈Lt Cs(A, B) )",". (5) The average performance drops by 0.1 (Ter-B leu)/2 points, compared to the interchange eperiment. 550 System GALE-web P1R6-web MT08 news MT08 web Avg. Bleu (T-B) /2 Bleu (T-B) /2 Bleu (T-B) /2 Bleu (T-B) /2 (T-B) /2 Single T2S 32.6 11.6 16.9 23.5 37.7 7.8 28.1 14.5 14.4 Hiero 33.7 10.2 17.0 23.1 39.2 6.3 28.8 13.7 13.3 F2S 34.0 10.3 17.3 23.2 39.6 6.3 29.2 13.6 13.4 Joint Liu:09 34.1 9.7 17.0 23.0 38.8 6.7 29.0 13.2 13.2 Gen. 34.4 9.7 17.8 22.6 40.0 6.1 29.6 13.1 12.9 Spe. 35.1 9.4 18.1 22.2 40.2 5.8 29.6 12.9 12.6 Int. 34.9 9.4 17.9 22.3 40.0 6.2 29.6 12.9 12.7 Table 2: All results of single and joint decoding systems. 6.1 Setup The training corpus consists of 16 million sentence pairs available within the DARPA BOLT Chinese-English task. The corpus includes a mix of newswire, broadcast news, webblog and comes from various sources such as LDC, HK Law, HK Hansard and UN data. The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). Language models are trained on the English side of the parallel corpus, and on monolingual corpora, such as Gigaword (LDC2011T07) and Google News, altogether comprising around 10 billion words.","We use a modified version of the Berkeley parser (Petrov and Klein, 2007) to obtain a parse forest for each training sentence, then we prune it with the marginal probability-based inside-outside algorithm to contain only 3n CFG nodes, where n is the sentence length. Finally, we apply the forest-based GHKM algorithm (Mi and Huang, 2008; Galley et al., 2004) to extract tree-to-string translation rules from forest-string pairs.","In the decoding step, we prune the input hypergraphs to 10n nodes before we use fast pattern-matching (Zhang et al., 2009) to convert the parse forest into the translation forest.","We tune on 1275 sentences, each with 4 references, from the LDC2010E30 corpus, initially released under the DARPA GALE program.","All MT experiments are optimized with MIRA (Crammer et al., 2006) to maximize (Ter-B leu)/2.","We test on four different test sets: GALE-web test set from LDC2010E30 corpus (1239 sentences, 4 references), P1R6-web test set from LDC2012E124 corpus (1124 sentences, 1 reference), NIST MT08 newswire portion (691 sentences, 4 references), and NIST MT08 web portion (666 sentences, 4 references). 6.2 Results Table 2 shows all results of single and joint decoding systems. The Bleu score of the single hiero baseline is 39.2 on MT08-news, showing that it is a strong system. The single F2S baseline achieves comparable scores on all four test sets.","Then, for reference, we present results of joint Hiero and T2S decoding, which is, to our knowledge, a strong and competitive reimplementaion of the work described by Liu et al. (2009). Finally, we present results of joint decoding of hiero and F2S in three interaction schemes: generalization, specification, and interchange.","All three combination schemes significantly improve results of any single system on all four testsets. On average and measured in (Ter-B leu)/2, our systems improve the best single system by 0.4 (generalization), 0.7 (specification), and 0.6 (interchange).","The specification comes out as the strongest interaction scheme, beating the second interchange on 2 testsets by 0.1 and 0.4 (Ter-B leu)/2 points and on 3 testsets by 0.2 Bleu points. 6.3 Discussion of Results Interpretations of model behavior with thousands of parameters that may possibly overlap and interfere should be always attempted with caution. In this section we highlight some interesting observations, ac-551 Specification Generalization Interchange X → ∗ ∗ → X X → ∗ ∗ → X VP IP VV NR ADVP QP CC DVP NP P ... CS CP AD VRD PU ADJP DNP PP PRN DP 0.069 0.059 0.053 0.032 0.025 0.023 0.017 0.017 0.017 0.012 ... -0.005 -0.007 -0.011 -0.012 -0.028 -0.028 -0.045 -0.064 -0.069 -0.092 QP PP NN DP NR DNP NP LC DEC DEG ... VV PRN PN BA VP VRD JJ VC DFL PU 0.057 0.054 0.048 0.044 0.034 0.032 0.030 0.025 0.023 0.023 ... -0.010 -0.011 -0.013 -0.015 -0.015 -0.028 -0.035 -0.037 -0.054 -0.073 VV VP NN QP ADVP LCP NP P IP NR ... VSB PN PU M VRD DNP ADJP PP DP PRN 0.062 0.044 0.034 0.025 0.022 0.021 0.018 0.017 0.016 0.016 ... -0.004 -0.004 -0.004 -0.007 -0.014 -0.023 -0.039 -0.058 -0.070 -0.080 NN PP CP LCP DEG DP DEC QP LC NP ... FLR DVP BA JJ AS VRD ADVP PN DFL PU 0.048 0.041 0.035 0.035 0.031 0.028 0.027 0.027 0.021 0.019 ... -0.006 -0.009 -0.010 -0.011 -0.014 -0.017 -0.021 -0.033 -0.038 -0.103 Table 3: Examples of specification, generalization, and interchange weights. POS tags in italics. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ADVP","CLP","ADJP","FLR","DFL","DP","VCP","VSB","VRD","VCD QP","NP","DVP","DNP","LCP","PP","VP","CP","PRN IP","FRAG Average span length Figure 4: Average span length for selected syntactic labels on GALE-web test set. companying them with our subjective judgements and speculations.","Table 3 shows the specification and generalization features tuned for the three combination schemes, then sorted by their weights λX→Y or λY→X. Features shown at the top of the table are very expensive (the #Interactions Generalization Inter. gen. F2S → glue 5557 4202 F2S → hiero 695 1178 total gen. 6252 5380 Specification Inter. spec. phrase → F2S 2763 2235 glue → F2S 946 841 hiero → F2S 683 839 total spec. 4392 3915 Table 5: Rule interactions on GALE-web test set. system tries to avoid them), while inexpensive features are at the bottom (the system is encouraged to use them).","The most expensive interactions for the specification belong to constituents (IP, VP) that usually occur higher in a syntactic tree (see Figure 4 for average span lengths of selected syntactic labels), and often require non-local reorderings. This indicates that the decoder is discouraged from switching from hiero into F2S derivation at these higher-level spans. 552 rule type Generalization Specification Interchange F2S 18,807 58% 19,399 70% 18,400 61% Hiero 3,730 12% 2,330 8% 3,133 10% Glue 7,367 23% 571 2% 4,714 16% Phrase 2,274 7% 5,484 20% 3,868 13% total 32,178 27,784 30,115 Table 4: Rule counts on GALE-web test set. 100̂ 101̂ 102̂ 103̂ 104̂ 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 Number of rules Span length Generalization F2S Hiero Glue Phrase 100̂ 101̂ 102̂ 103̂ 104̂ 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 Number of rules Span length Specification F2S Hiero Glue Phrase 100̂ 101̂ 102̂ 103̂ 104̂ 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 Number of rules Span length Interchange F2S Hiero Glue Phrase Figure 5: Rule distributions on GALE-web test set.","The third most expensive feature belongs to a part-of-speech tag—the preterminal VV. We may hypothesize that it shows the importance of lexical information for the precision of reordering typically carried out within (parent) VP nodes, and/or the importance of POS information for succesful disambiguation of word senses in translation. Ideally, the system can use a VP rule with a lexicalized VV. Less preferably, the VV part has to be translated by another T2S rule (losing the lexical constraint). In the worst case, the system has to use a hiero hypothesis to translate the VV part (losing the syntactic constraint), risking imprecise translation, since the hiero rule is not constrained to senses corresponding to the source POS VV. Again, the high penalty discourages from using the hiero derivation in this context.","On the other hand, the bottom of the table shows labels that encourage using hiero–DP, PP, DNP, ADJP, etc.–shorter phrases that tend to be monotone and less ambiguous.","Similar interpretations seem plausible when examining the generalization experiment. Expensive features related to preterminals (NR, NN, CD) may suggest two alternative principles: First, using F2S rules for thes POS categories and then switching to hiero is discouraged, since these contexts are more reliably handled by hiero due to better lexical coverage and common adjacency in nominal categories. Second, since there is only one attempt to switch from F2S derivation to hiero, letting F2S complete even larger spans (and maybe switching to hiero later) is favorable.","The tail of generalization feature weights is more difficult to interpret. The discount on VP encourages decoder to use F2S for entire verb phrases before switching to hiero, on the other hand, other verb-related preterminals occupy the tail as well, hurrying into early switching from F2S to hiero. 553","Finally, the feature weights tuned for the interchange experiment are divided into two subcolumns. Both generalization and specification weights show similar trends as in the previous two interaction schemes, although blurred (VP and IP descending from the absolute top). Since transitions in both ways are allowed, the search space is bigger and the system may behave differently. It is even possible for a path in the hypergraph to zigzag between F2S and hiero nodes to collect interaction dis-counts, “diluting” the syntactic homogeneity of the hypothesis.","Figure 5 and Tables 4 and 5 show rule distributions, total rule counts, and numbers of interactions of different types for the three interaction schemes on the GALE-web test set. The scope of phrase rules is limited to 6 words. The scope of hiero rules is limited to 20 words by the commonly used maxParse parameter, leaving longer spans to the glue rule.","The trends of F2S and glue rules show the most obvious difference. In the generalization, F2S rules translate spans of up to 50 words. Glue rules prevail on spans longer then 7 words. The specification is reversed, pushing the longest scope of hiero and glue rules down to 40 words, completing the longest sentences entirely with F2S. The interchange comes out as a mixture of the previous two trends.","All three schemes prefer using F2S rules at shorter spans, to the contrary of our original assump-tion of phrasal and hiero rules being stronger on local contexts in general. Here we may refer again to the specification feature weights for preterminals VV, NR, CC and P in Table 3 and to our previously stated hypothesis about the importance of preserving lexical and syntactic context.","Hiero rules usage on longer spans drops fastest for specification, slowest for generalization, and in between for interchange.","It is also interesting to notice the trends on very short spans (2–4 words) shown by rule distributions and reflected in numbers of interaction types. While specification often transitions from a single phrase rule directly into F2S, the interchange has relatively higher counts of hiero rules, another sign of the hiero and F2S interaction.","Synthesizing from several sources of indications is difficult, however, we arrive at the conclusion that joint decoding of hiero and F2S significantly improves the performance. While the single systems show similar performance, their roles are not balanced in joint decoding. It seems that the role of hiero consists in enabling F2S in most contexts.","We have focused on three special cases of interaction. We see a great potential in further studies of other schemes, allowing more flexible interaction than simple specification, but still more constrained than the interchange. It seems also promising to refine the interaction modeling with features taking into account more information than a single syntactic label, and to explore additional ways of parameter estimation."]},{"title":"7 Conclusion","paragraphs":["We have proposed flexible interaction of hypergraphs as a novel technique combining hiero and forest-to-string translation models within one decoder. We have explored three basic interaction schemes—specification, generalization, and interchange—and described soft constraints controlling the interactions. We have carried out experiments on large training data and with strong baselines. Of the three schemes, the specification shows the highest gains, achieving improvements from 0.5 to 0.9 (Ter-B leu)/2 points over the best single system. We have conducted a detailed analysis of each system output based on different indications of interactions, discussed possible interpretations of results, and finally offered our conclusion and proposed future lines of research."]},{"title":"Acknowledgments","paragraphs":["We thank Jiřı́ Havelka for proofreading and helpful suggestions. We would like to acknowledge the support of DARPA under Grant HR0011-12-C-0015 for funding part of this work. The views, opinions, and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the DARPA."]},{"title":"References","paragraphs":["David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of HLT-NAACL, pages 218–226. 554","David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270, Ann Arbor, Michigan, June.","Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.","John Denero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In In Proceedings NAACL-HLT, pages 975–983.","Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT-NAACL, pages 273–280.","Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL, pages 961–968, Sydney, Australia, July.","Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems. In Proceedings of EMNLP, pages 98–107, October.","Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of AMTA, pages 66–73.","Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL, pages 127–133.","Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proceedings of COLING-ACL, pages 609– 616.","Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009. Joint decoding with multiple translation models. In Proceedings of ACL-IJCNLP, pages 576–584, August.","Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP, pages 206–214.","Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL: HLT, pages 192–199.","Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL, pages 404–411.","Antti-Veikko Rosti, Spyros Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In Proceedings of ACL, pages 312–319, Prague, Czech Republic, June.","Taro Watanabe and Eiichiro Sumita. 2011. Machine translation system combination by confusion forest. In Proceedings of ACL 2011, pages 1249–1257.","Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan. 2009. Fast translation rule matching for syntax-based statistical machine translation. In Proceedings of EMNLP, pages 1037–1045, Singapore, August. 555"]}]}
