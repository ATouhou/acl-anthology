{"sections":[{"title":"","paragraphs":["Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 97–104, Vancouver, October 2005. c⃝2005 Association for Computational Linguistics"]},{"title":"A Large-Scale Exploration of Effective Global Features for a Joint Entity Detection and Tracking Model Hal Daum·e III and Daniel Marcu Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 hdaume,marcu @isi.edu Abstract","paragraphs":["Entity detection and tracking (EDT) is the task of identifying textual mentions of real-world entities in documents, extending the named entity detection and coreference resolution task by considering mentions other than names (pronouns, denite descriptions, etc.). Like NE tagging and coreference resolution, most solutions to the EDT task separate out the mention detection aspect from the coreference aspect. By doing so, these solutions are limited to using only local features for learning. In contrast, by modeling both aspects of the EDT task simultaneously, we are able to learn using highly complex, non-local features. We develop a new joint EDT model and explore the utility of many features, demonstrating their effectiveness on this task."]},{"title":"1 Introduction","paragraphs":["In many natural language applications, such as automatic document summarization, machine transla-tion, question answering and information retrieval, it is advantageous to pre-process text documents to identify references to entities. An entity, loosely dened, is a person, location, organization or geopolitical entity (GPE) that exists in the real world. Being able to identify references to real-world entities of these types is an important and difcult natural language processing problem. It involves nding text spans that correspond to an entity, identifying what type of entity it is (person, location, etc.), identifying what type of mention it is (name, nominal, pronoun, etc.) and nally identifying which other mentions in the document it corefers with. The difculty lies in the fact that there are often many ambiguous ways to refer to the same entity. For example, consider the two sentences below:","Bill ClintonNAM","PER1 gave a speech today to","the SenateNAM","ORG2 . The PresidentNOM","PER1 outlined","hisPRO PER1 plan for budget reform to themPRO","ORG2 .","There are ve entity mentions in these two sentences, each of which is underlined (the corresponding mention type and entity type appear as superscripts and subscripts, respectively, with coreference chains marked in the subscripts), but only two entities:","Bill Clinton, The president, his","and","the Senate, them",". The mention detection task is to identify the entity mentions and their types, without regard for the underlying entity sets, while coreference resolution groups a given mentions into sets.","Current state-of-the-art solutions to this problem split it into two parts: mention detection and coreference (Soon et al., 2001; Ng and Cardie, 2002; Florian et al., 2004). First, a model is run that attempts to identify each mention in a text and assign it a type (person, organization, etc.). Then, one holds these mentions xed and attempts to identify which ones refer to the same entity. This is typically accomplished through some form of clustering, with clustering weights often tuned through some local learning procedure. This pipelining scheme has the signicant drawback that the mention detection module cannot take advantage of information from the coreference module. Moreover, within the coreference 97 task, performing learning and clustering as separate tasks makes learning rather ad-hoc.","In this paper, we build a model that solves the mention detection and coreference problems in a simultaneous, joint manner. By doing so, we are able to obtain an empirically superior system as well as integrate a large collection of features that one cannot consider in the standard pipelined approach. Our ability to perform this modeling is based on the Learning as Search Optimization framework, which we review in Section 2. In Section 3, we describe our joint EDT model in terms of the search procedure executed. In Section 4, we describe the features we employ in this model; these include the standard lexical, semantic (WordNet) and string matching features found in most other systems. We additionally consider many other feature types, most interestingly count-based features, which take into account the distribution of entities and mentions (and are not expressible in the binary classication method for coreference) and knowledge-based features, which exploit large corpora for learning name-to-nominal references. In Section 5, we present our experimental results. First, we compare our joint system with a pipelined version of the system, and show that joint inference leads to improved performance. Next, we perform an extensive feature comparison experiment to determine which features are most useful for the coreference task, showing that our newly introduced features provide useful new information. We conclude in Section 6."]},{"title":"2 Learning as Search Optimization","paragraphs":["When one attempts to apply current, standard machine learning algorithms to problems with combinatorial structured outputs, the resulting algorithm implicitly assumes that it is possible to nd the best structures for a given input (and some model parameters). Furthermore, most models require much more, either in the form of feature expecta-tions for conditional likelihood-based methods (Lafferty et al., 2001) or local marginal distributions for margin-based methods (Taskar et al., 2003). In many casesincluding EDT and coreferencethis is a false assumption. Often, we are not able to nd the best solution, but rather must employ an approximate search to nd the best possible solution, given time and space constraints. The Learning as Search","Algo Learn(problem, initial, enqueue,",",",",",")","nodes MakeQueue(MakeNode(problem,initial))","while nodes is not empty do","node RemoveFront(nodes)","if none of nodes","node","is","-good or","GoalTest(node) and node is not","-good then","sibs siblings(node,",")"," update(",",",", sibs, node","nodes)","nodes","MakeQueue(sibs)","else","if GoalTest(node) then return","next Operators(node)","nodes","enqueue(problem, nodes, next,",")","end if","end while Figure 1: The generic search/learning algorithm. Optimization (LaSO) framework exploits this difculty as an opportunity and seeks to nd model parameters that are good within the context of search.","More formally, following the LaSO framework, we assume that there is a set of input structures and a set of output structures","(in our case, elements","will be documents and elements will be documents marked up with mentions and their coreference sets). Additionally, we provide the structure of a search space","that results in elements of","(we will discuss our choice for this component later in Section 3). The LaSO framework relies on a monotonicity assumption: given a structure and a node","in the search space, we must be able to calculate whether it is possible for this node","to eventually lead to","(such nodes are called","-good).","LaSO parameterizes the search process with a weight vector",", where weights correspond to features of search space nodes and inputs. Specifically, we write","as a function that takes a pair of an input","and a node in the search space","and produces a vector of features. LaSO takes a standard search algorithm and modies it to incorporate learning in an online manner to the algorithm shown in Figure 1. The key idea is to perform search as normal until a point at which it becomes impossible to reach the correct solution. When this happens, the weight vector","is updated in a corrective fashion. The algorithm relies on a parameter update formula; the two suggested by (Daum·e III and Marcu, 2005) are a standard Perceptron-style update and an approximate large margin update of the sort proposed by (Gentile, 2001). In this work, we only use the large margin update, since in the original LaSO work, it consistently outperformed the sim-98 pler Perceptron updates. The update has the form given below:"," proj"," proj "," sibs   sibs  "," nodes   nodes ","Where","is the update number,","is a tunable param-","eter and proj projects a vector into the unit sphere."]},{"title":"3 Joint EDT Model","paragraphs":["The LaSO framework essentially requires us to specify two components: the search space (and corresponding operations) and the features. These two are inherently tied, since the features rely on the search space, but for the time being we will ignore the issue of the feature functions and focus on the search. 3.1 Search Space We structure search in a left-to-right decoding framework: a hypothesis is a complete identication of the initial segment of a document. For in-stance, on a document with","words, a hypothesis that ends at position","is essentially what you would get if you took the full structured output and chopped it off at word",". In the example given in the introduction, one hypothesis might correspond to Bill Clinton gave a (which would be a","-good hypothesis), or to Bill Clinton gave a (which would not be a","-good hypothesis).","A hypothesis is expanded through the application of the search operations. In our case, the search procedure rst chooses the number of words it is going to consume (for instance, to form the mention Bill Clinton, it would need to consume two words). Then, it decides on an entity type and a mention type (or it opts to call this chunk not an entity (NAE), corresponding to non-underlined words). Finally, as-suming it did not choose to form an NAE, it decides on which of the foregoing coreference chains this entity belongs to, or none (if it is the rst mention of a new entity). All these decisions are made simultaneously, and the given hypothesis is then scored. 3.2 An Example For concreteness, consider again the text given in the introduction. Suppose that we are at the word them and the hypothesis we are expanding is correct. That is, we have correctly identied Bill Clinton with entity type person and mention type name; that we have identied the Senate with entity type organization and mention type name; and that we have identied both The President and his as entities with entity type person and mention types nominal and pronoun, respectively, and that The President points back to the chain","Bill Clinton and that his points back to the chain  Bill Clinton, The President",".","At this point of search, we have two choices for length: one or two (because there are only two words left: them and a period). A rst hypothesis would be that the word them is NAE. A second hypothesis would be that them is a named person and is a new entity; a third hypothesis would be that them is a named person and is coreference with the Bill Clinton chain; a fourth hypothesis would be that them is a pronominal organization and is a new entity; next, themcould be a pronominal organization that is coreferent with the Senate; and so on. Similar choices would be considered for the string them . when two words are selected. 3.3 Linkage Type One signicant issue that arises in the context of as-signing a hypothesis to a coreference chain is how to compute features over that chain. As we will discuss in Section 4, the majority of our coreference-specic features are over pairs of chunks: the proposed new mention and an antecedent. However, since in general a proposed mention can have well more than one antecedent, we are left with a decision about how to combine this information.","The rst, most obvious solution, is to essentially do nothing: simply compute the features over all pairs and add them up as usual. This method, how-ever, intuitively has the potential for over-counting the effects of large chains. To compensate for this, one might advocate the use of an average link computation, where the score for a coreference chain is computed by averaging over its elements. One might also consider a max link or min link scenario, where one of the extrema is chosen as the value. Other re-search has suggested that a simple last link, where a mention is simply matched against the most recent mention in a chain might be appropriate, while rst link might also be appropriate because the rst mention of an entity tends to carry the most information.","In addition to these standard linkages, we also 99 consider an intelligent link scenario, where the method of computing the link structure depends on the mention type. The intelligent link is computed as follow, based on the mention type of the current mention,",":","If","NAM then: match rst on NAM elements in the chain; if there are none, match against the last NOM element; otherwise, use max link.","If","NOM then: match against the max NOM in the chain; otherwise, match against the most last NAM; otherwise, use max link.","If","PRO then: use average link across all PRO or NAM; if there are none, use max link.","The construction of this methodology as guided by intuition (for instance, matching names against names is easy, and the rst name tends to be the most complete) and subsequently tuned by experimenta-tion on the development data. One might consider learning the best link method, and this may result in better performance, but we do not explore this op-tion in this work. The initial results we present will be based on using intelligent link, but we will also compare the different linkage types explicitly."]},{"title":"4 Feature Functions","paragraphs":["All the features we consider are of the form base-feature","decision-feature, where base features are functions of the input and decisions are functions of the hypothesis. For instance, a base feature might be something like the current chunk contains the word ’Clinton’ and a decision feature might be something like the current chunk is a named person. 4.1 Base Features For pedagogical purposes and to facility model comparisons, we have separated the base features into eleven classes: lexical, syntactic, pattern-based, count-based, semantic, knowledge-based, class-based, list-based, inference-based, string match features and history-based features. We will deal with each of these in turn. Finally, we will discuss how these base features are combined into meta-features that are actually used for prediction. Lexical features. The class of lexical features contains simply computable features of single words. This includes: the number of words in the current chunk; the unigrams (words) contained in this chunk; the bigrams; the two character prexes and sufxes; the word stem; the case of the word, computed by regular expressions like those given by (Bikel et al., 1999); simple morphological features (number, person and tense when applicable); and, in the case of coreference, pairs of features between the current mention and an antecedent. Syntactic features. The syntactic features are based on running an in-house state of the art part of speech tagger and syntactic chunker on the data. The words include unigrams and bigrams of part of speech as well as unigram chunk features. We have not used any parsing for this task. Pattern-based features. We have included a whole slew of features based on lexical and part of speech patterns surrounding the current word. These include: eight hand-written patterns for identifying pleonastic it and that (as in It is raining or It seems to be the case that . . . ); identication of pluralization features on the previous and next head nouns (this is intended to help make decisions about entity types); the previous and next content verb (also intended to help with entity type identication); the possessor or possessee in the case of simple possessive constructions (The president ’s speech would yield a feature of president on the word speech, and vice-versa; this is indented to be a sort of weak sub-categorization principle); a similar feature but applied to the previous and next content verbs (again to provide a weak sort of sub-categorization); and, for coreference, a list of part of speech and word sequence patterns that match up to four words between nearby mentions that are either highly indicative of coreference (e.g., of, said, am , a) or highly indicative of non-coreference (e.g., ’s, and, in the, and the). This last set was generated by looking at intervening strings and nding the top twenty that had maximal mutual information with with class (coreferent or not coreferent) across the training data. Count-based features. The count-based features apply only to the coreference task and attempt to capture regularities in the size and distribution of coreference chains. These include: the total number of entities detected thus far; the total number of mentions; the entity to mention ratio; the entity 100 to word ratio; the mention to word ratio; the size of the hypothesized entity chain; the ratio of the number of mentions in the current entity chain to the total number of mentions; the number of intervening mentions between the current mention and the last one in our chain; the number of intervening mentions of the same type; the number of intervening sentence breaks; the Hobbs distance computed over syntactic chunks; and the decayed density of the hypothesized entity, which is computed as     ",", where","ranges over all previous mentions (constrained in the numerator to be in the same coreference chain as our mention) and","is the number of entities away this mention is. This feature is captures that some entities are referred to consistently across a document, while others are mentioned only for short segments, but it is relatively rare for an entity to be mentioned once at the beginning and then ignored again until the end. Semantic features. The semantic features used are drawn from WordNet (Fellbaum, 1998). They include: the two most common synsets from WordNet for all the words in a chunk; all hypernyms of those synsets; for coreference, we also consider the distance in the WordNet graph between pairs of head words (dened to be the nal word in the mention name) and whether one is a part of the other. Finally, we include the synset and hypernym information of the preceding and following verbs, again to model a sort of sub-categorization principle. Knowledge-based features. Based on the hypothesis that many name to nominal coreference chains are best understood in terms of background knowledge (for instance, that George W. Bush is the President), we have attempted to take advantage of recent techniques from large scale data mining to extract lists of such pairs. In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman’s web page to generate features between names and nominals (this list contains","pairs mined from","GBs of news data). Since this data set tends to focus mostly on person instances from news, we have additionally used similar data mined from a","GB web corpus, for which more general ISA relations were mined (Ravichandran et al., 2005). Class-based features. The class-based features we employ are designed to get around the sparsity of data problem while simultaneously providing new information about word usage. The rst class-based feature we use is based on word classes derived from the web corpus mentioned earlier and computed as described by (Ravichandran et al., 2005). The second attempts to instill knowledge of collocations in the data; we use the technique described by (Dunning, 1993) to compute multi-word expressions and then mark words that are commonly used as such with a feature that expresses this fact. List-based features. We have gathered a collection of about 40 lists of common places, organization, names, etc. These include the standard lists of names gathered from census data and baby name books, as well as standard gazetteer information list-ing countries, cities, islands, ports, provinces and states. We supplement these standard lists with lists of airport locations (gathered from the FAA) and company names (mined from the NASDAQ and NYSE web pages). We additionally include lists of semantically plural but syntactically singular words (e.g., group) which were mined from a large corpus by looking for patterns such as (members of the . . . ). Finally, we use a list of persons, organizations and locations that were identied at least 100 times in a large corpus by the BBN IdentiFinder named entity tagger (Bikel et al., 1999).","These lists are used in three ways. First, we use simple list membership as a feature to improve detection performance. Second, for coreference, we look for word pairs that appear on the same list but are not identical (for instance, Russia and England appearing on the country list but not being identical hints that they are different entities). Finally, we look for pairs where one element in the pair is the head word from one mention and the other element in the pair is a list. This is intended to capture the notion that a word that appears on out country list is often coreferent with the word country. Inference-based features. The inference-based features are computed by attempting to infer an underlying semantic property of a given mention. In particular, we attempt to identify gender and semantic number (e.g., group is semantically plural although it is syntactically singular). To do so, we cre-101 ated a corpus of example mentions labels with number and gender, respectively. This data set was automatically extracted from our EDT data set by looking for words that corefer with pronouns for which we know the number or gender. For instance, a mention that corefers with she is known to be singular and female, while a mention that corefers with they is known to be plural. In about 5% of the cases, this was ambiguous these cases were thrown out. We then used essentially the same features as described above to build a maximum entropy model for predicting number and gender. The predictions of this model are used both as features for detection as well as coreference (in the latter case, we check for matches). Additionally, we use several pre-existing classiers as features. This are simple maximum entropy Markov models trained off of the MUC6 data, the MUC7 data and our ACE data. String match features. We use the standard string match features that are described in every other coreference paper. These are: string match; sub-string match; string overlap; pronoun match; and normalized edit distance. In addition, we also use a string nationality match, which matches, for in-stance Israel and Israeli, Russia and Russian, England and English, but not Netherlands and Dutch. This is done by checking for common sufxes on nationalities and matching the rst half of the of the words based on exact match. We additionally use a linguisticallymotivated string edit distance, where the replacement costs are lower for vowels and other easily confusable characters. We also use the Jaro distance as an additional string distance metric. Finally, we attempt to match acronyms by looking at initial letters from the words in long chunks. History-based features. Finally, for the detection phase of the task, we include features having to do with long-range dependencies between words. For instance, if at the beginning of the document we tagged the word Arafat as a person’s name (perhaps because it followed Mr. or Palestinian leader), and later in the document we again see the word Arafat, we should be more likely to call this a person’s name, again. Such features have previously been explored in the context of information extraction from meeting announcements using conditional random elds augmented with long-range links (Sutton and McCallum, 2004), but the LaSO framework makes no Markov assumption, so there is no extra effort required to include such features. 4.2 Decision Features Our decision features are divided into three classes: simple, coreference and boundary features. Simple. The simple decision features include: is this chunk tagged as an entity; what is its entity type; what is its entity subtype; what is its mention type; what is its entity type/mention type pair. Coreference. The coreference decision features include: is this entity the start of a chain or continuing an existing chain; what is the entity type of this started (or continued) chain; what is the entity subtype of this started (or continued) chain; what is the mention type of this started chain; what is the mention type of this continued chain and the mention type of the most recent antecedent. Boundary. The boundary decision features include: the second and third order Markov features over entity type, entity subtype and mention type; features appearing at the previous (and next) words within a window of three; the words that appear and the previous and next mention boundaries, specied also by entity type, entity subtype and mention type."]},{"title":"5 Experimental Results 5.1 Data","paragraphs":["We use the ofcial 2004 ACE training and test set for evaluation purposes; however, we exclude from the training set the Fisher conversations data, since this is very different from the other data sets and there is no Fisher data in the 2004 test set. This amounts to","training documents, consisting of ","sentences and","words. There are a total of","mentions in the data corresponding to entities (note that the data is not annotated for cross-document coreference, so instances of Bill Clinton appearing in two different documents are counted as two different entities). Roughly half of the entities are people, a fth are organizations, a fth are GPEs and the remaining are mostly locations or facilities. The test data is","documents,","sentences and ","words, with","mentions to","entities. In all cases, we use a beam of 16 for training and test, 102 and ignore features that occur fewer than ve times in the training data. 5.2 Evaluation Metrics There are many evaluation metrics possible for this data. We will use as our primary measure of quality the ACE metric. This is computed, roughly, by rst matching system mentions with reference mentions, then using those to match system entities with reference entities. There are costs, once this matching is complete, for type errors, false alarms and misses, which are combined together to give an ACE score, ranging from","to",", with","being perfect (we use v.10 of the ACE evaluation script). 5.3 Joint versus Pipelined We compare the performance of the joint system with the pipelined system. For the pipelined system, to build the mention detection module, we use the same technique as for the full system, but simply don’t include in the hypotheses the coreference chain information (essentially treating each mention as if it were in its own chain). For the stand-alone coreference system, we assume that the correct mentions and types are always given, and simply hypothesize the chain (though still in a left-to-right manner).1","Run as such, the joint model achieves an ACE score of","and the pipelined model achieves an ACE score of",", a reasonably substantial improvement for performing both task simultaneously. We have also computed the performance of these two systems, ignoring the coreference scores (this is done by considering each mention to be its own entity and recomputing the ACE score). In this case, the joint model, ignoring its coreference output, achieves an ACE score of","and the pipelined model achieves a score of",". The joint model","1","One subtle difculty with the joint model has to do with the online nature of the learning algorithm: at the beginning of training, the model is guessing randomly at what words are entities and what words are not entities. Because of the large number of initial errors made in this part of the task, the weights learned by the coreference model are initially very noisy. We experimented with two methods for compensating for this effect. The rst was to give the mention identication model as head start: it was run for one full pass through the training data, ignoring the coreference aspect and the following itera-tions were then trained jointly. The second method was to only update the coreference weights when the mention was identied correctly. On development data, the second was more efcient and outperformed the rst by","","ACE score, so we use this for the experiments reported in this section. Figure 2: Comparison of performance as different feature classes are removed. does marginally better, but it is unlikely to be statistically signicant. In the 2004 ACE evaluation, the best three performing systems achieved scores of",",","and","; it is unlikely that our system is signicantly worse than these. 5.4 Feature Comparison for Coreference In this section, we analyze the effects of the different base feature types on coreference performance. We use a model with perfect mentions, entity types and mention types (with the exception of pronouns: we do not assume we know pronoun types, since this gives away too much information), and measure the performance of the coreference system. When run with the full feature set, the model achieves an ACE score of","and when run with no added features beyond simple biases, it achieves",". The best performing system in the 2004 ACE competi-tion achieved a score of","on this task; the next best system scored",", which puts us squarely in the middle of these two (though, likely not statistically signicantly different). Moreover, the best performing system took advantage of additional data that they labeled in house.","To compute feature performance, we begin with all feature types and iteratively remove them one-by-one so that we get the best performance (we do not include the history features, since these are not relevant to the coreference task). The results are shown in Figure 2. Across the top line, we list the ten feature classes. The rst row of results shows the performance of the system after removing just 103 one feature class. In this case, removing lexical features reduces performance to",", while removing string-match features reduces performance to",". The non-shaded box (in this case, syntactic features) shows the feature set that can be removed with the least penalty in performance. The second row repeats this, after removing syntactic features.","As we can see from this gure, we can freely remove syntax, semantics and classes with little decrease in performance. From that point, patterns are dropped, followed by lists and inference, each with a performance drop of about","or",". Removing the knowledge based features results in a large drop from","down to","and removing count-based features drops the performance another","points. Based on this, we can easily conclude that the most important feature classes to the coreference problem are, in order, string matching features, lexical features, count features and knowledge-based features, the latter two of which are novel to this work. 5.5 Linkage Types As stated in the previous section, the coreference-only task with intelligent link achieves an ACE score of",". The next best score is with min link (",") followed by average link with a score of",". There is then a rather large drop with max link to",", followed by another drop for last link to","and rst link performs the poorest, scoring","."]},{"title":"6 Discussion","paragraphs":["In this paper, we have applied the Learning as Search Optimization (LaSO) framework to the entity detection and tracking task. The framework is an excellent choice for this problem, due to the fact that many relevant features for the coreference task (and even for the mention detection task) are highly non-local. This non-locality makes models like Markov networks intractable, and LaSO provides an excellent framework for tackling this problem. We have introduced a large set of new, useful features for this task, most specically the use of knowledge-based features for helping with the name-to-nominal problem, which has led to a substantial improvement in performance. We have shown that performing joint learning for mention detection and coreference results in a better performing model that pipelined learning. We have also provided a comparison of the contributions of our various feature classes and compared different linkage types for coreference chains. In the process, we have developed an efcient model that is competitive with the best ACE systems.","Despite these successes, our model is not perfect: the largest source of error is with pronouns. This is masked by the fact that the ACE metric weights pronouns low, but a solution to the EDT problem should handle pronouns well. We intend to explore more complex features for resolving pronouns, and to incorporate these features into our current model. We also intend to explore more complex models for automatically extracting knowledge from data that can help with this task and applying this technique to a real application, such as summarization. Acknowledgments: We thank three anonymous reviewers for helpful comments. This work was supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276."]},{"title":"References","paragraphs":["D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34.","H. Daum·e III and D. Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In ICML.","T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1).","C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge, MA.","M. Fleischman, E. Hovy, and A. Echihabi. 2003. Ofine strategies for online question answering: Answering questions be-fore they are asked. In ACL.","R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A statistical model for multilingual entity detection and tracking. In NAACL/HLT.","C. Gentile. 2001. A new approximate maximal margin classication algorithm. JMLR, 2:213242.","J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random elds: Probabilistic models for segmenting and labeling sequence data. In ICML.","V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In ACL.","D. Ravichandran, P. Pantel, and E. Hovy. 2005. Randomized algorithms and NLP: Using locality sensitive hash functions for high speed noun clustering. In ACL.","W. Soon, H. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521 544.","C. Sutton and A. McCallum. 2004. Collective segmentation and labeling of distant entities in information extraction. In ICML workshop on Statistical Relational Learning.","B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In NIPS. 104"]}]}