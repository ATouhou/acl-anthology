{"sections":[{"title":"","paragraphs":["Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 658–667, Singapore, 6-7 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Can Chinese Phonemes Improve Machine Transliteration?:A Comparative Study of English-to-Chinese Transliteration ModelsJong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro TorisawaLanguage Infrastructure Group, MASTAR Project,National Institute of Information and Communications Technology (NICT)3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan{rovellia,uchimoto,torisawa}@nict.go.jpAbstract","paragraphs":["Inspired by the success of English grapheme-to-phoneme research in speech synthesis, many researchers have proposed phoneme-based English-to-Chinese transliteration models. However, such approaches have severely suffered from the errors in Chinese phoneme-to-grapheme conversion. To address this issue, we propose a new English-to-Chinese transliteration model and make system-atic comparisons with the conventional models. Our proposed model relies on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration."]},{"title":"1 Introduction1.1 Motivation","paragraphs":["Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of English-to-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007).","Inspired by the success of English grapheme-to-phoneme research in speech synthesis, many researchers have proposed phoneme-based English-to-Chinese transliteration models. In these approaches, Chinese phonemes are generated from English graphemes or phonemes, and then the Chinese phonemes are converted into Chinese graphemes (or characters), where Chinese Pinyin strings1","are used for representing a syllable-level Chinese phoneme sequence. Despite its high accuracy in generating Chinese phonemes from English, this approach has severely suffered from errors in Chinese phoneme-to-grapheme conversion, mainly caused by Chinese homophone confusion – one Chinese Pinyin string can correspond to several Chinese characters (Li et al., 2004). For example, the Pinyin string “LI” corresponds to such different Chinese characters as , , and .For this reason, it has been reported that English-to-Chinese transliteration without Chinese phonemes outperforms that with Chinese phonemes (Li et al., 2004).","Then “Can Chinese phonemes improve English-to-Chinese transliteration, if we can reduce the errors in Chinese phoneme-to-grapheme conversion?” Our research starts from this question. 1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chinese phoneme-to-grapheme conversion. However, the simple use of Chinese phonemes doesn’t always provide a good clue to reduce the ambiguity in Chinese phoneme-to-grapheme conversion. Let us explain with an example, the Chinese transliteration of Greeley in Table 1, where Chinese phonemes are represented in terms of Chinese Pinyin strings and English phonemes are represented by ARPAbet symbols2",".","In Table 1, Chinese Pinyin string “LI” corresponds to two different Chinese characters, and 1 Pinyin, the most commonly used Romanization sys-","tem for Chinese characters, faithfully represents Chinese 658 Table 1: Chinese Pinyin string “LI” and its corresponding Chinese characters in Chinese transliteration of Greeley English grapheme g ree ley English phoneme G RIY LIY Chinese Pinyin GE LI LI Chinese character    ",". It seems difficult to find evidence for select-ing the correct Chinese character corresponding to each Chinese Pinyin string “LI” by just looking at the sequence of Chinese Pinyin strings “GE LI LI.” However, English graphemes (ree and ley)or phonemes (“R IY” and “L IY”) corresponding to Chinese Pinyin string “LI”, especially their consonant parts (r and l in the English graphemes and “R” and “L” in the English phonemes), provide strong evidence to resolve the ambiguity. Thus, we can easily find rules for the conversion from Chinese Pinyin string “LI” to and as follows: •⟨ “R IY”, LI ⟩→  •⟨ “L IY”, LI ⟩→ ","Based on the observation, we propose an English-to-Chinese transliteration model based on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. We define a set of English-to-Chinese transliteration models and categorize them into the following three classes: • MI:","Models Independent of Chinese phonemes","• MS: Models based on Simple use of Chinese phonemes","• MJ: Models based on Joint use of Chinese phonemes and English graphemes and phonemes that correspond to our proposed model. Our comparison among the three types of transliteration models can be summarized as follows.","• The MI models relying on either English graphemes or phonemes could not outperform those based on both English graphemes and phonemes. phonemes and syllables (Yin and Felley, 1990). 2","http://www.cs.cmu.edu/l̃aura/pages/ arpabet.ps","• The MS models always showed the worst performance due to the severe error rate in Chinese phoneme-to-grapheme conversion.","• The MJ models significantly reduced errors in Chinese phoneme-to-grapheme conversion; thus they achieved the best performance.","The rest of this paper is organized as follows. Section 2 introduces the notations used through-out this paper. Section 3 describes the transliteration models we compared. Section 4 describes our tests and results. Section 5 concludes the paper with a summary."]},{"title":"2 Preliminaries","paragraphs":["Let EG be an English word composed of n English graphemes, and let EP be a sequence of English phonemes that represents the pronunciation of EG. Let CG be a sequence of Chinese graphemes corresponding to the Chinese transliteration of EG, and let CP be a sequence of Chinese phonemes that represents the pronunciation of CG.","CP corresponds to a sequence of the Chinese Pinyin strings of CG. Because a Chinese Pinyin string represents the pronunciation of a syllable consisting of consonants and vowels, we divide a Chinese Pinyin string into consonant and vowel parts like “L+I”, “L+I+N”, and “SH+A.” In this paper, we define a Chinese phoneme as the vowel and consonant parts in a Chinese Pinyin string (e.g., “L”, “SH”, and “I”). A Chinese character usually corresponds to multiple English graphemes, English phonemes, and Chinese phonemes (i.e., corresponds to English graphemes ree, English phonemes “R IY”, and Chinese phonemes “L I” in Table 1). To represent these many-to-one correspondences, we use the well-known BIO labeling scheme to represent a Chinese character, where B and I represent the beginning and inside/end of the Chinese characters, respectively, and O is not used. Each Chinese phoneme corresponds to a Chinese character with B and I labels. For example, Chinese character “” in Table 1 can be represented as “:B” and “:I”, where “:B” and “:I” correspond to Chinese phonemes “L” and “I”, respectively. In this paper, we define a Chinese grapheme as a Chinese character represented with a BIO label, e.g., “:B” and “:I.” 659 Table 2: egi and its corresponding epi, cpi, and cgi in Greeley and its corresponding Chinese transliteration “” i 1 2 3 4 5 6 7 EG g r e e l e y EP G R IY φ L IY φ CP GE L I φ L I φ GE LI φ LI φ CG  :B  :B  :I φ  :B  :I φ   φ  φ","Then EP , CP , and CG can be segmented into a series of sub-strings, each of which corresponds to an English grapheme in EG. We can thus write","• EG = eg1, ··· ,egn = egn 1","• EP = ep1, ··· ,epn = epn 1","• CP = cp1, ··· ,cpn = cpn 1","• CG = cg1, ··· ,cgn = cgn 1 where egi, epi, cpi, and cgi represent the ith English grapheme, English phonemes, Chinese phonemes, and Chinese graphemes corresponding to egi, respectively.","Based on the definition, we model English-to-Chinese transliteration so that each English grapheme is tagged with its corresponding English phonemes, Chinese phonemes, and Chinese graphemes. Table 2 illustrates egi, epi, cpi, and cgi with the same example listed in Table 1 (English word Greeley and its corresponding Chinese transliteration “”)3",", where φ represents an empty string."]},{"title":"3 Transliteration Model","paragraphs":["We defined eighteen transliteration models to be compared. These transliteration models are classified into three classes, MI, MS, and MJ as described in Section 1.2; each class has three basic transliteration models and three hybrid ones. In this section, we first describe the basic transliteration models in each class by focusing on the main difference among the three classes and then describe the hybrid transliteration models.","3","We performed alignment between EG and EP and between EP and CP in a similar manner presented in Li et al. (2004). Then the two alignment results were merged using EP as a pivot. Finally, we made a correspondence relation among egi, epi, cpi, and cgi using the merged alignment result and the Pinyin table. 3.1 Basic Transliteration Models The basic transliteration models in each class are denoted as M (x, y). • (x, y) ∈ X × Y • x ∈ X = {EG,EP ,EGP } • y ∈ Y = {φ, C P ,JCP } x is an English-side parameter representing English grapheme (EG), English phoneme (EP ), and the joint use of English grapheme and phoneme (EGP = ⟨ EG,EP ⟩ ) that contributes to generating Chinese phonemes or Chinese graphemes in a transliteration model. y is a Chinese-phoneme parameter that represents a way of using Chinese phonemes to generate Chinese graphemes in a transliteration model. Since M (x, φ ) represents a transliteration model that does not rely on Chinese phonemes, it falls into MI, while M (x, CP ) corresponds to a transliteration model in MS that only uses Chinese phonemes in Chinese phoneme-to-grapheme conversion. M (x, J CP ) is a transliteration model in the MJ class that generates Chinese transliterations based on joint use of x and Chinese phoneme CP , where x ∈ X. Thus, M (x, J CP ) can be rewritten as M (x, ⟨ x, CP ⟩ ), where the joint representation of x and CP , ⟨ x, CP ⟩ , is used in Chinese phoneme-to-grapheme conversion. The three basic models in MJ can be interpreted as follows: • M (EG,JCP ) = M (EG, ⟨ EG,CP ⟩ ) • M (EP ,JCP ) = M (EP , ⟨ EP ,CP ⟩ ) • M (EGP ,JCP ) = M (EGP , ⟨ EGP ,CP ⟩ ) M (EG,JCP ) directly converts English graphemes into Chinese phonemes without the help of English phonemes and then generates Chinese transliterations based on the joint representation of English graphemes and Chinese phonemes. The main difference between M (EP ,JCP ) and M (EGP ,JCP ) lies in the use of English graphemes to generate Chinese phonemes and graphemes. English graphemes are only used in English grapheme-to-phoneme conversion, and English phonemes play a crucial role for generating Chinese transliteration in M (EP ,JCP ). Chinese phoneme-to-grapheme conversion that relies on the joint use of English graphemes, English phonemes, and Chinese 660","PM(EG,JCP )(CG|EG)=∑ ∀ CP P (CP |EG) × P (CG|EG,CP ) (1)","PM(EP ,JCP )(CG|EG)=∑ ∀ CP ∑","∀ EP P (EP |EG) × P (CP |EP ) × P (CG|EP ,CP ) (2)","PM(EGP ,JCP )(CG|EG)=∑ ∀ CP ∑","∀ EP P (EP |EG) × P (CP |EG,EP ) × P (CG|EG,EP ,CP ) (3)","PM(EG,CP )(CG|EG)=∑ ∀ CP P (CP |EG) × P (CG|CP ) (4)","PM(EP ,CP )(CG|EG)=∑ ∀ CP ∑","∀ EP P (EP |EG) × P (CP |EP ) × P (CG|CP ) (5)","PM(EGP ,CP )(CG|EG)=∑ ∀ CP ∑","∀ EP P (EP |EG) × P (CP |EG,EP ) × P (CG|CP ) (6) phonemes is the key feature of M (EGP ,JCP ). Because M (x, J CP ) can be interpreted as M (x, ⟨ x, CP ⟩ ), English-side parameter x determines the English graphemes and phonemes, or both jointly used with Chinese phonemes in Chinese phoneme-to-grapheme conversion. Then we can represent the three basic transliteration models as in Eqs. (1)–(3), where P (CG|EG,CP ), P (CG|EP ,CP ), and P (CG|EG,EP ,CP ) are the key points in our proposed models, MJ.","The three basic transliteration models in MS – M (EG,CP ), M (EP ,CP ), and M (EGP ,CP ) – are formulated as Eqs. (4)–(6). Chinese phoneme-based transliteration models in the literature fall into either M (EG,CP ) or M (EP ,CP ) (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ, except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG|CP ) in Eqs. (4)–(6). PM(EG,φ )(CG|EG)=P (CG|EG) (7) PM(EP ,φ )(CG|EG) (8)","= ∑ ∀ EP P (EP |EG) × P (CG|EP ) PM(EGP ,φ )(CG|EG) (9)","= ∑ ∀ EP P (EP |EG) × P (CG|EG,EP ) The three basic transliteration models in MI are represented in Eqs. (7)–(9). Because the MI models are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG,φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 <α< 1. We denote a hybrid transliteration model between two basic transliteration models M (x1,y) and M (x2,y) as M (x1 + x2,y,α), where y ∈ Y = {φ, C P ,JCP }, x1 ̸= x2, and x1,x2 ∈ X = {EG,EP ,EGP }. In this paper, we define three types of hybrid transliteration models in each class: M (EG + EP ,y,α), M (EG + EGP ,y,α), and M (EP + EGP ,y,α). PM(x1+x2,y,α )(CG|EG) (10) = α × PM(x1,y)(CG|EG) +(1− α ) × PM(x2,y)(CG|EG) 3.3 Probability Estimation Because Eqs. (1)–(9) can be estimated in a similar way, we limit our focus to Eq. (3) in this section. Assuming that P (EP |EG), P (CP |EG,EP ), and P (CG|EG,EP ,CP ) in Eq. (3) depend on the size of the context window, k (k =3in this paper), 661","Table 3: Feature functions for P (cgi|cgi−1","i−k, ⟨ eg, ep, cp⟩ i+k","i−k) with an example in Table 2, where i =2","f1 gram3(egi) egi+2","i = “ree” cgi =“:B”","f2 pair11(cpi−1,cgi−1) cpi−1 = “G”, cgi−1 =“:B” cgi =“:B”","f3 pair12(cgi−1,cpi−1) cpi","i−1 = “GE L”, cgi−1=“:B” cgi =“:B”","f4 pair22(cpi−1,cgi−2) egi","i−1 = “gr”, epi","i−1 =“GR” cgi =“:B”","f5 triple1(egi,cpi,cgi−1) egi = “r”, cpi−1 = “GE”, cgi−1=“:B” cgi =“:B”","f6 triple2(egi−1,cgi−1,cpi−1) egi−1 = “g”, cpi","i−1= “GE L”, cgi−1=“:B” cgi =“:B” they can be simplified into a series of products in Eqs. (11)–(13).","The maximum entropy model is used to estimate the probabilities in Eqs. (11)–(13) (Berger et al., 1996). Generally, a conditional maximum entropy model is an exponential model that gives the conditional probability, as described in Eq. (14), where λ i is the parameter to be estimated and fi(a, b) is a feature function corresponding to λ i (Berger et al., 1996; Ratnaparkhi, 1997): P (EP |EG) ≈ ∏","i P (epi|epi−1 i−k,egi+k","i−k ) (11) P (CP |EG,EP ) (12) ≈ ∏","i P (cpi|cpi−1 i−k, ⟨ eg, ep⟩ i+k","i−k) P (CG|EG,EP ,CP ) (13) ≈ ∏","i P (cgi|cgi−1 i−k, ⟨ eg, ep, cp⟩ i+k","i−k) P (b|a)=","exp(∑","i λ ifi(a, b))","∑","b′ exp(∑","i λ ifi(a, b′",")) (14) fi(a, b) is a binary function returning TRUE or FALSE based on context a and output b. If fi(a, b)=1, its corresponding model parameter λ i contributes toward conditional probability P (b|a) (Berger et al., 1996; Ratnaparkhi, 1997). The feature functions used here are defined in terms of context predicates — a function returning TRUE or FALSE that depends on the presence of the information in the current context (Ratnaparkhi, 1997). Context predicates and their descriptions used are given in Table 4.","N-GRAM includes gram1(uj), gram2(uj), and gram3(uj) corresponding to a unigram, a bigram, and a trigram, respectively. PAIR includes a pair of unigrams (pair11), unigram and bigram (pair12), and bigrams (pair22). TRIPLE includes a triple of three unigrams (triple1) and a triple of two unigrams and one bigram (triple2). Note that if different context predicates represent the same context, we accept one of them and ignore the others Table 4: Context predicates and their descriptions Category Context predicates Description N-GRAM gram1(uj) uj gram2(uj) uj+1 j gram3(uj) uj+2 j PAIR pair11(uj,vk) uj, vk pair12(uj,vk)","uj, vk+1 k pair22(uj,vk) uj+1 j , vk+1","k TRIPLE triple1(uj,vk,wl) uj, vk, wl triple2(uj,vk,wl)","uj, vk, wl+1 l (e.g., pair12(uj,uj+1) = trigram(uj) = uj+2","j ). Table 3 represents the examples of feature functions for P (cgi|cgi−1","i−k, ⟨ eg, ep, cp⟩ i+k","i−k).","We used the “Maximum Entropy Modeling Toolkit”4","to estimate the probabilities and the LBFGS algorithm to find λ i in Eq. (14). For each transliteration model, we produced n-best transliterations using a stack decoder (Schwartz and Chow, 1990). 3.4 Summary In this paper, we defined eighteen transliteration models to be compared. There are six transliteration models, three basic and three hybrid ones, in each class, MI, MS, and MJ. We compared the transliteration models from the viewpoint of Chinese phonemes or the class of transliteration models in our experiments."]},{"title":"4 Testing and Results","paragraphs":["We used the same test set used in Li et al. (2004) for our testing5",". It contains 37,694 pairs of English words and their official Chinese transliterations 4 Available at http://homepages.inf.ed.ac.","uk/s0450736/maxent_toolkit.html 5 This test set was also used in “NEWS09 machine translit-","eration shared task” for English-to-Chinese transliteration (Li","et al., 2009) 662 extracted from the “Chinese Transliteration of For-eign Personal Names” (Xinhua News Agency, 1992), which includes names in English, French, German, and many other foreign languages (Li et al., 2004). We used the same test data as in Li et al. (2004). But we randomly selected 90% of the training data used in Li et al. (2004) as our training data and the remainder as the development data, as shown in Table 5. Table 5: Number of English-Chinese transliteration pairs in each data set","Ours Li et al. (2004) Training data 31,299 34,777 Development data 3,478 N/A Blind test data 2,896 2,896","We used the training data for training the transliteration models. For each model, we tuned the parameters including the number of iterations for training the maximum entropy model and a Gaussian prior for smoothing the maximum entropy model using the development data. Further, the development data was used to select parameter α of the hybrid transliteration models. We varied parameter α from 0 to 1 in 0.1 intervals (i.e., α =0, 0.1, 0.2, ···,1) and tested the performance of the hybrid models with the development data. Then we chose α that showed the best performance in each hybrid model. The blind test data was used for evaluating the performance of each transliteration model. The CMU Pronounc-ing Dictionary6",", which contains about 120,000 English words and their pronunciations, was used for estimating P (EP |EG).","We conducted two experiments. First, we compared the overall performance of the transliteration models. Second, we investigated the effect of training data size on the performance of each transliteration model.","The evaluation was done for word accuracy in top-1 (ACC), Chinese pronunciation accuracy (CPA) and a mean reciprocal rank (MRR) metric (Kantor and Voorhees, 2000; Li et al., 2009; Chang et al., 2009). ACC measures how many correct transliterations appeared in the top-1 result of each system. CPA measures the Chinese pronunciation accuracy in the top-1 of the n-best Chinese pronunciation. We used CPA for com-6 Available at http://www.speech.cs.cmu.edu/","cgi-bin/cmudict paring the performance between systems based on Chinese phonemes. MRR, mean reciprocal ranks of n-best results of each system over the test entries, is an evaluation measure for n-best transliterations. If a transliteration generated by a system matches a reference transliteration7","at the rth","posi-tion of the n-best results, its reciprocal rank equals 1/r ; otherwise its reciprocal rank equals 0, where 1 ≤ r ≤ n. We produced 10-best Chinese transliterations for each English word in our experiments. 4.1 Comparison of the Overall Performance Table 6 represents the overall performance of one system in a previous work (Li et al., 2004) and eighteen systems based on the transliteration models defined in this paper. ACC, MRR, and CPA represent the evaluation results for each model trained by our training data. To test transliteration models without the errors introduced by incorrect Chinese phonemes, we carried out the experiments with the correct Chinese pronunciation (or the correct Chinese phoneme sequence) in Chinese phoneme-to-grapheme conversion. In the experiment, we put the correct Chinese pronunciation into the top-1 of the n-best Chinese pronunciation with the highest probability, say P (CP |EG)=1; thus CPA was assumed to be 100%. The ACC of the transliteration models under this condition is denoted as ACC’ in Table 6. TRAIN represents the evaluation results of the transliteration models trained by our training data. To compare Li et al. (2004) and transliteration models defined in this paper under the same condition, we also carried out experiments with the same training data in Li et al. (2004). Since the training data used in Li et al. (2004) is identical as the union of our training and development data, we denoted it as TRAIN+DEV in Table 6. In both TRAIN and TRAIN+DEV, we used the same parameter setting that was obtained by using the development data.","LI04 represents a system in Li et al. (2004), and its ACC’ in TRAIN+DEV is taken from the literature. The systems based on the transliteration models defined in our paper are represented from the second row in Table 6. The phoneme-based transliteration models in the literature correspond to either M (EG,CP ) (Wan and Verspoor, 1998; Lee and Chang, 2003; Jiang et al., 2007) or M (EP ,CP ) (Meng et al., 2001; Gao et al., 2004; 7 In our test set, an English word corresponds to one refer-","ence Chinese transliteration. 663 Table 6: Comparison of the overall performance Class Model TRAIN TRAIN+DEV ACC MRR CPA ACC’ ACC MRR CPA ACC’ LI04 N/A N/A N/A N/A 70.1 N/A N/A N/A M (EG,JCP ) 71.9 80.4 72.3 88.2 72.3 80.7 73.1 88.9 M (EP ,JCP ) 61.1 70.3 62.4 82.8 61.1 70.6 63.1 83.8 MJ M (EGP ,JCP ) 72.3 80.9 73.2 89.6 73.5 81.5 73.9 90.4 M (EG+EP ,JCP , 0.7) 72.8 80.7 73.8 89.7 73.2 81.0 74.7 90.5 M (EG+EGP ,JCP , 0.6) 73.5 81.7 74.2 90.6 73.7 81.8 74.8 91.2 M (EP +EGP ,JCP , 0.1) 71.6 80.3 73.3 89.8 72.5 80.8 73.8 90.1 M (EG,φ) 70.0 78.5 N/A N/A 70.6 79.0 N/A N/A M (EP ,φ) 58.5 69.3 N/A N/A 59.4 70.1 N/A N/A MI M (EGP ,φ) 71.2 79.9 N/A N/A 72.3 80.7 N/A N/A M (EG+EP ,φ, 0.7) 70.7 79.1 N/A N/A 72.0 80.0 N/A N/A M (EG+EGP ,φ, 0.4) 72.0 80.3 N/A N/A 72.8 80.9 N/A N/A M (EP +EGP ,φ, 0.1) 71.0 79.6 N/A N/A 72.0 80.4 N/A N/A M (EG,CP ) 58.9 70.2 72.3 78.4 59.1 70.4 73.1 78.4 M (EP ,CP ) 50.2 62.3 62.4 78.4 50.4 62.6 63.1 78.5 MS M (EGP ,CP ) 59.1 70.4 73.2 78.4 59.3 70.5 73.9 78.5 M (EG+EP ,CP , 0.8) 59.7 71.3 73.8 79.0 60.3 71.7 74.7 79.0 M (EG+EGP ,CP , 0.6) 59.8 71.7 74.2 78.9 60.6 72.1 74.8 78.9 M (EP +EGP ,CP , 0.1) 58.8 70.4 73.3 78.9 59.4 70.7 73.8 78.8 Virga and Khudanpur, 2003).","A comparison between the basic and hybrid transliteration models showed that the hybrid ones usually performed better (the exception was M (EP +EGP ,y,α) but the performance still comparable to the basic ones in each class). Especially, the hybrid ones based on the best two basic transliteration models, M (EG+EGP ,y,α), showed the best performance.","A comparison among the MI, MS, and MJ models showed that Chinese phonemes did contribute to the performance improvement of English-to-Chinese transliteration when Chinese phonemes were used together with their corresponding English graphemes and phonemes in Chinese phoneme-to-grapheme conversion. A one-tail paired t-test between the MI and MJ models showed that the results of the MJ models were always significantly better than those of the MI models if the MI and MJ models shared the same English-side parameter, x ∈ {EG,EP ,EGP } (level of significance = 0.001). In the results obtained by the MS and MJ models, the figures in CPA are the same when the MS and our MJ models share the same English-side parameter. Moreover, the difference between the figures in ACC and CPA can be interpreted as the error rate of Chinese phoneme-to-grapheme conversion. Our proposed MJ models generated Chinese transliterations with a very low error rate in Chinese phoneme-to-grapheme conversion, while the MS models suffered from a significant error rate in Chinese phoneme-to-grapheme conversion. ACC’ showed that the MJ models still outperformed the MS models even without errors in generating Chinese pronunciation from the English words. These results indicate that the joint use of Chinese phonemes and their corresponding English graphemes and phonemes significantly improved the performance in Chinese phoneme-to-grapheme conversion and English-to-Chinese transliteration.","Table 7 shows the Chinese transliterations generated by M (EG,φ), M (EGP ,φ), M (EG,JCP ), and M (EGP ,JCP ) where English or Chinese phonemes contributed to the correct transliteration. In this table, the first column show the English words and their English phonemes, and the second and third columns represent the Chinese transliterations and their phonemes. Note that the Chinese phonemes in the second and third columns of the MI models are not used in transliteration. They are shown in the table to indicate the difference in the Chinese phonemes of Chinese 664 Table 7: Top-1 results of M (EG,φ), M (EGP ,φ), M (EG,JCP ), and M (EGP ,JCP ), where * represents incorrect transliterations M(EGP,JCP) M(EG,JCP) MJ models   * (LAI YIN HA TE)   * (LAI YIN HA TE) Reinhardt (R AI N HH AA R T)  (AI WEI) ","* (YI WEI) Ivy (AY V IY)   * (AI MI LI)  * (AI MI LI) Emily (EH M IH L IY)   LAI YIN HA TE   LAI YIN HA TE Reinhardt (R AI N HH AA R T)  AI WEI ","* YI WEI Ivy (AY V IY)   AI MI LI  AI MI LI Emily (EH M IH L IY) M(EGP,) M(EG,) MI models transliterations between the MI and MJ models.","For Emily and Reinhardt, the MJ models generated correct Chinese transliterations, but the MI models did not. Figure 1 shows the probability distribution when a transliteration model generates the first Chinese character in the Chinese transliteration of Reinhardt with and without Chinese phonemes. Two Chinese characters, and , were strong candidates and is the correct one in this case. Without Chinese phonemes, M (EG,φ), which is based on P(cg|Reinhardt) in Figure 1(a) preferring to , generated the incorrect transliteration as shown in Table 7. However, Figure 1(b) shows that can be selected if the correct Chinese phoneme sequence “LAI YIN ...” is given. Three Chinese phoneme sequences starting with “LAI YIN ...”, “LAI NA ...”, and “LAI NEI ...” were generated from Reinhardt, where “LAI YIN ...” was the best Chinese phoneme sequence based on the probability distribution in Figure 1(c). As a result, M (EG,JCP ), which jointly used Chinese phonemes with English graphemes, generated the correct Chinese transliteration of Reinhardt based on two probability distribution in Figures 1(b) and 1(c). In the case of Ivy, English phonemes contributed to generating the correct transliteration in the M (EGP ,φ) and M (EGP ,JCP ) models.","Chinese transliterations sometimes reflect the English word’s pronunciation as well as the Chinese character’s meaning (Li et al., 2007). Li 0 0.2 0.4 0.6 0.8 P(|Reinhardt) P(|Reinhardt) (a) Probability distribution when Chinese phonemes are not given 00.20.40.60.81   P(cg|Reinhardt, \"LAI YIN ..\") P(cg|Reinhardt, \"LAI NA ..\") P(cg|Reinhardt, \"LAI NEI ..\") (b) Probability distribution when Chinese phonemes are given 0 0.20.4 0.6 0.8 1 P(\"LAI YIN ..\"|Reinhardt) P(¬\"LAI YIN ..\"|Reinhardt) (c) Probability distribution for Chinese phoneme sequence “LAI YIN ...” and others Figure 1: Probability distribution for the first Chinese character in the Chinese transliteration of Reinhardt: M (EG,φ) vs. M (EG,JCP ) et al. (2007) defined such a Chinese transliteration as a phonetic-semantic transliteration (semantic transliteration) to distinguish it from a usual phonetic transliteration. One fact that affects semantic transliteration is gender association (Li et al., 2007). For example, (meaining jasmine) is frequently used in Chinese transliterations of female names but seldom in common person names. Because Emily is often used in female names, the results obtained by the M (EG,JCP ) and M (EGP ,JCP ) models are acceptable. This indicates that Chinese phonemes coupled with English graphemes or those coupled with English graphemes and phonemes could provide evidence required for semantic transliteration as well as phonetic transliteration. As a result, M (EGP ,φ), M (EG,JCP ), 665 and M (EGP ,JCP ), which used phonemes coupled with English graphemes, achieved higher performance than M (EG,φ), which relied only on English graphemes. 4.2 Effect of Training Data Size 80 70 60 50 40 30 20 80 60 40 20 MRR Training Data Size (%) M(EG,φ) M(EP,φ) M(EGP,φ) M(EG,CP) M(EP,CP) M(EGP,CP) M(EG,JCP) M(EP,JCP) M(EGP,JCP) (a) Basic transliteration models 80 70 60 50 40 30 80 60 40 20 MRR Training Data Size (%)","M(EG+EP,φ,0.7) M(EG+EGP,φ,0.4) M(EP+EGP,φ,0.1) M(EG+EP,CP,0.8) M(EG+EGP,CP,0.6) M(EP+EGP,CP,0.1) M(EG+EP,JCP,0.7) M(EG+EGP,JCP,0.6) M(EP+EGP,JCP,0.1) (b) Hybrid transliteration models Figure 2: Performance of each system with different training data size","We investigated the effect of training data size on the performance of each transliteration model. We randomly selected training data with ratios from 10 to 90% and compared the performance of each system trained by different sizes of training data. The results for the basic transliteration models in Figure 2(a) can be categorized into three groups. M (EGP ,φ) and M (EGP ,JCP ) fall into the best group, where they showed the best performance regardless of training data size. M (EG,φ) and M (EG,JCP ) belong to the middle group, where they showed lower performance than the best group if the training data size is small, but their performance is comparable to the best group if the size of the training data is large enough. The others always showed lower performance than both the best and middle groups. Figure 2(b) shows that hybrid transliteration models, on average, were less sensitive to the training data size than the basic ones, because the two different basic transliteration models used in the hybrid ones boosted transliteration performance by complementing each other’s weak points."]},{"title":"5 Conclusion","paragraphs":["We proposed a new English-to-Chinese transliteration model based on Chinese phonemes and their corresponding English graphemes and phonemes. We defined eighteen English-to-Chinese transliteration models including our proposed model and classified them into three classes based on the role of Chinese phonemes in the transliteration models. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration.","Now we can answer Ye s to this paper’s key question, “Can Chinese phonemes improve machine transliteration?” Actually, this is the second time the same question has been answered. The previous answer, which was unfortunately reported as No by Li et al. (2004), has been accepted as true for the last five years; the research issue has been considered closed. In this paper, we found a new answer that contradicts the previous answer. We hope that our answer promotes research on phoneme-based English-to-Chinese transliteration."]},{"title":"Appendix: Illustration of BasicTransliteration Models in MJ and MS","paragraphs":["EG CP EG EP EG EP CG CP CP CG CG :)JC,Μ(Ε PG :)JC,Μ(Ε PP :)JC, Μ(Ε P GP (a) MJ models EG CP EG EP EG EP CG CP CP CG CG :)C, Μ(Ε P GP :)C,Μ(Ε PP :)C,Μ(Ε PG (b) MS models 666"]},{"title":"References","paragraphs":["Y. Al-Onaizan and Kevin Knight. 2002. Translating named entities using monolingual and bilingual resources. In Proc. of ACL ’02, pages 400–408.","A. L. Berger, S. D. Pietra, and V. J. D. Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.","M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009. Unsupervised constraint driven learning for transliteration discovery. In Proceedings of NAACL HLT’ 09. Wei Gao, Kam-Fai Wong, and Wai Lam. 2004. Phoneme-based transliteration of foreign names for OOV problem. In Proc. of IJCNLP 2004, pages 110–119.","Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng Niu. 2007. Named entity translation with web min-ing and transliteration. In Proc. of IJCAI ’07, pages 1629–1634.","Paul B. Kantor and Ellen M. Voorhees. 2000. The trec-5 confusion track: Comparing retrieval methods for scanned text. Information Retrieval, 2:165–176.","Sarvnaz Karimi, Falk Scholer, and Andrew Turpin. 2007. Collapsed consonant and vowel models: New approaches for English-Persian transliteration and back-transliteration. In Proceedings of ACL ’07, pages 648–655.","Chun-Jen Lee and Jason S. Chang. 2003. Acquisition of English-Chinese transliterated word pairs from parallel-aligned texts using a statistical machine transliteration model. In Proc. of HLT-NAACL 2003 Workshop on Building and Using Parallel Texts, pages 96–103.","Haizhou Li, Min Zhang, and Su Jian. 2004. A joint source-channel model for machine transliteration. In Proceedings of the 42th Annual Meeting of the Association of Computational Linguistics, pages 160– 167.","Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui Dong. 2007. Semantic transliteration of personal names. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.","Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009. Whitepaper of NEWS 2009 machine transliteration shared task. In Proc. of ACL-IJCNLP 2009 Named Entities Workshop.","M.G. Abbas Malik. 2006. Punjabi machine transliteration. In Proceedings of the COLING/ACL 2006, pages 1137–1144.","H.M. Meng, Wai-Kit Lo, Berlin Chen, and K. Tang. 2001. Generating phonetic cognates to handle named entities in English-Chinese cross-language spoken document retrieval. In Proc. of Automatic Speech Recognition and Understanding, 2001. ASRU ’01, pages 311–314.","Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara. 2006. A comparison of different machine transliteration models. Journal of Artificial Intelligence Research (JAIR), 27:119–151.","Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximal entropy models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 1– 10.","Richard Schwartz and Yen-Lu Chow. 1990. The N-Best algorithm: an efficient procedure for finding top N sentence hypotheses. In Proc. of ICASSP ’90, pages 81–84.","Tarek Sherif and Grzegorz Kondrak. 2007. Substring-based transliteration. In Proceedings of ACL ’07, pages 944–951.","Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of proper names in cross-lingual information retrieval. In Proc. of ACL 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition, pages 57–64.","Stephen Wan and Cornelia Maria Verspoor. 1998. Automatic English-Chinese name transliteration for development of multilingual resources. In Proc. of COLING ’98, pages 1352–1356.","Xinhua News Agency. 1992. Chinese transliteration of foreign personal names. The Commercial Press.","Binyong Yin and Mary Felley. 1990. Chinese Romanization: Pronunciation and Orthography. Sinolingua.","Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat. 2007. Multilingual transliteration using feature based phonetic method. In Proceedings of ACL’07, pages 112–119. 667"]}]}
