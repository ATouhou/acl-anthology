{"sections":[{"title":"","paragraphs":["Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1213–1221, Edinburgh, Scotland, UK, July 27–31, 2011. c⃝2011 Association for Computational Linguistics"]},{"title":"Computation of Infix Probabilitiesfor Probabilistic Context-Free GrammarsMark-Jan NederhofSchool of Computer ScienceUniversity of St AndrewsUnited Kingdommarkjan.nederhof@gmail.com Giorgio SattaDept. of Information EngineeringUniversity of PaduaItalysatta@dei.unipd.itAbstract","paragraphs":["The notion of infix probability has been introduced in the literature as a generalization of the notion of prefix (or initial substring) probability, motivated by applications in speech recognition and word error correction. For the case where a probabilistic context-free grammar is used as language model, methods for the computation of infix probabilities have been presented in the literature, based on various simplifying assumptions. Here we present a solution that applies to the problem in its full generality."]},{"title":"1 Introduction","paragraphs":["Probabilistic context-free grammars (PCFGs for short) are a statistical model widely used in natural language processing. Several computational problems related to PCFGs have been investigated in the literature, motivated by applications in model-ing of natural language syntax. One such problem is the computation of prefix probabilities for PCFGs, where we are given as input a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G starts with w, that is, has w as a prefix. This quantity is defined as the possibly infinite sum of the probabilities of all strings of the form wx, for any string x over the alphabet of G.","The problem of computation of prefix probabilities for PCFGs was first formulated by Persoon and Fu (1975). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word or part-of-speech, when a prefix of the input has already been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acous-tic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error correction, when one is processing ‘noisy’ text and the parser recognizes an error that must be recovered by operations of in-sertion, replacement or deletion.","Motivated by the above applications, the problem of the computation of infix probabilities for PCFGs has been introduced in the literature as a generalization of the prefix probability problem. We are now given a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G has w as an infix. This probability is defined as the possibly infinite sum of the probabilities of all strings of the form xwy, for any pair of strings x and y over the alphabet of G. Besides applications in computation of the probability distribution for the next word token and in word error correction, infix probabilities can also be exploited in speech understanding systems to score partial hypotheses in algorithms based on beam search, as discussed by Corazza et al. (1991).","Corazza et al. (1991) have pointed out that the computation of infix probabilities is more difficult than the computation of prefix probabilities, due to the added ambiguity that several occurrences of the given infix can be found in a single string generated by the PCFG. The authors developed solutions for the case where some distribution can be defined on","1213 the distance of the infix from the sentence boundaries, which is a simplifying assumption. The problem is also considered by Fred (2000), which provides algorithms for the case where the language model is a probabilistic regular grammar. However, the algorithm in (Fred, 2000) does not apply to cases with multiple occurrences of the given infix within a string in the language, which is what was pointed out to be the problematic case.","In this paper we adopt a novel approach to the problem of computation of infix probabilities, by removing the ambiguity that would be caused by multiple occurrences of the given infix. Although our result is obtained by a combination of well-known techniques from the literature on PCFG parsing and pattern matching, as far as we know this is the first algorithm for the computation of infix probabilities that works for general PCFG models without any restrictive assumption.","The remainder of this paper is structured as follows. In Section 2 we explain how the sum of the probabilities of all trees generated by a PCFG can be computed as the least fixed-point solution of a non-linear system of equations. In Section 3 we recall the construction of a new PCFG out of a given PCFG and a given finite automaton, such that the language generated by the new grammar is the intersection of the languages generated by the given PCFG and the automaton, and the probabilities of the generated strings are preserved. In Section 4 we show how one can efficiently construct an unambiguous finite automaton that accepts all strings with a given infix. The material from these three sections is combined into a new algorithm in Section 5, which allows computation of the infix probability for PCFGs. This is the main result of this paper. Several extensions of the basic technique are discussed in Section 6. Section 7 discusses implementation and some experiments."]},{"title":"2 Sum of probabilities of all derivations","paragraphs":["Assume a probabilistic context-free grammar G, represented by a 5-tuple (Σ, N, S, R, p), where Σ and N are two finite disjoint sets of terminals and nonterminals, respectively, S ∈ N is the start symbol, R is a finite set of rules, each of the form A → α, where A ∈ N and α ∈ (Σ ∪N )∗",", and p is a function from rules in R to real numbers in the interval [0, 1].","The concept of left-most derivation in one step is represented by the notation α π","⇒G β, which means that the left-most occurrence of any nonterminal in α ∈ (Σ ∪ N )∗","is rewritten by means of some rule π ∈ R. If the rewritten nonterminal is A, then π must be of the form (A → γ) and β is the result of replacing the occurrence of A in α by γ. A left-most derivation with any number of steps, using a sequence d of rules, is denoted as α d","⇒G β. We omit the subscript G when the PCFG is understood. We also write α ∗","⇒ β when the involved sequence of rules is of no relevance. Henceforth, all derivations we discuss are implicitly left-most.","A complete derivation is either the empty sequence of rules, or a sequence d = π1 · · · πm, m ≥ 1, of rules such that A d","⇒ w for some A ∈ N and w ∈ Σ∗",". In the latter case, we say the complete derivation starts with A, and in the former case, with d an empty sequence of rules, we assume the complete derivation starts and ends with a single terminal, which is left unspecified. It is well-known that there exists a bijective correspondence between left-most complete derivations starting with nonterminal A and parse trees derived by the grammar with root A and a yield composed of terminal symbols only.","The depth of a complete derivation d is the length of the longest path from the root to a leaf in the parse tree associated with d. The length of a path is defined as the number of nodes it visits. Thus if d = π for some rule π = (A → w) with w ∈ Σ∗",", then the depth of d is 2.","The probability p(d) of a complete derivation d = π1 · · · πm, m ≥ 1, is: p(d) = m ∏ i=1 p(πi). We also assume that p(d) = 1 when d is an empty sequence of rules. The probability p(w) of a string w is the sum of all complete derivations deriving that string from the start symbol: p(w) = ∑","d: S d ⇒wp(d). With this notation, consistency of a PCFG is de-1214","fined as the condition: ∑","d,w: S d ⇒wp(d) = 1. In other words, a PCFG is consistent if the sum of probabilities of all complete derivations starting with S is 1. An equivalent definition of consistency considers the sum of probabilities of all strings: ∑ w p(w) = 1. See (Booth and Thompson, 1973) for further discussion.","In practice, PCFGs are often required to satisfy the additional condition:","∑ π=(A→α) p(π) = 1, for each A ∈ N . This condition is called properness. PCFGs that naturally arise by parameter estimation from corpora are generally consistent; see (Sánchez and Benedı́, 1997; Chi and Geman, 1998). However, in what follows, neither properness nor consistency is guaranteed.","We define the partition function of G as the function Z that assigns to each A ∈ N the value","Z(A) = ∑ d,w p(A d","⇒ w). (1) Note that Z(S) = 1 means that G is consistent. More generally, in later sections we will need to compute the partition function for non-consistent PCFGs.","We can characterize the partition function of a PCFG as a solution of a specific system of equations. Following the approach in (Harris, 1963; Chi, 1999), we introduce generating functions associated with the nonterminals of the grammar. For A ∈ N and α ∈ (N ∪ Σ)∗",", we write f (A, α) to denote the number of occurrences of symbol A within string α. Let N = {A1, A2, . . . , A|N|}. For each Ak ∈ N , let mk be the number of rules in R with left-hand side Ak, and assume some fixed order for these rules. For each i with 1 ≤ i ≤ mk, let Ak → αk,i be the i-th rule with left-hand side Ak.","For each k with 1 ≤ k ≤ |N |, the generating function associated with Ak is defined as gAk (z1, z2, . . . , z|N|) = mk ∑ i=1","(","p(Ak → αk,i) · |N| ∏ j=1 zf(Aj,αk,i)","j )",". (2) Furthermore, for each i ≥ 1 we recursively define functions g(i)","Ak (z1, z2, . . . , z|N|) by g(1) Ak (z1, z2, . . . , z|N|) = gAk (z1, z2, . . . , z|N|), (3) and, for i ≥ 2, by g(i) Ak (z1, z2, . . . , z|N|) = (4)","gAk ( g(i−1) A1 (z1, z2, . . . , z|N|), g(i−1) A2 (z1, z2, . . . , z|N|), . . . , g(i−1) A|N| (z1, z2, . . . , z|N|) ). Using induction it is not difficult to show that, for each k and i as above, g(i)","Ak (0, 0, . . . , 0) is the sum of the probabilities of all complete derivations from Ak having depth not exceeding i. This implies that, for i = 0, 1, 2, . . ., the sequence of the g(i)","Ak (0, 0, . . . , 0) monotonically converges to Z(Ak).","For each k with 1 ≤ k ≤ |N | we can now write Z(Ak) =","= lim i→∞ g(i)","Ak (0, . . . , 0)","= lim","i→∞ gAk ( g(i−1) A1 (0, 0, . . . , 0), . . . , g(i−1) A|N| (0, 0, . . . , 0) )","= gAk ( limi→∞ g(i−1)","A1 (0, 0, . . . , 0), . . . ,","limi→∞ g(i−1)","A|N| (0, 0, . . . , 0) ) = gAk (Z(A1), . . . , Z(A|N|)). The above shows that the values of the partition function provide a solution to the system of the following equations, for 1 ≤ k ≤ |N |: zk = gAk (z1, z2, . . . , z|N|). (5)","In the case of a general PCFG, the above equations are non-linear polynomials with positive (real) coefficients. We can represent the resulting system in vector form and write X = g(X). These systems","1215 are called monotone systems of polynomial equations and have been investigated by Etessami and Yannakakis (2009) and Kiefer et al. (2007). The sought solution, that is, the partition function, is the least fixed point solution of X = g(X).","For practical reasons, the set of nonterminals of a grammar is usually divided into maximal subsets of mutually recursive nonterminals, that is, for each A and B in such a subset, we have A ∗","⇒ uBα and B ∗","⇒ vAβ, for some u, v, α, β. This corresponds to a strongly connected component if we see the connection between the left-hand side of a rule and a nonterminal member in its right-hand side as an edge in a directed graph. For each strongly connected component, there is a separate system of equations of the form X = g(X). Such systems can be solved one by one, in a bottom-up order. That is, if one strongly connected component contains nonterminal A, and another contains nonterminal B, where A ∗","⇒ uBα for some u, α, then the system for the latter component must be solved first.","The solution for a system of equations such as those described above can be irrational and nonexpressible by radicals, even if we assume that all the probabilities of the rules in the input PCFG are rational numbers, as observed by Etessami and Yannakakis (2009). Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al. (1999). This corresponds to the so-called fixed-point iteration method, which is well-known in the numerical calculus literature and is frequently applied to systems of non-linear equations because it can be easily implemented.","When a number of standard conditions are met, each iteration of (4) adds a fixed number of bits to the precision of the solution; see Kelley (1995, Chapter 4). Since each iteration can easily be implemented to run in polynomial time, this means that we can approximate the partition function of a PCFG in polynomial time in the size of the PCFG itself and in the number of bits of the desired precision.","In practical applications where large PCFGs are empirically estimated from data sets, the standard conditions mentioned above for the polynomial time approximation of the partition function are usually met. However, there are some degenerate cases for which these standard conditions do not hold, resulting in exponential time behaviour of the fixed-point iteration method. This has been firstly observed in (Etessami and Yannakakis, 2005).","An alternative iterative algorithm for the approximation of the partition function has been proposed by Etessami and Yannakakis (2009), based on Newton’s method for the solution of non-linear systems of equations. From a theoretical perspective, Kiefer et al. (2007) have shown that, after a certain number of initial iterations, Newton’s method adds a fixed number of bits to the precision of the approximated solution, even in the above mentioned cases in which the fixed-point iteration method shows exponential time behaviour. However, these authors also show that, in some degenerate cases, the number of itera-tions needed to compute the first bit of the solution can be at least exponential in the size of the system.","Experiments with Newton’s method for the approximation of the partition functions of PCFGs have been carried out in several application-oriented settings, by Wojtczak and Etessami (2007) and by Nederhof and Satta (2008), showing considerable improvements over the fixed-point iteration method."]},{"title":"3 Intersection of PCFG and FA","paragraphs":["It was shown by Bar-Hillel et al. (1964) that context-free languages are closed under intersection with regular languages. Their proof relied on the construction of a new CFG out of an input CFG and an input finite automaton. Here we extend that construction by letting the input grammar be a probabilistic CFG. We refer the reader to (Nederhof and Satta, 2003) for more details.","To avoid a number of technical complications, we assume the finite automaton has no epsilon transitions, and has only one final state. In the context of our use of this construction in the following sections, these restrictions are without loss of general-ity. Thus, a finite automaton (FA) M is represented by a 5-tuple (Σ, Q, q0, qf , ∆), where Σ and Q are two finite sets of terminals and states, respectively, q0 is the initial state, qf is the final state, and ∆ is a finite set of transitions, each of the form s a","↦→ t, where s, t ∈ Q and a ∈ Σ.","A complete computation of M accepting string","1216 w = a1 · · · an is a sequence c = τ1 · · · τn of transitions such that τi = (si−1 ai","↦→ si) for each i (1 ≤ i ≤ n), for some s0, s1, . . . , sn with s0 = q0 and sn = qf . The language of all strings accepted by M is denoted by L(M). A FA is unambiguous if at most one complete computation exists for each accepted string. A FA is deterministic if there is at most one transition s a","↦→ t for each s and a.","For a FA M as above and a PCFG G = (Σ, N, S, R, p) with the same set of terminals, we construct a new PCFG G′","= (Σ, N ′",", S′",", R′",", p′","), where N ′","= Q × (Σ ∪ N ) × Q, S′","= (q0, S, qf ), and R′","is the set of rules that is obtained as follows.","• For each A → X1 · · · Xm in R and each sequence s0, . . . , sm with si ∈ Q, 0 ≤ i ≤ m, and m ≥ 0, let (s0, A, sm) → (s0, X1, s1) · · · (sm−1, Xm, sm) be in R′","; if m = 0, the new rule is of the form (s0, A, s0) → ε. Function p′ assigns the same probability to the new rule as p assigned to the original rule.","• For each s a ↦→ t in ∆, let (s, a, t) → a be in R′",".","Function p′ assigns probability 1 to this rule.","Intuitively, a rule of G′","is either constructed out of a rule of G or out of a transition of M. On the basis of this correspondence between rules and transitions of G′",", G and M, it is not difficult to see that each derivation d′","in G′","deriving string w corresponds to a unique derivation d in G deriving the same string and a unique computation c in M accepting the same string. Conversely, if there is a derivation d in G deriving string w, and some computation c in M accepting the same string, then the pair of d and c corresponds to a unique derivation d′","in G′","deriving the same string w. Furthermore, the probabilities of d and d′","are equal, by definition of p′",".","Let us now assume that each string w is accepted by at most one computation, i.e. M is unambiguous. If a string w is accepted by M, then there are as many derivations deriving w in G′","as there are in G. If w is not accepted by M, then there are zero derivations deriving w in G′",". Consequently: ∑ d′ ,w:","S′ d′ ⇒G′w","p′ (d′",") = ∑ d,w: S d ⇒Gw∧w∈L(M) p(d), or more succinctly: ∑","w p′ (w) = ∑ w∈L(M) p(w).","Note that the above construction of G′","is exponential in the largest value of m in any rule from G. For this reason, G is usually brought in binary form before the intersection, i.e. the input grammar is transformed to let each right-hand side have at most two members. Such a transformation can be realized in linear time in the size of the grammar. We will return to this issue in Section 7."]},{"title":"4 Obtaining unambiguous FAs","paragraphs":["In the previous section, we explained that unambiguous finite automata have special properties with respect to the grammar G′","that we may construct out of a FA M and a PCFG G. In this section we discuss how unambiguity can be obtained for the special case of finite automata accepting the language of all strings with given infix w ∈ Σ∗",":","Linfix (w) = {xwy | x, y ∈ Σ∗ }.","Any deterministic automaton is also unambiguous. Furthermore, there seem to be no practical algorithms that turn FAs into equivalent unambiguous FAs other than the algorithms that also determinize them. Therefore, we will henceforth concentrate on deterministic rather than unambiguous automata.","Given a string w = a1 · · · an, a finite automaton accepting Linfix (w) can be straightforwardly constructed. This automaton has states s0, . . . , sn, transitions s0 a","↦→ s0 and sn a","↦→ sn for each a ∈ Σ, and transition si−1 ai","↦→ si for each i (1 ≤ i ≤ n). The initial state is s0 and the final state is sn. Clearly, there is nondeterminism in state s0.","One way to make this automaton deterministic is to apply the general algorithm of determinization of finite automata; see e.g. (Aho and Ullman, 1972). This algorithm is exponential for general FAs. An alternative approach is to construct a deterministic finite automaton directly from w, in line with the Knuth-Morris-Pratt algorithm (Knuth et al., 1977; Gusfield, 1997). Both approaches result in the same deterministic FA, which we denote by Iw. However, the latter approach is easier to implement in such a","1217 way that the time complexity of constructing the automaton is linear in |w|.","The automaton Iw is described as follows. There are n + 1 states t0, . . . , tn, where as before n is the length of w. The initial state is t0 and the final state is tn. The intuition is that Iw reads a string x = b1 · · · bm from left to right, and when it has read the prefix b1 · · · bj (0 ≤ j ≤ m), it is in state ti (0 ≤ i < n) if and only if a1 · · · ai is the longest prefix of w that is also a suffix of b1 · · · bj. If the automaton is in state tn, then this means that w is an infix of b1 · · · bj.","In more detail, for each i (1 ≤ i ≤ n) and each a ∈ Σ, there is a transition ti−1 a","↦→ tj, where j is the length of the longest string that is both a prefix of w and a suffix of a1 · · · ai−1a. If a = ai, then clearly j = i, and otherwise j < i. To ensure that we remain in the final state once an occurrence of infix w has been found, we also add transitions tn a","↦→ tn for each a ∈ Σ. This construction is illustrated in Figure 1."]},{"title":"5 Infix probability","paragraphs":["With the material developed in the previous sections, the problem of computing the infix probabilities can be effectively solved. Our goal is to compute for given infix w ∈ Σ∗","and PCFG G = (Σ, N, S, R, p): σinfix (w, G) = ∑ z∈Linfix (w) p(z). In Section 4 we have shown the construction of finite automaton Iw accepting Linfix (w), by which we obtain: σinfix (w, G) = ∑ z∈L(Iw) p(z).","As Iw is deterministic and therefore unambiguous,","the results from Section 3 apply and if G′","= (Σ, N ′",",","S′ , R′",", p′",") is the PCFG constructed out of G and Iw","then:","σinfix (w, G) = ∑ z p′","(z). Finally, we can compute the above sum by applying the iterative method discussed in Section 2."]},{"title":"6 Extensions","paragraphs":["The approach discussed above allows for a number of generalizations. First, we can replace the infix w by a sequence of infixes w1, . . . , wm, which have to occur in the given order, one strictly after the other, with arbitrary infixes in between:","σisland (w1, . . . , wm, G) = ∑ x0,...,xm∈Σ∗ p(x0w1x1 · · · wmxm). This problem was discussed before by (Corazza et al., 1991), who mentioned applications in speech recognition. Further applications are found in computational biology, but their discussion is beyond the scope of this paper; see for instance (Apostolico et al., 2005) and references therein. In order to solve the problem, we only need a small addition to the procedures we discussed before. First we construct separate automata Iwj (1 ≤ j ≤ m) as explained in Section 4. These automata are then composed into a single automaton I(w1,...,wm). In this composition, the outgoing transitions of the final state of Iwj , for each j (1 ≤ j < m), are removed and that final state is merged with the initial state of the next automaton Iwj+1. The initial state of the composed automaton is the initial state of Iw1, and the final state is the final state of Iwm. The time costs of constructing I(w1,...,wm) are linear in the sum of the lengths of the strings wj.","Another way to generalize the problem is to replace w by a finite set L = {w1, . . . , wm}. The objective is to compute: σinfix (L, G) = ∑ w∈L,x,y∈Σ∗ p(xwy) Again, this can be solved by first constructing a deterministic FA, which is then intersected with G. This FA can be obtained by determinizing a straightforward nondeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time.","Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case,","1218 t0 t1 t2 t3 t4 a b a c b, c a a, b, c c b, c a b Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987)."]},{"title":"7 Implementation","paragraphs":["We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time or space demand, and that might be improved. The experiments were run on a desktop with a 3.0 GHz Pentium 4 processor. The implementation language is C++.","The set-up of the experiments is similar to that in (Nederhof and Satta, 2008). A probabilistic context-free grammar was extracted from sections 2-21 of the Penn Treebank version II. Subtrees that generated the empty string were systematically removed. The result was a CFG with 10,035 rules, 28 nonterminals and 36 parts-of-speech. The rule probabilities were determined by maximum likelihood estimation. The grammar was subsequently binarized, to avoid exponential behaviour, as explained in Section 3.","We have considered 10 strings of length 7, randomly generated, assuming each of the parts-of-speech has the same probability. For all prefixes of those strings from length 2 to length 7, we then computed the infix probability. The duration of the full computation, averaged over the 10 strings of length 7, is given in the first row of Table 1.","In order to solve the non-linear systems of equations, we used Broyden’s method. It can be seen as an approximation of Newton’s method. It requires more iterations, but seems to be faster over-all, and more scalable to large problem sizes, due to the avoidance of matrix inversion, which sometimes makes Newton’s method prohibitively expensive. In our experiments, Broyden’s method was generally faster than Newton’s method and much faster than the simple iteration method by the relation in (4). For further details on Broyden’s method, we refer the reader to (Kelley, 1995).","The main obstacle to computation for infixes substantially longer than 7 symbols is the memory consumption rather than the running time. This is due to the required square matrices, the dimension of which is the number of nonterminals. The number of nonterminals (of the intersection grammar) naturally grows as the infix becomes longer.","As explained in Section 2, the problem is divided into smaller problems by isolating disjoint sets of mutually recursive nonterminals, or strongly connected components. We found that for the applica-tion to the automata discussed in Section 4, there were exactly three strongly connected components that contained more than one element, throughout the experiments. For an infix of length n, these components are:","• C1, which consists of nonterminals of the form (ti, A, tj), where i < n and j < n,","• C2, which consists of nonterminals of the form (ti, A, tj), where i = j = n, and","• C3, which consists of nonterminals of the form (ti, A, tj), where i < j = n. This can be easily explained by looking at the structure of our automata. See for example Figure 1, with cycles running through states t0, . . . , tn−1, and cycles through state tn. Furthermore, the grammar extracted from the Penn Treebank is heavily recursive,","1219 infix length 2 3 4 5 6 7 total running time 1.07 1.95 5.84 11.38 23.93 45.91 Broyden’s method for C1 0.46 0.90 3.42 6.63 12.91 24.38 Broyden’s method for C2 0.08 0.04 0.07 0.04 0.03 0.09 Broyden’s method for C3 0.20 0.36 0.81 1.74 5.30 9.02 Table 1: Running time for infixes from length 2 to length 7. The infixes are prefixes of 10 random strings of length 7, and reported CPU times (in seconds) are averaged over the 10 strings. so that almost every nonterminal can directly or in-directly call any other.","The strongly connected component C2 is always the same, consisting of 2402 nonterminals, for each infix of any length. (Note that binarization of the grammar introduced artificial nonterminals.) The last three rows of Table 1 present the time costs of Broyden’s method, for the three strongly connected components.","The strongly connected component C3 happens to correspond to a linear system of equations. This is because a rule in the intersection grammar with a left-hand side (ti, A, tj), where i < j = n, must have a right-hand side of the form (ti, A′",", tj), or of the form (ti, A1, tk) (tk, A2, tj), with k ≤ n. If k < n, then only the second member can be in C3. If k = n, only first member can be in C3. Hence, such a rule corresponds to a linear equation within the system of equations for the entire grammar.","A linear system of equations can be solved an-alytically, for example by Gaussian elimination, rather than approximated through Newton’s method or Broyden’s method. This means that the running times in the last row of Table 1 can be reduced by treating C3 differently from the other strongly connected components. However, the running time for C1 dominates the total time consumption.","The above investigations were motivated by two questions, namely whether any part of the computation can be precomputed, and second, whether infix probabilities can be computed incrementally, for infixes that are extended to the left or to the right. The first question can be answered affirmatively for C2, as it is always the same. However, as we can see in Table 1, the computation of C2 amounts to a small portion of the total time consumption.","The second question can be rephrased more precisely as follows. Suppose we have computed the infix probability of a string w and have kept intermediate results in memory. Can the computation of the infix probability of a string of the form aw or wa, a ∈ Σ, be computed by relying on the existing results, so that the computation is substantially faster than if the computation were done from scratch?","Our investigations so far have not found a positive answer to this second question. In particular, the systems of equations for C1 and C3 change fundamentally if the infix is extended by one more symbol, which seems to at least make incremental computation very difficult, if not impossible. Note that the algorithms for the computation of prefix probabilities by Jelinek and Lafferty (1991) and Stolcke (1995) do allow incrementality, which contributes to their practical usefulness for speech recognition."]},{"title":"8 Conclusions","paragraphs":["We have shown that the problem of infix probabilities for PCFGs can be solved by a construction that intersects a context-free language with a regular language. An important constraint is that the finite automaton that is input to this construction be unambiguous. We have shown that such an automaton can be efficiently constructed. Once the input probabilistic PCFG and the FA have been combined into a new probabilistic CFG, the infix probability can be straightforwardly solved by iterative algorithms. Such algorithms include Newton’s method, and Broyden’s method, which was used in our experiments. Our discussion ended with an open question about the possibility of incremental computation of infix probabilities."]},{"title":"References","paragraphs":["S. Abney, D. McAllester, and F. Pereira. 1999. Relating","probabilistic grammars and automata. In 37th Annual","1220 Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 542–549, Maryland, USA, June.","A.V. Aho and M.J. Corasick. 1975. Efficient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333–340, June.","A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1 of The Theory of Parsing, Translation and Compiling. Prentice-Hall, Englewood Cliffs, N.J.","A. Apostolico, M. Comin, and L. Parida. 2005. Conservative extraction of overrepresented extensible motifs. In Proceedings of Intelligent Systems for Molecular Biology (ISMB05).","Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Y. Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application, chapter 9, pages 116–150. Addison-Wesley, Reading, Massachusetts.","T.L. Booth and R.A. Thompson. 1973. Applying probabilistic measures to abstract languages. IEEE Transactions on Computers, C-22:442–450.","Z. Chi and S. Geman. 1998. Estimation of probabilistic context-free grammars. Computational Linguistics, 24(2):299–305.","Z. Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.","A. Corazza, R. De Mori, R. Gretter, and G. Satta. 1991. Computation of probabilities for an islanddriven parser. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(9):936–950.","K. Etessami and M. Yannakakis. 2005. Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations. In 22nd International Symposium on Theoretical Aspects of Computer Science, volume 3404 of Lecture Notes in Computer Science, pages 340–352, Stuttgart, Germany. Springer-Verlag.","K. Etessami and M. Yannakakis. 2009. Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations. Journal of the ACM, 56(1):1–66.","A.L.N. Fred. 2000. Computation of substring probabilities in stochastic grammars. In A. Oliveira, editor, Grammatical Inference: Algorithms and Applica-tions, volume 1891 of Lecture Notes in Artificial Intelligence, pages 103–114. Springer-Verlag.","D. Gusfield. 1997. Algorithms on Strings, Trees, and Sequences. Cambridge University Press, Cambridge.","T.E. Harris. 1963. The Theory of Branching Processes. Springer-Verlag, Berlin, Germany.","F. Jelinek and J.D. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315–323.","C.T. Kelley. 1995. Iterative Methods for Linear and Nonlinear Equations. Society for Industrial and Applied Mathematics, Philadelphia, PA.","S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the convergence of Newton’s method for monotone systems of polynomial equations. In Proceedings of the 39th ACM Symposium on Theory of Computing, pages 217–266.","D.E. Knuth, J.H. Morris, Jr., and V.R. Pratt. 1977. Fast pattern matching in strings. SIAM Journal on Computing, 6:323–350.","M.-J. Nederhof and G. Satta. 2003. Probabilistic parsing as intersection. In 8th International Workshop on Parsing Technologies, pages 137–148, LORIA, Nancy, France, April.","M.-J. Nederhof and G. Satta. 2008. Computing partition functions of PCFGs. Research on Language and Computation, 6(2):139–162.","E. Persoon and K.S. Fu. 1975. Sequential classification of strings generated by SCFG’s. International Journal of Computer and Information Sciences, 4(3):205–217.","P. Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proc. of the fifteenth International Conference on Computational Linguistics, Nantes, August, pages 418–424.","J.-A. S ánchez and J.-M. Bened ı́. 1997. Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(9):1052–1055, September.","Y. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proc. of the fifteenth International Conference on Computational Linguistics, Nantes, August, pages 426–432.","A. Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):167–201.","K. Vijay-Shanker, D.J. Weir, and A.K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In 25th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 104–111, Stanford, California, USA, July.","D. Wojtczak and K. Etessami. 2007. PReMo: an an-alyzer for Probabilistic Recursive Models. In Tools and Algorithms for the Construction and Analysis of Systems, 13th International Conference, volume 4424 of Lecture Notes in Computer Science, pages 66–71, Braga, Portugal. Springer-Verlag.","1221"]}]}
