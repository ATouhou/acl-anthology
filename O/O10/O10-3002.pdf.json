{"sections":[{"title":"","paragraphs":["Computational Linguistics and Chinese Language Processing Vol. 15, No. 1, March 2010, pp. 19-36 19 © The Association for Computational Linguistics and Chinese Language Processing"]},{"title":"基於對照表以及語言模型之簡繁字體轉換 Chinese Characters Conversion System based on Lookup Table and Language Model 李民祥","paragraphs":["∗"]},{"title":"、吳世弘","paragraphs":["∗"]},{"title":"、曾議慶","paragraphs":["∗"]},{"title":"、楊秉哲","paragraphs":["+"]},{"title":"、谷圳","paragraphs":["+ "]},{"title":"Min-Hsiang Li, Shih-Hung Wu, Yi-Ching Zeng, Ping-che Yang, and Tsun Ku 摘要","paragraphs":["中國大陸與台灣的文字同屬於華文字體,但字體上卻分為簡體字與繁體字。中 國大陸與台灣近年來在中文書籍及網路上皆有大量的資訊交流。基於閱讀習慣, 文字勢必需要執行簡繁轉換後才利於雙方的讀者閱讀。傳統的簡繁轉換擁有簡 體一字對繁體多字的歧異問題以及兩岸用語不同的問題。因此,本研究設計一 個具有擴展性的簡繁轉換系統,透過人工擷取維基百科新增對照表內容來改善 兩岸用語不同的問題,以及使用語言模型改善簡體字一個字對繁體字多個字的 歧異問題。此系統可以降低各種中文電子書籍執行簡繁轉換後人工校正的成本。 具有彈性的架構使得系統可以持續擴充改進。 關鍵詞:簡繁轉換,語言模型,維基百科,對照表"]},{"title":"Abstract","paragraphs":["The character sets used in China and Taiwan are both Chinese, but they are divided into simplified and traditional Chinese characters. There are large amount of  ∗ 朝陽科技大學資訊工程系, Department of Computer Science and Information Engineering, Chaoyang University of Technology E-mail: {s9827608, shwu, s9927605}@cyut.edu.tw The author for corrrespondence is Shih-Hung Wu. + 資訊工業策進會, Institute for Information Industry E-mail: {maciaclark,cujing}@iii.org.tw   20 李民祥 等 information exchange between China and Taiwan through books and Internet. To provide readers a convenient reading environment, the character conversion between simplified and traditional Chinese is necessary. The conversion between simplified and traditional Chinese characters has two problems: one-to-many ambiguity and term usage problems. Since there are many traditional Chinese characters that have only one corresponding simplified character, when converting simplified Chinese into traditional Chinese, the system will face the one-to-many ambiguity. Also, there are many terms that have different usages between the two Chinese societies. This paper focus on designing an extensible conversion system, that can take the advantage of community knowledge by accumulating lookup tables through Wikipedia to tackle the term usage problem and can integrate language model to disambiguate the one-to-many ambiguity. The system can reduce the cost of proofreading of character conversion for books, e-books, or online publications. The extensible architecture makes it easy to improve the system with new training data. Keywords: Chinese Character Conversion, Language Model, Wikipedia, Lookup Table."]},{"title":"1. 緒論","paragraphs":["由於中國大陸與台灣數位出版合作的開啟,中文書籍相互流通的機會增加,簡體字與繁 體字的轉換技術開始變得重要。近年來,隨著台灣與中國大陸兩岸交流逐漸頻繁,以及 網路資訊快速發展,文字書信已經成為兩岸溝通的媒介之一。然而,由於中國大陸普遍 使用簡體中文,台灣主要使用繁體中文,因此雙方在文字溝通上勢必要先經過簡繁轉換 的程序後才利於閱讀。一般來說,簡繁轉換是依據簡繁字對照表來進行轉換,這個方法 主要使用單字詞一對一的方式進行簡繁轉換。不過在許多情況下,簡體字對應繁體字經 常是一對多的狀況,所以僅使用一對一的方式進行轉換常常會出現字詞不適用的狀況, 稱此為「非對稱簡繁字」。","中國大陸以及台灣已著手研究簡繁轉換工具的有:中國大陸的中國科學院軟體所、 四通利方資訊技術有限公司、新天地公司;台灣的 IBM 公司、倚天資訊股份公司,以及 其它研發團隊等。目前也有許多文書處理軟體包含著內建的簡繁轉換系統,例如: Microsoft Office、Sun 的 OpenOffice;以及網路上可查詢到的雙語字典,例如:Google Translate。然而,這些轉換的結果通常參差不齊,簡繁轉換後依然需要依靠人工來校正 不精確轉換的錯誤(王曉明、魏林梅,2008)。根據文獻,王寧擷取了 150 萬字的小說簡 體字語料,使用 Office Word2003 執行簡體轉換繁體的功能,發現許多簡體字對繁體字 一對多的情況無法正確被轉換(王寧、王曉明,2005)。簡而言之,簡繁轉換的困難在於 簡體字存在著非對稱簡繁字的情況,使得在不同名詞或是動詞搭配時,無法正確轉換出 應該對應到的字,例如:簡體字的「下面」在敘述位置時,轉換為繁體字後為「下面」;   基於對照表以及語言模型之簡繁字體轉換 21 而簡體字的「下面」使用在動詞時,轉換為繁體字後則是「下麵」(李樹德,2009)。並 且,簡繁用詞問題需要依靠蒐集大量的簡體以及繁體用詞的對照表來提供轉換,例如「坐 公車」互相對應「坐公交車」。","本論文提供以繁體字語料庫建構的語言模型以及收集維基百科簡繁詞彙對照表,並 計算語言模型的分數來達到提升簡體字轉換繁體字正確性的方法。例如:繁體的「坐公 車」轉換為簡體的「坐公交車」,簡體的「吃面」轉換為繁體的「吃麵」而非「吃面」。 實驗部分,由於繁體字轉換簡體字為多對一的問題,僅需查表即可完成轉換。所以我們 著重於簡體字轉換繁體字時非對稱簡繁字的選擇辦法。我們以包含多種非對稱簡繁字轉 換的常用字的句子進行簡體字轉換繁體字的測試,並且與目前幾種知名的翻譯工具進行 比較,接著分析本系統轉換錯誤的問題。接著,我們引入斷詞系統(中科院計算所,2009), 改善原本系統無法轉換正確的幾種情況,並且分析無法正確轉換的非對稱簡繁字。最後, 透過調整語言模型以及對照表的大小來驗證語言模型以及對照表大小對於簡繁轉換正確 率是否有相對的影響。"]},{"title":"2. 系統設計與方法 2.1 系統流程設計","paragraphs":["我們根據傳統簡繁轉換系統架構做為本系統的基礎,並加入擷取自維基百科繁體中文以 及簡體中文的對應條目,利用新聞語料庫訓練 unigram 和 bigram 的機率,計算出分數最 高的一對多轉換字。本篇論文的系統中,簡繁轉換的文字編碼皆使用 Unicode 的編碼方 式,因為 Unicode 為國際編碼,它給予每一個字符唯一的編碼表示,並且包含了現有規 範中所有簡體字與繁體字的日常用字。所以使用 Unicode 可以省去繁體字編碼(BIG5)與 簡體字編碼(GB)的轉換步驟。轉換過程分為三個步驟:一、首先使用對照表判斷是否需 要進行專有名詞以及一般名詞轉換。二、判斷是否含有非對稱簡繁字。三、使用語言模 型計算分數。","對照表內容以維基百科簡體字與繁體字條目名稱做為專有名詞以及一般名詞的對 照表、以及維基百科提供的簡繁轉換非對稱簡繁字。非對稱簡繁字為簡體字轉繁體字一 對多的狀況,例如:簡體字的「皇后」轉換為繁體字有兩種可能,分別為「皇后」以及 「皇後」,因為簡體字的「后」對應的繁體字為「后」以及「後」,所以轉換上出現這 兩種字詞。最後,我們蒐集 1998 年至 2001 年的新聞語料庫做為我們的建構語言模型的 語料庫,並使用語言模型計算簡體字轉換繁體字時出現非對稱簡繁字的分數。系統流程 圖如圖一所示:   22 李民祥 等  "]},{"title":"圖一、系統流程圖 2.2 對照表收集","paragraphs":["中國大陸以及台灣同屬華文市場,但書籍內容用字遣詞仍有很多差異。智慧型的文體轉 換,必須要解決編碼、詞彙以及簡體字對繁體字轉換時一對多及多對一的問題。兩岸不 同詞彙的比對和轉換,包括人名地名組織名以及領域專有名詞(劉匯丹、吳健,2008)。 目前的技術多只著重編碼的轉換,以及專有名詞的轉換。因此,對照表的內容需要豐富 的轉換對照,包括成語、中外人名、地名、組織名。   基於對照表以及語言模型之簡繁字體轉換 23","對照表部份,維基百科擁有大量的條目資料,並且提供了對於一般名詞以及專業名 詞的準確度,Martin Hepp (2007)提到,維基百科中有 92.67%的條目名稱即使過了一段時 間後,條目名稱依然沒有改變,有 6.67%是改變了名稱,但語義上保持不變,僅剩下的 0.67%為可被刪除的條目。因此,維基百科中所有條目名稱中有 99.34%的條目名稱是可 以被信任的。基於這個理由,專有名詞轉換的部份我們主要依靠維基百科做為轉換的輸 出。","維基百科提供非對稱簡繁字以及一對一單字詞的轉換對應字。因此我們以人工方式 蒐集維基百科的簡體字與繁體字的非對稱簡繁字以及一對一單字詞,用來做為簡體字與 繁體字相互轉換時所依據的來源。繁體字與簡體字用詞對照部份,一共收集 7180 筆對照 詞彙;非對稱簡繁字以及一對一單字詞一共收集 6619 筆,其中非對稱簡繁字一共有 475 筆,一對一單字詞為 6144 筆。表一、表二以及表三分別為部分對照表內容、部分非對稱 簡繁字以及部分簡繁轉換一對一單字詞的範例。"]},{"title":"表一、部分對照表內容","paragraphs":["簡繁用詞對照表  繁體字用詞 簡體字用詞 快閃記憶體 闪存 網際網路 因特网 解碼 译码 印表機 打印机 埠 端口 蟻后 蚁后"]},{"title":"表二、部分非對稱簡繁字","paragraphs":["非對稱簡繁字  繁體字單字詞 簡體字單字詞 板闆 板 辟闢 辟 表錶 表 發髮 发 并並併竝 并 乾干幹榦 干 面麵麪麫 面     24 李民祥 等"]},{"title":"表三、部分簡繁轉換一對一單字詞","paragraphs":["簡繁轉換一對一單字詞  繁體字單字詞 簡體字單字詞 獃 呆 僱 雇 韓 韩 號 号 輓 挽 兩 两 嚴 严","如圖二所示,上面的句子為簡體中文,下面的句子為將轉換的繁體中文。透過對照 表的簡繁用詞轉換以及一對一轉換,可以精確的轉換出正確用詞。但是,非對稱簡繁字 的對照表僅提供可能轉換的字詞,並沒有提供如何正確轉換非對稱簡繁字。"," 簡體: 我想解释剪 发 的好处,那当然是有很多的。     繁體: 我想解釋剪 發髮 的好處,那當然是有很多的。 "]},{"title":"圖二、非對稱簡繁字的轉換問題","paragraphs":["因此,我們使用語言模型計算非對稱簡繁字 bigram 以及 unigram 的機率值,取得圖 二例子中「剪發」、「發的」以及「剪髮」、「髮的」出現機率較高的 bigram 機率值, 藉由較高 bigram 機率值來做為選擇字的轉換方式。"]},{"title":"2.3 語言模型","paragraphs":["我們使用統計式語言模型的方法(Statistical language model) (Rosenfeld, 1992),篩選出正 確性較高的翻譯方式(陳勇志、吳世弘、盧家慶、谷圳,2009)(洪大弘,2008)。系統使用 N-gram 語言模型計算一個句子中字詞組合的機率,機率越高代表越可能符合正確文法, 反之則代表可能越不符合正確文法。首先建立語言模型,我們使用 Maximum Likelihood Estimation (MLE) (Katz, 1987),計算出語料庫中每個字出現的相對頻率並且藉此計算機 率值,如公式(1)所示:","1","1 1 1 1 1 () (| ) ()","n","n nN n","nnN n","nN Cw w Pw w Cw","−","− −+","−+ −","−+= (1)   基於對照表以及語言模型之簡繁字體轉換 25","其中,C 代表某個字 W 出現的頻率。由於一個句子是由 n 個字組成,因此一個句子的機","率可以計算為如公式(2)所示:","112( ) ( , ,..., )n","nPw Pw w w≡ (2)","其中 nw 表示句子中第 n 個字。 1()n","P w 表示 1 到 n 個字出現的機率值。","假設字詞的機率為獨立事件,一個句子條件機率可由連乘得到,如公式(3)所示:","21","112131 1","1) 1 2 1 () ()( |)(| ) ( | ) () ( | )","nn n","kn kk Pw Pw Pw w Pw w Pw w Pw Pw w − − = =×× ="]},{"title":"∏","paragraphs":["... (3)","然而,組成一個句子的字詞是有限的,無法由過去歷史出現的無限字來做預測,因此我","們將公式(3)改寫為如公式(4)所示:","11","11(| ) (| )nn","nnnNPw w Pw w−−","−+≈ (4)","代表依據前(n-1)個字出現的機率來預測目前第 n 個字所出現的機率,而所謂的 N-gram","就是當 N=2 時,稱為 bigram,如公式(4)所示:","1","11(| ) (| )n","nnnPw w Pw w−","−≈ (5)","當 N=3 時,稱為 trigram,如公式(5)所示:","1 112(| ) (| )n","nnnnPw w Pw w w−","−−≈ (6) 依此類推至 N-gram。","如圖三舉例,利用前兩個字出現的情況下,預測下一個字出現的機率。如圖三所示: 此圖舉例說明以「世」 與「界」為例:由「世」出現的情況下來推測「界」出現的機率, 稱為 bigram。同理,「界」與「盃」也是 bigram;若是由「世」與「界」出現的情況下 來推測「盃」出現的機率,則稱為 trigram。然而,建構 trigram 的語言模型會造成語言 模型的內容龐大,造成系統速度降低。因此,我們利用中文字出現二字詞的比例很高的 特性,本研究使用的語言模型為計算 bigram 的出現頻率。 "]},{"title":"圖三、說明 bigram 以及 trigram 計算方法 2.4 Smoothing","paragraphs":["然而,MLE 在字詞出現頻率正常的情況下可以運作良好,但由於訓練資料稀疏,有些字 詞出現的頻率會很低甚至是零,因此語言模型計算分數時可能會發生找不到要計算的字   26 李民祥 等 詞,導致無法正確預測下一個字的錯誤狀況,使得正確率降低。頻率是零的情況有兩種, 一種是代表兩個字之間無意義的結合,也就是真正的零;另一種是假的零,意思就是雖 然這個字在文集中沒出現過,但是卻是真實世界中存在的字詞,只是訓練語料裡沒有出 現。為了避免出現機率相乘後為零的狀況,我們使用 Smoothing 的方法,分配 bigram 以 及 unigram 出現機率的權重值,以 bigram 的機率為主要的機率分配,並給予較高的權重 值;而 unigram 則給予較小的權重值,因為 unigram 提供的線索比起 bigram 提供的線索 所含有的資訊來得較低。","Smoothing 的方法可分成折扣的方法和模型結合的方法,折扣的方法就是調整機率, 將機率較高者分配其值給機率為零者;而模型結合的方式就是利用內插法和補插法,當 trigram 無效時,使用 bigram,bigram 無效時則使用 unigram。本系統的 Smoothing 為模 型結合的方法。我們會使用 Interpolated Kneser-Ney smoothing 的演算法(Goodman, 2001)。 Interpolated Kneser-Ney smoothing 公式如公式(7)所示: int 1 2 1 2 1(| ) (| )(1 )[ (| )(1 ) ()]erpolate i i tirgram i i bigram i unigramP www P www P ww P wλ λμ λ−− −− −= +− +− (7) 其中 λ 以及 μ 表示分配的權重值。分別計算 trigram、bigram 以及 unigram 的機率值,並 給予權重分配的 λ 和 μ ,藉以避免發生 trigram 或是 bigram 的機率值為零的狀況。而我 們主要是使用 bigram 語言模型,因此本系統使用 Interpolated Kneser-Ney smoothing 的公 式時需要稍作修改,我們將公式(7)改寫為:","int 1 1(| ) (| )(1 )erpolate i bigram i unigramPwwPww Pλ λ−−=+− (8) 我們刪除沒有使用到的 trigram,只計算 bigram 以及 unigram 的機率值。其中,我們認為 bigram 的頻率的資訊強度大於 unigram 的資訊強度,因此 λ 設定為 0.9。"]},{"title":"2.5 Entropy以及Modified Entropy","paragraphs":["評估的內容中有一項很重要的標準─Entropy,它被廣泛的使用在測量資訊上面(Berger, Vincent J. Della Pietra & Stephen A. Della Pietra, 1996)。其定義為下列公式(9):","2() ()log () xTH XPxPx ∈=−"]},{"title":"∑  ","paragraphs":["(9) 其中隨機變數 X 涵蓋的範圍包含可預測的 T 集合(例如字母,字詞或部分的語音),這裡 表示非對稱簡繁字合併非對稱簡繁字前後單字詞的字串。 ()Px 為 MLE 所計算出來的機 率值,x 表示 X 的 bigram。因為只需要取得機率連乘後的最大值,所以我們減少公式(9) 的計算量,加快計算時間。我們實際計算時使用公式(9)的 Entropy 改寫後的公式(10) Modified Entropy。","'","10() log () xTH XPx ∈=−"]},{"title":"∑  ","paragraphs":["(10)   基於對照表以及語言模型之簡繁字體轉換 27"]},{"title":"3. 實驗結果與分析","paragraphs":["由於統計模板頻率需要大量的語料資料,因此我們蒐集新聞語料庫做為我們的語料庫。 語料庫的整理如表四所示:"]},{"title":"表四、新聞語料庫資料明細","paragraphs":["資料來源 年份 新聞社 文件數 檔案大小 新聞語料庫 1998-1999 China times 38,163 209MB China times Commercial 25,812 China times Express 5,747 Central Daily News 27,770 China Daily News 34,728 1998-1999 United Daily News 249,508 320MB 2000-2001 United Daily News 172,421 1.03GB United Express 91,958 Ming Hseng News 168,807 Economic Daily News 463,873","對照表部份,我們使用維基百科提供的非對稱簡繁字對照表,其中非對稱簡繁字的 數量為 475 筆,當系統判斷要轉換的字為非對稱簡繁字時使用語言模型進行 bigram 的計 算,選擇出分數最佳的對應字;如果系統判斷要轉換的字為一對一單字詞時,則使用維 基百科提供的 6144 筆一對一單字詞對照表直接進行轉換。系統的輸入以及輸出皆使用 Unicode 編碼的文字。我們也從維基百科中抽取了 7180 筆簡繁用詞對照表,使用方式如 前述。如圖四所示:先判斷句中是否含有專有名詞以及一般動名詞,如果有則先轉換, 否則進行一對一單字詞轉換以及非對稱簡繁字轉換。如圖四中出現可以對應的一般名詞 的轉換,「公車」相互對應「公交车」;接著,判斷句中是否含有非對稱簡繁字,如果 有則使用語言模型計算出最佳的對應字,否則直接進行轉換。如圖四中出現可以直接轉 換的「時」與「时」,以及需要使用語言模型進行計算分數的「麵」與「面、麵、麪、 麫」。  簡體: 我坐 公交车 吃 面 时 。          繁體: 我坐 公車 吃 面麵麪麫 時 。 "]},{"title":"圖四、系統轉換的例子  ","paragraphs":["28 李民祥 等","語言模型計算分數部分,我們計算當簡體字轉換為繁體字需要選擇非對稱簡繁字時 候的分數,使用前述的 Modified Entropy 做為最後計算的分數。Modified Entropy 越低代 表該字的組合機率越高。因此,我們選擇計算 Modified Entropy 最低的組合。表五為圖 四中非對稱簡繁字的語言模型分數計算例子。我們計算出「吃面」與「面。」、「吃麵」 與「麵。」、「吃麪」與「麪。」以及「吃麫」與「麫。」等四種非對稱簡繁字組合在 語言模型中出現的機率相乘的 Modified Entropy 分數。其中,「麪」以及「麫」在訓練 的語料庫當中出現次數皆為 0,所以只會計算「吃」以及「。」的 unigram 機率,才會 造成「吃麪。」以及「吃麪。」Modified Entropy 分數相同的狀況。"]},{"title":"表五、非對稱簡繁字的Modified Entropy計算分數","paragraphs":["簡體=>繁體 Modified Entropy 吃面。=>吃面。 18.164046939 吃面。=>吃麵。 12.016282836 吃面。=>吃麪。 62.00000001 吃面。=>吃麫。 62.00000001 圖四最後轉換的結果如圖五所示:  簡體: 我坐 公交车 吃 面 。      繁體: 我坐 公車 吃 麵 。 "]},{"title":"圖五、轉換結果","paragraphs":["簡繁轉換大部分的問題是出在非對稱簡繁字的問題上,因此我們的測試集主要針對 句中包含非對稱簡繁字的簡體句子進行簡體轉換繁體的測試。然而,各個領域皆有其適 用的簡繁轉換對照,這部份透過收集大量的對照表即可正確轉換。而在王寧(2005)中總 共有 15 萬的句子,我們使用王寧(2005)提供的 271 句,其中 271 句為常見轉換字詞出現 的問題包含非對稱簡繁字的簡體中文小說句子進行測試。因為小說使用的文字多為一般 讀者較常接觸的一般動名詞,因此可以較準確的評估我們系統的正確性。圖六為我們進 行測試的部分資料,表六為測試集的資料整理。其中,一對一單字詞為僅有一種可能的 轉換結果,因此我們主要評估非對稱簡繁字轉換的正確與否。評估部分,我們使用 Accuracy,如公式(11)所定義。   基於對照表以及語言模型之簡繁字體轉換 29 "]},{"title":"圖六、部分測試集 表六、測試集資料整理","paragraphs":["非對稱簡繁字字數 一對一單字詞字數 756 4418","Accuracy 100%tc W=× (11) tc 表示正確轉換非對稱簡繁字的字數,W 表示非對稱簡繁字的字數。我們比較過去文獻 以及目前市面上的簡繁轉換方式,並未發現同樣使用語言模型的轉換方式。目前轉換品 質較佳的系統如 Google Translate、Microsoft Word 2007、溫普敦、同文堂等四種知名的 翻譯軟體。圖七為這四種系統與我們系統的比較。  88.62% 86.77% 74.87% 65.34% 94.84% 0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00% Google Translate 同文堂 溫普敦 Word 2007 本系統"]},{"title":"圖七、與其它系統比較的結果  ","paragraphs":["30 李民祥 等 圖七的實驗結果顯示,我們的系統對於簡繁轉換的效果比其它幾種效果來得好。因此, 我們找出未被成功轉換的非對稱簡繁字,如表七所示。其中帶有底線的為轉換錯誤的字。"]},{"title":"表七、部份轉換錯誤的句子","paragraphs":["轉換錯誤的句子 已經幹了的道路 這是從前麵茶棚裡留聲機上放出來的 外麵糊了紙 現在他剛從六百裡外的煤礦回來 她摸出表來看 但是她依舊昂然自得地畫動槳 好一出大悲劇 接著,我們針對錯誤的部分,以手動方式蒐尋可能造成錯誤的對照表內容,發現對照表 中含有容易因為斷詞不佳時會造成轉換錯誤的對照詞彙,如表八所示:"]},{"title":"表八、斷詞不佳時容易造成轉換錯誤的對照詞彙","paragraphs":["繁體用詞 幹了 麵茶 麵糊 裡外 簡體用詞 干了 面茶 面糊 里外","因為它們包含了非對稱簡繁字的單字詞,因此簡體字的「干了」並非只能轉換為如 對照表中繁體字的「幹了」,而是可以轉換為「乾了」或是「幹了」。;簡體字的「面 茶」並非只能轉換為繁體字的「麵茶」。這是因為簡體字的「面」在繁體字時經常使用 在「裡面」、「外面」、「上面」等詞彙。但是,簡體字的「面」與後一個字成詞時則 成為「麵茶」、「麵糊」等詞彙;同理,其它容易因為斷詞不佳而造成轉換錯誤的對照 表內容也是一樣的狀況。因此,斷詞的正確與否,對於簡繁轉換有著絕對的影響力。所 以我們進一步引入中國科學院開發的簡體字斷詞系統(中科院計算所,2009),嘗試改善 上述的問題。我們將斷詞後不成詞的連續單字詞合併,避免文字資訊遺失。如圖八所示, 經由合併連續單字詞的步驟可以保留原有的文字資訊,使得圖八例子中簡體字的「看表」 可以找到對照表中繁體字的「看錶」。 "]},{"title":"圖八、合併斷詞後的連續單字詞   ","paragraphs":["基於對照表以及語言模型之簡繁字體轉換 31","引入斷詞系統後,表七所示的幾種狀況可獲得改善。例如簡體字的「前面茶棚」應 該為繁體字的「前面」以及「茶棚」,但未引用斷詞的系統會因為對照表中含有簡體字 「面茶」對應至繁體字「麵茶」的關係,造成簡體字的「前面茶棚」轉換為繁體字的「前 麵茶棚」的錯誤結果;引入斷詞系統後可以正確斷出「前面」以及「茶棚」,使得轉換 結果正確。再次執行實驗後,其結果如圖九所示。","實驗結果顯示,引入斷詞系統雖然可以改善系統效能,但成效不大。因此,我們關 心剩下沒被成功轉換的非對稱簡繁字的類型。我們發現主要的錯誤轉換為要轉換為「錶」 卻轉換為「表」、要轉換為「划」卻轉換為「劃」、要轉換為「齣」卻轉換為「出」。 因此,我們找出這些非對稱簡繁字被錯誤轉換時,使用語言模型計算分數的 bigram 組合 字以及其句子。表九為錯誤轉換的類型以及被轉換錯誤的 bigram 組合字的 Modified Entropy 分數,其中帶有底線的為轉換錯誤的字。表九中錯誤轉換的非對稱簡繁字是因為 進行計算的 bigram 在語言模型的機率低於被轉換的 bigram 的機率,因而轉換為不正確 的字。然而,語言模型中所有的 bigram 皆由 unigram 組合起來。因此,unigram 頻率較 高的字,自然會擁有較多的 bigram。基於這個理由,我們找出「表」、「出」、「劃」、 「錶」、「齣」、「划」等六個主要被錯誤轉換的 unigram 頻率。我們發現,由於「表」、 「出」、「劃」在語言模型中 unigram 的頻率皆為「錶」、「齣」、「划」的一百倍以 上,造成大多數的 bigram 皆由 unigram 頻率高的那方組成,使得「錶」、「齣」、「划」 擁有較少可以依據的 bigram 頻率資訊來做為能夠被正確轉換的 bigram 頻率。至此,我 們透過對照表、語言模型以及加入斷詞後的系統,仍有無法解決的非對稱簡繁字類型, 這些類型是我們認為困難的問題。適當的頻率可以使得簡繁轉換一對多的非對稱簡繁字 被正確轉換,過度的頻率會使得轉換錯誤,過度頻率只如表十所示。  "]},{"title":"圖九、第二次實驗比較系統斷詞前與系統斷詞後的結果","paragraphs":["94.84% 95.77% 94.20% 94.40% 94.60% 94.80% 95.00% 95.20% 95.40% 95.60% 95.80% 96.00% 本系統_使用斷詞前 本系統_使用斷詞後   32 李民祥 等"]},{"title":"表九、部分錯誤轉換的類型","paragraphs":["轉換錯誤的句子 計算的組合字 Modified Entropy 分數 醇王府的汝窯大瓶您不是唱一出《鎖五龍》就搬來了 嗎? 一出《 9.949089426 一齣《 12.5808102 開蒙第一出學的《武家坡》。 一出學 10.672644324 一齣學 14.336685057 佩珠打算回去,她摸出表來看,快到拾二點鐘了 摸出表來 18.568075432 摸出錶來 23.575641152 摸齣表來 45.15278956 摸齣錶來 69.91470532 然後自己坐到船尾,把住槳慢慢地劃起來。 地划起 14.946993213 地劃起 13.737994911"]},{"title":"表十、過度的頻率比較","paragraphs":["頻率比較  出 齣 0.004183773 0.000012075 表 錶 0.002347138 0.000006012 劃 划 0.000268397 0.000015572","我們使用語料庫的 10%(158MB)、30%(475MB)、50%(792MB)、70%(1109MB)以及 100%(1584MB)大小來建構語言模型,以及將對照表的大小分為 10%(718 筆)、30%(2154 筆)、50%(3590 筆)、70%(5026 筆)以及 100%(7180 筆),利用不同的語言模型大小以及對 照表大小來判斷是否會影響簡繁轉換的準確性。評估方式如公式(11)定義的 Accuracy。 如圖七所示:橫軸為使用不同的語言模型大小,每一條線分別代表使用不同的對照表大 小,縱軸為 Accuracy。","從表十一中我們可以看出,簡繁轉換的 Accuracy 隨著語言模型以及對照表使用的數 量越來越大時,Accuracy 也越來越高。使用我們系統建構的 1584MB 的語言模型大小以 及 7180 筆的對照表大小時 Accuracy 可達到 95.77%。其中我們注意到,當對照表大小從 50%開始,對照表對於 Accuracy 的提升較無語言模型大小 158MB 以及 475MB 時來得顯 著。這是因為我們的測試集主要針對非對稱簡繁字進行轉換的測試,大部分句子沒有包 含需要簡繁用詞轉換的專有名詞以及一般名詞。劉匯丹(2008)提到,一個好的簡繁轉換 系統必須要有足夠的知識,方能轉換出正確的詞彙。意思是說,因為中國大陸與台灣因   基於對照表以及語言模型之簡繁字體轉換 33 為文化關係,許多專有名詞以及一般動名詞使用不同名詞但是意思相同的詞彙,例如先 前提到的「公車」互相對應「公交車」。因此需要大量的對照表來提供應該正確轉換的 詞彙。語言模型大小部分,因為測試集主要針對非對稱簡繁字進行轉換的測試,因此當 語言模型越大時,我們可以看出 Accuracy 有顯著的提升。"]},{"title":"表十一、簡體文字轉換繁體文字使用不同大小的語言模型以及對照表的結果 ","paragraphs":["Lookup table size  Language model size 718筆 2154筆 3590筆 5026筆 7180筆 158MB 91.14% 91.80% 92.59% 92.72% 92.72% 475MB 91.93% 92.86% 93.65% 93.65% 93.92% 792MB 92.46% 92.99% 93.92% 93.92% 94.05% 1109MB 93.65% 94.05% 94.97% 94.97% 95.11% 1584MB 94.18% 94.58% 95.50% 95.50% 95.77%","繁體轉簡體的實驗部分,我們以 Google、Yahoo 網路新聞語料作為測試資料。如圖 十:包含新聞類別、科技類別以及旅遊類別一共 70 篇文章,字數一共 51350 字,其中包 括了新聞文章(34 篇文章)、科技文章(26 篇文章)、旅遊文章(10 篇文章)。經由 Google 翻譯及本系統系統轉換後,一共找出 55 句裡面詞彙轉換不相同的地方。接著請 以簡體字為第一語文的使用者,選出轉換後用法較佳的詞彙,而 Google 轉換較佳詞彙有 32 個詞彙,本系統轉換後較佳詞彙有 23 個詞彙。 "]},{"title":"圖十、第三次實驗繁轉簡實驗結果","paragraphs":["51.18% 41.82% 0.00% 10.00% 20.00% 30.00% 40.00% 50.00% 60.00% Google 本系統   34 李民祥 等 轉換結果顯示,本系統被認為轉換不佳的詞彙的原因,在於我們使用的對照表,是 Wikipedia 提供的簡繁轉換對照表。因此,雖然我們系統成功的依照 Wikipedia 提供的對 照表完成轉換,但是我們發現了兩種情況:","1. 以簡體字為第一語文的使用者依然不習慣對照的詞彙。例如:表十二、第 6 點:公車 和公交車,使用者覺得公車為較佳,公交車並非轉換錯誤。","2. 某些詞彙需要依靠上下文來決定是否需要進行對照詞彙的轉換,或者直接轉換為簡體 字即可。如表十二、第 7 點:電腦轉換出的詞彙,前面加上 Windows,計算器在簡體 用法廣義只電子產品,所以電腦為較佳詞彙。"]},{"title":"表十二、相同句子本系統和Google系統轉換後比較,此為Google轉換為較佳例子。","paragraphs":["Google轉換後的句子 本系統轉換後的句子 1 就是最重要的关键 就是最重要的牛鼻子 2 和我尝试的一个蓝牙设备也能够顺利 地连接 和我尝试的一个蓝牙设备也能够顺利地 访问 3 更欣赏他的爱家爱老婆 更欣赏他的爱家爱爱人 4 台中地检署也请相关单位搜集资料进 行了解 台中地检署也请相关单位蒐集材料进行 了解 5 新台币汇率 新台币外汇牌价 6 公车或火车转程就能抵达 公交车或火车转程就能抵达 7 我很容易就实现了和一台 Windows 电脑 的媒体文件同步 我很容易就实现了和一台Windows计算器 的媒体文件同步 另外轉換結果顯示,本系統優於 Google 在於 Google 所使用的對照表是將詞彙中的字直 接做轉換。例如:表十三、第 1 點:繁體的網路在簡體用法是使用互聯網。"]},{"title":"表十三、相同句子本系統和Google系統轉換後比較,此為本系統轉換為較佳例子。","paragraphs":["Google轉換後的句子 本系統轉換後的句子 1 MV 在网路上引发负面讨论后 MV 在互联网上引发负面讨论后 2 扫除所有垃圾资讯 扫除所有垃圾信息 3 澳洲航空(Qantas)指出 澳大利亚航天(Qantas)指出 4 并考量一般使用之超音波简易测定的 数值并不精准 并考量一般使用之超声波简易测定的数 值并不精准 5 除国语名称与「QQ」出奇相似外 除普通话名称与「QQ」出奇相似外 6 而且用病毒方式传播软体 而且用病毒方式传播软件 7 目前除了使用在软性可弯曲的萤幕 目前除了使用在软性可弯曲的屏幕   基於對照表以及語言模型之簡繁字體轉換 35"]},{"title":"4. 結論","paragraphs":["本篇論文的研究主要是改善傳統簡繁轉換僅執行一對一編碼轉換,而沒有考慮非對稱簡 繁字的問題,造成非對稱簡繁字一直無法有效的被正確轉換。因此,以實驗的測試集為 例,測試集包含的非對稱簡繁字為日常用字佔測試集中所有字數約 15%,我們的系統可 以將這 15%的非對稱簡繁字執行 94.84%的正確轉換。第二次實驗加入了斷詞後僅能夠提 高約 1%的正確率,並且餘下轉換錯誤的類型是我們認為困難的轉換字,需要倚靠其它方 法來解決。","實驗部分我們調整語言模型以及對照表的大小來測試是否對於簡繁轉換的效能有 影響。從實驗結果來看,語言模型數量越大對於轉換結果有正向幫助,但是如果語言模 型數量過大,卻會影響系統轉換的速度。這也是本系統建構語言模型時只考慮 bigram 分 數的原因,因為建構 trigram 會使得語言模型數量過大,造成轉換速度下降。對照表部分, 由於中國大陸與台灣有許多用詞不同的狀況,因此需要大量的對照表提供正確轉換的詞 彙。但是對照表數量過於龐大,也會造成系統轉換速度下降。因此,對照表的建構可以 針對特定領域蒐集對照的詞彙,例如醫學領域的對照詞彙、資訊科學的對照詞彙...等, 如此一來,針對需要轉換的用詞領域來蒐集對照表,減去不必要的資訊儲存於對照表中, 避免對照表數量過大的情況。","本研究提供的方式可以讓其它研究人員以及使用者自行選擇建構語言模型的大小 以及語料庫,對照表也能夠讓各人員自行選擇想要使用的對照表。因此本研究具有彈性 的架構使得系統可以持續擴充改進。在未來,我們將著手建構簡體中文以及繁體中文的 平行語料庫,利用簡體中文以及繁體中文的文法幾乎相同的特性,使用一些找尋新詞的 方式,嘗試找出繁體中文內被判斷為新詞的詞彙,但是簡體中文對列句子的相同位置卻 沒有發現可能是名詞的詞彙,接著利用繁體中文句子中新詞的上下字為線索,找尋出簡 體中文對列句子中可能為對應詞彙的新詞。將發現的新詞加入系統的對照表中,藉以自 動擴展對照表的內容。","而在繁轉簡的實驗「沒有字轉換的錯誤」,只有「詞彙不同」的問題。例如:關鍵 轉換成牛鼻子,原因在對照表關鍵定義為牛鼻子。因此我們觀察一些不同的轉換結果, 發現對照表當中有些詞彙,並不是雙向轉換都適合的,並可加入詞意的判別,增加對照 表中的詞彙單向對雙向的轉換,例:當計算器前面出現 Windows 時,應轉換成电脑,利 用前後文來增加在轉換的準確性。"]},{"title":"致謝","paragraphs":["本研究依經濟部補助財團法人資訊工業策進會「100 年度數位匯流服務開放平台技術研 發計畫」辦理。     36 李民祥 等"]},{"title":"參考文獻","paragraphs":["王曉明、魏林梅(2008,12 月)。談簡繁轉換的幾個關鍵問題。 5TH CDF 研討會數位社","群雙效(CD2E)。","王寧、王曉明(2005,10 月)。兩岸四地漢字的轉換與溝通。第三屆兩岸四地中文數位化","合作論壇,台北。","李樹德(2009)。Word“中文簡繁轉換”存在的問題與解決對策。2009 年 9 月 2 日,取自 http://www.yywzw.com/show.aspx?id=1570&cid=142.","劉匯丹、吳健(2008,12 月)。基於詞語消歧的分層次漢字簡繁轉換系統。5TH CDF 研討","會數位社群雙效(CD2E)。","陳勇志、吳世弘、盧家慶、谷圳(2009,九月)。中文混淆字集應用於別字偵錯模板自動產","生。第二十一屆自然語言與語音處理研討會,台北。","洪大弘(2008)。基於語言模型及正反面語料知識庫之中文錯別字自動偵錯系統。朝陽科技 大學資訊工程系碩士論文。","Hepp, M., Siorpaes, K., & Bachlechner, D. (2007). Harvesting Wiki Consensus: Using Wikipedia Entries as Vocabulary for Knowledge Management. IEEE Internet Computing, 11(5), 54-65.","Rosenfeld, R. (1992). Adaptive Statistical Language Modeling: a Maximum Entropy Approach. Ph.D. Thesis Proposal, Carnegie Mellon University..","Katz, S. (1987). Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. IEEE Transactions on ACOUSTICS, SPEECH, and SIGNAL PROCESSING, 35(3), 400-401.","Goodman, J. (2001). A Bit of Progress in Language Modeling, Extended Version. (Technical Report MSR-TR-2001-72). Microsoft Research, 2001.","Berger, A. L., Pietra, V. K. D., & Pietra, S. A. D. (1996). A maximum entropy approach to natural language processing. Computational Linguistics, 22(1), 39-71."]},{"title":"語料庫與工具","paragraphs":["中科院計算所 ICTCLA2009, http://ictclas.org/index.html Google translate. http://translate.google.com.tw/#zh-CN|zh-CN| Microsoft Office, http://office.microsoft.com/zh-tw/ 溫普敦, http://www.winperturn.com.tw/ 同文堂, http://tongwen.openfoundry.org/ "]}]}