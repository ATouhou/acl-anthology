{"sections":[{"title":"","paragraphs":["1  1","2","2 1  2  69308027@cc.ntnu.edu.tw, ymyeh@ice.ntnu.edu.tw, berlin@csie.ntnu.edu.tw   (Speech Robustness)  (Histogram Equalization)   (Data Fitting)(Inverse Function)   (Moving Average) (Non-stationary Noise)    AURORA-2 "]},{"title":"1. ","paragraphs":["(Automatic Speech Recognition, ASR)  (Environment Mismatch) (Background Noise) (Channel Effect)(Speech Robustness)    (1)(Additive Noise)(2) (Convolutional Noise) (Linearly Additive)  2   [1] (1) (Speech Enhancement)  (Uncorrelated) (Noisy Speech) (Clean Speech)(Spectral Subtraction, SS)[2](Wiener Filter, WF)[3] (2) (Robust Speech Feature)  (Cepstrum Mean Subtraction, CMS)[4](Cepstrum Mean and Variance Normalization, CMVN)[5] (3) (Acoustic Model Adaptation) (Adaptation Data) (Mean Vector)(Covariance Matrix)  (Maximum a Posteriori, MAP)[6](Maximum Likelihood Linear Regression, MLLR)[7] (CMS) (CMVN) (Moment)   [8] [9]  (Histogram Equalization)[10]  (Test Speech)(Cumulative Density Function, CDF) (Training Speech) (Reference Distribution) ()   ()  "," ts th tn tnthtsty 3   [11][12] (Vector Quantization) (Distributed Speech Recognition, DSR)  (Quantization Distortion)  (Table Look-Up based Histogram Equalization, THEQ)[10]   (Quantile-based Histogram Equalization, QHEQ)[13,14]  (Grid Search)  (Data Fitting)     "]},{"title":"2.      ","paragraphs":["(Log Energy) (Mel Filter-Bank) (Mel-Frequency Cepstral Coefficient)   4 [10][15](THEQ) (QHEQ)"]},{"title":"2.1 (Histogram Equalization, HEQ) ","paragraphs":["()(Mel- Frequency Cepstral Coefficients, MFCC)[16][17][18] [10][19][20] (Transformation Function) [19][20] x (Probability Density Function, PDF) xpTest xF x y ypTrain xpTest (1) "," dy yFd yFp","dydx xpyp TestTestTrain 1 1 ","  (1)","yF 1 ","xF (Inverse Function) (Cumulative Probability Density Function, CDF)    yC dyyp dy dy ydF yFp dxxpxC Train y xFyTrain xF Test","x TestTest    "]},{"title":"‡ ‡ ‡","paragraphs":["f f   f    |'' ' ' ' ' '' 1 1 (2) xCTest"]},{"title":"yC","paragraphs":["Train 'y 'xF xF","xCCxF TestTrain1 )( (3)","1 TrainC TrainC  1.0           xCTest 1.0 xCTrain x y  5 (Cumulative Histogram) i (Quantiles)(Mean)  i  (Equalization) (Table-Lookup)(Table Look-Up based Histogram Equalization, THEQ)  "]},{"title":"2.2 (Quantile-based Histogram equalization, QHEQ) ","paragraphs":["(Nonparametric) [19][20] xH   ‚‚ „ • ¤¤ ' § ‚‚ „ • ¤¤ ' §","‚‚ „ • ¤¤ ' §  KK K Qx","Qx QxH DD J 1 (4) KQ D J xH (5) (Quantile Correction) (Minimum Mean Square Error, MMSE) D J (5) D J"]},{"title":"‘̂","paragraphs":["‘̂","‚ „ • ¤ '","§ "]},{"title":"ƒ ","paragraphs":["1 1 2 ,minarg, K k train kk QQH JDJD (5)","K kQ k train kQ k (5) D J (4) (Weight Average) J (5) D J "]},{"title":"3.  ","paragraphs":["6   [21]"]},{"title":"3.1 (Polynomial-Fit Histogram Equalization, PHEQ) ","paragraphs":["ii vu , (Response Variable) iv (Explanatory Variable) iu (Regression Model) iu iv iuG (Coefficients) iuG iv~ (Minimum Sum of Squares Error) iu iv~ iv (Least Squares Regression) iuG M "]},{"title":"ƒ  ","paragraphs":["M m m im M iMiiii uauauauaauGv","0 2 210~ (6)","Maaa ,,, 10 (Coefficients) 2 E"]},{"title":"ƒƒ ","paragraphs":["‚ „ • ¤ '","§ N i M m m imi uavE","1 2","02 (7)  (Polynomial-Fit Histogram Equalization, PHEQ) iy iy iTrain yC (7) "]},{"title":"ƒ  ","paragraphs":["M m m","iTrainmiiTrain yCayyCG 0~ (8)","2 'E "]},{"title":"ƒƒ ","paragraphs":["‚ „ • ¤ '","§ N i M m m","iTrainmi yCayE 1 2","02 ' (9) N (Frame) Maaa ,,, 10 (10) Maaa ,,, 10 Mm aE m 1,0 '2  ww (10) "]},{"title":">@","paragraphs":["NyyyY ,, 21  Y S Y iy 7    N yS yC","ipos i | (11) ipos yS iy S N (11)(8) (9)(10) iy iyC ((8))     "]},{"title":"3.2 (Moving Average, MA)  ","paragraphs":["(Non-stationary Noise)(Sharp Peak)(Valley)     [22] (Feature Normalization) (Mean Subtraction) (Variance Normalization) (Auto-Regression Moving Average, ARMA)  [22] z (Non-Casual Moving Average) 8  ~ , 12 ~ ˆ  fl  fi› d "]},{"title":"ƒ","paragraphs":["otherwisey LTtLif L y y t L","Li it","t (12) z (Casual Moving Average)  ~ , 1 ~ ˆ 0  fl  fi› d "]},{"title":"ƒ","paragraphs":["otherwisey TtLif L y y t L i it t (13) z (Non-Casual Auto Regression Moving Average)  ~ , 12 ~ˆ ˆ 10  fl  fi › d   "]},{"title":"ƒƒ","paragraphs":["otherwisey LTtLif L yy y t L i L j jtit","t (14) z (Casual Auto Regression Moving Average)  ~ , 12 ~ˆ ˆ 10  fl  fi › d   "]},{"title":"ƒƒ","paragraphs":["otherwisey TtLif L yy y t L i L j jtit","t (15) iy~ iŷL Order of Moving Average"]},{"title":"3.3    ","paragraphs":["(16) ii yy |̂ D ty [19][20] D  1̂ ttt yyy uu DD (16) iy iŷD D ''E  "]},{"title":"ƒ ","paragraphs":["N i iCleaniNoisy yyE 1 2 )()('' (17) )(inoisyy i (16) )(icleany (16)(17) (Outlier)(Threshold)  9"]},{"title":"4. 4.1 ","paragraphs":["Aurora-2 (European Telecommunications Standards Institute, ESTI)[23] (Airport) (Babble)(Car)(Exhibition)(Restaurant)(Subway)(Street) (Train Station)(Signal-to-Noise Ratio, SNR) 20dB15dB10dB5dB0dB -5dB -G.712 MIRSAurora-2 Set ASet B Set CSet A (Stationary)Set B (Nonstationary)Set C (Acoustic Models)(1~9 zero oh) (left-to-right) (Continuous Density Hidden Markov Model, CDHMM) 16 (State) 3 (Gaussian Mixture Distribution)(Silence) (Pause) HTK [24] (Front-End Processing) (Mel-Frequency Cepstral Coefficients, MFCCs)(Frame Length) 25(Frame Shift)103912 (Log Energy)13 (Delta Coefficient)(Acceleration Coefficient)                   iy  )(iTrainTrain yC )( iTestTest yC iyCG maa ,,1 i"]},{"title":"y~","paragraphs":["i"]},{"title":"yˆ ","paragraphs":["10   3 5 7 9 22.39 21.54 21.08 21.30 1000 21.80 21.46 21.13 21.16 100 22.68 21.31 20.75 20.55  (Clean-Condition) 10 23.42 22.20 22.54 23.42 10.80 10.34 10.43 10.54 1000 10.48 10.32 10.40 10.45 100 10.73 10.45 10.36 10.45  (Multi-Condition) 10 11.65 10.61 10.79 11.58"]},{"title":"4.1  ","paragraphs":["(Polynomial Order)  (Histogram Bins)100010010 (Order) (Word Error Rate, WER) Aurora-2(Sets A, B C)(20dB0dB) (End Behavior)   (Overfit) 7100 7 100"]},{"title":"4.2   ","paragraphs":["0  [22](Non-Casual ARMA)  20%(Relative Improvement) 5%  "]},{"title":"4.3 ","paragraphs":["(16) D 1 D 11  (Word Error Rate, WER) 0 1 2 3 4 5 Non-Casual MA 20.75 17.75 16.83 17.26 18.15 19.66 Casual MA 20.75 19.23 18.28 17.44 17.12 17.28 Non-Casual ARMA 20.75 17.83 16.90 16.38 16.99 17.34    Casual ARMA 20.75 17.93 16.84 19.20 17.44 19.20 Non-Casual MA 10.36 9.88 9.88 10.24 10.94 11.69 Casual MA 10.36 10.13 9.74 9.76 9.78 10.12 Non-Casual ARMA 10.36 9.88 9.78 9.84 9.94 10.11    Casual ARMA 10.36 9.95 9.71 10.84 9.76 10.68              (dB)  (%) Alpha_0 Alpha_0.1 Alpha_0.2 Alpha_0.3 Alpha_0.4 Alpha_0.5 Alpha_0.6 Alpha_0.7 Alpha_0.8 Alpha_0.9 Alpha_1            (dB)    (% )             12  WER(%) Set A Set B Set C MFCC 41.06 41.52 40.03 41.04 AFE 38.69 44.25 28.76 38.93 CMVN 27.73 24.60 27.17 26.37 MS+VN+ARMA(3) 18.38 16.14 21.81 18.17 THEQ 19.72 18.57 19.24 19.16 QHEQ 23.53 21.90 22.36 22.64 PHEQ 20.98 20.17 21.43 20.75 PHEQ+MA 16.83 15.10 20.02 16.78   PHEQ+ D +MA 16.19 15.17 19.72 16.49 MFCC 14.78 16.01 19.33 16.18 AFE 10.64 10.76 12.85 11.13 CMVN 12.70 12.45 14.52 12.98 MS+VN+ARMA(3) 9.49 10.37 10.06 9.95 THEQ 10.02 10.41 10.34 10.24 QHEQ 10.20 10.75 10.76 10.53 PHEQ 9.91 9.41 13.14 10.36 PHEQ+MA 9.41 9.53 11.21 9.82   PHEQ+ D +MA 9.15 9.08 11.53 9.60 0 D    (Smoothing)  (16) (12)~(15)"]},{"title":"4.4  ","paragraphs":["(Advanced Front-End Processing, AFE)(CMVN) (THEQ)(QHEQ)3 (MS+VN+ARMA) (PHEQ) 3 (PHEQ+MA)D 0.6 1 (PHEQ+ D +MA) [18][14]   13 (PHEQ+MA PHEQ+D +MA)"]},{"title":"4.5  ","paragraphs":["(Stereo-based Piecewise Linear Compensation, SPLICE)[25] (Heteroscedastic Linear Discriminant Analysis, HLDA) [26] (Maximum Likelihood Linear Transformation, MLLT)[27] (Discrete Cosine Transform, DCT)(Partial Decorrection) [28] t 4 tz (Feature Supervector) tz T ty t","T t zy T (18)   25%  Aurora-2  512 kr [25] tx 4tx 4tx ‹› ̆ ! tz !T t","T t zy T ! ty 4tx 1tx 1tx 4txtx!  14  "]},{"title":"ƒ ƒ    ","paragraphs":["N t t N t ttt k ykp yxykp r 1 1 | | (18) N ty t tx k k tyc tyĉ  ktt kktk k ryy yNck ˆ̂ ,;maxargˆ c c 6c P (19) tyc (Likelihood) k  (18) Set C  "]},{"title":"5.         ","paragraphs":["ktt ryy c cˆ    "]},{"title":"ƒ ƒ      ","paragraphs":["1 0 1 0 | | T t t T t ttt k ykp yxykp r  "," kr  k ty'ˆ"," ty'  kktk k yNck 6c ,;maxarĝP (Stereo Data)"," tx"," ty"," ty  15  WER(%) Set A Set B Set C HLDA-MLLT+CMVN 21.63 21.37 21.59 21.52 HLDA-MLLT+PHEQ-MA 15.98 15.96 15.91 15.96 SPLICE+CMVN 16.34 14.95 21.18 16.75   SPLICE+PHEQ-MA 13.40 13.41 17.08 14.14 HLDA-MLLT+CMVN 9.49 9.51 10.40 9.68 HLDA-MLLT+PHEQ-MA 9.06 8.87 8.55 8.88 SPLICE+CMVN 10.40 11.00 13.80 11.32   SPLICE+PHEQ-MA 9.54 10.88 12.18 10.60 HLDA+MLLT 8.88% SPLICE 14.14%"]},{"title":"6.","paragraphs":["[1] Y. Gong, “Speech Recognition in noisy environments: A survey,” Speech communication, Vol.16, 1995. [2] S.F. Boll, “Supperssion of Acoutstic Noise in Speech Using Spectral Subtraction,” IEEE Trans. on ASSP, Vol.27, No.2, pp.133-120, 1979. [3] X. Huang, A. Acero and H. Hon, “Spoken Language Processing: A Guide to Theory, Algorithm and System Development,” Prentice Hall PTR Upper Saddle River, NJ, USA, 2001. [4] S. Furui, “Cepstral Analysis Techniques for Automatic Speaker Verification,” IEEE Trans. on ASSP, 1981. [5] A. Viikki and K. Laurila, “Cepstral Domain Segmental Feature Vector Normalization for Noise Robust Speech Recognition,” Speech Communication, Vol. 25, 1998. [6] J.L. Gauian and C.H. Lee, “Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains,” IEEE Trans. on Speech and Audio Processing, 1994. [7] C.J. Leggetter and P.C. Woodland, “Maximum Likelihood Linear Regression for Speaker Adaptation of Continuous Density Hidden Markov Models,” Computer Speech and Language, 1995. [8] Y. H. Suk, S. H. Choi, H. S. Lee, “Cepstrum Third-order Normalization Method for Noisy Speech Recognition,” Electronics Letters, Vol. 35, no. 7, pp. 527-528, April 1999. [9] C.W. Hsu and L.S. Lee, “Higher Order Cepstral Moment Normalization (HOCMN) for Robust Speech Recognition,” in Proc. ICASSP 2004. [10] S. Dharanipargda and M. Padmanabhan, “A Nonlinear Unsupervised Adaptation Technique for Speech Recognition,” in Proc. ICSLP 2000. 16 [11] C. Y. Wan and L.S. Lee, “Joint Uncertainty Decoding (JUD) with Histogram-Based Quantization (HQ) for Robust and/or Distributed Speech Recognition,” in Proc. ICASSP 2006. [12] C.Y. Wan and L.S. Lee, “Histogram-based quantization (HQ) for robust and scalable distributed speech recognition,” in Proc. EUROSPEECH 2005. [13] F. Hilger, H. Ney, “Quantile based histogram equalization for noise robust speech recognition,” in Proc. EUROPSEECH 2001. [14] F. Hilger et al., “Quantile Based Histogram Equalization for Noise Robust Large Vocabulary Speech Recognition,” IEEE Trans. on Speech and Audio Processing, Vol. 14(3), 2005. [15] A. de la Torre et al., “Non-linear Transformation of the Feature Space for Robust Speech Recognition,” in Proc. ICASSP 2002. [16] S. Molau et al., “Histogram Based Normalization in the Acoustic Feature Space,” in Proc. ASRU 2001. [17] S. Molau et al., “Feature Space Normalization in Adverse Acoustic Conditions,” in Proc. ICASSP 2003. [18] S. Molau et al., “Histogram Normalization in the Acoustic Feature Space,” in Proc. ICASSP 2002. [19] J. C. Segura et al., “Cepstral domain segmental nonlinear feature transformations for robust speech recognition,” IEEE Signal Processing Letters, Vol. 11(5), 2004. [20] A. de la Torre et al., “Histogram equalization of the speech representation for robust speech recognition,” IEEE Trans. on Speech and Audio Processing, Vol. 13(3), 2005. [21] S.H. Lin, Y.M. Yeh and B. Chen, “Exploiting Polynomial-Fit Histogram Equalization and Temporal Average for Robust Speech Recognition,” in Proc. ICSLP 2006. [22] C.P. Chen, J. Bilmes and K. Kirchhoff, “Low-Resource Noise-Robust Feature Post-Processing on Aurora 2.0,” in Proc. ICSLP 2002. [23] H. G. Hirsch, D. Pearce, “The AURORA Experimental Framework for the Performance Evaluations of Speech Recognition Systems under Noisy Conditions,” in Proc. ISCA ITRW ASR 2000. [24] S. Young et al., “The HTK Book Version 3.3,” 2005. [25] L. Deng, A. Acero, M. Plumpe and X. Huang. “Large-Vocabulary Speech Recognition under Adverse Acoustic Environments,” in Proc. ICSLP 2000. [26] M. J. F. Gales, “Maximum Likelihood Multiple Projection Schemes for Hidden Markov Models,” Cambridge University Technical Report RT-365, 2001. [27] G. Saon et al., “Maximum Likelihood Discriminant Feature Spaces,” in Proc. ICASSP 2000. [28] , “,” , 2005."]}]}