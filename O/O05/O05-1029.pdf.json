{"sections":[{"title":"Translation Divergence Analysis and Processing for Mandarin-English Parallel Text Exploitation Shun-Chieh Lin, Jia-Ching Wang, and Jhing-Fa Wang Department of Electrical Engineering, National Cheng Kung University. 701 No.1, Ta-Hsueh Road, Tainan City Taiwan R.O.C. Tel: 886-6-2757575 Ext. 62341, Fax: 886-6-2761693 E-mail: wangjf@csie.ncku.edu.tw  Abstract","paragraphs":["Previous work shows that the process of parallel text exploitation to extract transfer mappings between language pairs raises the capability of language translation. However, while this process can be fully automated, one thorny problem called “divergence” causes indisposed mapping extraction. Therefore, this paper discuss the issues of parallel text exploitation, in general, with special emphasis on divergence analysis and processing. In the experiments on a Mandarin-English travel conversation corpus of 11,885 sentence pairs, the perplexity with the alignments in IBM translation model is reduced averagely from 13.65 to 5.18 by sieving out inappropriate sentences from the collected corpus.  1. Introduction Over the past decade, research has focused on the automatic acquisition of translation knowledge from parallel text corpora. Statistical-based systems build alignment models from the corpora without linguistic analysis [1,2]. Another class of systems analyzes sentences in parallel texts to obtain transfer structures or rules [6]. Previous work shows that the process of parallel text exploitation to extract transfer mappings (models or rules) between language pairs can raise the capability of language translation. However, previous work is still hampered by the difficulties in transfer mapping extraction of achieving accurate lexical alignment and acquiring reusable structural correspondences. Although automatic extraction methods of lexical alignment and structural correspondences are introduced, they are not capable of handling exceptional cases like “divergence” presented in [4]. In general, divergence arises with variant lexical usage of role, position, and morphology between two languages. Therefore, while mapping extraction can be fully automated from parallel texts, divergence causes indisposed mapping extraction. Furthermore, the existence of translation divergences also makes adaptation from source structures into target structures difficult [5,7,8]. For parallel text exploitation, these divergences make the training process of transfer mapping extraction between languages impractical including parsing and word-level alignment, lexical-semantic lexicography, and syntactic structures. Therefore, study of parallel text exploitation needs a careful study of translation divergence. The framework of this paper is as follows. A brief overview of parallel text exploitation is discussed in Section 2. In Section 3, translation divergence analysis and processing for Mandarin-English parallel texts is presented. Section 4 shows experimental results with the alignments in IBM translation model. Finally, generalized conclusions are presented in Section 5. 2. Overview of Statistical-based Parallel Text Exploitation The goal of parallel text exploitation is to acquire the knowledge for translation of a text given in some source (“Mandarin”) string of words, m into a target (“English”) string of words, e. For the presented statistical approach [1] to string translation of"]},{"title":"( )","paragraphs":["me |Pr , among all possible target strings, the string will be chosen with the highest probability which is given by Bayes’ decision rule as follows:",")|Pr()Pr(maxarĝemee e= (1) Pr(e) is the language model of target language and Pr(m| e) is the translation model. In order to estimate the correspondence between the words of the target sentence and the words of the source sentence, a sort of pair-wise dependence by considering all word pairs for a given sentence pair [m, e] is assumed, referred to as alignment models. Figure 1 shows an example for the translation parameters of a sentence pair. In general, these parameters are lexicon probability, ex."]},{"title":"( )","paragraphs":["ij emp | , sentence length probability, ex."]},{"title":"()","paragraphs":["em"]},{"title":"llp |","paragraphs":[", and alignment probability, ex."]},{"title":"( )","paragraphs":["em"]},{"title":"llijp ,,|","paragraphs":[". Therefore, given more parallel texts, more probability parameters could be estimated for translation."]},{"title":"English: :","paragraphs":["21 eli"]},{"title":"eeeee LL= (How much)","paragraphs":["1"]},{"title":"(for)","paragraphs":["2"]},{"title":"(a)","paragraphs":["3"]},{"title":"(night)","paragraphs":["4"]},{"title":"?4=","paragraphs":["e"]},{"title":"l Mandarin: :","paragraphs":["21 mlj"]},{"title":"mmmmm LL= ()","paragraphs":["1"]},{"title":"()","paragraphs":["2"]},{"title":"()","paragraphs":["3"]},{"title":"()","paragraphs":["4"]},{"title":"?4=","paragraphs":["m"]},{"title":"l ()","paragraphs":["42"]},{"title":"| emp )4|4( ==","paragraphs":["em"]},{"title":"llp ()","paragraphs":["em"]},{"title":"llijp ,,4|2 == : lexicon probability : sentence length probability : alignment probability ","paragraphs":["Fig. 1. An example for the translation parameters of a sentence pair However, it is difficult to achieve straightforward and correct estimation for these probability parameters. In the above example, the English word “for” is one major factor called “divergence” makes the estimation process between sentence pairs impractical. Therefore, in the next section, we present the analysis and processing of the translation divergence for improving the performance on parallel text exploitation.  3. Translation Divergence Analysis and Processing 3.1 Analysis of Divergence Problems Dorr’s work [3] of divergence analysis is based on English-Spanish and English-German translations. Based on these two language pairs, 5 different categories have been identified. In this section, we discuss more multiform examples among the 5 types of divergences in Mandarin-English parallel texts. For each example, three sentences are given: e means an original English sentence in parallel texts, m means a Mandarin sentence, and ẽ means an amended English sentence which is better for translation parameter training with m.  3.1.1 Identification of Thematic Divergence Thematic divergence often involves a “swap” of the subject and object position and obtains unpredictable word-level alignment. For example,  e: (Is)1 (credit card)2 (acceptable)3 (to)4 (them)5 ? m: ()1 ()2 ()3 ()4 ? ẽ: (Do)1 (they)2 (accept)3 (credit card)4 ?  Here, credit card appears in subject position in e and in object position (“”) in m; analogously, the object them appears as the subject they (“”). Therefore, for the thematic divergence, the position alignments of 2↔3 and 5↔1 are obtained in a sentence pair [m, e]. However, if a sentence pair [m, ẽ] can be provided, the position alignments of 1↔2, 2↔3, and 3↔4 are better for straightforward parameter estimation of"]},{"title":"( )","paragraphs":["em"]},{"title":"llijp ,,|","paragraphs":[".  3.1.2 Identification of Morphological Divergence Morphological divergence involves the selection of a target-language word that is a morphological variant of the source-language equivalent and it raises the ambiguity of lexical-semantic lexicography.  e: (May)1 (I)2 (have)3 (your)4 (signature)5 (here)6 ? m: ()1 ()2 ()3 ()4 ()5 ()6 ? ẽ: (Could)1 (you)2 (sign)3 (here)4 ?  In this example, the predicate is nominal (signature) in e but verbal (“”) in m. While inputting two sentence pairs [m, e] and [m, ẽ], the parameter estimation of"]},{"title":"( )","paragraphs":["ij"]},{"title":"emp |","paragraphs":["should be reformulated with two morphological translation conditions:"]},{"title":"( )NeeVmmp","paragraphs":["iijj"]},{"title":"∈∈ ,|,","paragraphs":["and"]},{"title":"( )VeeVmmp","paragraphs":["iijj"]},{"title":"∈∈ ,|,","paragraphs":[". Therefore, with growing of various morphological translations, more conditions would raise more complexity of lexicon transfer parameter estimation and cause more ambiguity of lexical-semantic lexicography.  3.1.3 Identification of Structural Divergence In structural divergence, a verbal argument has a different syntactic realization in the target language and the appearance of the divergence causes additional syntactic structural mapping constructions.  e: (About)1 (the)2 (center)3 . m: ()1 ()2 ()3 . ẽ: (About)1 (in)2 (the)3 (center)4 .  Observe that the place object is realized as a noun phrase (the center) in e and as a prepositional phrase (“ ”) in m. For this example, the divergence causes the alignment of 0←2, which is a null mapping for Mandarin lexicon “”. In addition, the divergence also causes alignments of 2↔3 and 3↔3, which result in non-equal mapping number q-to-n (q>1, n>1, and q≠n). For a raised null mapping, the parameter estimation of"]},{"title":"( )","paragraphs":["em"]},{"title":"llijp ,,|","paragraphs":["and"]},{"title":"( )","paragraphs":["em"]},{"title":"llp |","paragraphs":["become more complicated by further considering translation of lexicon insertion (i=0) and deletion (j=0). More raised non-equal mapping number in parallel texts, more parameter estimation of"]},{"title":"( )","paragraphs":["em"]},{"title":"llp |","paragraphs":["and more length generation condition for translation.  3.1.4 Identification of Conflational Divergence Conflation is the incorporation of necessary participants (or arguments) of a given action. A conflational divergence arises when there is a difference in incorporation properties between two languages. In addition, there are word compounds in Chinese language by embedding some semantic contiguity. For this divergence, the complexity of training process for transfer mapping extraction is extremely increased.  e: (Please)1 (have)2 (him)3 (call)4 (me)5 . m: ()1 ()2 ()3 ()4 ()5 ()6 ()7 ()8 . ẽ: (Please)1 (tell)2 (him)3 (to)4 (give)5 (me)6 (a)7 (call)8 .  This example illustrates the conflation of a constitution in e that must be overly realized in m: the effect of the action (give me a call) is indicated by the word “ ” whereas this information is incorporated into the main verb (call me) in e. Therefore, this divergence causes most complexity on parameter estimation of translation including"]},{"title":"( )","paragraphs":["ij"]},{"title":"emp |","paragraphs":[","]},{"title":"()","paragraphs":["em"]},{"title":"llp |","paragraphs":[", and"]},{"title":"()","paragraphs":["em"]},{"title":"llijp ,,|","paragraphs":[".   3.1.5 Identification of Lexical Divergence For lexical divergence, the event is lexically realized as the main verb in one language but as a different verb in other language. It typically raises the ambiguity of lexical-semantic lexicography and also can be viewed as a side effect of other divergences. Thus, the formulation thereof is considered to be some combination of those given above, such as a conflational divergence forces the occurrence of a lexical divergence.  e: (Nothing)1 (can)2 (beat)3 (Phantom of the Opera)4 . m: ()1 (什)2 ()3 ()4 . ẽ: (Nothing)1 (can)2 (compare)3 (with)4 (Phantom of the Opera)5 .  Here the main verb “beat” in e but as a different verb “” (to compare with) in m. Other examples are like “cash”, “have”, “take”, and etc. in English but “ 金”, “”, “”, and etc. in Mandarin, respectively.  3.2 Processing of Divergence Evaluation According to the above divergence analysis, the divergent mappings between sentence pairs are composed of non-equal mapping number (q-to-n, q>1, n>1, qn), different position mapping (i↔j, i j), and null mapping (i→0 or 0→j). Unlike non-equal mapping number and different position mapping, the null mapping cannot provide target language translation information for lexical item selection and position generation. Therefore, we want to use a simple and straightforward measurement method to evaluate the possible null mappings. For example to the Mandarin-English parallel text corpus, given a Mandarin sentence mlj"]},{"title":"mmmmm LL","paragraphs":["21"]},{"title":"=","paragraphs":["and an English sentence"]},{"title":":","paragraphs":["21 eli"]},{"title":"eeeee LL=","paragraphs":[", direct lexical mappings in the mapping space can be extracted using the relevant bilingual dictionary [13]. The mapping function is defined as follows: "]},{"title":"()( )⎩⎨⎧ =∋Θ∈∃ =−= otherwise0 , if1 ,","paragraphs":["k kjp kjij"]},{"title":"m mem","paragraphs":["i"]},{"title":"σσ σδτ","paragraphs":["(2) where j"]},{"title":"m","paragraphs":["is j-th Mandarin segmented term; i"]},{"title":"e","paragraphs":["is the i-th English phrase, and ip"]},{"title":"Θ","paragraphs":["is represented as a Mandarin lexicon set of the English phrase i"]},{"title":"e","paragraphs":["in the chosen bilingual dictionary. The mapping function"]},{"title":"( )","paragraphs":["ij"]},{"title":"em ,τ","paragraphs":["has the factor k"]},{"title":"σ","paragraphs":[", which represents k-th Mandarin lexicon in ip"]},{"title":"Θ","paragraphs":[". Therefore, if the translation of i"]},{"title":"e","paragraphs":["found in the bilingual dictionary is the same to j"]},{"title":"m","paragraphs":[","]},{"title":"( )","paragraphs":["ij"]},{"title":"em ,τ","paragraphs":["is assigned to 1; otherwise,"]},{"title":"( )","paragraphs":["ij"]},{"title":"em ,τ","paragraphs":["is assigned to 0. And we can obtain the direct lexical mapping sequence"]},{"title":"{ }","paragraphs":["| 0 and 0i MjaiI jJ∆= ≤≤ ≤≤ (3) where i j"]},{"title":"a","paragraphs":["is a mapping referred to as the alignment"]},{"title":"ji →","paragraphs":["if"]},{"title":"( ) 1, =","paragraphs":["ij"]},{"title":"emτ","paragraphs":["or"]},{"title":"0→i","paragraphs":["and"]},{"title":"j→0","paragraphs":["if"]},{"title":"( ) 0, =","paragraphs":["ij"]},{"title":"emτ","paragraphs":[". If the lexical mapping sequence M"]},{"title":"∆","paragraphs":["contains more than a particular number, named n"]},{"title":"ε","paragraphs":[", of null mappings ("]},{"title":"0→i","paragraphs":["and"]},{"title":"j→0","paragraphs":["), then the degree of divergence between the sentence pairs [m, e] becomes significant. Hence, the content of m or e should be updated to improve the accuracy and effectiveness of exploration of mapping order between word sequences and derivation of transfer mappings. In this paper, we choose to sieve out the divergent sentence pairs from the parallel texts.  4. Experimental Results Table 1 shows the basic characteristics of the collected parallel texts extended by travel conversation [11]. The Mandarin words in the corpora were obtained automatically using a Mandarin morphological analyzer at CKIP [10] and an English morphological analyzer referred to LinkGrammar [12]. Table 1. Basic characteristics of the collected parallel texts Mandarin English Number of sentences 11,885 11,885 Total number of words 80,699 66915 Number of word entries 6,278 5,118 Average number of words per sentence 6.79 5.63  The percentage of the various types of divergences for the collected parallel texts is shown on Fig. 2. For the collected corpus of travel conversation, almost two out of three parallel sentences (65 percent) occur the conflational divergence and less than one out of five parallel sentences (19 percent) occur the lexical divergence. In order to assess the effect of translation divergence in the parallel texts, the system also utilizes an alignment training tool called GIZA, which is a program in an EGYPT toolkit","designed by the Statistical Machine Translation team [9]1 . Based on segmented Chinese, we use the original GIZA for testing in this paper. In relation to the IBM models in GIZA, this study uses models 1-4 and ten iterations of each training models for the collected corpus. The parallel sentences with various types of divergences are sieved out from the collected corpus and perplexity in IBM original GIZA training model with comparison of sieving various types of divergences is shown on Table 2. The perplexity with sieving thematic divergence is similar to that with sieving structural divergence and the perplexity with sieving morphological divergence is similar to that with sieving lexical divergence. For sieving conflational divergence, a noticeable perplexity reduction is obtained among other types of divergence but the cost is that almost two out of three parallel sentences (65 percent) are sieved out from the collected corpus. Table 3 lists the perplexity of the original parallel sentences and that of the evaluated parallel sentences from GIZA. The results demonstrate that more null mappings can result in higher perplexity, i.e. more translation choices for a lexical item, thus increasing the translation ambiguity and lowering the accuracy of lexical mapping extraction. Two amended translation probabilities with evaluation of n"]},{"title":"ε","paragraphs":["<1 are shown in Table 4. The number of translation choices of “have” and “back” are reduced from 7 to 4 and 7 to 3, respectively. After evaluating the divergence of each sentence pair in parallel texts and retaining those with n"]},{"title":"ε","paragraphs":["<1, i.e. no null mappings in a sentence pair, the perplexity in the alignment training model can be reduced from 13.65 to 5.18 on average.  33% 46% 26% 65% 19% 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%","Thematic Morphological Structural Conflational Lexical Divergence Type Percen tage o f Div e rg en ces  Fig. 2. The percentage of the various types of divergences for the collected parallel texts  1 This toolkit could be downloaded from http://www.clsp.jhu.edu/ws99/projects/mt/toolkit/","","Table 2. Perplexity in IBM original GIZA training model with comparison of sieving various types of divergences.","Thematic Divergence Morphological Divergence Structural Divergence Conflational Divergence","Lexical","Divergence Model 1 9.91 10.17 9.41 8.46 10.70 Model 2 7.89 10.22 7.84 6.51 9.72 Model 3 8.72 11.83 8.56 7.48 12.35 Model 4 8.65 11.79 8.61 7.44 12.29 Average 8.79 11.00 8.61 7.47 11.26 ","Table 3. Perplexity in IBM original GIZA training model with comparison of original ( n"]},{"title":"ε <","paragraphs":["∞) /evaluated parallel sentences.  n"]},{"title":"ε","paragraphs":["<∞ n"]},{"title":"ε","paragraphs":["<4 n"]},{"title":"ε","paragraphs":["<3 n"]},{"title":"ε","paragraphs":["<2 n"]},{"title":"ε","paragraphs":["<1 No. of (m, e) 11,885 10,976 9,874 8,618 7,639 Model 1 10.94 10.09 8.26 6.79 5.98 Model 2 12.92 8.52 6.57 5.24 4.43 Model 3 15.39 9.47 7.13 6.21 5.16 Model 4 15.33 9.45 7.11 6.20 5.15 Average 13.65 9.38 7.27 6.11 5.18  Table 4. Examples of two amended English word translation probabilities. Have Translation probability trained with original parallel sentences","Translation probability trained","with evaluated parallel sentences 0.4312746 0.4612446 0.346279 0.398176 0.1231011 0.1035049 0.0975747 0.0370745 0.00146905 0.000294352 2.95704e-08   Back Translation probability trained with original parallel sentences","Translation probability trained","with evaluated parallel sentences 來 0.937283 來 0.9392834 0.0379813 0.042981 0.01650713 0.01871713 0.00786959 2.5874e-06 0.000294352 1.66024e-07   5. Conclusion In this work, we discuss one issue of parallel text exploitation, in general, with special emphasis on divergence analysis and processing. Experiments were performed for the languages of Mandarin and English with the travel conversation corpus of 11,885 sentence pairs. The experimental results show that the analysis and evaluation of divergence for retaining low divergent parallel sentences can reduce the perplexity in IBM translation model averagely from 13.65 to 5.18. For sieving conflational divergence, a noticeable perplexity reduction is obtained among other types of divergence but the cost is that almost two out of three parallel sentences (65 percent) are sieved out from the collected corpus. Future studies will attempt to implement a translation decoder to assess the influence of divergence evaluation on BLEU sore. References [1] H. Ney, S. Nießen, F. J. Och, H. Sawaf, C. Tillmann, and S. Vogel, “Algorithms for statistical translation of spoken language,” IEEE Trans. Speech and Audio Processing, vol. 8, no. 1, pp. 24–36, Jan. 2000. [2] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer, “The mathematics of statistical machine translation: Parameter estimation,” Computational Linguistics, vol. 19, no. 2, pp. 263–311, 1993. [3] B. J. Dorr, P. W. Jordan and J. W. Benoit, “A survey of current paradigms in machine translation,” in Advances in Computers, vol. 49, M. V. Zelkowitz, Ed. Academic Press, 1999. [4] B. J. Dorr, Machine translation: A view from the lexicon. Cambridge, MA: The MIT press, 1993. [5] B. J. Dorr, “Machine Translation Divergences: A Formal Description and Proposed Solution,” ACL Vol. 20, No. 4, pp. 597–631, 1994. [6] A. Menezes and S. D. Richardson, “A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora,” in Proc. Workshop on Data-driven Machine Translation at 39th Annual Meeting of the Association for Computational Linguistics, 2001, pp. 39–46. [7] J. F. Wang and S. C. Lin, “Bilingual corpus evaluation and discriminative sentence vector expansion for machine translation,” in Proc. ICAIET, 2002, pp.117–120. [8] D. Gupta and N. Chatterjee, “Study of divergence for example based English-Hindi machine translation,” in Proc. STRANS, 2002, pp. 132-140. [9] EGYPT toolkit, developed by the Statistical Machine Translation team, Center for Language and Speech Processing, Johns-Hopkins University, MD, 1999. [10] L. L. Chang, “The modality words in modern Mandarin,” Chinese Knowledge Information Processing Group, Institute of Information Science Academia Sinica, Taiwan, Tech. Rep. 93-06, 1993. [11] , 旅 Easy Go, 讀, 2001. [12] D. D. Sleator and D. Temperley, “Parsing English with a Link Grammar,” Carnegie Mellon University, Pittsburgh, PA, Tech. Rep. CMU-CS-91-196, 1993. [13] Dr. Eye 6.0, developed by Inventec Corporation, 2004."]}]}