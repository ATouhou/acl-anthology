{"sections":[{"title":"Word-Transliteration Alignment Tracy Lin","paragraphs":["Dep. of Communication Engineering National Chiao Tung University,","1001, Ta Hsueh Road,","Hsinchu, 300, Taiwan tracylin@cm.nctu.edu.tw"]},{"title":"Chien-Cheng Wu","paragraphs":["Department of Computer Science National Tsing Hua University","101, Kuangfu Road,","Hsinchu, 300, Taiwan g904374@oz.nthu.edu.tw"]},{"title":"Jason S. Chang","paragraphs":["Department of Computer Science National Tsing Hua University","101, Kuangfu Road,","Hsinchu, 300, Taiwan jschang@cs.nthu.edu.tw "]},{"title":"Abstract","paragraphs":["The named-entity phrases in free text represent a formidable challenge to text analysis. Translating a named-entity is important for the task of Cross Language Information Retrieval and Question Answering. However, both tasks are not easy to handle because named-entities found in free text are often not listed in a monolingual or bilingual dictionary. Although it is possible to identify and translate named-entities on the fly without a list of proper names and transliterations, an extensive list certainly will ensure the high accuracy rate of text analysis. We use a list of proper names and transliterations to train a Machine Transliteration Model. With the model it is possible to extract proper names and their transliterations in a bilingual corpus with high average precision and recall rates."]},{"title":"1. Introduction","paragraphs":["Multilingual named entity identification and (back) transliteration has been increasingly recognized as an important research area for many applications, including machine translation (MT), cross language information retrieval (CLIR), and question answering (QA). These transliterated words are often domainspecific and many of them are not found in existing bilingual dictionaries. Thus, it is difficult to handle transliteration only via simple dictionary lookup. For CLIR, the accuracy of transliteration highly affects the performance of retrieval.  Transliteration of proper names tends to be varied from translator to translator. Consensus on transliteration of celebrated place and person names emerges over a short period of inconsistency and stays unique and unchanged thereafter. But for less known persons and unfamiliar places, the transliterations of names may vary a great deal. That is exacerbated by different systems used for Ramanizing Chinese or Japanese person and place names. For back transliteration task of converting many transliterations back to the unique original name, there is one and only solution. So back transliteration is considered more difficult than transliteration. Knight and Graehl (1998) pioneered the study of machine transliteration and proposed a statistical transliteration model from English to Japanese to experiment on back transliteration of Japanese named entities. Most previous approaches to machine transliteration (Al-Onaizan and Knight, 2002; Chen et al., 1998; Lin and Chen, 2002); English/Japanese (Knight and Graehl, 1998; Lee and Choi, 1997; Oh and Choi, 2002) focused on the tasks of transliteration and back-transliteration. Very little has been touched upon for the issue of aligning and acquiring words and transliterations in a parallel corpus.  The alternative to on-the-fly (back) machine transliteration is simple lookup in an extensive list automatically acquired from parallel corpora. Most instances of (back) transliteration of proper names can often be found in a parallel corpus of substantial size and relevant to the task. For instance, fifty topics of the CLIR task in the NTCIR 3 evaluation conference contain many named entities (NEs) that require (back) transliteration. The CLIR task involves document retrieval from a collection of late 1990s news articles published in Taiwan. Most of those NEs and transliterations can be found in the articles from the Sinorama Corpus of parallel Chinese-English articles dated from 1990 to 2001, including “Bill Clinton,” “Chernobyl,” “Chiayi,” “Han dynasty,” “James Soong,” “Kosovo,” “Mount Ali,” “Nobel Prize,” “Oscar,” “Titanic,” and “Zhu Rong Ji.” Therefore it is important for CLIR research that we align and extract words and transliterations in a parallel corpus.  In this paper, we propose a new machine transliteration method based on a statistical model trained automatically on a bilingual proper name list via unsupervised learning. We also describe how the parameters in the model can be estimated and smoothed for best results. Moreover, we show how the model can be applied to align and extract words and their transliterations in a parallel corpus.  2 The remainder of the paper is organized as follows: Section 2 lays out the model and describes how to apply the model to align word and transliteration. Section 3 describes how the model is trained on a set of proper names and transliterations. Section 4 describes experiments and evaluation. Section 5 contains discussion and we conclude in Section 6."]},{"title":"2. Machine Transliteration Model","paragraphs":["We will first illustrate our approach with examples. A formal treatment of the approach will follow in Section 2.2."]},{"title":"2.1 Examples","paragraphs":["Consider the case where one is to convert a word in English into another language, says Chinese, based on its phonemes rather than meaning. For instance, consider transliteration of the word “Stanford,” into Chinese. The most common transliteration of “Stanford” is “jO.” (Ramanization: [shi-dan-fo]). We assume that transliteration is a piecemeal, statistical process, converting one to six letters at a time to a Chinese character. For instance, to transliterate “Stanford,” the word is broken into “s,” “tan,” “fo\\r,” and “d,” which are converted into zero to two Chinese characters independently. Those fragments of the word in question are called transliteration units (TUs). In this case, the TU “s” is converted to the Chinese character “j,” “tan” to “O,” “for” to “,̄” and “d” to the empty string λ. In other words, we model the transliteration process based on independence of conversion of TUs. Therefore, we have the transliteration probability of getting the transliteration “jO” given “Stanford,” P(jŌ| Stanford), P(jŌ| Stanford) = P(j | s) P(O | tan) P(̄| for) P( λ | d)  There are several ways such a machine transliteration model (MTM) can be applied, including (1) transliteration of proper names (2) back transliteration to the original proper name (3) word-transliteration alignment in a parallel corpus. We formulate those three problems based on the probabilistic function under MTM:  3 Transliteration problem (TP) Given a word w (usually a proper noun) in a language (L1), produce automatically the transliteration t in another language (L2). For instance, the transliterations in (2) are the results of solving the TP for four given words in (1). (1) Berg, Stanford, Nobel, †M (2) i, jO,̄ łL¢, Tsing Hua  Back transliteration Problem (BTP) Given a transliteration t in a language (L2), produce automatically the original word w in (L1) that gives rise to t. For instance, the words in (4) are the results of solving the BTP for two given transliterations in (3).  (3) ƒ¶}fi„̂, Lin Ku-fang (4) Michelangelo, JF  Word Transliteration Alignment Problem (WTAP) Given a pair of sentence and translation counterpart, align the words and transliterations therein. For in-stance, given (5a) and (5b), the alignment results are the three word-transliteration pairs in (6), while the two pairs of word and back transliteration in (8) are the results of solving WTAP for (7a) and (7b) (5a) Paul Berg, professor emeritus of biology at Stanford University and a Nobel laureate, ... (5b) jO{̄¶¤nI£̃‡CłL¢—i1","C (6) (Stanford, ¥v⁄ƒ”), (Nobel, łL¢), (Berg, i)","(7a) PRC premier Zhu Rongji's saber-rattling speech on the eve of the election is also seen as having aroused re-","sentment among Taiwan's electorate, and thus given Chen Shui-bian a last-minute boost.","(7b) ƒ⁄⁄ƒ@‘̀†zƒ¶Ł́¿«e«fĺ¥xW̆¿¥“̀”‰‚̋¡A⁄]‡Q»{‹‹O‡yƒ¤¿¥⁄̀•̌P¡Aƒ́⁄«ø‡fl⁄«“”{̀“ø⁄@","‚}¡C2","","(8) (Zhu Rongji, ƒ¶Ł́), (Chen Shui-bian, ‡fl⁄«)  Both transliteration and back transliteration are important for machine translation and cross language information retrieval. For instance, the person and place names are likely not listed in a dictionary, there-fore should be mapped to the target language via run-time transliteration. Similarly, a large percentage of  1 Scientific American, US and Taiwan editions. What Clones? Were claims of the first human embryo premature? Gary Stix and NkV(Trans.) December 24, 2001. 4 keywords in a cross language query are person and place names. It is important for an information system to produce appropriate counterpart names in the language of documents being searched. Those counterparts can be obtained via direct transliteration based on the machine transliteration and language models (of proper names in the target language).  The memory-based alternative is to find those word-transliteration in the aligned sentences in a parallel corpus (Chuang, You, and Chang 2002). Word-transliteration alignment problem certainly can be dealt with based on lexical statistics (Gale and Church 1992; Melamed 2000). However, lexical statistics is known to be very ineffective for low-frequency words (Dunning 1993). We propose to attack WTAP at the sub-lexical, phoneme level."]},{"title":"2.2 The Model","paragraphs":["We propose a new way for modeling transliteration of an English word w into Chinese t via a Machine Transliteration Model. We assume that transliteration is carried out by decomposing w into k translation units (TUs), ω1, ω2, ..., ωk which are subsequently converted independently into τ1, τ2, ..., τk respectively. Finally, τ1, τ2, ..., τk are put together, forming t as output. Therefore, the probability of converting w into t can be expressed as P(t | w) ="]},{"title":")|(max","paragraphs":[",1...,..., k1k1 ii kik"]},{"title":"P ωτ","paragraphs":["ττωω ="]},{"title":"Π","paragraphs":[", where w = ω1ω2 ...ωk , t = τ1τ2 ...τk , |t| ≤ k ≤ |t|+|w|, τ i ω i ≠ λ. See Equation (1) in Figure 1 for more details.  Based on MTM, we can formulate the solution to the Transliteration Problem by optimizing P(t | w) for the given w. On the other hand, we can formulate the solution to the Back Transliteration Problem by optimizing P(t | w) P( w) for the given t. See Equations (2) through (4) in Figure 1 for more details.   2 Sinorama Chinese-English Magazine, A New Leader for the New Century--Chen Elected President, April 2000, p. 13. 5 The word-transliteration alignment process may be handled by first finding the proper names in English and matching up with the transliteration for each proper name. For instance, consider the following sentences in the Sinorama Corpus:  (9c) uÆc你fiS„„zAwÿnIẑC你„„œIPCv...mN{C̆œ”","B»¡ (9e) \"When you understand all about the sun and all about the atmosphere and all about the rotation of the earth, you","may still miss the radiance of the sunset.\" So wrote English philosopher Alfred North Whitehead.  It is not difficult to build part of speech tagger or named entity recognizer for finding the following proper names (PN):  (10a) Alfred, (10b) North, (10c) Whitehead.","","We use Equation (5) in Figure 1 to model the alignment of a word w and its transliteration t in s based on the alignment probability P(s, w) which is the product of transliteration probability P(σ | ω) and a","trigram match probability, P(m i | m i-2, m i-1), where m i is the type of the i-th match in the alignment path. We define three match types based on lengths a and b, a = | τ |, b = | ω |: match(a, b) = H if a = 0, match(a, b) = V if b = 0, and match(a, b) = D if a > 0 and b > 0. The D-match represents a non-empty TU ω matching a transliteration character τ, while the V-match represents the English letters omitted in the transliteration process. 6  MACHINE TRANSLITERATION MODEL: The probability of transliteration t of the word w P(t | w) = , (1)"]},{"title":")|(","paragraphs":[",1...,...,"]},{"title":"max","paragraphs":["k1k1 ii kik"]},{"title":"P ωτ","paragraphs":["ττωω"]},{"title":"Π","paragraphs":["=","where w = ω1ω2... ωk , t = τ1τ2...τk , | t | ≤ k ≤ | t | + | w |, | τi ωi | ≥ 1."," TRANSLITERATION: Produce the phonetic translation equivalent t for the given word w t ="]},{"title":"arg","paragraphs":["P(t | w) (2) t"]},{"title":"max BACK TRANSLITERATION:","paragraphs":["Produce the original word w for the given transliteration t P(w | t) ="]},{"title":")P( )P()|P( t wwt ","paragraphs":["(3) w ="]},{"title":") P() | P(maxarg )P( ) P() | P( maxarg wwt t wwt","paragraphs":["tt"]},{"title":"=","paragraphs":["(4) WORD-TRANSLITERATION ALIGNMENT: Align a word w with its transliteration t in a sentence s P(s, w) = P(σ"]},{"title":"Π","paragraphs":["= kik ,1...,...,"]},{"title":"max","paragraphs":["k1k1 σσωω i | ωi) P(m i | m i-2, m i-1), (5)","where w = ω1ω2...ωκ , s = σ1σ2...σκ , (both ω i and σ i can be empty) | s | ≤ k ≤ | w | + | s |, |ωi σi| ≥ 1, m i is the type of the (ω i , σ i) match, m i = match (|ω i |, | σ i | ),","match(a, b) = H, if b = 0,","match(a, b) = V, if a = 0,","match(a, b) = D, if a > 0 and b > 0, P(m i | m i-2, m i-1) is trigram Markov model probabiltiy of match types.","α(i, j ) = P(s1:i-1, w1:j-1). (6)","α(1, 1) = 1, μ(1, 1) = (H, H). (7)","α(i, j ) = α(i-a, j-b) P(s",",60 b0,1, a max== j-a:j-1 | wi-b:i-1) P( match(a, b) | μ(i-a, j-b) ). (8)","μ(i, j) = (m, match(a*, b*)), where μ(i-a*, j-b*) = (x, m), (9)","where (a*, b*) = α(i-a, j-b) P(s",",60 b0,1, a maxarg","== j-a:j-1 | wi-b:i-1) P( match(a, b) | μ(i-a, j-b) ).","","Figure 1. The equations for finding the Viterbi path of matching a proper name and its translation in a sentence","","Æc 你 fi S „ „ ... N { ̆C œ” »¡ w h i t e h e a d ","Figure 2. The Viterbi alignment path for Example (9c) and the proper name “Whitehead” (10c) in the sentence (9e),","consisting of one V-match (te-λ), three D-matches (whi−, hea−C, d−œ”), and many H-matches. 7 To compute the alignment probability efficiently, we need to define and calculate the forward probability α(i, j) of P(s, w) via dynamic programming (Manning and Schutze 1999), α(i, j) denotes the probability of aligning the first i Chinese characters of s and the first j English letters of w. For the match type trigram in Equation (5) and (8), we need also compute μ(i, j), the types of the last two matches in the Viterbi alignment path. See Equations (5) through (9) in Figure 1 for more details.  For instance, given w = “Whitehead” and s = “uÆc你fiS„„zAwÿnIẑC 你 „„œIPCv...mN{C̆œ”B»¡ ,” the best Viterbi path indicates a decomposition of word “Whitehead” into four TUs, “whi,” “te,” “hea,” and “d” matching “,” λ, “C,” “œ”” respectively. By extracting the sequence of D- and V-matches, we generate the result of word-transliteration alignment. For instance, we will have (Cœ”, Whitehead) as the output. See Figure 2 for more details."]},{"title":"3. Estimation of Model Parameters","paragraphs":["In the training phase, we estimate the transliteration probability function P(τ | ω), for any given TU ω and transliteration character τ, based on a given list of word-transliterations. Based on the Expectation Maximization (EM) algorithm (Dempster et al., 1977) with Viterbi decoding (Forney, 1973), the iterative parameter estimation procedure on a training data of word-transliteration list, (E k, C k), k = 1 to n is described as follows:"," Initialization Step:"]},{"title":"Initially, we have a simple model P","paragraphs":["0"]},{"title":"(τ | ω)","paragraphs":["P0 (τ | ω) = sim( R(τ) | ω) = dice(t 1 t 2 ...ta, w 1 w 2 ...w b) (8) = ba c +2","where R(τ) = Romanization of Chinese character τ R(τ) = t 1 t 2 ...ta ω = w 1 w 2 ...w b c = # of common letters between R(τ) and ω  8 For instance, given w = ‘Nayyar’ and t = ‘[o,’ we have and R(τ1) = ‘na’ and R(τ2) = ‘ya’ under Yanyu Pinyin Romanization System. Therefore, breaking up w into two TUs, ω1 = ‘nay’ ω 2 = ‘yar’ is most probable, since that maximizes P0(τ 1 | ω 1) × P0(τ 2 | ω 2)  P0(τ 1 | ω 1)= sim( na | nay) = 2 × 2 / (2+3) = 0.8 P0(τ 2 | ω 2)= sim( ya | yar) = 2 × 2 / (2+3) = 0.8  Expectation Step: In the Expectation Step, we find the best way to describe how a word get transliterated via decomposition into TUs which amounts to finding the best Viterbi path aligning TUs in E k and characters in C k for all pairs (E k, C k), k = 1 to n, in the training set. This can be done using Equations (5) through (9). In the training phase, we have slightly different situation of s = t. Table 1. The results of using P0(τ |ω) to align TUs and transliteration characters","w s=t ω-τ match on Viterbi path Spagna ·'‹¥§¤̈s-· pag-'‹ n-¥§ a-¤̈ Kohn ⁄fiƒ koh-⁄ n-fiƒ Nayyar fl¶̇fi nay-fl̇yar-¶fi Alivisatos “§Q”ß...»ƒ«· a-“ li-§Q vi-”ß sa-...» to-ƒ« s-· Rivard ¤‰¥...̧w ri-¤‰ var-¥̧d-...w Hall N”‚ ha-N ll-”‚ Kalam ¥d ̄́ ka-¥d lam- ̄́ Salam  ̃́'i sa- ̃́ la- m-'i Adam ¤•̈ a-¤̈dam-• Gamoran »\\† ̄́ ga-»\\ mo-† ran- ̄́ Heller »fi̇hel-»fi ler-̇ Adelaide “–o„p...w a-“ de-–o lai-„p de-...w Nusser §V• nu-§V sser-• Nechayev fl¥̇d›C⁄ ne-fl̇cha-¥d ye-›C v-⁄ Hitler §fl̆Ṡhi-§̆t-flS ler-̇ Hunt “CflS hun-“C t-flS Germain “N ger-“N main-Massoud ¤‹̃...w ma-¤ ssou-‹̃ d-...w Malong ”¿¶' ma-”¿ long-¶' Gore “”‚ go-“ re-”‚ Teich fiı‡\\ tei-fiı ch-‡\\ Laxson '§J·̧la-' x-§J son-·̧  The Viterbi path can be found via a dynamic programming process of calculating the forward probability function α(i, j) of the transliteration alignment probability P(E k , C k) for 0 < i < | C k | and 0 < j < | E k |. After calculating P(C k , E k) via dynamic programming, we also obtain the TU matches (τ, ω) on the 9 Viterbi path. After all pairs are processed and TUs and translation characters are found, we then re-estimate the transliteration probability P(τ | ω) in the Maximization Step  Maximization Step: Based on all the TU alignment pairs obtained in the Expectation Step, we update the maximum likelihood estimates (MLE) of model parameters using Equation (9)."]},{"title":"∑∑ ∑∑","paragraphs":["= =","= n i CE n i CE MLE 1 ),(in matches' 1 ),(in matches )count( ),count( )|(P ii ii ω ωτ ωτ ωτ","ωτ (9) The Viterbi EM algorithm iterates between the Expectation Step and Maximization Step, until a stopping criterion is reached or after a predefined number of iterations. Re-estimation of P(τ | ω) leads to convergence under the Viterbi EM algorithm."]},{"title":"3.1 Parameter Smoothing","paragraphs":["The maximum likelihood estimate is generally not suitable for statistical inference of parameters in the proposed machine transliteration model due to data sparseness (even if we use a longer list of names for training, the problem still exists). MLE is not capturing the fact that there are other transliteration possibilities that we may have not encountered. For instance, consider the task of aligning the word “Michelangelo” and the transliteration “J̃N” in Example (11):  (11) (Michelangelo, ƒ¶}fi„̂)  It turns out in the model trained on some word-transliteration data provides the MLE parameters in the MTM in Table 2. Understandably, the MLE-based model assigns 0 probability to a lot of cases not seen in the training data and that could lead to problems in word-transliteration alignment. For instance, relevant parameters for Example (11) such as P(J | che) and P(N | lan) are given 0 probability. Good Turing estimation is one of the most commonly used approaches to deal with the problems caused by data sparseness and zero probability. However, GTE assigns identical probabilistic values to all unseen events, which might lead to problem in our case. 10 Table 2. PMLE(t | n) value relevant to Example (11)","English TU ω Transliteration τ PMLE(τ | ω) mi ƒ 0.00394 mi –K 0.00360 mi 'œ 0.00034 mi ‡̀0.00034 mi ̀0.00017 che ‡̇0.00034 che ⁄̀0.00017 che ¤ 0.00017 che '_ 0.00017 che «· 0.00017 che ‹ 0.00017 che ¶} 0 lan ı̃ 0.00394 lan  ̄́ 0.00051 lan › 0.00017 lan fi 0 ge fi 0.00102 ge '_ 0.00085 ge ƒN 0.00068 ge 0.00017 ge »\\ 0.00017 lo ‹¥ 0.00342 lo „̂ 0.00171 lo ' 0.00017  We observed that although there is great variation in Chinese transliteration characters for any given English word, the initial, mostly consonants, tend to be consistent. See Table 3 for more details. Based on that observation, we use the linear interpolation of the Good-Turing estimation of TU-to-TU and the class-based initial-to-initial function to approximate the parameters in MTM. Therefore, we have"," ))init(|)(init(P5.0)|(P5.0)|(P ececec MLEGTli +="]},{"title":"4 Experiments and evaluation","paragraphs":["We have carried out rigorous evaluation on an implementation of the method proposed in this paper. Close examination of the experimental results reveal that the machine transliteration is general effective in aligning and extracting proper names and their transliterations from a parallel corpus.  The parameters of the transliteration model were trained on some 1,700 proper names and transliterations from Scientific American Magazine. We place 10 H-matches before and after the Viterbi alignment 11 path to simulate the word-transliteration situation and trained the trigram match type probability. Table 4 shows the estimates of the trigram model. Table 3. The initial to initial correpsondence of ω amd R(τ) ω τ R(τ) Init(ω) Init(R(τ)) mi ƒ mi m m mi –K mi m m mi 'œ min m m mi ‡̀mai m m mi ̀mai m m che ‡̇jei ch j che ⁄̀chei ch ch che ¤ chi ch ch che '_ chi ch ch che «· chi ch ch che ‹ ke ch k che ¶} kai ch k lan ı̃ lan l l lan  ̄́ lan l l lan › lun l l lan fi lang l l ge fi ge g g ge '_ chi g ch ge ƒN ji g j ge ji g j ge »\\ gai g g lo ‹¥ lo l l lo „̂ Lo l l lo ' La l l Table 4. The stastical estimates of trigram match types","Match Type Trigram m1 m2 m3 Count P( m3 | m1 m2 ) DDD 1886 0.51 DDH 1627 0.44 DDV 174 0.05 DHD 0 0.00 DHH 1702 1.00 DHV 0 0.00 DVD 115 0.48 DVH 113 0.47 DVV 12 0.05 HDD 1742 0.96 HDH 7 0.01 HDV 58 0.03 HHD 1807 0.06 HHH 29152 0.94 HHV 15 0.00 HVD 15 1.00 HVH 0 0.00  The model was then tested on three sets of test data:  12 (1) 200 bilingual examples in Longman Dictionary of Comtemporary Dictionary, English-Chinese Edi-","tion. (2) 200 aligned sentences from Scientific American, US and Taiwan Editions. (3) 200 aligned sentences from the Sinorama Corpus.  Table 5 shows that on the average the precision rate of exact match is between 75-90%, while the precision rate for character level partial match is from 90-95%. The average recall rates are about the same as the precision rates. Table 5. The experimental results of word-transliteration alignement Test Data # of words ( # of characters) # of matches (# of characters) Word precision (Characters) LODCE 200 (496) 179 (470) 89.5% (94.8%) Sinorama 200 (512) 151 (457) 75.5% (89.3%) Sci. Am. 200 (602) 180 (580) 90.0% (96.3%)"]},{"title":"5. Discussion","paragraphs":["The success of the proposed method for the most part has to do with the capability to balance the conflicting needs of capturing lexical preference of transliteration and smoothing to cope with data sparseness and generality. Although we experimented with a model trained on English to Chinese transliteration, the model seemed to perform reasonably well even with situations in the opposite direction, Chinese to English transliteration. This indicates that the model with the parameter estimation method is very general in terms of dealing with unseen events and bi-directionality.  We have restricted our discussion and experiments to transliteration of proper names. While it is commonplace for Japanese to have transliteration of common nouns, transliteration of Chinese common nouns into English is rare. It seems that is so only when the term is culture-specific and there is no counterparts in the West. For instance, most instances “”X‡T” and “‰G“Ø̄” found in the Sinorama corpus are mapped into lower case transliterations as shown in Example (11) and (12):  13 (11a) ⁄⁄ŒŒ“A¡X¡X”X‡Tflu“”¤S‚¤⁄F¶¡H (11b) Are ch'i-p'aos--the national dress of China--really out of fashion?  (12a) ⁄@·T‰G“Ø̄fi“k‰»̆s«~ (12b) a scroll of shou chin ti calligraphy  Without capitalized transliterations, it remains to be seen how word-transliteration alignment related to common nouns should be handled."]},{"title":"6. Conclusion","paragraphs":["In this paper, we propose a new statistical machine transliteration model and describe how to apply the model to extract words and transliterations in a parallel corpus. The model was first trained on a modest list of names and transliteration. The training resulted in a set of ‘syllabus’ to character transliteration probabilities, which are subsequently used to extract proper names and transliterations in a parallel corpus. These named entities are crucial for the development of named entity identification module in CLIR and QA.  We carried out experiments on an implementation of the word-transliteration alignment algorithms and tested on three sets of test data. The evaluation showed that very high precision rates were achieved.  A number of interesting future directions present themselves. First, it would be interesting to see how effectively we can port and apply the method to other language pairs such as English-Japanese and English-Korean. We are also investigating the advantages of incorporate a machine transliteration module in sentence and word alignment of parallel corpora."]},{"title":"Acknowledgement","paragraphs":["We acknowledge the support for this study through grants from National Science Council and Ministry of Education, Taiwan (NSC 90-2411-H-007-033-MC and MOE EX-91-E-FA06-4-4). 14"]},{"title":"References","paragraphs":["Al-Onaizan, Y. and K. Knight. 2002. Translating named entities using monolingual and bilingual resources. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 400-408. Chen, H.H., S-J Huang, Y-W Ding, and S-C Tsai. 1998. Proper name translation in cross-language information retrieval. In Proceedings of 17th COLING and 36th ACL, pages 232-236. Chuang, T., G.N. You, J.S. Chang (2002) Adaptive Bilingual Sentence Alignment, Lecture Notes in Artificial Intelligence 2499, 21-30. Cibelli, J.B. R.P. Lanza, M.D. West, and C. Ezzell. 2002. What Clones? SCIENTIFIC AMERICAN, Inc., New York, January. http://www.sciam.com. Dagan, I., Church, K. W., and Gale, W. A. 1993. Robust bilingual word alignment for machine aided translation. In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, pages 1-8, Columbus Ohio. Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38. Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1-38. Forney, G.D. 1973. The Viterbi algorithm. Proceedings of IEEE, 61:268-278, March. Knight, K. and J. Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599-612. Lee, J.S. and K-S Choi. 1997. A statistical method to generate various foreign word transliterations in multilingual information retrieval system. In Proceedings of the 2nd International Workshop on Information Retrieval with Asian Languages (IRAL'97), pages 123-128, Tsukuba, Japan. Lin, W-H Lin and H-H Chen. 2002. Backward transliteration by learning phonetic similarity. In CoNLL-2002, Sixth Conference on Natural Language Learning, Taipei, Taiwan. 15 Manning, Ch. and H. Schutze. 1999. Foundations of Statistical Natural Language Processing, MIT Press; 1st edition. Oh, J-H and K-S Choi. 2002. An English-Korean transliteration model using pronunciation and contextual rules. In Proceedings of the 19th International Conference on Computational Linguistics (COLING), Taipei, Taiwan. Proctor, P. 1988. Longman English-Chinese Dictionary of Contemporary English, Longman Group (Far East) Ltd., Hong Kong. Sinorama. 2002. Sinorama Magazine. http://www.greatman.com.tw/sinorama.htm. Stalls, B.G. and K. Knight. 1998. Translating names and technical terms in Arabic text. In Proceedings of the COLING/ACL Workshop on Computational Approaches to Semitic Languages. Tsujii, K. 2002. Automatic extraction of translational Japanese-KATAKANA and English word pairs from bilingual corpora. International Journal of Computer Processing of Oriental Languages, 15(3):261-279. 16"]}]}