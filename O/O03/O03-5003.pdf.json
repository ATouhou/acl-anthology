{"sections":[{"title":"","paragraphs":["Computational Linguistics and Chinese Language Processing Vol. 8, No. 2, August 2003, pp. 61-76 61  The Association for Computational Linguistics and Chinese Language Processing "]},{"title":"Building A Chinese WordNet Via Class-Based Translation Model Jason S. Chang* , Tracy Lin+ , Geeng-Neng You** , Thomas C. Chuang++ , Ching-Ting Hsieh***  Abstract","paragraphs":["Semantic lexicons are indispensable to research in lexical semantics and word sense disambiguation (WSD). For the study of WSD for English text, researchers have been using different kinds of lexicographic resources, including machine readable dictionaries (MRDs), machine readable thesauri, and bilingual corpora. In recent years, WordNet has become the most widely used resource for the study of WSD and lexical semantics in general. This paper describes the Class-Based Translation Model and its application in assigning translations to nominal senses in WordNet in order to build a prototype Chinese WordNet. Experiments and evaluations show that the proposed approach can potentially be adopted to speed up the construction of WordNet for Chinese and other languages."]},{"title":"1. Introduction","paragraphs":["WordNet has received widespread interest since its introduction in 1990 [Miller 1990]. As a large-scale semantic lexical database, WordNet covers a large vocabulary, similar to a typical college dictionary, but its information is organized differently. The synonymous word senses are grouped into so-called synsets. Noun senses are further organized into a deep IS-A hierarchy. The database also contains many semantic relations, including hypernyms, hyponyms, holonyms, meronyms, etc. WordNet has been applied in a wide range of studies on  * Department of Computer Science, National Tsing Hua University 101, Sec. 2, Kuang Fu Road, Hsinchu, Taiwan, ROC E-mail: jschang@cs.nthu.edu.tw + Department of Communication Engineering, National Chiao Tung University 1001, University Road, Hsinchu, Taiwan, ROC E-mail: tracylin@mail.nctu.edu.tw ** Department of Information Manangement, National Taichung Institute of Technology San Ming Road, Taichung, Taiwan, ROC E-mail: gny@mail.ntit.edu.tw ++ Dept of Computer Science, Van Nung Institute of Technology 1 Van-Nung Road, Chung-Li, Taiwan, ROC E-mail: tomchuang@cc.vit.edu.tw *** Panasonic Taiwan Laboratories Co., Ltd. (PTL) E-Mail: chingting@ptl.com.tw   62 J. S. Chang et al.  such topics as word sense disambiguation [Towell and Voothees, 1998; Mihalcea and Moldovan, 1999], information retrieval [Pasca and Harabagiu, 2001], and computer-assisted language learning [Wible and Liu, 2001].","Thus, there is a universally shared interest in the construction of WordNet in different languages. However, constructing a WordNet for a new language is a formidable task. To exploit the resources of WordNet for other languages, researchers have begun to study ways of speeding up the construction of WordNet for many European languages [Vossen, Diez-Orzas, and Peters, 1997]. One of many ways to build a WordNet for a language other than English is to associate WordNet senses with appropriate translations. Many researchers have proposed using existing monolingual and bilingual Machine Readable Dictionaries (MRD) with an emphasis on nouns [Daude, Padro & Rigau, 1999]. Very little study has been done on using corpora or on covering other parts of speech, including adjectives, verbs, and adverbs. In this paper, we describe a new method for automating the process of constructing Chinese WordNet. The method was developed specifically for nouns and is capable of assigning Chinese translations to some 20,000 nominal synsets in WordNet.","The rest of this paper is divided into four sections. The next section provides the background on using a bilingual dictionary to build a Chinese WordNet and semantic concordance. Section 3 describes a class-based translation model for assigning translations to WordNet senses. Section 4 describes the experimental setup and results. A conclusion is provided in Section 5 along with directions of future work."]},{"title":"2. From Bilingual MRD and Corpus to Bilingual Semantic Database","paragraphs":["In this section, we describe the proposed method for automating the construction process of a Chinese WordNet. We have experimented to find the simplest way of attaching an appropriate translation to each WordNet sense under a Class-Based Translation Model. The translation candidates are taken from a bilingual word list or Machine Readable Dictionaries (MRDs). We will use an example to show the idea, and a formal description will follow in Section 3."]},{"title":"Table 1. Words in the same conceptual class that often share common Chinese characters in their translations. Code (set title) Hyponyms","paragraphs":["Chinese translation fish (aquatic vertebrate) carp Û‡‰ fish (aquatic vertebrate) catfish ‡̨‰ fish (aquatic vertebrate) eel `̄‡‰ complex (building) factory ⁄u...t complex (building) cannery ł̄Y⁄u...t complex (building) mill »s‡y...t speech (communication) discussion Q‰;‡̃‰   Building A Chinese WordNet Via Class-Based Translation Model 63  speech (communication) argument ‰;‰Í;“§‰ speech (communication) debate Ḡ‰","Let us consider the example of assigning appropriate translations for the nominal senses of “plant” in WordNet 1.7.1. The noun “plant” in WordNet has four senses: 1. plant, works, industrial plant (buildings for carrying on industrial labor); 2. plant, flora, plant life (a living organism lacking the power of locomotion); 3. plant (something planted secretly for discovery by another person);","4. plant (an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience).","The following translations are listed for the noun “plant” in the Longman Dictionary of Contemporary English (English-Chinese Edition) [Longman Group 1992]: 1. ·“«, 2. ‡]‡,̆ 3. „, 4. ⁄u...t, 5. ⁄”‰u⁄H, and 6. fi“”B̄ .","For words such as “plant” with multiple senses and translations, the question arises: Which translation goes with which synset? We make the following observations that are crucial to the solution of the problem:","1. Each nominal synset has a chain of hypernyms which give ever more general concepts of the word sense. For instance, plant-1 is a building complex, which in turn is a structure and so on and so forth, while plant-2 can be generalized as a life form.","2. The hyponyms of a certain top concept in WordNet form a set of semantically related word senses.","3. Semantically related senses tend to have surface realization in Chinese with shared characters.","For instance, building complex spawns the hyponyms factory, mill, assembly plant, cannery, foundry, maquiladora, etc., all of which realize in Chinese using the characters “...t” or “⁄u...t.” Therefore, we can say that there is a high probability that senses which are direct or indirect hyponyms of building complex share the Chinese characters “⁄u” and “...t” in their Chinese translations. Therefore, it is clear that one can determine that plant-1, a hyponym of building complex, should have “⁄u...t” instead of “·“«” as its translation. See Table 1 for more examples. That intuition can be expanded into a systematic way of assigning the most appropriate translation to a given word sense. Figure 1 shows how the method works for four senses of plant.","In the following, we will consider the task of assigning the most appropriate translation to plant-1, the first sense of the noun “plant.” First, the system looks up “plant” in the Translation Table (T Table) for candidate translations of plant-1:   64 J. S. Chang et al.  (plant, ·“«), (plant, „), (plant, ‡]‡)̆, (plant, ⁄u...t), (plant, ⁄”‰u⁄H), (plant, fi“”B̄).","Next, the semantic class g to which plant-1 belongs is determined by consulting the Semantic Class Table (SC Table). In this study we use some 1,145 top hypernyms h to represent the class of word senses that are direct or transitive hyponyms of h. The path designator of h in WordNet is used to represent the class. The hypernyms are chosen to correspond roughly to the division of sets of words in the Longman Lexicon of Contemporary English (LLOCE) [McArthur 1992]. Table 2 provides examples of classes related to plant and their class codes."]},{"title":"Table 2. Words in four classes related to the noun plant. English WN sense Class Code Words in the Class","paragraphs":["Plant 1 N001004003030 factory, mill, assembly plant, ... Plant 2 N001001005 flora, plant life, ... Plant 3 N001001015008 thought, idea, ... Plant 4 N001001003001001 producer, supernatural, ... Plant 4 N001003001002001 announcer, conceiver, ...","For instance, plant-1 belongs to the class g represented by the WordNet synset (structure, construction): g = N001004003030.","Subsequently, the system evaluates the probabilities of each translation conditioned on the semantic class g: P(“·“«” | N001004003030), P(“„” | N001004003030), P(“‡]‡”̆ | N001004003030), P(“⁄u...t” | N001004003030), P(“⁄”‰u⁄H” | N001004003030), P(“fi“”B̄” | N001004003030).","These probabilities are not evaluated directly. The system takes apart the characters in a translation and looks up P( u | g ), the probabilities for each translation character u conditioned on g: P(“·” | N001004003030) = 0.000025, P(““«” | N001004003030) = 0.000025, P(“” | N001004003030) = 0.00278, P(“„” | N001004003030) = 0.00278, P(“‡]” | N001004003030) = 0.00306,   Building A Chinese WordNet Via Class-Based Translation Model 65  P(“‡”̆ | N001004003030) = 0.00075, P(“⁄u” | N001004003030) = 0.00711, P(“...t” | N001004003030) = 0.01689, P(“⁄”” | N001004003030) = 0.00152, P(“‰u” | N001004003030) = 0.00152, P(“⁄H” | N001004003030) = 0.00152, P(“fi” | N001004003030) = 0.00152, P(““”” | N001004003030) = 0.00152, P(“B̄” | N001004003030) = 0.00152.","Note that to deal with lookup failure, a smoothing probability is given (0.000025, derived using the Good-Turing method). By using a statistical estimate based on simple linear interpolation, we can get P(“⁄u...t” | plant-1) ≈ P (“⁄u...t” | N001004003030) ≈"]},{"title":"21","paragraphs":["P(“⁄u” | N001004003030) +"]},{"title":"21","paragraphs":["P(“...t” | N001004003030) ="]},{"title":"21","paragraphs":["(0.0178+0.0073) = 0.0124. Similarly, we have P(“·“«” | N001004003030) = 0.0013, P(“„” | N001004003030) = 0.0023, P(“‡]‡”̆ | N001004003030) = 0.0028, P(“⁄”‰u⁄H” | N001004003030) = 0.0014, P(“fi“”B̄” | N001004003030) = 0.0001.","Finally, by choosing the translation with the highest probabilistic value for g, we can get an entry for Chinese WordNet (CWN Table): (plant, ⁄u...t, n, 1, “buildings for carrying on industrial labor”)","After we get the correct translation of plant-1 and many other word senses in g, we will be able to re-estimate the class-based translation probability for g and produce a new CT Table. However, the reader may wonder how we can get the initial CT Table. This dilemma can be resolved by adopting an iterative algorithm that establishes an initial CT Table and makes revision until the values in the CT Table converge. More details will be provided in Section 3.   66 J. S. Chang et al.   "]},{"title":"Fig. 1 Using CBTM to build Chinese WordNet. This example shows how the first sense of plant receives an appropriate translation via the Class-Based Translation Model and how the model can be trained iteratively. 3. The Class-Based Translation Model","paragraphs":["In this section, we will formally describe the proposed class-based translation model, how it can be trained, and how it can be applied to the task of assigning appropriate translations to different word senses. Given Ek, the kth sense of an English word E in the WordNet, the probability of its Chinese translation is denoted as P( C | Ek). Therefore, the best Chinese   T Table  SC Table","","CT Table  English Word Chinese Word English Word","WN","Sense POS Class Code Class Translation","Character Prob. plant ·“« plant 1 n N001004003030 N001004003030 0.0178 plant „ plant 2 n N001001005 N001004003030 ...t 0.0174 plant ‡]‡̆plant 3 n N001001015008 N001004003030 ¥ 0.0088 plant ⁄u...t plant 4 n N001001003001001 N001004003030 ⁄u 0.0073 plant ⁄”‰u⁄H plant 4 n N001003001002001 ¡K ¡K ¡K plant fi“”B̄ N001001005 “« 0.0161 N001001005 · 0.0161 ¡K ¡K ¡K  Translation Table Semantic Class Table Class Translation Table   BST Table CWN Table  English Word Sense No. POS Chinese Word","Prob. English Word Sense No. POS","Chinese","Word plant 1 n ⁄u...t 0.0124 plant 1 n ⁄u...t plant 1 n ‡]‡̆0.0028 plant 2 n ·“« plant 1 n „ 0.0023 plant 1 n ⁄”‰u⁄H 0.0014 plant 1 n ·“« 0.0013 plant 1 n fi“”B̄ 0.0001   Bilingual Semantic Translation Table Bilingual WordNet    Building A Chinese WordNet Via Class-Based Translation Model 67 ","translation C* is",")|(maxarg)( k",")(k*","ECPEC","ETC∈ ≅ , (1) where T(X) is the set of Chinese translations of sense X listed in a bilingual dictionary.","Based on our observation that semantically related senses tend to be realized in Chinese using shared Chinese characters, we tie together the probability functions of translation words in the same semantic class and use the class-based probability as an approximation. Thus, we have )|()|( k gCPECP ≅ , (2) where g = g(Ek) is the semantic class containing Ek.","The probability of P(C|g) can be estimated using the Expectation and Maximization Algorithm as follows: (Initialization)"]},{"title":"m ECP 1 )|(","paragraphs":["k"]},{"title":"=","paragraphs":[", m = | T(E) | and C ∈ T(E); (3) (Maximization)"]},{"title":"∑ ∑ ∈ ∈= =","paragraphs":["ikE ikE"]},{"title":"gEIECP gEICCIECP gCP","paragraphs":[",, kki ,, kiki"]},{"title":")()|( )()()|( )|(","paragraphs":[", (4) where C i = the ith translation of Ek in T(Ek) , I(x) = 1 if x is true and 0 otherwise; (Expectation)"]},{"title":")|()|(","paragraphs":["k1"]},{"title":"gCPECP =","paragraphs":[", (5) where g = g (Ek) is the class that contains Ek ; (Normalization)"]},{"title":"∑","paragraphs":["∈"]},{"title":"=","paragraphs":[")( k1 k1 k k"]},{"title":")|( )|( )|(","paragraphs":["ETD"]},{"title":"EDP ECP ECP","paragraphs":[". (6)","In order to avoid the problem of data sparseness, P(C|g) is estimated indirectly via the unigrams and bigrams in C. We also weigh the contribution of each unigram and bigram to avoid the domination of a particular character in the semantic class. Therefore, we rewrite Equations 4 and 5 as follows: (Maximization)"]},{"title":"∑ ∑","paragraphs":["∈ =∈ = jikE jikE u EuPgEI m EuPuuIgEI m guP ,,, kji,k ,,, kji,ji,k )|()( 1 )|()()( 1 )|( , (4a) where u i,j = the jth unigram of the ith translation in T(Ek) , m = the number of characters in the ith translation in T(Ek),   68 J. S. Chang et al.  "]},{"title":"∑ ∑","paragraphs":["∈ − =∈ − = jikE jikE b EbPgEI m EbPbbIgEI m gbP ,,, kji,k ,,, kji,ji,k )|()( 1 1 )|()()( 1 1 )|( , (4b) where bi,j = the jth overlapping bigram of the ith translation in T(Ek); (Expectation)"]},{"title":"∑","paragraphs":["= ≅≅ m i u m guP gCPECP 1 i k1",")|( )|()|( (unigram), (5a)"]},{"title":"∑∑","paragraphs":["− == −+≅≅ 1 1 i 1 i k1 )1(2 )|( 2 )|( )|()|( m i b m i u m gbP","m guP gCPECP (+bigram), (5b) where ui is a unigram, bi is an on overlapping bigram of C, and m is the number of characters in C .","For instance, assume that we have the first sense trunk-1 of the word trunk in WordNet and the translations in LDOCE as follows:","trunk-1 (the main stem of a tree; usually covered with bark; the bole is usually the part that is commercially useful for lumber), Translations of trunk — ⁄j¥‰c, ⁄jƒ‰c, •F, and ¶H» . Initially, the probabilities of each translation for trunk-1 are as follows: P( ⁄j¥‰c | trunk-1 ) = 1/4, P( ⁄jƒ‰c | trunk-1 ) = 1/4, P( •F | trunk-1 ) = 1/4, P( ¶H» | trunk-1 ) = 1/4.","Table 3 shows the words in the semantic class N001004001018013014 (stalk, stem), containing trunk-1 and relevant translations. Following Equations 4a and 4b, we took the unigrams and overlapping bigrams from these translations to calculate the probability of unigram and bigram translations for (stalk, stem). Although initially irrelevant translations such as bulb-„q¿O“w(light bulb) can not be excluded, after one iteration of the maximization step, the noise is suppressed substantially, and the top ranking translations shown in Tables 4 and 5 seem to be the “genus” terms of the class. For instance, the top ranking unigrams for N001004001018013014 include † (stem), “K (branch), –ł (branch), fi (stump) (tree) •F (trunk) etc. Similarly, the top ranking bigrams include †y† (bulb), “K (branch), ‹h –ł (willow branch), and •F (trunk). All indicate the general concepts of the class.","With the unigram translation probability P( u | g), one can apply Equations 5a and 6 to proceed with the Expectation Step and calculate the probability of each translation candidate for a word sense as shown in Example 1: Example 1.","P1(•F|trunk-1)=1/2*(P(|N001004001018013014)+P(•F| N001004001018013014)) =1/2*(0.0145+0.0103) = 0.0124,   Building A Chinese WordNet Via Class-Based Translation Model 69 ","P1(¶H»|trunk-1) =1/2*(P(¶H|N001004001018013014)+P(» |N001004001018013014 )) =1/2* (0.00054+0.00054) = 0.00054,","P1(⁄j¥‰c|trunk-1) =1/3*(P(⁄j|N001004001018013014)+P(¥|N001004001018013014 ) + P(‰c |N001004001018013014)) , =1/3*(0.0074+0.00036+0.00072) = 0.00283,","P1(⁄jƒ‰c|trunk-1) =1/3*(P(⁄j|N001004001018013014)+P(ƒ|N001004001018013014 ) + P(‰c | N001004001018013014)) =1/3*(0.0074 + 0.00043 + 0.00072) = 0.00285 P ( •F | trunk-1 ) = 0.0124/(0.0124+0.00054+0.00283+0.00285) = 0.665950591, P ( ¶H» | trunk-1 ) = 0.0124/(0.0124+0.00054+0.00283+0.00285) = 0.0290010741, P ( ⁄j¥‰c | trunk-1 ) = 0.0124/(0.0124+0.00054+0.00283+0.00285) = 0.1519871106, P ( ⁄jƒ‰c | trunk-1 ) = 0.0124/(0.0124+0.00054+0.00283+0.00285) = 0.1530612245.","Using simple linear interpolation of translation unigrams and bigrams (Equation 5b), the probability of each translation candidate for a word sense can be calculated as shown in Example 2: Example 2.","P1( •F | trunk-1 ) = 1/2 * {1/2 * (P( | N001004001018013014 ) +P( •F | N001004001018013014 ) ) +P( •F | N001004001018013014 ) } = 1/2 * (0.0124 + 0.0145) = 0.01345,","P1( ¶H» | trunk-1 ) = 1/2 * {1/2 * (P( ¶H | N001004001018013014 ) +P( » | N001004001018013014 ) ) +P( ¶H» | N001004001018013014 ) } = 1/2 * (0.00054 + 0.00107) = 0.000805,","P1( ⁄j¥‰c | trunk-1 ) = 1/2 * {1/3 * (P( ⁄j | N001004001018013014 ) + P( ¥ | N001004001018013014 )) + P( ‰c | N001004001018013014 )} + 1/2 * (P( ⁄j¥ | N001004001018013014 ) +P( ¥‰c | N001004001018013014 ) ) } = 1/2 * (0.00283 + 0.00054) = 0.001685,","P1( ⁄jƒ‰c | trunk-1 ) = 1/2 * {1/3 * (P( ⁄j | N001004001018013014 ) + P( ƒ | N001004001018013014 )) + P( ‰c | N001004001018013014 ) } + 1/2 * (P( ⁄jƒ | N001004001018013014 )   70 J. S. Chang et al.  +P( ƒ‰c | N001004001018013014 ) ) } = 1/2 * (0.00285 + 0.00054) = 0.001695 P (•F|trunk-1) = 0.01345/(0.01345+0.000805+0.001685+0.001695)= 0.76268783669, P (¶H»|trunk-1) = 0.000805/(0.01345+0.000805+0.001685+0.001695) = 0.045647859371, P (⁄j¥‰c|trunk-1) = 0.001685/(0.01345+0.000805+0.001685+0.001695) = 0.095548624894, P (⁄jƒ‰c|trunk-1) = 0.001695/(0.01345+0.000805+0.001685+0.001695) = 0.096115679047."]},{"title":"Table 3. Words and their translations in the semantic class N001004001018013014 English E WN sense k G(E k) Chinese Translation","paragraphs":["Beanstalk 1 N001004001018013014 ¤§† Bole 2 N001004001018013014 •F Branch 2 N001004001018013014 ⁄“K Branch 2 N001004001018013014 ‡¡“ø Branch 2 N001004001018013014 “K Brier 2 N001004001018013014 fl·̆ Bulb 1 N001004001018013014 †y†“‹“« Bulb 1 N001004001018013014 „q¿O“w Cane 2 N001004001018013014 –̃ł Cutting 2 N001004001018013014 ‡̄ł Cutting 2 N001004001018013014 ·¡“K Stick 2 N001004001018013014 ⁄p“K Stick 2 N001004001018013014 ⁄Ø Stem 2 N001004001018013014 fia¤t Stem 2 N001004001018013014 •F "]},{"title":"Table 4. Probabilities of each unigram for the semantic class containing trunk-1, etc. Unigram (u) Semantic Class Code (g) P( u | g )","paragraphs":["† N001004001018013014 0.0706 “K N001004001018013014 0.0274 ¤§ N001004001018013014 0.0216 –ł N001004001018013014 0.0162 N001004001018013014 0.0145 fi N001004001018013014 0.0134   Building A Chinese WordNet Via Class-Based Translation Model 71  •F N001004001018013014 0.0103 ̃N001004001018013014 0.0080 ¡K ¡K¡K¡K¡K¡K¡K¡K¡K¡K ¡K "]},{"title":"Table 5. Probabilities of each bigram for the semantic class containing trunk-1, etc. Bigram (b) Semantic Class Code (g) P( b | g )","paragraphs":["†y† N001004001018013014 0.0287 ‹h–ł N001004001018013014 0.0269 •F N001004001018013014 0.0145 “K N001004001018013014 0.0144 „“K N001004001018013014 0.0134 ... .............................. ...","Both examples show that the class-based translation model produces reasonable probabilistic values. The examples also show that for trunk-1, the linear interpolation method gives a higher probabilistic value for the correct translation “•F” than the unigram-based approach does (0.76268783669 vs. 0.665950591). In this case, linear interpolation is a better parameter estimation scheme. Our experiments showed, in general, that combining both unigrams and bigrams does lead to better overall performance."]},{"title":"4. Experiments","paragraphs":["We carried out two experiments to see how well CBTM can be applied to assign appropriate translations to nominal senses in WordNet. In the first experiment, the translation probability was estimated using Chinese character unigrams, while in the second experiment, both unigrams and bigrams were used. The linguistic resources used in the experiments included:","1. WordNet 1.6: WordNet contains approximately 116,317 nominal word senses organized into approximately 57,559 word meanings (synsets).","2. Longman English-Chinese Dictionary of Contemporary English (LDOCE E-C): LDOCE is a learner’s dictionary with 55,000 entries. Each word sense contains information, such as a definition, the part-of-speech, examples, and so on. In our method, we take advantage of its wide coverage of frequently used senses and corresponding Chinese translations. In the experiments, we tried to restrict the translations to lexicalized words rather than descriptive phrases. We set a limit on the length of a translation: nine Chinese characters or less. Many of the nominal entries in WordNet are not covered by learner dictionaries; therefore, the experiments focused on those senses for which Chinese translations are available in LDOCE. 3. Longman Lexicon of Contemporary English (LLOCE): LLOCE is a bilingual   72 J. S. Chang et al.  taxonomy, which brings together words with related meanings and lists them in topical/semantic classes with definitions, examples, and illustrations. The three tables shown in Figure 1 were generated in the course of the experiments:","1. The Translation Table has 44,726 entries and was easily constructed by extracting Chinese translations from LDOCE E-C [Proctor 1988].","2. We obtained the Sense Class Table by finding the common hypernyms of sets of words in LLOCE. 1,145 classes were used in the experiments.","3. The Class Translation Table was constructed using the EM algorithm based on the T Table and SC Table. The CT Table contains 155,512 entries.","Table 6 shows the results of using CBTM and Equation 1 to find the best translations for a word sense. We are concerned with the coverage of word senses in average text. In that sense, the translation of plant-3 is incorrect, but this error is not very significant, since this word sense is used infrequently. We chose the WordNet semantic concordance, SEMCOR, as our testing corpus. There are 13,494 distinct nominal word senses in SEMCOR. After the translation probability calculation step, our results covered 10,314 word senses in SEMCOR; thus, the coverage rate was 76.43%."]},{"title":"Table 6. The results and appropriate translations for each sense of the English word. English WN sense Chinese Translation Appropriate Chinese Translation","paragraphs":["Plant 1 ⁄u...t ⁄u...t Plant 2 ·“« ·“« Plant 3 ⁄”‰u⁄H fi“”B̄ Plant 4 ⁄”‰u⁄H ⁄”‰u⁄H Spur 1 „“y „“y Spur 2 ¿Ey ¤o, w Spur 4 ¤¤o ¤¤o Spur 5 ⁄‰u ⁄‰u Bank 1 »ƒ̈ »ƒ̈ Bank 2 '̂Y ¤F‹w Bank 3 fiw fiw, xƒs' Scale 1 O...“̆k'•̨̇O...“̆k'•̨̇ Scale 2 ⁄æ¤ ‡W... Scale 3 ⁄æ¤ ⁄æ¤ Scale 5 †⁄U“”fiŒ¥fih †⁄U“”fiŒ¥fih Scale 6 ›¶¥ ›¶¥","To see how well the model assigns translations to WordNet senses appearing in average text, we randomly selected 500 noun instances from SEMCOR as our test data. There were 410 distinct words. Only 75 words had a unique sense in WordNet. There were 77 words with   Building A Chinese WordNet Via Class-Based Translation Model 73  two senses in WordNet, while 70 words had three senses in WordNet, and so on. The average degree of sense ambiguity was 4.2."]},{"title":"Table 7. The degree of ambiguity and number of words in the test data with different degree of ambiguity. Degree of ambiguity # of senses in WordNet # of word types in the test data Examples","paragraphs":["1 75 aptitude, controversy, regret 2 77 camera, fluid, saloon 3 70 drain, manner, triviality 4 51 confusion, fountain, lesson 5 35 isolation, pressure, spur 6 25 blood, creation, seat 7 28 column, growth, mind 8 9 contact, hall. program 9 7 body, company, track 10 8 bank, change, front >10 25 control, corner, deaft","Among our 500 test data, 280 entries were the first sense, while 112 entries were the second sense. Over half of the words had the meaning of the first sense. Therefore, the first sense was most frequently used. Therefore, it was found to be more important to get the first and the second senses right. We manually gave each word sense an appropriate Chinese translation whenever one was available from LDOCE. From these translations, we found the following:","1. There were 491 word senses for which corresponding translations were available from LDOCE.","2. There were 5 word senses for which no relevant translations could be found in LDOCE due to the limited coverage of this learner’s dictionary. Those word senses and relevant translations included assignment-2 ( ̄́), marriage-3 (–B §́), snowball-1(‚́†y†), prime-1(‰Ł...)̆, and program-7 (‹F”ı).","3. There were 4 words, that have no translations due to the particular cross-referencing scheme of LDOCE. Under this scheme, some nouns in LDOCE are not directly given a definition and translation, but rather a pointer to a more frequently used spelling. For instance, “groom” is given a pointer to “BRIDEGROOM” rather than the relevant definition and translation (“•s›ƒ”).","In the first experiment, we started out by ranking the relevant translations for each noun sense using the class-based translation model. If two translations had the same probabilistic value, we gave them the same rank. For instance, Table 8 shows that the top 1 translation for plant-1 was “⁄u...t.”   74 J. S. Chang et al.  "]},{"title":"Table 8. The rank of each translation corresponding to each word sense. (plant-2, fi “”B̄) and (plant-2, ‡]‡)̆ have the same probability and rank. English Semantic class WN sense Chinese Translation Probability Rank","paragraphs":["Plant N001004003030 (structure) 1 ⁄u...t 0.012372 1 Plant N001004003030 (structure) 1 ‡]‡̆0.002823 2 Plant N001004003030 (structure) 1 „ 0.002270 3 Plant N001004003030 (structure) 1 ⁄”‰u⁄H 0.001375 4 Plant N001004003030 (structure) 1 ·“« 0.001278 5 Plant N001004003030 (structure) 1 fi“”B̄ 0.000130 6 Plant N001001005 (flora) 2 ·“« 0.016084 1 Plant N001001005 (flora) 2 „ 0.002623 2 Plant N001001005 (flora) 2 ⁄u...t 0.000874 3 Plant N001001005 (flora) 2 ‡]‡̆0.000525 4 Plant N001001005 (flora) 2 fi“”B̄ 0.000525 4 Plant N001001005 (flora) 2 ⁄”‰u⁄H 0.000360 5 "]},{"title":"Table 9. The recall rate in the first experiment The number of top-ranking translations Correct Entries (Total entries =500) Recall rate (unigram) Recall rate (unigram+bigram)","paragraphs":["Top 1 344 68.8% 70.2% Top 2 408 81.6% 83.2% Top 3 441 88.2% 89.0% Top 4 449 89.8% 91.4% Top 5 462 92.4% 93.2%","We used the same method to evaluate the recall rate in the second experiment, where both unigrams and bigrams were used. The experimental results show a slight improvement over the results obtained using only unigrams.","In these experiments, we estimated the translation probability based on unigrams and bigrams. The evaluation results confirm our observation that we can exploit shared characters in translations of semantically related senses to obtain relevant translations. We evaluated the experimental results based on whether the Top 1 to Top 5 translations covered all appropriate translations. If we selected the Top 1 translation in the first experiment as the most appropriate translation, there were 344 correct entries, and the recall rate was 68.8%. The Top 2 translations covered 408 correct entries, and the recall rate was 81.6%. Table 9 shows the recall rate with regard to the number of top-ranking translations used for the purpose of evaluation.   Building A Chinese WordNet Via Class-Based Translation Model 75 "]},{"title":"5. Conclusion","paragraphs":["In this paper, a statistical class-based translation model for the semi-automatic construction of a Chinese WordNet has been proposed. Our approach is based on selecting the appropriate Chinese translation for each word sense in WordNet. We observe that a set of semantically related words tend to share some Chinese characters in their Chinese translations. We propose to rely on the knowledge base of a Class Based Translation Model derived from statistical analysis of the relationship between semantic classes in WordNet and translations in the bilingual version of the Longman Dictionary of Contemporary English (LDOCE). We carried out two experiments that show that CBTM is effective in speeding up the construction of a Chinese WordNet.","The first experiment was based on the translation probability of unigrams, and the second was based on both unigrams and bigrams. Experimental results show that the method produces a Chinese WordNet covering 76.43% of the nominal senses in SEMCOR, which implies that a high percentage of the word senses can be effectively handled. Among our 500 testing cases, the recall rate was around 70%, 80% and 90%, respectively, when the Top 1, Top 2, and Top 3 translations were evaluated. The recall rate when using both unigrams and bigrams was slightly higher than that when using only unigrams. Our results can be used to assist the manual editing of word sense translations.","A number of interesting future directions present themselves. First, obviously, there is potential for combining two or more methods to get even better results in connecting WordNet senses with translations. Second, although nouns are most important for information retrieval, other parts of speech are important for other applications. We plan to extend the method to verbs, adjectives and adverbs. Third, the translations in a machine readable dictionary are at times not very well lexicalized. The translations in a bilingual corpus cauld be used to improve the degree of lexicalization."]},{"title":"Acknowledgement","paragraphs":["This study was partially supported by grants from the National Science Council (NSC 90-2411-H-007-033-MC) and the MOE (project EX 91-E-FA06-4-4)."]},{"title":"References","paragraphs":["Daudé, J., L. Padró and G. Rigau, “Mapping Multilingual Hierarchies using Relaxation Labelling,” Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, 1999   76 J. S. Chang et al. ","Daudé, J., L. Padró and G. Rigau, “Mapping WordNets using Structural Information,” Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, 2000.","McArthur, T., “Longman Lexicon of Contemporary English,” Longman Group (Far East) Ltd., Hong Kong, 1992.","Mihalcea, R. and D. Moldovan., “A method for Word Sense Disambiguation of unrestricted text,” Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, 1999, pp. 152-158. Miller, G., “Five papers on WordNet,” International Journal of Lexicography, 3(4), 1990.","Pasca, M. and S. Harabagiu, “The Informative Role of WordNet in Open-Domain Question Answering,” in Proceedings of the NAACL 2001 Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, June 2001, Carnegie Mellon University, Pittsburgh PA, pp. 138-143.","Proctor, P., “Longman English-Chinese Dictionary of Contemporary English,” Longman Group (Far East) Ltd., Hong Kong, 1988.","Towell, G. and E. Voothees, “Disambiguating Highly Ambiguous Words,” Computational Linguistics, 24(1) 1998, pp. 125-146.","Vossen, P., P. Diez-Orzas and W. Peters, “The Multilingual Design of the EuroWordNet Database,” Processing of the IJCAI-97 workshop Multilingual Ontologies for NLP Applications, 1997.","Wible, D. and A. Liu, “A syntax-lexical semantics interface analysis of collocation errors,” PacSLRF 2001."]}]}