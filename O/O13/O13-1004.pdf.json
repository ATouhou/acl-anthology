{"sections":[{"title":"基基基於於於 Sphinx 可可可快快快速速速個個個人人人化化化行行行動動動數數數字字字語語語音音音辨辨辨識識識系系系統統統 Quickly Personalizable Mobile Digit Speech Recognition System Based on Sphinx","paragraphs":["顏宗芃 Tsung-Peng Yen 陳噜噴平 Chia-Ping Chen","國立中山大學資訊工程系 Department of Computer Science and Engineering","National Sun Yat-Sen University","m003040029@student.nsysu.edu.tw, cpchen@cse.nsysu.edu.tw"]},{"title":"摘摘摘要要要","paragraphs":["本論文建立了一個透過網路提供數字語音辨識服務的系統|ヲ 除了語音辨識功能也 提供線上個人化調適功能來克服在不呜呦環境中的噪音強健性。以英文數字辨識來說使 只需要經過少許的調適就能夠在少許的時間內打造出正確率達 80% 以上的個人化英文 數字語音辨識系統。Sphinx-4 是屜屢門為了研究聜聦開發的工具|ヲ 具杜杴延展性、模組化、 可插拔的架構|ヲ 因為這些特性我們選擇使用 Sphinx-4 做為語音辨識系統的核心。為了 讓選擇聲學模型與訓練語料及調適聲學模型上杜杴一個依據|ヲ 使用 AURORA2 語料庫訓 練模型|ヲ 台灣口音英語語料庫與 Android 裝置錄製的語料進行調適實驗|ヲ 結果顯示使 用 EAT 語者獨立的語料經 100 句調適後正確率能夠由 80% 進步到約 90% ;多環境模 型經 Android 單人語料經 25 句能從平均 70% 提升到約 95% 的正確率。 關關關鍵鍵鍵詞詞詞::: 行動化、語音辨識、個人化、調適、噪音強健性、Sphinx"]},{"title":"Abstract","paragraphs":["In this paper, we introduce a system for on-line digit speech recognition services. Besides the speech recognition service in our system, we also provide adaptation function to improve the noise-robustness between different environment. In the case of English digit recognition, our recognition system can achieve over 80% accuracy for a specific speaker by using a few adaptation data. We use Sphinx-4 as a speech recognition kernel in our system. Because Sphinx-4 is a system prepared exclusively for researchers, it is a flexible, modular and pluggable framework. We provide our experiment results on AURORA2, EAT and Android device recording. We use AURORA2 database training models that adapt by EAT and Android device recording. The experimental results show we can get high accuracy after a few adaptation. keywords: mobile, speech recognition, personalizable, adapt, noise-robustness, Sphinx Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 39"]},{"title":"一一一、、、研研研究究究背背背景景景、、、動動動機機機","paragraphs":["拜科技的演進及網路發展所賜|ヲ 語音辨識成為了生活上日漸重要的角色如 Google voice search [1]、Iphone Siri [2] 及其它相關應用 [3] [4] [5]|ヲ衍生了許多可以連上網際網 路的科技產品 (如 PDA 、智慧型手機、平版電腦)。這些科技產品都已經成為了現代人 的生活必需品|ヲ 但是大多數都是使用傳統的捜捴 鍵來進行操作|ヲ 想要利用捜捴 鍵靈活的操 作這些不呜呦的裝置是非常困難的。但如果我們的裝置不侷限於捜捴 鍵輸入聜聦使用語音輸 入來控制這些裝置|ヲ 甚至不需要把手機從包包中拿出來就能夠撥出電話與朋友交談。 把語音變成隨身攜帶的萬用遙控器能夠大大的改善使用上的便利性|ヲ 即使是身體杜杴殘 缺的人只需要透過口語|ヲ也能利用這個系統來操作這些現代科技的手持裝置。","現在大多數的即時語音辨識系統都是建立在網際網路上|ヲ 在辨識的過程中使用者透 過個人電腦或是其它裝置將語音傳至伺服器上|ヲ 待伺服器辨識完成後將結果回傳|ヲ 把 語音辨識相關等較較耗費資源的工作都交給伺服器運算。這種架構讓使用者不需要使 用高效能的裝置就能使用語辨識的服務|ヲ 像雲端運算服務 [6] 多數都建立於大型的分散 式伺服器上。在 [7] 中提到|ヲ 人類可用語音輸入來控制瀏覽器指標以增進使用者與網頁 的互動。","在本論文屜屢注在建立一個能夠兼具服務與研究的語音辨識網路系統|ヲ 研究不呜呦口 音、噪音環境、調適句數對辨識率的影響以提供給使用者在選擇聲學模型、訓練、調 適上能夠杜杴一個依據|ヲ 利用網際網路結呜呢自動語音辨識 (Automatic Speech Recognition, ASR) 系統|ヲ釋出一個網路語音辨識系統。"]},{"title":"二二二、、、系系系統統統架架架構構構","paragraphs":["回應 辨識結果 語料庫 Sphinx-4 SphinxTrain 使 用 者 介 面 使用者 語料 聲學模型 伺 服 端 應 用 程 式 用 戶 端 應 用 程 式 訓練 調適 產生 辨識 網 際 網 路 收集語料 命令 語料 控制 需求 控制命令 圖 1、 系統架構圖","現在常見到的主流的語音辨識研究大多是以隱藏式馬可夫模型 (Hidden Markov Model, HMM)如 [8] [9] [10]|ヲ與高斯混呜呢模型 (Gaussian Mixture Model, GMM) [11] [12] 統 詜詢模型的方法建立的|ヲ 這一類的辨識工具杜杴HTK [13]、CMU Sphinx 等等|ヲ 在本篇論 文中選擇使用 CMU Sphinx 的 Sphinx-4 [14] 作為核心辨識工具。 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 40","主從式架構是一種運用網路技術、開放的架構來降低成本的一種小型化電腦系統|ヲ 用戶端可能是一台個人電腦或小型工作站|ヲ 本身就具備完整獨立作業的能力;伺服端 則是一台較大型的伺服器或電腦主機|ヲ 聜聦在用戶端及伺服端之間則藉著可靠的通信協 定連結。","本 系 統 以 HTTP (HyperText Transfer Protocol)的 方 式 建 立 主 從 式 架 構 |ヲ 一 個 伺 服 端 (server) 透過網路來呜呦時服務多個用戶端 (client)|ヲHTTP 是網際網路應用最為廣泛 的一種網路協定|ヲ 它的好處在於能夠容易的使用網頁伺服器架構出用戶端給瀏覽器使 用|ヲ 聜聦且在其它裝置上也很容易能夠設詜詢出符呜呢條件的用戶端|ヲ 圖 1 表示了整個系統 架構|ヲ 用戶端與伺服端分別使用不呜呦的應用程式來控制|ヲ 使用者透過使用者介面 (user interface) 與用戶端應用程式溝通|ヲ 用戶端應用程式將語料及命令以需求的方式送出至 伺服端|ヲ 伺服端應用程式收到需求後針對所需控制辨識工具做辨識或調適的動作|ヲ 完 成後把將辨識結果或完成訊息回應給用戶端應用程式以操控使用者介面。"]},{"title":"乜乴乜乴乜乴、、、實實實驗驗驗","paragraphs":["訓練 AURORA2 測試結果 調適 測試 AURORA2 訓練語料 EAT DIGIT 調適語料 NOISE1 調適語料 基礎模型 EAT Digit 調適模型 NOISE1 調適模型 AURORA2 測試語料 EAT DIGIT 測試語料 NOISE1 測試語料 EAT DIGIT 測試結果 NOISE1 測試結果 圖 2、 實驗流程","本論文中的實驗一共用到了 AURORA2、EAT DIGIT、NOISE1 乜乴個語料庫|ヲ圖 2 簡 明的表示了整個實驗的流程|ヲ 第一步是使用 AURORA2 的語料進行訓練作為調適的基 礎模型|ヲ 再分別使用 EAT DIGIT 及 NOISE1 語料來調適進行不呜呦的腔調、噪音、裝置 實驗。 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 41 (一一一)、、、語語語料料料庫庫庫介介介紹紹紹","本篇論文使用 AURORA2 [15] 產生基礎聲學模型 (baseline model)|ヲ 台灣口音英語 語料 (English Across Taiwan, EAT) [16] 英文數字部分 (簡稱 EAT DIGIT) 及自行錄製 的 NOISE1 語料做為調適語料進行一系列調適的實驗。 AURORA2 :使用不呜呦加成性噪音、訊噪比來測試語音辨識系統強健性的語料庫。 EAT DIGIT :從 EAT 中的可用語料中過濾出純英文數用的部分|ヲ句數如表 1。 表 1、 台灣口音英語語料庫可用純英文數字語料句數 環境\\分類 非英語系學生 英語系學生 女 男 加總 女 男 加總 總和 gsm 212 334 546 386 156 542 1088 pstn 168 171 439 341 136 477 916 mic16k 421 697 1118 794 318 1112 2230","NOISE1 :從 AURORA2 語料庫中選擇 NOISE1 的文本 (corpus) 錄製(故稱 NOISE1 語 料庫)|ヲ使用裝置 DHD (HTC Desire HD)及 WFS(Wildfire S)錄製 8KHz 16bits PCM 格 式語音檔案|ヲ整個語料庫如表 2 所示。 表 2、 NOISE1 語料庫錄製環境杜杴關閉所杜杴家電與門窗的安靜宿舍 中(dormitory) 、 安 靜 宿 舍 於 開 啟 的 電 風 扇 旁 (fan) 、 下 班 時 段 西 子 灣 捷 運 二 號 出 口 旁 的 公 車 等 候 亭 (road) 、中山大學下午五點的籃球場 (basketball) 、中山大學中午 11 點半 L 型停車 場 (parkingLot) 及中山大學電資大樓 F5017b 實驗室 (laboratory)。 (a) 各環境與裝置句數 分類 環境 語者 使用裝置 DHD WFS clean dormitory tpyen 1001 X noise fan tpyen 50 X road tpyen 50 X basketball tpyen 50 X parkingLot tpyen 50 X f5017b laboratory 如表 2(b) 200 200 (b) f5017b 環境語料各語者與裝置句數 語者 性別 使用裝置 DHD WFS jcdeng 女 50 50 mkwu 男 50 50 tpyen 男 50 50 yhhuang 男 50 50 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 42 (二二二)、、、實實實驗驗驗設設設定定定","本實驗所使用的特徵參數為梅爾倒頻譜係數 (Mel-scale Frequency Cepstral Coefficients, MFCC) |ヲ 如表 3 所示|ヲ 在擷取所杜杴音檔特徵參數除了於 EAT 中 mic16k 所使用 的取樣頻率為 16000 外其餘所使用的參數都是一致的。後端的聲學模型上使用高斯 混呜呢模型來訓練|ヲ 所使用的字典 (dictionary) 與音素 (phone) 列於表 4中|ヲ 隱藏式馬可 夫模型每一個音素一個模型、每一個模型 3 個狀態、每個狀態包含 8 個高斯混呜呢分 佈 (Gaussian mixture distribution) 訓練上下文相關 (context dependent) 的模型。 表 3、 擷取特徵參數所使用的參數檔案 參參參數數數 說說說明明明 設設設定定定值值值 alpha 預強調參數 0.97 dither 增加 1/2-bit 雜訊避免零能量音框 yes ncep 倒譜係數 13 lowerf 下截止頻率 64 upperf 上截止頻率 4000 nfft 快速傅利葉轉換大小 512 wlen 漢明窗長度 0.025 input endian 輸入資料的位元組序|ヲ在 NIST 與 MS Wav 格式中忽略 big samprate 取樣頻率 8000 feat Sphinx 參數格式 1s c d dd","分別對 AURORA2 的乾淨訓練語料與多環境訓練語料使用 SphinxTrain 得到乾淨 語料 (clean) 及多環境語料 (multi) 兩個聲學模型|ヲ 使用 AURORA2 的測試語料所得到 的辨識率如表 5 所示|ヲ 本論文使用字模型 (word dependant model) 作為基礎模型|ヲ 把 相呜呦發音的音素模型共用所得到的結果也非常相近於現在的結果。實驗中利用 EAT DIGIT 與 NOISE1 語料做一系列的調適實驗|ヲ 在這些調適實驗中所用到的調適法為最 大後驗 (Maximum A Posteriori, MAP) [17] 方法來進行。 (乜乴乜乴乜乴)、、、AURORA2 聲聲聲學學學模模模型型型使使使用用用 EAT DIGIT 語語語料料料庫庫庫語語語料料料調調調適適適實實實驗驗驗","為了解外國語言腔調在受過訓練後與未受過訓練的差異進行英語系與非英語系學 生的腔調比較、調適效果與句數關係、跨環境調適效果乜乴項實驗。EAT DIGIT 一共 杜杴gsm、pstn、mic16k 乜乴種錄音方式|ヲ再細分為 gsm 英語系學生的語料 (gsmE) 、gsm 非 英語系學生的語料 (gsmN)、pstnE、pstnN、mic16kE、mic16kN|ヲ 最後把這六種條件的 語料分成測試語料以及調適語料|ヲ 如表 6 所示。把每一種條件的語料再各分成兩半|ヲ 一半做測試語料一半做調適語料。分別在後面加上 t 與 a 代表測試語料及調適語 料|ヲ將每一種環境的語料一共分成四份 (E test、E adapt、NE test、NE adapt) 。 1、、、EAT DIGIT 英英英語語語系系系與與與非非非英英英語語語系系系腔腔腔調調調比比比較較較 利用各環境 E a 及 N a 的部分調適成英語系模型及非英語系模型|ヲ 再使用 E t 與 N t 的 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 43 表 4、 字典與音素 字典 文字 音素 eight EY eight, T eight five F five, AY five, V five four F four, OW four, R four nine N nine, AY nine, N nine 2 oh OW oh one W one, AX one, N one seven S seven, EH seven, V seven, E seven, N seven six S six, I six, K six, S six 2 three TH three, R three, II three two T two, OO two zero Z zero, II zero, R zero, OW zero filler <s> SIL<sil> </s> 表 5、 乾淨語料與多環境語料模型辨識率(Avg.:0-20db 的平均值) 乾淨語料模型 多環境語料模型 dB \\測試集 A B C Avg. A B C Avg. clean 99.5 99.5 99.3 99.5 99.0 99.0 98.8 99.0 20 95.3 96.8 95.4 95.9 98.3 98.4 97.7 98.2 15 87.1 90.2 87.1 88.3 97.6 97.5 97.0 97.4 10 63.6 71.3 64.3 66.8 95.0 95.0 94.3 94.9 5 24.6 31.6 26.7 27.8 84.5 84.5 84.4 84.5 0 2.1 4.8 3.5 3.5 47.8 48.4 50.5 48.6 -5 0.4 1.0 0.6 0.7 8.2 11.7 10.7 10.1 Avg. 54.5 58.9 55.4 56.5 84.6 84.8 84.8 84.7 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 44 表 6、 將 EAT 六種條件的語料進一步分成測試語料及調適語料 測試語料 調適語料 女 男 總句數 女 男 總句數 總詜詢 gsmN 106 167 273 106 167 273 546 gsmE 193 78 271 193 78 271 542 pstnN 84 135 219 84 136 220 439 pstnE 170 68 238 171 68 239 477 mic16kN 210 348 558 211 349 560 1118 mic16kE 397 159 556 397 159 556 1112 語料測試|ヲ得到的英語系與非英語系腔調差異做比較如表 7:","由這些數據中發現不論是使用英語系或非英語系的語料來調適|ヲ 辨識率都是英語系 語料優於非英語系語料。 AURORA2 的錄製語者都是以英文為母語|ヲ 因此擁杜杴較標準 的口音得到較高的辨識率是可預期的現象。在以英語系語料為測試語料的條件下|ヲ 不 論是使用英語系或非英語系語料都能得到良好的調適效果;相對於使用非英語系語料 測試時使用英語系語料調適的效果就沒那麼優異。","這個實驗的結果表示出口音對辨識率的影響很大|ヲ 在訓練過與未訓練過辨識率的差 距可以多達 10%|ヲ 聜聦且即使是訓練過的口音或多或少還是會受到母語口音的影響|ヲ 在 選擇調適語料的時後以挑選與使用者母語相呜呦的語料為佳。 2、、、EAT DIGIT 調調調適適適效效效果果果與與與句句句數數數關關關係係係 在調適效果與句數關係的實驗中|ヲ 使用不呜呦的句數來觀察各個條件下使用不呜呦句數調 適的辨識率。在這邊調適句數單位 1 是代表一句男生語料加上一句女生語料 (5 單位就 代表 5 句男生語料加 5 句女生語料|ヲ 以此類推)|ヲ 如單一性別語料不足則使用另一種性 別的語料補足。對乜乴種環境做句數調適效果的測試|ヲ 我們讓乜乴種環境中的英語系及非 英語系的語料輪流當測試語料及訓練語料|ヲ結果如表 8 9。","由實驗結果中能夠看出調適句數與正確率成長起初正確率略微下降、後來快速 成長、到最後逐漸趨緩的整個過程趨勢|ヲ 由圖中也可得知在使用語者無關 (context independent) 調適語料時使用約 50 單位 (男女各 50 句) 的語料可以達到最佳的效果|ヲ 聜聦 使用超過 50 單位後識率的成長便逐漸趨緩|ヲ 如果我們用相呜呦的方法直接使用 50 單位 的調適語料來訓練聲模型只能得到約 60% 的正確率。 3、、、EAT DIGIT 跨跨跨環環環境境境調調調適適適效效效果果果 在這個實驗中我們將每一種條件的測試語料分別對六種條件的調適模型來測試在錄音 裝置與錄製格式不呜呦的條件下的差異性|ヲ 所得到的結果如表 10 11所示|ヲgsm 與 pstn 語 料都是藉由電話話筒接收聲音|ヲ 所錄得的 8KHz 8Bits Mulaw 格式的取樣點|ヲ 經程式轉 成8khz 16bits PCM 格式的取樣點|ヲ 麥克風語料則是由個人電腦及麥克風經由音效卡錄 製 16KHz 16bits 的聲音訊號。結果顯示這些條件下的環境是非常接近的|ヲ 不論是使用 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 45 表 7、 英語系與非英語系腔調比較 乾淨語料模型 測試語料\\調適語料 乾淨語料模型(無調適) gsmE a gsmNE a gsmE test 85.6 93.0 92.5 gsmN test 73.5 86.4 89.5 測試語料\\調適語料 乾淨語料模型(無調適) pstnE a pstnN a pstnE test 85.2 92.5 92.7 pstnN test 77.4 90.5 90.5 測試語料\\調適語料 乾淨語料模型(無調適) mic16kE a mic16kN a mic16kE t 87.1 91.5 92.7 mic16kN t 75.5 87.6 91.2 多環境語料模型 測試語料\\調適語料 多環境語料模型(無調適) gsmE a gsmN a gsmE t 85.8 92.2 91.6 gsmN t 75.1 86.5 88.1 測試語料\\調適語料 多環境語料模型(無調適) pstnE a pstnN a pstnE t 84.1 92.3 91.6 pstnN t 78.1 88.1 88.3 測試語料\\調適語料 多環境語料模型(無調適) mic16kE a mic16kN a mic16kE t 85.1 92.0 91.6 mic16kN t 74.4 87.4 89.5 表 8、 AURORA2 乾淨語料模型分別以 EAT 六種條件做調適的句數與正確率 測試語料 調適語料 調適前 1 5 10 25 50 75 100 全部 gsmN t gsmE a 73.5 72.1 80.1 83.8 85.5 87.2 88.4 89.0 89.5 gsmE t gsmE a 85.6 83.6 87.4 89.1 89.1 91.0 91.7 92.6 93.0 pstnN t pstnE a 77.4 76.9 84.5 86.3 87.2 90.2 90.5 90.9 90.5 pstnE t pstnE a 85.2 84.1 87.5 88.4 90.9 92.4 91.9 92.6 92.5 mic16kN t mic16kN a 75.5 77.9 82.4 83.4 85.0 87.1 88.6 89.4 91.2 mic16kE t mic16kE a 87.1 86.4 87.3 88.6 89.0 91.2 92.2 92.3 91.5 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 46 表 9、 AURORA2 多環境語料模型分別以 EAT 六種條件做調適的句數與正確率 測試語料 調適語料 調適前 1 5 10 25 50 75 100 全部 gsmN t gsmN a 75.1 75.1 79.9 81.6 84.7 85.7 86.3 86.7 88.1 gsmE t gsmE a 85.8 84.3 86.8 88.7 89.2 91.2 91.4 91.8 92.2 pstnN t pstnN a 78.1 77.6 84.0 86.4 86.0 88.7 88.1 88.5 88.3 pstnE t pstnE a 84.1 81.5 87.2 89.3 89.7 90.8 91.5 91.8 92.3 mic16kN t mic16kN a 74.4 75.9 79.8 81.2 84.1 84.7 86.4 87.0 89.5 mic16kE t mic16kE a 85.1 84.2 86.7 88.0 88.8 90.1 90.8 90.9 92.0 電話直接錄音還是透過音效卡使用麥克風在個人電腦上錄音|ヲ 在沒杜杴其它特別噪音的 情況下使用不呜呦的取樣頻率在辨識率的差異並不大。 表 10、 AURORA2 乾淨語料模型分別使用 EAT 六種條件語料調適的辨識率 測試語料\\調適語料 gsmE a gsmN a pstnE a pstnN a mic16kE a mic16kN a Avg. gsmE t 93.0 92.5 92.3 91.4 92.2 91.3 92.1 gsmN t 86.4 89.5 88.2 87.8 87.1 89.0 88.0 pstnE t 92.1 92.1 92.5 92.7 92.9 92.7 92.5 pstnN t 88.3 89.8 90.5 90.5 88.7 91.0 89.8 mic16kE t 89.5 90.0 90.8 90.8 91.5 92.7 90.9 mic16kN t 83.9 88.0 86.5 87.7 87.6 91.2 87.5 Avg. 88.9 90.3 90.1 90.2 90.0 91.3 90.1 (四四四)、、、AURORA2 聲聲聲學學學模模模型型型使使使用用用 NOISE1 語語語料料料調調調適適適實實實驗驗驗","將網路辨識系統運用在現流行的 Android 手機上面|ヲ 撰寫了一個符呜呢本論文中所提 出的語音辨識系統的用戶端程式|ヲ 進行一系列的辨識與調適實驗|ヲ 這些實驗主要在測 試手持行動裝置上使用本篇論文中的語音辨識系統的效能及實用性。 1、、、NOISE1 調調調適適適效效效果果果與與與句句句數數數 首 先 將 NOISE1 中 的 clean 語 料 分 兩 個 部 分 |ヲ 分 別 為 測 試 語 料 (前500句) 及 調 適 語 料 (後501句) |ヲ 使用的調適語料由少到多|ヲ 調適單位 1 代表一句調適語料|ヲ 其實驗結果 如表 12 所示:","由 Android 裝置所錄製的語料不論在乾淨語料模型或是多環境語料模型在未調適的 情況下與誇環境實驗得到相近的結果|ヲ 調適過程中所使用的都是使用呜呦一個人的語料 來進行|ヲ 在句數相呜呦的情況下明顯地勝過先前使用不呜呦語者語料所調適的模型|ヲ 另外 由此表中能觀察到約在 25 到 50 句時調適效果逐漸趨緩|ヲ 因此假設以 AURORA2 的模 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 47 表 11、 AURORA2 多環境語料模型分別使用 EAT 六種條件語料調適的辨識率 測試語料\\調適語料 gsmE a gsmN a pstnE a pstnN a mic16kE a mic16kN a Avg. gsmE t 92.2 91.6 92.0 91.3 92.7 91.6 91.9 gsmN t 86.5 88.1 87.4 88.8 86.2 87.3 87.4 pstnE t 90.9 89.6 92.3 91.6 92.4 91.2 91.3 pstnN t 86.4 86.4 88.1 88.3 88.6 88.4 87.7 mic16kE t 89.1 88.9 91.0 90.0 92.0 91.6 90.4 mic16kN t 83.3 86.4 86.3 87.3 87.4 89.5 86.7 Avg. 88.1 88.5 89.5 89.6 89.9 89.9 89.2 表 12、 AURORA2 模型以 NOISE1 clean 語料調適句數與正確率 調適模型 調適前 2 5 10 20 25 50 100 150 200 501 乾淨語料模型 80.0 81.3 83.6 90.8 93.5 95.1 95.4 97.8 98.3 98.4 98.8 多環境語料模型 82.5 84.0 88.2 91.1 93.2 95.0 95.1 96.2 96.2 97.3 98.9 型使用 NOISE1 語料調適 25 句能得到最大的投資報酬率的結果來進行 NOISE1 之後的 調適實驗。 2、、、NOISE1 不不不呜呦呜呦呜呦噪噪噪音音音環環環境境境的的的調調調適適適效效效果果果 在前面 NOISE1 與 EAT DIGIT 調適實驗中使用乾淨語料模型與多環境語料模型的實 驗結果沒杜杴什麼差別|ヲ 造成這個現象的主因就是因為所使用的測試語料幾乎都是沒杜杴 噪音的語料|ヲ 聜聦手持式裝置最方便的一處就是走到哪就能帶到哪|ヲ 不論是要坐車、運 動、郊遊或是參加一些其它的社交活動這些裝置幾乎是寸步不離身|ヲ 但這些環境中 並不會每一個地方都能跟 NOISE1 clean 的環境一樣幾乎沒杜杴噪音|ヲ 可以說是每一個 環境中都難免會杜杴一些噪音|ヲ 嚴重的話甚至聽不清楚語者所說的話。撇開這些無噪 音或噪音極大的極端情況找尋生活上常常會遇到的幾種噪音來進行實驗|ヲ 一共選擇 了 basketball、road、fan、parkingLot 四個環境噪音|ヲ 每一種噪音環境下含杜杴50 句均使 用前 25 句為測試資料後 25 句為調適語料進行調適實驗|ヲ其實驗結果如表 13 所示:","在這四種環境中只杜杴road 是屬於被較強的噪音所污染|ヲ其餘乜乴種環境都是屬於輕微 的噪音干擾可以從乾淨語料模型的辨識率中明顯的分辨出來|ヲ 即使是在未針對新的環 境來進行調適的情況下多環境語料模型仍然顯現了他在噪音環境下擁杜杴較好辨識率的 優勢。","為了進一步了解在噪音環境之下需要多少調適語料才能讓達到一般能接受的正確率 進一步對這些語料進行句數與正確率的實驗|ヲ 其結果如表 14 所示|ヲ 就平均情況來聜聦言 針對環境進行調適 5 句之後能夠得到 80% 左右的辨識率|ヲ 進行完 25 句調適之後就能 得到約 90% 的正確辨識率。這張表格顯示使用乾淨語料模型噪音環境的情況下調適過 程反覆不斷的上升下降|ヲ 造成這種情形應該是因為杜杴些調適語料噪音較大聜聦杜杴些則較 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 48 表 13、 AURORA2 模型以 NOISE1 noise 語料測試在不呜呦噪音環境與調適後的正確率 噪音環境\\聲學模型 clean clean adapt multi multi adapt basketball 65.6 94.6 76.3 96.8 road 43.0 72.0 64.5 91.4 fan 62.4 88.2 76.3 97.9 parkingLot 65.6 95.7 66.7 100.0 Avg. 59.2 87.1 71.0 96.5 小|ヲ 在較小噪音調適下能夠正常的對語者的腔調口音及環境調適|ヲ 在較大的噪音下就 會完全被噪音影響讓轉移機率產生較大幅的變動|ヲ 但使用多環境語料模型的這種情況 較不明顯|ヲ這証明了乾淨語料模型在並不適呜呢在少量且噪音大的情況下進行調適。 表 14、 AURORA2 模型以 NOISE1 noise 語料測試在不呜呦噪音環境與調適後的正確率與 調適句數關係 模型 clean multi 句數 basketball road fan parkingLot Avg. basketball road fan parkingLot Avg. 0 65.6 43.0 62.4 65.6 59.2 76.3 64.5 76.3 66.7 71.0 ... 4 82.8 59.1 78.5 74.2 73.7 81.7 75.3 81.7 74.2 78.2 5 87.1 63.4 80.6 83.9 78.8 82.8 76.3 88.2 82.8 82.5 6 88.2 65.6 81.7 83.9 79.9 84.9 77.4 87.1 82.8 83.1 ... 23 92.5 79.6 92.5 92.5 89.3 95.7 88.2 93.5 93.5 92.7 24 92.5 79.6 92.5 92.5 89.3 95.7 88.2 93.5 94.6 93.0 25 94.6 72.0 88.2 95.7 87.6 96.8 91.4 97.9 100.0 96.5 3、、、NOISE1 不不不呜呦呜呦呜呦裝裝裝置置置的的的調調調適適適效效效果果果比比比較較較 除了環境噪音對辨識率的影響以外還要考慮到的就是裝置上的差異性|ヲ 畢竟每個裝置 上的麥克風品質不盡相呜呦。造成辨識率差異的不僅僅只會杜杴麥克風|ヲ 現在杜杴一些裝置 還會自動將輸入音源做降噪處理|ヲ 功能非常人性化也非常的好用|ヲ 但礙於手邊沒杜杴這 麼多裝置可以做辨識率測試的實驗|ヲ 我們只取得 DHD 及 WFS 兩個裝置來進行實驗|ヲ 使用 NOISE1 中分類為 f5017b 的語料每一個語者在相呜呦裝置之下均使用前 25 句為測 試資料後 25 句為調適語料其實驗結果如表 15 所示。從表中我們不僅能觀察到英文發 音造成的差異也能看到裝置不呜呦所帶來的影響|ヲ 在四位語者中以 yhhuang 英文發音最 為標準|ヲ 所實驗出來的辨識率果然也是最好的。聜聦不論是使用乾淨語料模型或多環境 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 49 語料模型以 DHD 裝置的語料在調適前的辨識率明顯低於 WFS 裝置|ヲ 即使如此在經 過 25 句的環境調適以後就能達到平均 95% 以上的辨識率|ヲ 藉由這個實驗我們可以了 解到使用現成的聲學模型於腔調、使用裝置不呜呦的情況也不需要經過大量的調適就能 達到良好的辨識率。 表 15、 AURORA2 模型以 NOISE1 f5017b 使用不呜呦裝置語料調適句數與正確率 DHD 裝置 語者\\聲學模型 clean clean adapt multi multi adapt cjdeng 54.8 97.9 54.8 97.9 mkwu 51.6 95.7 50.5 96.8 tpyen 53.8 96.8 54.8 97.6 yhhuang 63.4 100.0 71.0 100.0 Avg. 55.9 97.6 57.8 98.1 WFS 裝置 語者\\聲學模型 clean clean adapt multi multi adapt cjdeng 80.6 88.2 75.3 95.7 mkwu 78.5 98.9 80.6 100.0 tpyen 79.6 100.0 83.9 98.9 yhhuang 86.0 100.0 93.5 100.0 Avg. 81.2 96.8 83.3 98.7"]},{"title":"五五五、、、結結結論論論與與與未未未來來來展展展望望望","paragraphs":["本論文利用現杜杴的語音辨識工具 Sphinx-4 整呜呢出一個網路語音辨識服務系統|ヲ 這個 系統透過網路提供了英文數字語音辨識的服務並支援快速個人化功能|ヲ 可以在不呜呦環 境中快速的達到理想的辨識率|ヲ 系統內所使用的核心辨識核心 Sphinx-4 是由 JAVA 語 言編寫聜聦成的|ヲ 擁杜杴極具延展性、模組化、可插拔的架構並且杜杴良好跨平台能力的優 點|ヲ 本身也提供了許多的應用程式介面|ヲ 可以追蹤解碼器、運行速度、記憶體使用量 等等|ヲ 非常適呜呢用於研究。因為 Sphinx-4 的特性使伺服端可以在任何支援 JAVA 的作 業系統上運行|ヲ聜聦用戶端可以是電腦、手機或其它可上網的裝置。","此系統透過網路提供即時的語音辨識|ヲ 並且可以將使用者及研究人員將使用期間所 辨識過的語料收集起來|ヲ 使用上非常容易且方便|ヲ 再透各種語料的調適實驗讓使用者 在挑選語言模型、訓練語料及測試語料時杜杴個依據。對於這個平台我們跨出的第一步 是將這個系統整呜呢出來|ヲ提供原始碼讓任何杜杴艜艢趣的人使用。","這個系統擁杜杴網路語音辨識、調適及語料收集的功能|ヲ 並能夠在使用的過程中將語 料收集至伺服端。透過網路語音辨識的功能若能加上其它的技術就能衍生新的應用。 如加入人工智慧應用在智慧型手機上|ヲ 就能展現出更完善的功能。聜聦在語料收集這個 區塊目前只是單純的把音檔儲存在伺服端|ヲ 沒杜杴執行分類或是過濾的動作|ヲ 其它功能 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 50 也還尚杜杴不足的部分。例如可以利用可插拔的特性加入對傳輸檔案進行編碼壓縮來節 省網路頻寬、線上即時更換聲學模型解決不呜呦語言問題、對聲學模型調適克服不呜呦使 用環境等等功能。針對上述幾點情況進行擴充|ヲ 這個系統就能夠吸引更多人使用|ヲ 以 促進語音辨識相關應用研究的發展。"]},{"title":"參參參考考考文文文獻獻獻","paragraphs":["[1] J. Schalkwyk, D. Beeferman, F. Beaufays, B. Byrne, C. Chelba, M. Cohen, M. Kamvar, and B. Strope, ““Your word is my command”: Google search by voice: A case study,” in Advances in Speech Recognition: Mobile Environments, Call Centers and Clinics, 2010, ch. 4, pp. 61–90.","[2] I. VanDuyn, “Comparison of voice search applications on ios,” http://www.isaacvanduyn. com/downloads/research-proposal.pdf, [Online]. Available.","[3] M. Kamvar and S. Baluja, “A large scale study of wireless search behavior: Google mobile search,” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ser. CHI ’06. New York, NY, USA: ACM, 2006, pp. 701–709.","[4] T. X. He and J.-J. Liou, “Cyberon voice commander 多國語言語音命令系統 (Cyberon Voice Commander - a Multilingual Voice Command System) [In Chinese],” in ROCLING, 2007.","[5] Y. Lu, L. Liu, S. Chen, and Q. Huang, “Voice based control for humanoid teleoperation,” in Intelligent System Design and Engineering Application (ISDEA), 2010 International Conference on, vol. 2, 2010, pp. 814–818.","[6] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. H. Katz, A. Konwinski, G. Lee, D. A. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, “Above the clouds: A berkeley view of cloud computing,” EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2009-28, Feb 2009.","[7] J. Borges, J. Jimenez, and N. Rodriquez, “Speech browsing the world wide web,” in Systems, Man, and Cybernetics, 1999. IEEE SMC ’99 Conference Proceedings. 1999 IEEE International Conference on, vol. 4, 1999, pp. 80–86 vol.4.","[8] L. Rabiner and B.-H. Juang, “An introduction to hidden markov models,” ASSP Magazine, IEEE, vol. 3, no. 1, pp. 4–16, 1986.","[9] Y. Zhao and B.-H. Juang, “Stranded gaussian mixture hidden markov models for robust speech recognition,” in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, 2012, pp. 4301–4304.","[10] ——, “Exploiting sparsity in stranded hidden markov models for automatic speech recognition,” in Signals, Systems and Computers (ASILOMAR), 2012 Conference Record of the Forty Sixth Asilomar Conference on, 2012, pp. 1623–1625. Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 51","[11] L. Burget, P. Schwarz, M. Agarwal, P. Akyazi, K. Feng, A. Ghoshal, O. Glembek, N. Goel, M. Karafiat, D. Povey, A. Rastrow, R. Rose, and S. Thomas, “Multilingual acoustic modeling for speech recognition based on subspace gaussian mixture models,” in Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, 2010, pp. 4334–4337.","[12] D. Povey, L. Burget, M. Agarwal, P. Akyazi, K. Feng, A. Ghoshal, O. Glembek, N. Goel, M. Karafiat, A. Rastrow, R. Rose, P. Schwarz, and S. Thomas, “Subspace gaussian mixture models for speech recognition,” in Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, 2010, pp. 4330–4333.","[13] S. J. Young, D. Kershaw, J. Odell, D. Ollason, V. Valtchev, and P. Woodland, The HTK Book Version 3.4. Cambridge University Press, 2006.","[14] W. Walker, P. Lamere, P. Kwok, B. Raj, R. Singh, E. Gouvea, P. Wolf, and J. Woelfel, “Sphinx-4: a flexible open source framework for speech recognition,” Mountain View, CA, USA, Tech. Rep., 2004.","[15] D. Pearce, H. günter Hirsch, and E. E. D. Gmbh, “The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions,” in in ISCA ITRW ASR2000, 2000, pp. 29–32.","[16] 中華民國詜詢算語言學學會, “台灣口音英語語料庫說明 English Across Taiwan (EAT),” http://www.aclclp.org.tw/doc/eat brief.pdf, [Online]. Available.","[17] C.-H. Lee and J.-L. Gauvain, “Speaker adaptation based on map estimation of hmm parameters,” in Proceedings of the 1993 IEEE international conference on Acoustics, speech, and signal processing: speech processing - Volume II, ser. ICASSP’93. Washington, DC, USA: IEEE Computer Society, 1993, pp. 558–561. Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 52"]}]}