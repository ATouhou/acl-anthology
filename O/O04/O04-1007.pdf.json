{"sections":[{"title":"Finding Relevant Concepts for Unknown Terms Using a Web-based Approach","paragraphs":["Chen-Ming Hung1 and Lee-Feng Chien1, 2 1. Institute of Information Science, Academia Sinica","2. Dept. of Information Management, National Taiwan University","Taipei, Taiwan rglly@iis.sinica.edu.tw and lfchien@iis.sinica.edu.tw","Abstract. Previous research on automatic thesaurus construction most focused on extracting relevant terms for each term of concern from a small-scale and domain-specific corpus. This study emphasizes on utilizing the Web as the rich and dynamic corpus source for term association estimation. In addition to extracting relevant terms, we are interested in finding concept-level information for each term of concern. For a single term, our idea is that to send it into Web search engines to retrieve its relevant documents and we propose a Greedy-EM-based document clustering algorithm to cluster them and determine an appropriate number of relevant concepts for the term. Then the keywords with the highest weighted log likelihood ratio in each cluster are treated as the label(s) of the associated concept cluster for the term of concern. With some initial experiments, the proposed approach has been shown its potential in finding relevant concepts for unknown terms."]},{"title":"1. INTRODUCTION","paragraphs":["It has been well recognized that a thesaurus is crucial for representing vocabulary knowledge and helping users to reformulate queries in information retrieval systems. One of the important functions of a thesaurus is to provide the information of term associations for information retrieval systems. Previous research on automatic thesaurus construction most focused on extracting relevant terms for each term of concern from a small-scale and domain-specific corpus. In this study, there are several differences extended from the previous research. First, this study emphasizes on utilizing the Web as the rich and dynamic corpus source for term association estimation. Second, the thesaurus to be constructed has no domain limitation and is pursued to be able to benefit Web information retrieval, e.g. to help users disambiguate their search interests, when users gave poor or short queries. Third, in addition to extracting relevant terms, in this study we are interested in finding concept-level information for each term of concern. For example, for a term “National Taiwan University” given by a user, it might contain some different but relevant concepts from users’ point of view, such as “main page of National Taiwan University”, “entrance examination of NTU”, “the Hospital of NTU”, etc. The purpose of this paper is, therefore, to develop an efficient approach to deal with the above problem.","In information retrieval researching area, extracting concepts contained in one text always plays a key role. However, in traditional way, if the text is too short, it is almost impossible to get enough information to extract the contained concepts. In this paper, utilizing the abundant corpora on the World Wide Web, we attempt to find the concepts contained in arbitrary length of topic-specific texts, even only a single term. For a single term, our idea is that to send it into Web search engines to retrieve its relevant documents, and a Greedy-EM-based document clustering algorithm is developed to cluster these documents into an appropriate number of concept clusters, with the similarity of the documents. Then the terms with the highest weighted log likelihood ratio in each clustered document group are treated as the label(s) of the associated concept cluster for the term of concern. To cluster the extracted documents into an unknown number of concept mixtures is important, because it is hard to know an exact number of concepts should be contained in a single term.","Compared with general text documents, a single term is much shorter and typically do not contain enough information to extract adequate and reliable features. To assist the relevance judgment between short terms, additional knowledge sources would be exploited. Our basic idea is to exploit the Web. Adequate contexts of a single term, e.g., the neighboring sentences of the term, can be extracted from large amounts of Web pages. We found that it is convenient to implement our idea using the existent search engines. A single term could be treated as a query with a certain search request. And its contexts are then obtained directly from the highly ranked search-result snippets, e.g., the titles and descriptions of search-result entries, and the texts surrounding matched terms.","The proposed approach relies on an efficient document clustering technique. Usually, in document clustering techniques [1, 3, 4, 8], each text in a training set is transformed to a certain vector space, then begin agglomerated with another one text step by step depending on their cosine similarity [9]. Thus, a proper transformation from text to vector space, like TFIDF [9], takes the heavy duty of classification accuracy or concept extraction result. However, a good transformation, i.e. feature extraction, needs a well-labeled training data to support; this is not such an easy task in real world. The idea of this paper is to modify the vector space transformation as probabilistic framework.","With the extracted training data from the web, the Greedy EM algorithm [5, 7] is applied in this paper to automatically determine an appropriate number of concepts contained in the given single term through clustering the training documents. This is important while doing relevant concept extraction; otherwise, the number of concepts has to be assumed previously, it is difficult and impractical in real world. After clustering the training documents extracted from the Web into a certain number of mixtures, for each mixture, the representation of this mixture is straightforwardly defined as the term with the highest weighted log likelihood ratio in this mixture. With some initial experiments, the proposed approach has been shown its potential in finding relevant concepts for terms of concern.","The remainder of the paper is organized as follows. Section 2 briefly describes the background assumption, i.e. Naïve Bayes, and the modeling based on Naive Bayes. Section 3 describes the overall proposed approach in this paper, including the main idea of the greedy EM algorithm and its application to decide the number of concept domains contained in the training data from the web; in addition, generates keywords via comparing the weighted log likelihood ratio. Section 4 shows the experiments and their result. The summary and our future work are described in Section 5."]},{"title":"2. NAÏVE BAYES ASSUMPTION AND DOCUMENT CLASSIFICATION","paragraphs":["Before introducing our proposed approach, here introduce a well known way of text representation, i.e., Naive Bayes assumption. Naive Bayes assumption is a particular probabilistic generative model for text. First, introduce some notation about text representation. A document, i"]},{"title":"d","paragraphs":[", is considered to be an ordered list of words, ,1 ,2 ,| |"]},{"title":"{, ,, }","paragraphs":["ii ididd d"]},{"title":"ww w","paragraphs":[", where ,ijd"]},{"title":"w","paragraphs":["means the jth words and"]},{"title":"||","paragraphs":["i"]},{"title":"d","paragraphs":["means the number of words in i"]},{"title":"d","paragraphs":[". Second, every document is assumed generated by a mixture of components"]},{"title":"{}","paragraphs":["k"]},{"title":"C","paragraphs":["(relevant concept clusters), for k=1 to K. Thus, we can characterize the likelihood of document i"]},{"title":"d","paragraphs":["with a sum of total probability over all mixture components: 1"]},{"title":"(|) ( )(| ,)|","paragraphs":["kK","iik k"]},{"title":"ppCpCddθθθ","paragraphs":["="]},{"title":"= ∑","paragraphs":["(1) Furthermore, for each topic class kC of concern, we can express the probability of a document as: 1 ,1 ,2 ,| | ,, ||"]},{"title":"(| ,) ( , ,, | ,) (|,,, )","paragraphs":["ik k k j ii idi i ij iz dd d d dd"]},{"title":"pd C p w w w C pw C w z j θθ θ","paragraphs":["="]},{"title":"=< > =<∏  ","paragraphs":["(2) ,, ,"]},{"title":"(|,,, )(|,)","paragraphs":["kk ij iz ijdd d"]},{"title":"pw C w z j pw Cθθ<=","paragraphs":["(3)","Based on standard Naive Bayes assumption, the words of a document are generated independently of context, that is, independently of the other words in the same document given the class model. We further assume that the probability of a word is independent of its position within the document. Combine (1) and (2), , || 1"]},{"title":"(| ,) ( | ,)","paragraphs":["i ij d","i kdk j"]},{"title":"pd C pw Cθθ","paragraphs":["="]},{"title":"= ∏","paragraphs":["(4)","Thus, the parameters of an individual class are the collection of word probabilities, |"]},{"title":"(| ,)","paragraphs":["t k twC k"]},{"title":"pw Cθθ=","paragraphs":[". The other parameters are the weight of mixture class,"]},{"title":"(|)","paragraphs":["k"]},{"title":"pC θ","paragraphs":[", that is, the prior probabilities of class, k"]},{"title":"C","paragraphs":[". The set of parameters is |"]},{"title":"{,}","paragraphs":["t kk CwC"]},{"title":"θθ θ=","paragraphs":[". As will be described in next section, the proposed document clustering is designed fully based on the parameters."]},{"title":"3. RELEVANT CONCEPTS EXTRACTION","paragraphs":["In this section, we describe the overall framework of the proposed approach. Suppose given a single term,"]},{"title":"T","paragraphs":[", and its relevant concepts are our interest. The first step of the approach is to send T into search engines to retrieve the relevant documents as the corpus. Note that the retrieved documents are the so-called snippets defined in [2]. The detailed process of the approach is described below."]},{"title":"3.1 The proposed Approach","paragraphs":["Suppose given a single term, T; then the process of relevant-concept extractions is designed as: Step 1. Send T into search engines to retrieve N snippets as the Web-based corpus, DT. Step 2. Apply the Greedy EM algorithm to cluster DT into K mixtures (clusters), 1"]},{"title":"{}","paragraphs":["K kk"]},{"title":"C","paragraphs":["= , where K is","dynamically determined. Step 3. For each Ck, k=1 to K, choose the term (s) with the highest weighted log likelihood ratio as the label (s) of Ck."]},{"title":"3.2 The Greedy EM Algorithm","paragraphs":["Because we have no idea about the exact number of concepts strongly associated with each given term, thus for each term it's straightforward to apply the Greedy EM algorithm to clustering the relevant documents into an auto-determined number of clusters. The algorithm is a top-down clustering algorithm which is based on the assumptions of the theoretical evidence developed in [5, 7]. Its basic idea is to suppose that all the relevant documents belong to one component (concept cluster) at the initial stage, then successively adding one more component (concept cluster) and redistributing the relevant documents step by step until the maximal likelihood is approached.","Figure 1 shows the proposed approach and it is summarized in the following. a) Set K=1 and initialize"]},{"title":"1","paragraphs":["Ck"]},{"title":"θ =","paragraphs":["and |t kwC"]},{"title":"θ","paragraphs":["straightforwardly by t"]},{"title":"w","paragraphs":["’s frequency, for all t"]},{"title":"w","paragraphs":["shown in","DT. b) Perform EM steps until convergence, then 1|"]},{"title":"{,}","paragraphs":["iK","k Ct k kwC"]},{"title":"θθ θ","paragraphs":["="]},{"title":"=","paragraphs":["c) Calculate the likelihood,"]},{"title":"()","paragraphs":["i"]},{"title":"L θ","paragraphs":[".","d) Allocate one more mixture given initial modeling, i.e. 1 1 1|"]},{"title":"{,}","paragraphs":["K Ct K KwC"]},{"title":"θθ θ","paragraphs":["+ + +"]},{"title":"=","paragraphs":[", described in section","3.2.2. e) Keep i"]},{"title":"θ","paragraphs":["fixed, and use partial EM techniques, described in section 3.2.3, to update 1K"]},{"title":"θ","paragraphs":["+ . f) Set","11 1|"]},{"title":"{,}","paragraphs":["iK","k Ct k kwC"]},{"title":"θθθ","paragraphs":["++ ="]},{"title":"=","paragraphs":[". Calculate the likelihood, 1"]},{"title":"()","paragraphs":["i"]},{"title":"L θ","paragraphs":["+ . g) Stop if 1"]},{"title":"()","paragraphs":["i"]},{"title":"L θ","paragraphs":["+ <"]},{"title":"()","paragraphs":["i"]},{"title":"L θ","paragraphs":["; otherwise, return to c) and set K=K+1."]},{"title":"3.2.1 Likelihood Function","paragraphs":["As described previously, all relevant documents belong to one mixture initially; then check the likelihood to see if it is proper to add a new mixture. Thus, given K mixture components, the likelihood of K+1 is defined as: 11"]},{"title":"()(1 )() (, )","paragraphs":["KT KT TK"]},{"title":"LD LD Dααφθ","paragraphs":["++"]},{"title":"=− +","paragraphs":["(5) with"]},{"title":"α","paragraphs":["in (0,1), where 1 1|"]},{"title":"{,}","paragraphs":["K t KwC"]},{"title":"θθ α","paragraphs":["+ +"]},{"title":"=","paragraphs":["is the modeling of newly added mixture 1K"]},{"title":"C","paragraphs":["+ and 1"]},{"title":"(, )","paragraphs":["TK"]},{"title":"Dφθ","paragraphs":["+ is the likelihood in 1K"]},{"title":"C","paragraphs":["+ . If 1"]},{"title":"()","paragraphs":["KT"]},{"title":"LD","paragraphs":["+ <"]},{"title":"()","paragraphs":["KT"]},{"title":"LD","paragraphs":[", then stop the allocation of new mixture; otherwise, reallocate a new one."]},{"title":"3.2.2 Initialize Allocated Mixture","paragraphs":["In [7], a vector space model, initializing the newly added mixture is to calculate the first derivation with respect to"]},{"title":"α","paragraphs":["and to assume that the covariance matrix is a constant matrix. However, in our proposed","probability framework, it is much more complicated because of a large amount of word probabilities, |"]},{"title":"{ }","paragraphs":["t t kwC"]},{"title":"wθ ∀","paragraphs":[". Thus, we take the approximation of"]},{"title":"α","paragraphs":["in [6] as"]},{"title":"0.5α =","paragraphs":["for K=1 and"]},{"title":"2/( 1)Kα =+","paragraphs":["for"]},{"title":"2K ≥","paragraphs":[". The initialization of 1|"]},{"title":"{ }","paragraphs":["K t twC"]},{"title":"wθ","paragraphs":["+"]},{"title":"∀","paragraphs":["is randomized to satisfy |{}"]},{"title":"1","paragraphs":["tt kwCw"]},{"title":"θ =∑","paragraphs":["."]},{"title":"3.2.3 Update with Partial EM Algorithm","paragraphs":["In order to simplify the updating problem, we take advantage of partial EM algorithm for locally search the maxima of 1"]},{"title":"()","paragraphs":["KT"]},{"title":"LD","paragraphs":["+ . A notable property is that the original modeling for k=1 to K are fixed, only 1K"]},{"title":"θ","paragraphs":["+ is updated. || 1","1 1","||","|| 1 ||||","1 11 1 | 1"]},{"title":"{( | )} 1(,)(|) || (,)( | )","paragraphs":["T T V t tn n sn n","t K t K","V D","n DV","K sn t C K w"]},{"title":"pw C Nw d pC d VNwdpCd θ","paragraphs":["= + + =","+ == = +"]},{"title":"= + = +      ∑ ∑∑ ","paragraphs":["(6) 1|| 1 11"]},{"title":"1(|) () (1)||","paragraphs":["nn T K D K CK"]},{"title":"pC d pC KDθ","paragraphs":["= + ++"]},{"title":"+ == ++ ∑ ","paragraphs":["(7) where |V | and |DT| means the number of vocabularies and the number of documents shown in the T"]},{"title":"D","paragraphs":["respectively.","Since only the parameters of the new components are updated, partial EM steps constitute a simple and fast method for locally searching for the maxima of 1K"]},{"title":"L","paragraphs":["+ , without needing to resort to other computationally demanding nonlinear optimization methods."]},{"title":"3.3 Keyword Generation","paragraphs":["In the process, the documents in the training data T"]},{"title":"D","paragraphs":["will be clustered with their similarity into a set of clusters and keywords that can represent the concept of each cluster will be extracted. After clustering the relevant documents into several clusters, the distribution of each cluster in a probabilistic form can be calculated with the data in the cluster by applying the Greedy EM algorithm already described previously.","Next, we have to discover the hidden semantics inside each document cluster. However, retrieving the hidden semantics from a set of documents is a big issue. For convenience, we simply represent the meaning of a cluster with the word that has the highest weighted log likelihood ratio1","among the contained words in this cluster. With this assumption, the “representative” word could be chosen directly by comparing"]},{"title":"(| ) (| ) (| )log (| )","paragraphs":["tk tk tk","k t"]},{"title":"pw C WLR w C p w C pw C  =  ","paragraphs":["(8) for k=1 to K, where"]},{"title":"(|)","paragraphs":["kt"]},{"title":"Cpw","paragraphs":["means the probability of word t"]},{"title":"w","paragraphs":["in component k"]},{"title":"C","paragraphs":["and"]},{"title":"(| )","paragraphs":["k t"]},{"title":"Cpw","paragraphs":["means sum of the probabilities of word t"]},{"title":"w","paragraphs":["in those clusters except k"]},{"title":"C","paragraphs":["."]},{"title":"4. EXPERIMENTS","paragraphs":["In real world, for an unknown term, its associated concepts are what we are interested in; thus, in this section, we will show the experiment results obtained in evaluating a set of test terms. Before the larger amount of experiment, let’s preview the experiment of “ATM” to determine the number of retrieved relevant documents. Google (http://www.google.com) is the main search engine which we utilized in the following experiment."]},{"title":"4.1 Appropriate Number of Retrieved Relevant Documents ","paragraphs":["1 The sum of this quantity over all words is the Kullback-Leibler divergence between the distribution of words in k"]},{"title":"C","paragraphs":["and the distribution of words in k"]},{"title":"C","paragraphs":[", (Cover and Thomas, 1991).","We assume that too many retrieved documents will cause noises, but too few won't contain enough information about this unknown term. Thus, the appropriate number of retrieved relevant documents has to be decided. “ATM” in dictionary has six hidden semantics, which are “Automated Teller Machine”, “Asynchronous Transfer Mode”, “Act of Trade Marks”, “Air Traffic Management”, “Atmosphere” and “Association of Teachers of Mathematics” respectively. Table 1 shows the bi-gram extracted concepts via number of retrieved texts.","Table 1: Extracted concept clusters in “ATM” with respect to different numbers of retrieved relevant terms","# of training texts Extracted concept clusters","100 ATM {card, cell, internetworking, standards}, asynchronous transfer","200 ATM {access, information, networking, standards, locations}, credit union, debit cards , safety tips, teller machine, telescope makers","300 ATM {applications, cards, fees, networking, surcharges, locations}, adaptation layers, credit union, debit cards, token rings, teller machine,","400 ATM {applications, crashes, services, transactions, networking, security}, branch locator, financial institution, personal banking, public transport, telangiectasia mutated","500 ataxia telangiectasia, ATM {applications, asynchronous, crashes, products, protocol, resource}","600 ataxia telangiectasia, ATM {applications, crashes, protocol, technology}, atmospheric science, communication technology, electronics engineering, network interface, public transport, wan switches, rights reserved","700 air traffic, ataxia telangiectasia, ATM {crashes, debit, encryptor, protocol, surcharge, traffic}, atmospheric science, checking account, communication technology, electronics engineering, network interface, public transport","800 ataxia telangiectasia, ATM {adapters, crime, debit, protocol, surcharge, usage, cards}, atmospheric sciences, business checking, communication technology","900 24 hours, ataxia telangiectasia, ATM {adapters, connections, crashes, crime, debit, industry, protocol, resources}","1000 ATM networks, Arizona federal, 24 hour","Table 1 shows a challenge that choosing the term with the highest weighted log likelihood ratio as the label of one concept cluster can not effectively describe its complete semantics appropriately; in addition, for example, “Automated Teller Machine” is composed of many aspects, like security, location, cards, and etc. Thus, concept domain of “Automated Teller Machine” could be figured out while “ATM applications”, “ATM locations”, “ATM surcharges”, and some other aspects associated with “Automated Teller Machine” being extracted. Similarly, “Air Traffic Management” could be figured out while “public transport”, “air traffic” being extracted. Figure 1: Number of relevant documents v.s. Number of extracted concept clusters 0 1 2 3 4 5 6 7","100 200 300 400 500 600 700 800 900 1000 # of relevant documents #  o f ex t r ac t e d co n c ep t cl u s t e r s","Except the six hidden semantic clusters in ATM, some other concept clusters were also extracted, e.g. “Amateur Telescope Maker” because of “telescope makers” extracted and “Ataxia Telangiectasia Mutated” because of “ataxia teleangiectasia” extracted. One more interesting thing is that the more retrieved relevant documents not necessarily direct to the more extracted concept clusters (Figure 1). This phenomenon is caused from the extra noises extracted from the more relevant documents. The extra noises will not only worsen the performance of the greedy EM algorithm but also generate improper relevant terms from the clustered groups, which will not be considered as “good” categories manually. For each test term, considering the time cost and the marginal gain of extracted concepts, 600 relevant documents were retrieved from Web. Of course 600 relevant documents are not always appropriate for all cases, but for convenience, it was adopted."]},{"title":"4.2 Data Description","paragraphs":["The experiments took the \"Computer Science\" hierarchy in Yahoo! as the evaluation. There were totally 36 concepts in second level in the \"Computer Science\" hierarchy (as in Table 2), 177 objects in the third level and 278 objects in fourth level, all rooted at the concept \"Computer Science\". We divided the objects in third-level and fourth-level into three groups: full articles, which were the Web pages linked from Yahoo!'s website list under the Computer Science hierarchy, short documents, which were the site description offered by Yahoo!, and text segments, which were the directory names. We randomly chose 30 text segments from the third-level plus the fourth-level objects. The 30 proper nouns are shown in Table 3."]},{"title":"4.3 Relevant-Concepts Extraction","paragraphs":["In Section 3, the Greedy EM algorithm is treated as the unsupervised learning method which clusters retrieved relevant documents to extract hidden concepts for each test term.","Table 3: 30 terms from 3rd level and 4th","level in Yahoo!’s CS hierarchy ActiveX CMX CORBA Darwin Ebonics Eponyms Figlet GNU Hobo Signs Hurd IRIX ISDN Jini JXTA Mach Mesa PGP – Pretty Good Privacy PPP Puns QNX ROADS RSA Ray Tracing SETL SIP Trigonometry VHDL VMS WAIS Xinu","Table 4 shows the extracted bi-gram concept clusters for the 30 randomly chosen CS terms; this means that only bi-gram terms in the retrieved documents were extracted. The number of hidden concept clusters in each term was determined automatically by the Greedy EM algorithm.","Table 4: Bi-gram concept clusters for the test terms in Yahoo!’s CS hierarchy Test Terms Extracted Concept Clusters ActiveX ActiveX {control, vs, server} CMX CMX-RTX RTOS, multi-tasking operating, CMX {3000, 5000}, San Jose, Jose","BLVD CORBA application development, C++ software, CORBA {2.2, orbs, servers}, distributed","{applications, programming}, IDL compiler, language {IDL, mapping}, object-","oriented programming, request broker Darwin Charles Darwin, Darwin 6.0.2 Ebonics black English, African Americans, Ebonics X-mas Eponyms medical phenomena, on-line medical, aortic regurgitation, encyclopediaof","medical, Firkin Judith, esophageal surgery, historical allusions Figlet art characters, Figlet {Frank, frontend, package, RPM, tool}, assorted fonts GNU GNU {aspell, coding, compilers, documentation, desktop}, license GPL, public","licenseterms, reference card Hobo Signs ideogram carved, 45 signs Hurd Alexander Hurd, Debian developers, Debian Gnu, GNU operating, Hannah","Hurd, Hon Lord IRIX Sgi IRIX, IRIX reinstall, 2.6.5.7 Sgi, 3D graphics ISDN digital {access, networks, telephone}, Arca technologies, communication","standards, copper wire, data {applications, communications, services}, external","ISDN, integrated services Jini Jini technology, Jini Ji JXTA project JXTA, peer peer Mach Mustang Mach, disc golf Mesa Mesa Verde, Mesa Quad PGP encrypt messages, foaf files, keysigning party, PGP {backend, basics, comments,","corp}, ASCII armour PPP point-to-point protocol, ppp flea., Puns French word, Japanese spelling, social sciences, bilingual puns QNX Microkernel OS, QNX {applications, machine, voyager}, alternative vendor ROADS {Access, British, Hampton} ROADA, adverse weather RSA RSA security, RSA lighting Ray Tracing Computer graphics, Carlo Ray, recursive Ray SETL set language, ab le SIP control protocol, {bring, panel, partysip,} SIP, SIP {architecture, application,","client, standards}, Jonathan Rosenberg","Trigonometry Trigonometric functions, advanced algebra, Benjamin Bannekers, Banneker’s","trigonometry VHDL Asic design, circuit VHSIC, complete VHDL, hardware design, digital logic,","verilog simulation, synthesis tool VMS computational chemistry, shopping cart, administrator authentication, UNIX","translation, CCL VMS, equipment corporation WAIS area informationserver, laws enacted, public laws, WAIS {client, gateway,","searching, libraries}, presidential documents Xinu AMD élan, unix clone, software OS, Xinu {kernel, system}, II internetworking,","master distributor, demand paging","From Table 4, it is encouraging that the proposed approach extracted the main idea for most test CS terms. Taking \"Trigonometry\" for example, if we have no idea about “Trigonometry”, then from “function” and “algebra” in Table 4, there is not difficult to guess that it may be a kind of mathematical functions and developed by Benjamin Bannekers. Again, our proposed approach caught that “Darwin” is not only a British Naturalist but also the name of graphical software.","Even though the experiment result shows encouraging performance, the result was still bothered by many duplicated and noisy aspects. For example, “CORBA” means “Common Object Request Broker Architecture”; however, “C++ software” and “application development” actually only provide vague or not necessary information about “CORBA”. This was caused by the “too much effort” of the Greedy EM algorithm, which clusters the retrieved mixtures into too many groups."]},{"title":"5. CONCLUSIONS AND FUTURE WORK","paragraphs":["We have presented a potential approach to finding relevant concepts for terms via utilizing World Wide Web. This approach obtained an encouraging experimental result in testing Yahoo!’s computer science hierarchy. However, the work needs more in-depth study. As what we mentioned previously, choosing the word with the highest weighted log likelihood ratio as the concept of a clustered group after the Greedy EM algorithm does not provide enough representative. In addition, one concept usually contains many domains, e.g. “ATM” contains security, teller machine, transaction cost, and etc. Thus, distinguishing the extracted keywords into a certain concept still needs human intervention. On the other hand, in order to solve the problem of “too much effort” of the Greedy EM algorithm, we need to modify it with another convergence criterion."]},{"title":"References","paragraphs":["[1] A. Jain, M. Murty, and P. Flynn. Data Clustering: A Review. In ACM Computing Surveys, 31(3), September 1999.","[2] C. C. Huang, S. L. Chuang and L. F. Chien. LiveClassifier: Creating Hierarchical Text Classifiers through Web Corpora, WWW (2004).","[3] E. Rasmussen. Clustering Algorithms. In Information Retrieval Data Structures and Algorithms, William Frakes and Ricardo Baeza-Yates, editors, Prentice Hall, 1992. [4] E. Voorhees. The Cluster Hypothesis Revisited. In Proceedings of SIGIR 1985, 95-104.","[5] J. J. Verbeek, N. Vlassis and B. J. A. Krose. Efficient Greedy Learning of Gaussian Mixture Models. Neural Computation, 15 (2), pp.469-485, 2003.","[6] J. Q. Li and A. R. Barron. Mixture Density Estimation. In Advances in Neurarl Information processing Systems 12, The MIT Press, 2000.","[7] N. Vlassis and A. Likas A Greedy Algorithm for Gaussian Mixture Learning. In Neural Processing Letters (15), pp. 77-87, 2002.","[8] P. Willett. Recent Trends in Hierarchic Document Clustering: A Critical Review. In Information Processing and Management, 24(5), 577-597, 1988.","[9] T. Joachims. A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization. In Machine Learning: Proceedings of the Fourteenth International Conference (ICML ’97), pp. 143-151."]}]}