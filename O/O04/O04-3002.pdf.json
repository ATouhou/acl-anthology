{"sections":[{"title":"","paragraphs":["Computational Linguistics and Chinese Language Processing Vol. 9, No. 2 , August 2004, pp. 13-28 13 The Association for Computational Linguistics and Chinese Language Processing"]},{"title":"Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation Jhing-Fa Wang * , Shun-Chieh Lin* , Hsueh-Wei Yang* , and Fan-Min Li*  Abstract","paragraphs":["The critical issues involved in speech-to-speech translation are obtaining proper source segments and synthesizing accurate target speech. Therefore, this article develops a novel multiple-translation spotting method to deal with these issues efficiently. Term multiple-translation spotting refers to the task of extracting target-language synthesis patterns that correspond to a given set of source-language spotted patterns in conditional multiple pairs of speech patterns known to be translation patterns. According to the extracted synthesis patterns, the target speech can be properly synthesized by using a waveform segment concatenation-based synthesis method. Experiments were conducted with the languages of Mandarin and Taiwanese. The results reveal that the proposed approach can achieve translation understanding rates of 80% and 76% on average for Mandarin/Taiwanese translation and Taiwanese/Mandarin translation, respectively. Keywords: Multiple-Translation Spotting, Speech-to-Speech Translation"]},{"title":"1. Introduction","paragraphs":["Automatic speech-to-speech translation is a prospective application of speech and language technology [See JANUS III [Lavie et al. 1997], Verbmobil [W. Wahlster 2000], EUTRANS [Casacuberta et al. 2001] and ATR-MATRIX [Sugaya et al. 1999] ]. However, the unsolved problems in speech-to-speech translation are how to obtain proper source segments and how to generate accurate target sequences while the system performance is degraded by speech input. With the rising importance of parallel texts (bitexts) in language translation, an approach called translation spotting has been applied for proposing appropriate translations, referring to the TransSearch system [Macklovitch et al., 2000] and sub-sentential translation memory systems [M. Simard, 2003]. Previous works in this area have suggested that manual review or  * Corresponding author: Prof. Jhing-Fa Wang, Department of Electrical Engineering, National Cheng Kung University, No.1, Dasyue Rd., East District, Tainan City 70101, Taiwan, R.O.C. Email: wangjf@csie.ncku.edu.tw Tel: 886-6-2757575 ext. 62341 Fax: 886-6-2746867   14 Jhing-Fa Wang et al. crafting is required to obtain example bases of sufficient coverage and accuracy to be truly useful.","Translation spotting (TS) is a term coined by Véronis and Langlais [2000] and refers to the task of identifying word tokens in a target-language (TL) translation that correspond to some given word-patterns in a source-language (SL) text. This process takes as input a couple, i.e., a pair of SL and TL text segments known to be translation patterns, and an SL query, i.e., a subset of the patterns of the SL segment, on which the TS will focus its attention. In more formal terms:","The input to the TS process is a pair of SL and TL text segments TS , and a contiguous, non-empty input sequence of word-tokens in SL, nssq L 1 = . The output is a pair of sets of translation patterns"]},{"title":")(),( TrSr","paragraphs":["qq : the SL answer and TL answer, respectively. Table 1 shows some examples of TS, where the words in italics represent the SL input, and the words in bold are the SL and TL answers. As can be seen in these examples, the patterns in the input q and answers )( Sr q and )( Tr q may or may not be contiguous (examples 2 and 3), and the TL answer may possibly be empty (example 4) when there is no satisfactory way of linking TL patterns to the input. By varying the identification criteria, the translation spotting method can help evaluate units over various dimensions, such as frequency ranges, parts of speech and even speech features of spoken language. "]},{"title":"Table 1. Translation spotting examples. Sentence Pair Query SL (Mandarin) TL (Taiwanese)","paragraphs":["你 預計 要 待 幾 天 lie phahsngx bueq doax kuie jit 1. q : 待 幾 天",")( Sr q ={待,幾,天} )( Tr q ={doax,kuie,jit} 我 明天 要 訂 兩 間 有 淋浴 設備 的 單人房 minafzaix goar bueq dexng lerng kefng u sea sengqw e danjiin paang 2. q : 我 要 訂 兩 間 單人 房",")( Sr q ={我,要,訂,兩,間,單 人房}",")( Tr q ={goar,bueq,dexng,lerng,kefng,danjiin paang} 請 問 你們 今晚 有 一 間 雙人房 嗎 chviar bun lirn ehngf u cit kefng sianglaang paang but 3. q : 今晚 有 [...] 雙人房 嗎",")( Sr q ={今晚,有,雙人房, 嗎} )( Tr q ={ehngf,u,sianglaang,paang,but} 有 包括 早餐 在 內? u zafdngx but 4. q : 包括 ... 在 內 )( Sr q ={包括,在,內} )( Tr q ={"]},{"title":"f","paragraphs":["}    Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 15","However, translation spotting can only draw out the TL answer from the best translation; it can not handle an SL query whose word-tokens are distributed in different translations. Consequently, we propose conducting multiple-translation spotting of a speech input using multiple pairs of translation patterns. Figure 1 shows an example of multiple-translation spotting of a speech input. When a speaker inputs an SL speech query ”今晚會有三間單人房 嗎 ”, the proposed system can obtain a TL speech pattern set that includes five elements, ”ehngf”, ”kvaru”, ”svaf”, ”kefng”, and ”danjiinpaang”, according to the spotted SL speech patterns ”今晚”, “會有”, “間”, “嗎”, ”三”, and ”單人房”. The rest of this article is organized as follows. Section 2 presents the framework of the proposed system. Section 3 presents system data training for Mandarin and Taiwanese. Section 4 describes the proposed translation method for speech-to-speech translation. Section 5 presents experimental results. Finally, Section 6 draws conclusions. "]},{"title":"Figure 1. An example of multiple-translation spotting. 2. Framework of the Proposed System","paragraphs":["The proposed speech-to-speech translation system is divided into two phases – a training phase and a translation phase. In the training phase, the developed translation examples are imported to derive multiple-translation templates and develop speech data. In the following   16 Jhing-Fa Wang et al. step, the developed speech data are applied to construct multiple-translation spotting models and synthesis templates. Figure 2(a) shows a block diagram of the training phase.  (a) (b)"]},{"title":"Figure 2. Framework of the proposed system: (a) a training phase; (b) a translation phase.","paragraphs":["Figure 2(b) shows a block diagram of the translation phase. A one-stage based spotting method is adopted to identify input spoken phrases for each spotting template, and the template candidates are assigned in the following score normalization and ranking process. However, the hypothesized word sequence generally includes noise-like segments. Accordingly, the segments are adjusted by smoothing the hypothesized word sequences. After the hypothesized word sequences of all template candidates have been smoothed, the hypothesized target sequences are generated using the translation template with the maximum number of spotting tokens of speech input. The obtained target speech segments are used to produce target speech by means of the corresponding synthesis template in the final target generation process."]},{"title":"3. Data Training Phase","paragraphs":["As for the task of translating Mandarin and Taiwanese language pairs, although these languages both belong to the family of Chinese languages, their language usages still have various development by language families and their origins, Mandarin belongs to Altaic   Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 17 language family, and Taiwanese belongs to Sinitic language family [Sher et al., 1999]. Therefore, in the following section, we will consider their language usages for three template construction."]},{"title":"3.1 Multiple Translation Template Construction","paragraphs":["While translation templates can be fully constructed, one major issue in translation pattern exploitation, called “divergence,” makes straightforward transfer mapping extraction impractical. Dorr (1993) describes divergence in the following way: “translation divergence arises when the natural translation of one language into another result in a very different form than that of the original.” Therefore, we choose translations with no divergence to practice constructing templates. An example of a simple translation template derived from a practicable translated example is shown below.  Translated Example: SL: “我 朋友 要 訂 房間” « TL: ”goarn pengiuo bueq dexng pangkefng” Intention Translation: Mp1 要 訂 Mp2 « Tp1 bueq dexng Tp2 Variable Translation: If Mp1« Tp1, 我 朋友 « goarn pengiuo If Mp2« Tp2, 房間 « pangkefng ","The translation template is composed of a translated example, an intention translation, and two variable translations. The example shows how a sentence in Mandarin (SL) that contains an intention “要 訂” with two variables, Mp1 (我 朋友) and Mp2 (房間), can be translated into a sentence in Taiwanese (TL) with an intention “bueq dexng” and two variables, Tp1 (goarn pengiuo) and Tp2 (pangkefng). According to the template, the number of variable translations should be expanded to improve the capability for spotting the speech input. From the preceding example, variable translation expansion can be illustrated as follows:  Variable Translation Expansion: If Mp1 « Tp1, 我 « goar 我 朋友« goarn pengiuo If Mp2 « Tp2, 房間 « pangkefng 票 « phiaux alignments             18 Jhing-Fa Wang et al. Therefore, we can obtain corpus-specific multiple translations in a template constructed from three translation patterns, which are “我 朋 友 要 訂 房間«goarn pengiuo bueq dexng pangkefng”, “我«goar”, and “票«phiaux”."]},{"title":"3.2 Spotting Model Construction","paragraphs":["Taiwanese is a typical oral language and still has no uniform system of writing. In the literature, there are two ways to represent Taiwanese words: Chinese characters and alphabetic writing. [Sher et al., 1999]. Chinese characters have huge hieroglyph character sets; therefore, it is difficult to systematize developed examples. Although alphabetic writing would be an appropriate representation form, a universal phonemic transcription system is still not available.","Therefore, for the purpose of practical system construction, a collection of speech data is developed from derived text-form templates not only to obtain spotting models but also to transcribe text data as waveform-based representations. For one of the translating languages, the speech data, including intention speech and related variable speech, are used in chorus to construct spotting reference models for use in multiple-translation spotting. Such spotting reference models are embedded with latent grammars from the constructed templates. When dealing with Mandarin-Taiwanese speech feature models, we build the database by extracting LPCC features from recorded template speeches. Hence, when speech recognition is performed, the LPCC features are extracted from the recorded template speeches, and the LPCC features of speech input are used in combination to compute the degree of dissimilarity. After language pairs of both Taiwanese and Mandarin speech data are developed, the transfer mapping information for a pair of Taiwanese and Mandarin speech segments known to be similar in terms of text-form word alignment is constructed."]},{"title":"3.3 Synthesis Template Construction","paragraphs":["Both Mandarin and Taiwanese are tonal languages, and it is difficult to determine whether a morpheme will take its inherent tone or the derived tone when every word in a sentence is synthesized. [Wang et al., 1999; Sher et al., 1999]. Therefore, we utilize the obtained intention speech and variable speech as synthesis templates that include intention synthesis units and variable synthesis units. These synthesis units can be used to generate a speech output to be processed using a waveform segment concatenation-based synthesis method [Wang et al., 1999]. For each synthesis unit in the obtained speech data, the following features are stored:"]},{"title":"·","paragraphs":["the waveform and its length, · the code of the synthesis unit.   Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 19"]},{"title":"4. Translation Phase 4.1 Multiple-Translation Spotting Method","paragraphs":["To deal with the problem of spotting between a speech input L X1 and a translation pattern set"]},{"title":"{ }","paragraphs":["J j v j v j ts  1",")()( , = in the v-th translation template ( v"]},{"title":"r","paragraphs":["), we use the standard notation l to represent","the frame index of L","X1 , Ll ££1 , j to represent the spotting pair ( )()(",", v j v j ts ) index of v"]},{"title":"r","paragraphs":[","]},{"title":"Jj ££1","paragraphs":[", and k to represent the frame index of j-th spotting pattern )( v","js , 1 j Kk ££ . Then","for each input frame, the accumulated distance"]},{"title":"),,( jkld","paragraphs":["A is computed by "]},{"title":"( )","paragraphs":["),,1(min),,(),,(","2 jmldjkldjkld A kmkA -+= ££-"]},{"title":".","paragraphs":["(1)","For jKk ££ 2 , Jj ££1 , where ),,( jkld is the local distance between the l-th frame of T"]},{"title":"X","paragraphs":["1 and the k-th frame of source pattern )( v j"]},{"title":"s","paragraphs":[". The recursion of (1) is carried out of for all internal frames (i.e.,"]},{"title":"2‡k","paragraphs":[") of each source pattern. At the speech pattern boundary, i.e., 1=k , the recursion can be calculated as follows:"]},{"title":"( )[ ]","paragraphs":["),1,1(,),,1(minmin),1,(),1,(","1 jldmKldjldjld AmA JmA --+= ££"]},{"title":".","paragraphs":["(2) The final solution for the best path is"]},{"title":"( )[ ]","paragraphs":["jKLdd jA Jj v","G ,,min 1 )( ££= (3) The details of the multiple-translation spotting algorithm are given below: /* Parameter descriptions"]},{"title":"{ }","paragraphs":["J j v j  1",")( =t : the spotting results of"]},{"title":"{ }","paragraphs":["J j (v) js  1 = , where  = otherwise. ,0 .by spotted is pattern speech SL if ,1 1)( )( Lv jv j","Xs t ;"]},{"title":"{ }","paragraphs":["Jjtw v j v","jv ££=‹ 1 ,1| )()( t : the hypothesized TL synthesis patterns; */ /* Initialization */","1‹‹‹ jkl ; 0)( ‹v","jt , Jj ££1 ;"]},{"title":"{ }","paragraphs":["f‹vw ; /* v-th template spotting */","while ( Ll £ ) for each spotting pattern )( v","js while ( jKk £ ) if (k = 1)",")],,1()],,,1([minmin[),,(),,(","1 jkldjKldjkldjkld AjA JjA --+‹ ££   20 Jhing-Fa Wang et al.",")],,1()],,,1([minmin[arg),,(","1 jkldmKldjklp AmA Jm --‹ ££  else if (k > 1)",")),,1((min),,(),,(","2 jmldjkldjkld A kmkA -+‹ ££- ",")),,1((minarg),,(","2 jmldjklp A kmk -‹ ££-  else if (k = Kj) 1‹k ; else k++; end if end while end for l++; end while",")],,([min","1)(","jKLdd jA","Jj v","G ££‹ ;",")],,([minargˆ 1 jKLdj jA","Jj ££‹ ; /* Trace back and TL synthesis pattern extraction */"]},{"title":"{ }","paragraphs":[")],̂,[(back trace ̂","1 )( jKL τ j J j v","j ‹ = ; /*, )( v jt is assigned as 1 or 0*/","for each )( v jt , j=1,2,...,J","If ( 1)(","=v jt )"]},{"title":"{ }","paragraphs":[")( v jvv tww ‹̈ ; end if end for return v"]},{"title":"w","paragraphs":["and"]},{"title":"{ }","paragraphs":["J j v j τ  1 )( = ;"]},{"title":"4.2 Normalizing the Score and Ranking","paragraphs":["The length of the matching sequence can severely impact the cumulative dissimilarity measurement, so a length-conditioning weight is applied to overcome this defect. Scoring methods that involve the length measurement"]},{"title":"( )","paragraphs":[")(","1 , vL sX D ("]},{"title":"U","paragraphs":["J j (v) j v"]},{"title":"ss","paragraphs":["1 )( ="]},{"title":"=","paragraphs":[") [J.N.K. Liu and L. Zhou, 1998] can be defined in a number of similar ways:","),(minor )(,max(),( )( 1 )( 1 )( 1 vLvLvL sXsXsX =D"]},{"title":",","paragraphs":["(4) )( 1 )(","1 ),( vLvL","sXsX *=D"]},{"title":",","paragraphs":["(5)   Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 21","3/),(),(),( 11",")( 1 == +=D J j j J j","jvL","KLFKLNsX ,     ","(6)","where L X1 is the number of frames in speech input x; )( v","s is the total number of search frames in"]},{"title":"{ }","paragraphs":["J j v","js","1)( = ; ),( 1  = J j j KLN is the number of frames compared; and ),( 1  = J j j KLF is the number of frames that fail to be matched. To improve the flexibility and reliability of the dissimilarity measurement, an exponential"]},{"title":"( )","paragraphs":[")(","1 , vL sX D is exponentially defined as follows:",")( ,","),( )( 1","v","sXw","vL","sX ¶=D"]},{"title":",","paragraphs":["(7)","where )( , v","sXw ¶ is a weighting factor and","1 )()( 1, )( -  ł Ł","-= vvL","sX ssXw v . The weighting","factor of Eq. (7) has two features: one is length correlation normalization, and the other is","exponential score normalization. For length correlation normalization, the tendency to choose","a template )( v","s with the same length difference of L","X1 but smaller length multiplication","is eliminated. With exponential score normalization, when the difference between the speech","input and each template is larger, a higher dissimilarity score is obtained and spotting","discrimination improves. Finally, the normalized measured dissimilarity is determined as","follows:",")(",", )()( v","sXw v G v G dd ¶="]},{"title":".","paragraphs":["(8) The experimental analysis shown in Fig. 3 indicates that the interval"]},{"title":"¶","paragraphs":["that yields the most accurate dissimilarity measurement is"]},{"title":"[ ]","paragraphs":["dd +- 2.1,2.1 . Therefore, the value of"]},{"title":"¶","paragraphs":["chosen here is 1.2. The weighting factor is determined using the feature models of the first speaker for inside training. The feature models are different from the test data; thus,"]},{"title":"¶","paragraphs":["is a test-independent weighting factor. After all the templates are ranked, the retrieval accuracy is estimated using the criterion that the intention of the source speech is located in the set of the best N retrieved translation templates.         0 0.2 0.4 0.6 0.8 1","11.11.21.31.41.51.61.71.81.92 weight S p o t t i n g  A c c u r a c y  R a t e Top1 (1050 templates) Top5 (1050 templates) "]},{"title":"Figure 3. Time-conditioned weight convergence for dissimilarity measurement  ","paragraphs":["22 Jhing-Fa Wang et al."]},{"title":"4.3 Smoothing the Hypothesized Template","paragraphs":["The main weakness with the one-stage algorithm for multiple-translation spotting is that it provides no mechanism for controlling the resulting sequence length, that is, for determining the optimal token sequence of arbitrary length. The algorithm finds a single best path whose sequence length is arbitrary. Therefore, the hypothesized token sequence generally includes noise-like components. The components should be in the form of duplications, and their durations should be below a threshold. Based on this assumption, hypothesized token outputs with segmented durations below the threshold are considered for further smoothing. With Mandarin and Taiwanese, the duration of a syllable is 0.3 sec on average [Sher et al., 1999], and this value is set as the relevant threshold to sift out noise-like components whose durations are less than 0.3 sec. These are the preliminary speaker-dependent results of our experiments. This system is able to adjust the threshold when a speaker speaks at different rates. Additionally, this system is corpus-specific, and out of vocabulary (OOV) words are rejected based on their high"]},{"title":"dissimilarity","paragraphs":["scores. After the token sequences of all the TopN templates have been smoothed, the hypothesized target sequences is generated using the translation template with the maximum number of spotting tokens of speech input."]},{"title":"4.4 Target Speech Generation","paragraphs":["Once the hypothesized target sequences have been determined, the target speech generation process is straightforward, similar to the waveform segment concatenation-based synthesis method. In this method, waveform segments are extracted beforehand from the recorded intention synthesis units and variable synthesis units of the synthesis template, and they are rearranged with adequate overlapping portions to generate speech with the desired energy and duration. The merits of the method are the small computational cost in the synthesis process and the high level of intelligibility of the synthesized speech. The generation process includes complete matching, waveform replacement, and waveform deletion; thus, it is similar to the example-base translation method [J. Liu and L. Zhou, 1998]."]},{"title":"5. Experimental Results 5.1 The Task and the Corpus","paragraphs":["We built a collection of Mandarin sentences and their Taiwanese translations that usually appear in phrasebooks for foreign tourists. Because the translations were made sentence by sentence, the corpus was sentence-aligned at birth. Table 2 shows the basic characteristics of the collected corpus.     Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 23"]},{"title":"Table 2. Basic characteristics of the collected translated examples.    ","paragraphs":["In this work, the content of the high divergent example sentence pairs needed to be collated or sieved out to improve the accuracy and effectiveness of alignment exploration between word sequences and the derivation of multiple translation templates. Table 3 shows the basic characteristics of the derived multiple translation templates. The derived templates were used to develop the speech corpus, which was used to construct spotting models and synthesis templates. "]},{"title":"Table 3. Basic characteristics of the derived translation templates.","paragraphs":["Number of templates 1,050 Number of intentions 1,050 Total number of translation patterns 5,542 Number of translation entries 1,260 Average number of translations per template 5.28  In order to evaluate the system performance, a collection of 1,050 utterances were speaker-dependent trained, and 30 additional utterances of each language were collected by using one male speaker (Sp1) for inside testing and by using two bilingual male speakers (Sp2 and Sp3) for outside testing. All the utterances were sampled at an 8 kHz sampling rate with 16-bit precision on a Pentium","IV 1.8GHz, 1GB RAM, Windows","XP PC."]},{"title":"5.2 Translation Evaluations","paragraphs":["For the speech translation system, we found that the recognition performance of 39-dimension MFCCs and 10-dimension LPCCs was close. Therefore, we adopted 10-dimension LPCCs due to their advantages of faster operation and simpler hardware design. Speech feature analysis of recognition was performed using 10 linear prediction coefficient cepstrums (LPCCs) on a 32ms frame that overlapped every 8ms.","For estimating the computational load of the proposed MTS algorithm, a complexity analysis is shown in Table 4. Parts of the overall computation of the local frame distance Mandarin Taiwanese Number of sentences 2,084 2,084 Total number of words 14,219 14,317 Number of word entries 6,278 6,291 Average number of words per sentence 6.82 6.87   24 Jhing-Fa Wang et al. depend on the feature dimension, so we used O(LPCC_add) and O(LPCC_mul) to represent the complexity of additions and multiplications, respectively. We applied Itakura type in each internal dynamic programming path selection employed 3 additions to decide the last node and 1 addition to accumulate the node distance, and 3 multiplications for slope weighting. In Table 4, the second row, Distance computation, presents the computational complexity of computing the local distance, and the third row, Path selection, presents the computational complexity of selecting the best path, that is, the computational overload of MTS for each template. "]},{"title":"Table 4. Complexity analysis of the MTS algorithm.","paragraphs":["Computational Load  Addition Multiplication Distance computation"]},{"title":"()","paragraphs":["addLPCCOKL J j v j _ 1 )(   = "]},{"title":"()","paragraphs":["mulLPCCOKL J j v j _ 1 )(   =  Path selection = J j v j KL","1 )( 5 = J j v j KL","1 )( 3","Total for each template"]},{"title":"()()","paragraphs":["addLPCCOKL J j v j _5 1",")( +  = "]},{"title":"()()","paragraphs":["mulLPCCOKL J j v j _3 1",")( +  =  Total for all templates"]},{"title":"()()","paragraphs":["ł   Ł  + = v J j v j addLPCCOKL _5 1 )("]},{"title":"()()","paragraphs":["ł   Ł  + = v J j v j mulLPCCOKL _3 1 )( ","When input speech is being spotted, a major sub-problem in speech processing is determining the presence or absence of a voice component in a given signal, especially the beginnings and endings of voice segments. Therefore, the energy-based approach, which is a classic one and works well under high SNR conditions, was applied to eliminate unvoiced components in this research. The measurement results were divided into four parts: the dissimilarity measurement of linear prediction coefficient cepstrum (LPCC)-based (baseline), the baseline with unvoiced elimination (unVE), the baseline with the time-conditioned weight (TcW), and the combination of unVE and TcW considerations with the baseline. A given translation template is called a match when it contained the same intention as the speech input. The reason for adopting this strategy was that variables could be confirmed again while a dialogue was being processed, while wrong intentions could cause endless iterations of dialogue. The experimental results for proper template spotting are shown in Table 5 and Table 6.","Based on the constructed translation templates, when the template or vocabulary size increases, more templates would possibly lead to more feature models and more similarities in   Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 25 speech recognition, thus causing false recognition results and lower spotting accuracy. Additionally, multiple speaker dependent results were obtained using three speakers. The first speaker's feature models (spotting models) were used to perform tests on the other two speakers, and the results are shown in Table 7. The experimental results show that although the feature models were trained by Sp1, the spotting accuracy of Sp2 and Sp3 was only reduced by 10 to 15 percent. A bilingual evaluator was used to classify the target generation results into three categories [Yamabana et al., 2003]: Good, Understandable, and Bad. A Good generation needed to have no syntactic errors, and its meaning had to be correctly understood. Understandable generations could have some variable translation errors, but the main intention of the source speech had to be conveyed without misunderstanding. Otherwise, the translations were classified as Bad. With this subjective measure, the percentage of Good or Understandable generations for the Top 5 was 80% for Mandarin to Taiwanese (M/T) translation and 76% for Taiwanese to Mandarin (T/M) translation. The percentage of Good generations for the Top 1 was 63% for M/T translation, and it was 60% for T/M translation. We examined the translation templates in a specific domain and found that 100% translation accuracy could be achieved. In other words, translation errors occurred only as a result of speech recognition errors, such as word recognition errors and segmentation errors. The results show that T/M had poorer performance than M/T. This is perhaps because spoken Taiwanese has more tones than Mandarin; thus, it is harder for T/M translation spotting to find an appropriate translation template. "]},{"title":"Table 5. Average accuracy of baseline spotting and the improvement in Mandarin-to-Taiwanese Translation.         ","paragraphs":["Template Size","Top 1Top 5Top 1Top 5Top 1Top 5Top 1Top 5 1500.50.630.60.830.630.830.761 2500.50.630.60.830.630.830.761 3500.460.60.560.80.60.80.730.96 4500.460.60.560.80.60.80.730.96 5500.430.60.560.760.60.760.70.93 6500.430.560.530.730.560.760.70.93 7500.430.50.530.730.560.730.70.9 8500.40.50.50.70.530.730.660.86 9500.40.460.50.70.50.660.660.83 10500.40.430.460.660.460.660.630.8 Baseline + unVE +TcW Baseline","Baseline + unVE Baseline + TcW 1234    26 Jhing-Fa Wang et al. "]},{"title":"Table 6. Average accuracy of baseline spotting and the improvement in Taiwanese-to-Mandarin Translation. ","paragraphs":["Template Size","Top 1Top 5Top 1Top 5Top 1Top 5Top 1Top 5 1500.460.60.60.830.60.760.761 2500.460.60.60.830.60.70.730.96 3500.460.560.560.80.560.70.70.96 4500.430.560.560.760.560.660.70.93 5500.430.530.530.760.560.660.660.86 6500.430.530.530.730.530.60.660.86 7500.40.50.50.70.50.60.630.83 8500.40.50.50.70.50.560.60.8 9500.40.460.460.660.460.560.60.76 10500.360.430.430.660.460.560.60.76","1234 Baseline + unVE +TcW Baseline","Baseline + unVE","Baseline + TcW  "]},{"title":"Table 7. Average accuracy of spotting in multiple speaker testing. ","paragraphs":["Template Size (Sp1 model) (Top5) 150 250 350 450 550 650 750 850 950 1050 M2T 1 1 0.96 0.96 0.93 0.93 0.9 0.86 0.83 0.8 Sp1 T2M 1 0.96 0.96 0.93 0.86 0.86 0.83 0.8 0.76 0.76 M2T 0.9 0.86 0.83 0.8 0.76 0.73 0.73 0.7 0.66 0.66 Sp2 T2M 0.86 0.83 0.8 0.76 0.76 0.73 0.7 0.7 0.7 0.66 M2T 0.86 0.83 0.8 0.76 0.73 0.73 0.7 0.66 0.66 0.63 Baseline +unVE +TcW Sp3 T2M 0.83 0.8 0.76 0.76 0.73 0.7 0.7 0.66 0.63 0.63 "]},{"title":"6. Conclusion","paragraphs":["In this work, we have proposed an approach that retrieves identified target speech segments by carrying out multiple-translation spotting on a source input. According to the retrieved speech segments, the target speech can be further generated by using the waveform segment concatenation-based synthesis method. Experiments using Mandarin and Taiwanese were performed on Pentium","PCs. The experimental results reveal that our system can achieve an average translation understanding rate of about 78%.   Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 27 Acknowledgements This article is a partial result of Project NSC 90-2215-E-006-009, sponsored by the National Science Council, Taiwan, R.O.C."]},{"title":"References","paragraphs":["Lavie, A., A. Waibel, L. Levin, M. Finke, D. Gates, M. Gavalda, T. Zeppenfeld and P. Zahn, “JANUS III: Speech-to-Speech Translation in Multiple Languages,” Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, 22(I) 1997, pp. 99–102.","Wahlster, W., “Verbmobil: Foundations of Speech-to-Speech Translation,” New York: Springer-Verlag Press, 2000.","Casacuberta, F., D. Llorens, C. Martinez, S. Molau, F. Nevado, H. Ney, M. Pastor, D. Pico, A. Sanchis, E. Vidal and J. M. Vilar, ”Speech-to-Speech Translation Based on Finite-State Transducers,” Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, 26(I) 2001, pp. 613–616.","Sugaya, F., T. Takezawa, A. Yokoo and S. Yamamoto, “End-to-End Evaluation in ATR-MATRIX: Speech Translation System between English and Japanese,” Proceedings of European Conference on Speech Communication and Technology, 6(I) 1999, pp. 2431–2434.","Macklovitch, E., M. Simard and P. Langlais, “TransSearch: A Free Translation Memory on the World Wide Web,” Proceedings of International Conference on Language Resources & Evaluation, 3(I) 2000, pp. 1201–1208.","Michel, S., “Translation Spotting for Translation Memories,” Proceedings of HLT-NAACL Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, 2003, pp. 65–72.","Véronis, J. and P. Langlais, “Evaluation of Parallel Text Alignment Systems – The ARCADE Project,” in J. Véronis (ed.): Parallel Text Processing. Dordrecht: Kluwer Academic, 2000, pp. 369–388.","Dorr, B. J., “Machine Translation: A View from the Lexicon,” The MIT press, 1993.","Wang, J. F., B. Z. Houg and S. C. Lin, “A Study for Mandarin Text to Taiwanese Speech System,” Proceedings of the 12th Research on Computational Linguistics Conference , 1999, pp. 37–53.","Sher, Y. J., K. C. Chung and C. H. Wu, “Establish Taiwanese 7-Tones Syllable–based Synthesis Units Database for the Prototype Development of Text-to-Speech System, ” Proceedings of the 12th Research on Computational Linguistics Conference, 1999, pp. 15–35.","Liu, J. and L. Zhou, “A Hybrid Model for Chinese-English Machine Translation,” Proceedings of IEEE International Conference on Systems, Man, and Cybernetics , 2(I) 1998, pp.1201–1206.   28 Jhing-Fa Wang et al.","Yamabana, K., K. Hanazawa, R. Isotani, S. Osada, A. Okumura and T. Watanabe, “A Speech Translation System with Mobile Wireless Clients,” Proceedings of the Student Research Workshop at the 41st Annual Meeting of the Association for Computational Linguistics, 41(II) 2003, pp. 119–122.      "]}]}