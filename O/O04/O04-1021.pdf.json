{"sections":[{"title":"應用語料庫和語意相依法則於中文語音文件之摘要 Spoken Document Summarization Using Topic-Related Corpus and Semantic Dependency Grammar 黃建霖 謝嘉欣 吳宗憲 國立成功大學資訊工程系","paragraphs":["Email: {chicco, ngsnail, chwu}@csie.ncku.edu.tw","摘要","自動語音文件摘要技術,可應用於資訊的檢索、語意壓縮及資料記錄等方面。目前自動語音摘要存在 幾個問題,首先是語音辨識準確率的提升,以及如何對語音內容萃取重要資訊、生成在句法及語意上合理 的摘要結果。本論文提出ㄧ應用主題相關語料庫和語意相依法則於中文語音文件之摘要。首先,語音文件 透過大詞彙連續語音辨識的方法,將語音辨識成文字,並獲得摘要單元斷點、音節以及詞等資訊。語音摘 要部份,就語音本質從五個分數去分析,分別為:語音辨識信賴分數、詞重要性分數、語言分數、句法結 構分數及語意相依法則分數,而後利用動態規劃搜尋演算法(dynamic programming algorithm, DP)獲得初步 摘要結果。最後,為了使摘要語音串接輸出能具平滑特性,我們將摘要語音的有效語音段取出,計算語音 頻譜特徵,考慮串聯單元彼此間的流暢度,挑選語音文件中重複的單元以串接生成摘要語音。由實驗結果 得知,本研究所提出之自動語音摘要架構與人工摘要結果相比,能有效地萃取重要資訊,串接合成流暢的 摘要語音。"]},{"title":"1. 簡介","paragraphs":["近年來電腦、電信網路、通訊與多媒體等資訊科技的成熟發展,政府為提升行政效率,投入大量人 力物力從事資訊化,其中電子公文就是一個很好的例子。現今資訊科技進步,改變了人類溝通方式,也改 變了知識的管理和傳承,以及資訊的散播和儲存,對人類社會產生革命性的影響。目前國立故宮博物院、 國立歷史博物館等負責保存國家文物的機構,也積極地與產學界合作發展數位典藏計畫,將傳統文化創作 的保存工作,利用新科技以資訊化的方式長久保存。此外不乏一般的大型企業、新聞傳播事業等,本身都 保有大量累積的資訊。隨著科技的進步,資料型態可能已不再侷限於文字檔案,也包含各式的多媒體影音 資料,如:圖片、聲音及影像等。因此許多學者專家研究如何編碼壓縮,研究體積更小、容量更大的儲存 媒體,除此之外,文件檢索、摘要一直以來都是研究的重要主題。知識傳授,教育學習以及理念的傳播, 透過語音表達是最自然而且直接的方法。自動語音摘要技術對語音資料做語意上的壓縮,目的在於依使用 者需求,在大量的資料裡將無用多餘的資訊去除,保留具代表文章意涵的資訊並且自動建構出合乎文法及 語意的內容。","自動語音文件摘要研究的主題在於語音辨識、摘要模型以及語音串接。語音辨識雖然仍存在有許多瓶 頸,但由於過去學者的努力,已累積有相當的研究成果。目前中文語音辨識研究多以統計式模型的方法為 主,應用隱藏式馬可夫模型(hidden Markov model, HMM),來建立以音節或次音節為基礎的聲學模型,並 配合多連語言模型的應用,可將大量連續語音做詞彙的辨識。","摘要部份可分為文字摘要及語音摘要,文字摘要研究主要在於分析文章結構、字詞重要性,一般常見 的方法如:分析段落位置、句子長度、以詞頻和反轉文件頻率表示(term frequency * inverse document frequency, tf.idf)計算詞的重要性等[1][2]。相對於文字,語音摘要需要透過自動語音辨識,透過文字分析語 意層面的意涵,因此辨識的好壞會對摘要結果產生影響,且因為語音特性像是音高、周期或能量等[3],可 提供音韻上的分析來決定重要語句的選擇。過去日本東京工業大學的研究,就對語音摘要提出了很好的基 本概念,透過語音摘要參數的擷取,配合動態規劃搜尋演算法,找尋最佳的詞句組合[4][5]。但方法上缺乏 對語意成分的分析理解,且對於關鍵詞的選擇上並不十分合理強健,因此,我們要提出應用語意相依法則 和主題相關語料庫的方法於中文語音文件之摘要,同時分析文章中重要資訊的多寡來決定摘要比例,並且 利用語音頻譜特性考慮串接的流暢性[6]。"]},{"title":"2. 語音自動摘要","paragraphs":["本論文提出的自動語音摘要方法,首先,在語音辨識方面,利用最小錯誤鑑別訓練的方法來鑑別容易 渾淆的模型,提高語音辨識的正確率[7]。摘要的部份,從五個層面考慮摘要的生成:第一、考慮文章中重 要語意的保留,我們使用一組新聞語料知識庫,透過潛藏語意分析找出具代表性的重要文字。第二、語音  辨識正確率會影響文章語意的判斷,為了避免誤判情形的發生,經由計算辨識信賴分數,取辨識可信度較 高的詞作為摘要。第三、語音摘要詞與詞之間的串連間關係,可利用語言模型建立。第四、分析文章語意 相依的關係,建構合理的語意修飾關聯。第五、配合機率式文法規則,使句子具有文法規則,易於閱讀理 解。最後以動態規劃搜尋方法,產生最佳的摘要詞組。此外,為了使串接語音平滑輸出,在串接摘要單元 時,必須考慮串接流暢和平滑的程度。因此,在摘要單元串接的選擇上,我們考慮頻譜特性:分別使用頻 譜中心(spectral centroid)、頻譜滑動(spectral rolloff)、頻譜變遷(spectral flux)、時域上越零率(time domain zero crossing, ZCR)和梅爾倒頻譜參數(Mel-frequency ceptral coefficient, MFCC) ,找出相鄰差異最小的串接單元 以生成平滑之語音輸出。","然而,如何才能從語音文件中萃取出重要的詞句,建構出能夠代表文章意函的內容。本論文分別從語 音聲學(acoustics)、語言學(linguitic),句法(syntax)和語意(semantics)等方向去解決自動語音文件摘要可能面 對的問題,一篇語音文件透過特徵參數的計算,可被分析成五個主要的特徵分數,包含有:(1) 語音辨識 信賴 (confidence measure, ) 分數;(2)字詞相對於文章所代表的重要性 (word significance, ) 分 數;(3)語言學結構相鄰 (linguistic trigram,"]},{"title":"(","paragraphs":["m"]},{"title":"Cw)","paragraphs":[")","()mRw","21(| ,mm mLw w w− − ) 分數;(4)語意相依法則 (semantic dependency","grammars, ) 分數;以及(5)機率式文法規則 (probabilistic context-free grammars,1(,SDG m mBww− ) ()P S ) 分數。","因為分數值域大小並不相同,所以我們分別計算分數的最大值 score"]},{"title":"Max","paragraphs":["和最小值 score"]},{"title":"Min","paragraphs":[",依其不同值域對 各分數 scoreX 做正規化"]},{"title":"()/( )","paragraphs":["score score score score"]},{"title":"XMin MaxMin−−","paragraphs":["將每一個分數值調整為從 0 到 1 之間。語音文件 經過大詞彙連續語音辨識,產生一篇詞長為"]},{"title":"M","paragraphs":["的轉譯文件 123 1"]},{"title":"{ , , ,..., , }","paragraphs":["MM"]},{"title":"X ww w w w","paragraphs":["−"]},{"title":"=","paragraphs":[",辨識資訊包含有","次音節的語音斷點資訊。根據摘要比例,系統最後可以獲得長度為 NMPercentage= × 的摘要結果 。 123 1"]},{"title":"{ , , ,..., , }","paragraphs":["NN"]},{"title":"Ywww ww","paragraphs":["−"]},{"title":"=","paragraphs":["摘要流程如(圖 1)所示,分成下列四個步驟:首先就辨識結果將 stop word 去除,例如:的、及、了等, 不具語義表示的詞。再者,因為不同的語音文件可能包含的重要資訊量並不一致,所以摘要壓縮的比例會 對摘要結果有很大的影響。因此除了可以依據使用者需求設定摘要比例外,也可以藉由判斷字詞相對於文 章所代表的重要性 ,自動決定摘要比例。第三步驟,則是將語音辨識信賴分數、重要詞語分數、語 言學分數和語意相依分數等四種分數作結合,以動態規劃搜尋的方法,尋找可能的串接詞組。 ()mRw 21 1 1() { ( ) ( ) ( | , ) ( , )}M CmRmLmmm BSDGmm mSY Cw Rw Lw w w B w wλλλ λ−− − ==++ +"]},{"title":"∑","paragraphs":["(式 1) 其中,"]},{"title":",,","paragraphs":["CRL"]},{"title":"λ λλ","paragraphs":["和 B"]},{"title":"λ","paragraphs":["是代表各個特徵參數的權重(weight),用以結合這四個分數並且平衡各參數的重要性。  圖 1. 自動語音文件摘要程序 "]},{"title":"2.1 語音辨識信賴分數","paragraphs":["語音摘要需要透過辨識器得到語言上的資訊,但語音辨識可能會產生聲學上和語言學上的辨識錯誤, 擾亂最後摘要結果的意義。因此,我們將語音辨識的信賴分數 引入,目的在於選擇辨識較正確的結 果,作為判斷選擇摘要單元的分數之ㄧ。統計式語音辨識是基於貝氏法則,信賴分數是估算語音辨識中, 给定一串觀測語音序列 ()mCw 1"]},{"title":",...,","paragraphs":["tt"]},{"title":"x x= x","paragraphs":["m對於一字串 1"]},{"title":",...,","paragraphs":["m"]},{"title":"ww w=","paragraphs":[",計算其事後機率(posterior probability) 。辨識的階段中,我們期望能夠得到最大的事後機率值,也就是能夠有較小的誤差, 所以可得下列式子:"]},{"title":"(|","paragraphs":["mt"]},{"title":"pw x) ()max( |)max(| )()()max(| )(","paragraphs":["m mttmmttm"]},{"title":"Cw pw x px w pw px px w pw==⋅=⋅)","paragraphs":["m"]},{"title":") ","paragraphs":["(式 2) 其中,"]},{"title":"(","paragraphs":["m"]},{"title":"p w","paragraphs":["為語言模型的機率。 為聲學模型的機率。 為觀測到聲學特徵的機率。"]},{"title":"(| )","paragraphs":["tm"]},{"title":"px w ()","paragraphs":["t"]},{"title":"px 2.2 重要詞分數","paragraphs":["要對辨識結果做語音文件摘要的處理時,首先需要將語音文件內屬於重要的詞語保留下來,而把不具 備有表達文章意義的詞與字抽離。我們引用一組標題本文互相對照的新聞語料庫,來輔助判斷辨識結果的 詞句是否具有代表性。實驗從公共電視新聞收集 2001 到 2002 年的新聞,整理兩千零六則的新聞報導語料。 為了檢索出與摘要文章內容相似的新聞報導語料,我們參照資訊檢索的技術(Information Retrieval, IR) [1],","首先將平行語料的所有文章內容,分別轉換成以詞 和音節w d"]},{"title":"v","paragraphs":["s d"]},{"title":"v","paragraphs":["為單元的兩個向量,對於所要摘要的語音 文件也同樣地做轉換為兩個向量,可表示成 和12"]},{"title":"( , ,..., )","paragraphs":["Pww ww ddd d"]},{"title":"vtt t=","paragraphs":["12"]},{"title":"( , ,..., )","paragraphs":["Qssss ddd d"]},{"title":"vtt t=","paragraphs":["。其中,"]},{"title":"Q","paragraphs":["表示以音","節單元為基礎的向量 s dv 維度,依據四百零二個中文音節,並考慮詞長為二的所有配對組合,產生維度為","的向量。而 則表示以詞為基礎的向量 維度,根據辭典內所定義的詞,不 考慮虛詞(stop word)的部分,因為虛詞不會影響文章內容意義的檢索,去除用以降低計算的維度,得到結 果 (402 402 402) 162006Q =+×="]},{"title":"P","paragraphs":["w d"]},{"title":"v","paragraphs":["28000P = 。兩向量內的之數值以詞頻和反轉文件頻率表示(term frequency * inverse document frequency, tf.idf)[8]。同時,必須考慮語音辨識 ()jCw 可能造成的影響,將辨識不好的結果,減低分數。因此每一個索 引值的計算方法如下:"]},{"title":"()ln( 1)ln(/( 1)","paragraphs":["j jj w djw w"]},{"title":"tCw f Ndf=⋅+⋅ +)","paragraphs":["(式 3) 結合兩向量來做文件查詢,利用向量內積的計算,查詢平行語料內所有文章的關聯 ,"]},{"title":"(, )Rqd","paragraphs":["22(, ) cos( , ) (1 )cos( , ) ( ) /( ) (1 )( ) /( )","ww ww ss ss Rqd R qd","ww wT ww ww ss sT ss ss Rq d q d Rq d q d Rqd vS vS vS vS vS v vS vS vS v vS vS αα αα =+− =⋅ ⋅ +− ⋅ (式 4) 並且應用參數"]},{"title":"0.2","paragraphs":["R"]},{"title":"α =","paragraphs":["來平衡字與音節兩個向量的權重。依據此關聯分數 ,找出一篇文件描述的新 聞事件最接近的文章 ,之後,以潛藏式語意分析索引使用向量空間的方法[8],搜尋辨 識句子的詞與平行語料標題內的詞,兩者之間存在的關係。"]},{"title":"(, )Rqd","paragraphs":["*"]},{"title":"arg max ( , )","paragraphs":["d"]},{"title":"dR= qd","paragraphs":["方法說明如(圖 2)所示,首先根據平行語料和辭典,建立一個文章及詞的二維矩陣 w","tdA× ,維度為 。經由詞對應於文章以及文章對應詞的關係 ()"]},{"title":"2006 5104×","paragraphs":["()terms documents documents terms× ⋅×,最後可 以推導出詞對詞的關聯 T"]},{"title":"AAtermsterms=×","paragraphs":["。配合奇異值分解方法來達到維度的降低,將共同發生的事件 投影到相同的維度上。透過奇異值分解 ,將矩陣分解成三個矩陣 , 和 ( , 其中 。 ()T","td tn nn dnAUSV××××= tkU × kkS × )T","dkV × min( , )nt= d  圖 2. 奇異值分解  將取對角矩陣累計變異量之百分之九十作為維度降低的依據"]},{"title":"kn<","paragraphs":["。矩陣中每一個成分的值,用"]},{"title":"tf idf×","paragraphs":["代 表。詞對詞的矩陣透過降維度的資訊,來計算 2TT"]},{"title":"AAUSV=","paragraphs":["。透過新的詞對詞矩陣便可以得知兩個詞的相 似性 (, )LSI i jP ww ,其分數計算方法如下: 2"]},{"title":"(, )cos( , )","paragraphs":["ww ww ww wT ww ww LSI i j i j i j i j"]},{"title":"P ww US US US U US US== i","paragraphs":["(式 5) 最後歸納上述的步驟,透過下列程序的計算方法,我們可以從平行語料中萃取重要的資訊"]},{"title":"()","paragraphs":["i"]},{"title":"R w","paragraphs":["。其中 *t jw 代表平行語料標題內的詞,而 是輸入文章經語音識辨後的詞,因此可計算出 對於摘要文件的重要性: i"]},{"title":"w","paragraphs":["i"]},{"title":"w","paragraphs":["*"]},{"title":"( ) max{ ( , ) ( 1) ln( /( 1))}","paragraphs":["iit","iLSIijw w j"]},{"title":"Rw P w w f N df=⋅+⋅+","paragraphs":["(式 6)"]},{"title":"2.3 語言學結構相鄰分數","paragraphs":["我們利用三連語言模型 312 123 12"]},{"title":"(|,) (,,)/(,)Lw w w Fw w w Fw w=","paragraphs":[",建立詞與詞之間相接的情況,使 摘要最後結果更加符合語言學結構[9]。其中 ()F i 表示 frequency count。但為了避免許多詞統計不到 trigram 情況發生,引用 Jelinek et al.所提出的平滑方法(N gram smoothing)[10],內插 trigram, bigram 和 unigram 等 相關機率值。表示方法如下: 3 12 1 123 12 2 12 1 3 1"]},{"title":"( | , ) (, , ) (, ) (, ) () () ()","paragraphs":["i"]},{"title":"Lw ww p Fww w Fww p Fww Fw p Fw Fw=⋅ +⋅ +⋅ ∑","paragraphs":["(式 7) 其中, 表示正數的權重且合為一。 123"]},{"title":"1pp p++= 2.4 語意相依法則分數","paragraphs":["前面所言,利用重要詞的分數,找到一堆對於文章具有代表性的詞組,並且配合語言學結構相鄰的分 數,挑選彼此具有高度相鄰關聯性的詞組。但是這樣的資訊,對於生成一則合理且完整的摘要語句,並不 足夠。基於語言學的考量,句子應具備有語法(grammar)和句法上的關係,因此我們對中文語法結構做分析, 利用統計機率方法,計算機率式文法規則。語意學研究是字意和句意的描述,藉由語意特徵的探討,可以 幫助釐清彼此本質上的意涵。舉例而言,”這顆蘋果(NP) 吃了(V) 那個男人(NP)”的句子可能會令人難以置 信。由此可知在語意上,這個句子並不合理,但這並不是因為句法結構所造成的問題。因此,我們引入語 意相依法則,從句法和語意相依的概念來解決此問題。","語意相依法則(semantic dependency grammars, SDG),是透過剖析器(parser)將輸入的詞句,剖析出樹 狀的詞性架構,並標記出中心詞(head)所在的位置。以中心詞為基準,考慮其它詞與中心詞的關係。剖析 器是將詞句透過斷詞,找到相對應的詞性序列,並且利用動態規劃搜尋的方式,配合機率式文法規則模型, 建立對應的語法分析樹和其機率。參考 Collin 在 1996 年提出的相依模型[8],輸入一句子"]},{"title":"s","paragraphs":[",可剖析成文 法樹結構"]},{"title":"t","paragraphs":[",可表示為機率 。並可剖析成 B 個詞(terms of parsing tree)並存在有詞與詞相依的關係 D (dependency relation)。表示如下:"]},{"title":"(| )Pt s (|)(,|)(|)(|,Pt s PBD s PB s PD sB==×)","paragraphs":["(式 8) 假定每一個相依關係都是獨立的,且剖析後每一個詞 ,都相依於某一個中心詞 ,其相依的關聯可以 界定為 。因此,相依關聯可以重新定義成一個集合 ,表示如下: m"]},{"title":"w","paragraphs":["mwh","mw mwh"]},{"title":"R","paragraphs":[","]},{"title":"{( , , )}","paragraphs":["iiw iiw wh"]},{"title":"dwh R","paragraphs":[", 1"]},{"title":"(|,) (( , , ))","paragraphs":["jjw j n jw wh j"]},{"title":"PD sB Pdw h R","paragraphs":["="]},{"title":"= ∏","paragraphs":["(式 9) 在計算兩個詞 和i"]},{"title":"w","paragraphs":["j"]},{"title":"w","paragraphs":[",存在相依關係 的機率"]},{"title":"R (| , )","paragraphs":["ij"]},{"title":"F Rww","paragraphs":["時,同一關係可表示如下:"]},{"title":"(|,) (,,)(,","paragraphs":["ij ij ij"]},{"title":")F Rww CRww Cww=","paragraphs":["(式 10) 其中, 表示為兩個詞一起出現的頻率, 表示兩個詞一起出現時存在有的相依關係。 且為了避免資料稀疏(sparse data)的問題,進一步地利用知網的知識,將詞轉換成相對應的上位詞 (hypernym),以表示之"]},{"title":"(, )","paragraphs":["ij"]},{"title":"Cw w (, , )","paragraphs":["ij"]},{"title":"CRw w ()H i","paragraphs":[",得到下列式子: ( | ( ), ( )) ( , ( ), ( )) ( ( ), ( ))ij ij ijF RHwHw CRHwHw CHwHw= (式 11) 舉例而言,一句中文”我們遊覽台灣各個景點”,經過斷詞並且對應到相關的上位詞,和中研院 Treebank 內 建立的語意關係,配合中研院詞庫小組所提的「中心詞主導原則」(dead-driven principle) [11],最後可以建  構出如(圖 3)的語意相依網路,得 到 ”我們(first person) 遊覽(tour) 台灣(place) 各個(qValue) 景點(attribute)”。  圖 3. 語意相依關聯範例 表 1. 中心詞主導原則 1. 句子(S)和述詞片語(VP)的中心詞皆為述詞(V) 2. 名詞片語(NP)的中心語為名詞(N) 3. 介詞片語(PP)的中心語為介詞(P) 4. 方位詞片語(GP)的中心語為方位詞(Ng) 5. 對等連接詞(XP)的中心語為連接詞(C) 6. XP 的詞類由連接成份決定,連接成份為述詞片語(VP),則為述詞片語, 連接成份為名詞片語,則為名詞片語(NP)。 - S、VP 的中心語是述詞 - NP 多半以多右方的名詞為中心語 - PP 以介詞為中心語,其論元角色是 DUMMY,成雙岔結構 - GP 以 Ng 為中心語,其論元角色是 DUMMY,成雙岔結構 語意相依法則目的是建立詞組間語意關聯,即使詞組不相鄰,亦可得知詞與詞在語意上修飾的關係。 實際上,利用 HowNet 以及統計訓練好的語意相依機率。輸入一詞組,利用 HowNet 將其推展到上位詞的 型態[12],然後判斷詞組間是否有相依的關連。語意相依法則和機率式文法規則的訓練流程如(圖 4)所示:  圖 4. 語意相依法則及機率式文法規則訓練流程","","我們利用 Treebank 及公視新聞語料進行非監督式的訓練。 (,) 1"]},{"title":"1 (,) (,(,)) ((,)","paragraphs":["s k","i iab N","jj","SDG a b i a b T a bDR w w jiks"]},{"title":")B ww f TS w w f Sww N","paragraphs":["="]},{"title":"=∑∑∑ ×","paragraphs":["(式 12)  其中, iT"]},{"title":"f","paragraphs":["表示文法剖析 PCFG 之機率。 k iDR"]},{"title":"f","paragraphs":["表示語意相關法則之機率 s"]},{"title":"N","paragraphs":["表示句子總數。 表示 一個句子包含有 和 。 表示針對句子 可能剖析的文法樹。 k 表示存在的關連性索引。","指長度 的句子存在相依關係。 考慮訓練語料稀疏的問題(sparse data), 因此使 HowNet 內定義的上位詞(Hypernym)取代原本的詞組: j"]},{"title":"S","paragraphs":["a"]},{"title":"w","paragraphs":["b"]},{"title":"w","paragraphs":["i"]},{"title":"T","paragraphs":["j"]},{"title":"S","paragraphs":["{(,)|1 1k iiab wDDRww kN=≤≤}− j wN (,) ((),())"]},{"title":"(, ( , )) (, ( , ))","paragraphs":["kk iab i a bj","iab iabDR w w DR H w H w"]},{"title":"f TS w w f TS w w≅","paragraphs":["(式 13) 以(圖 3)為例, j"]},{"title":"S","paragraphs":["為:”我們遊覽台灣各個景點”,"]},{"title":"()H i","paragraphs":["表示推演到上位詞,如:台灣"]},{"title":"→","paragraphs":["place。 指 存在相依關係 (( ),( ))k iabDR H w H w"]},{"title":"f ,","paragraphs":["a"]},{"title":"ww","paragraphs":["b k i"]},{"title":"DR","paragraphs":[",如:各個 景點。最後,參照(式 11),(式 13)可由下式計算: quantifier"]},{"title":"→","paragraphs":["(( ),( )) 1, ( , ( , )) ( | ( ), ( )) / ( | ( ), ( ))w k iab","N","jk v","iab a b a u","DR H w H w","uuavf TS w w FR Hw Hw FR Hw Hw","=≠="]},{"title":"∑∑","paragraphs":["(式 14) 在摘要的第三個步驟中動態規劃搜尋程序,每次以兩個詞作為輸入,直接索引在此訓練好的語意相依法則 機率值。"]},{"title":"2.5 動態規劃搜尋方法","paragraphs":["以二維圖形說明動態規劃搜尋方法如(圖 5)所示,橫軸是摘要後的結果,每一個節點表示為一個詞, 計算每個節點的分數,並儲存累計分數和回溯路徑指標。縱軸是語音辨識後的結果共有十個詞,經過摘要 後為橫軸剩下五個詞。  圖 5. 語音摘要使用動態規劃搜尋方法之示意圖"]},{"title":"3. 摘要單元串接合成","paragraphs":["在挑選最佳的摘要單元之後,為了使摘要的原音重現,可能將原本並非屬於同一時間,也就是非連續 發音的語音片段串接合成。不過,如此單元串接可能會影響語音合成後音檔的品質,如聽覺上中斷、跳音、 摩擦等不連續情況。因此如何能夠從原本的音檔中,挑選出最適合作為串接的語音片段,使得整體語音可 以有流暢平滑的表現。我們考慮語音在頻譜上的特性,並參考[6]中對語音所定義的特徵參數,作為摘要單 元選擇的評量依據,以達到最佳平滑程度,串接出自然語音。分別求取五個特徵參數。包含有,頻譜中心 (SC)、頻譜滑動(SR)、頻譜變遷(SF)、時域上越零率(ZCR)和梅爾倒頻譜參數(MFCC)等。將此參數整合之 距離定義如下:"]},{"title":"(, ) min{ (, ) (, ) (, ) (, ) (, )}","paragraphs":["ij ij ij ij ij ij"]},{"title":"SSP w w SC w w SR w w SF w w ZCR w w MFCC w w=++++","paragraphs":["(式 15) 如(圖 6)所示,新聞內容經過斷詞以摘要單元為基礎,配合摘要結果來挑選新聞語音內所有的候選語音片 段,建立一個詞網格。然後,利用動態規劃搜尋的方式,找到最佳的串接語音。由(圖 6)可知,摘要結果共 選出六個摘要詞,其中\"耶誕節\"及\"消費\"在原本語音中共出現三個可挑選的串接候選,因此,我們利 用動態規劃搜尋的方式串接語音。  歡迎回到新聞現場,來看今年的耶誕消費市場, 每年耶誕節都是美國的消費旺季,而最近幾年, 台灣人過耶誕節的氣氛也越來越濃, 因此耶誕相關的商品消費也跟著旺了起來, 儘管今年台灣籠罩在不景氣的陰影之下, 新聞內容 耶誕節的商機還是很驚人。 歡迎 回到 新聞 現場,來看 今年 的 耶誕 消費(2-1) 市場(3-1), 每年 耶誕節(1-1) 都 是 美國 的 消費(2-2) 旺季,而 最近 幾年, 台灣人 過 耶誕節(1-2) 的 氣氛 也 越來越濃, 因此 耶誕 相關 的 商品 消費(2-3) 也 跟著 旺 了 起來, 儘管 今年 台灣(4-1) 籠罩 在 不景氣 的 陰影 之 下, 斷詞結果","耶誕節(1-3) 的 商機(5-1) 還是 很 驚人(6-1)。 摘要結果 耶誕節(1) 消費(2) 市場(3) 台灣(4) 商機(5) 驚人(6)  圖 6. 摘要語音串接示意圖","1) 頻譜中心,音訊經過短時域傅立葉轉換,取其頻譜的能量中心。頻譜中心可量測頻譜上特徵,頻心高 代表著亮度高、頻率高的訊號。 (, ) () ( )ij i jSC w w SC w SC w=− ; 11 1","1 () (( [] )/( []))FN N","it","tn nSC w M n n M n F == ==× ×"]},{"title":"∑∑ ∑","paragraphs":["t (式 16) 其中,"]},{"title":"[]","paragraphs":["t"]},{"title":"M n","paragraphs":["傅立葉轉換強度; 頻框索引; 音訊分頁索引。"]},{"title":"n t","paragraphs":["2) 頻譜滑動,同樣表示頻譜上特徵,可測量兩單元間的差異, 11","1 ( ) (0.85 [ ])FN","it","tnSR w M n F ===×"]},{"title":"∑∑","paragraphs":["。","3) 頻譜變遷,正規劃相鄰頻譜的平方差,目的在於量測頻譜上的局部變化, 2 1 11","1 () ([] [])FN","itt","tnSF w N n N n F − ===−"]},{"title":"∑∑","paragraphs":["。其中, 定義在第"]},{"title":"t","paragraphs":["音訊分頁的正規化傅立葉強度。"]},{"title":"[]","paragraphs":["t"]},{"title":"Nn","paragraphs":["4) 時域上越零率,一般用於噪音偵測,在此可知兩單元間,噪音改變程度。 11 11","() ( ([]) ([ 1]))","2 FN","it","tnZCR w sign s n sign s n F ===−"]},{"title":"∑∑","paragraphs":["t−。 5) 梅爾倒頻譜參數,應用語音辨識常用的梅爾倒頻譜參數,共取三十九維,主要是模擬人的聽覺模型, 1","1 () ()F","it","tMFCC w mfcc f F =="]},{"title":"∑","paragraphs":["。"]},{"title":"4. 實驗評估 4.1 語音辨識評估","paragraphs":["實驗用的摘要語料,收錄自公視晚間新聞共 120 小時,根據標記檔案,取出主播部分四小時三十分鐘, 其中三小時做為訓練語料,約 328MB;剩下約一小時三十分鐘,255 則新聞報導作為測試語料,約 166MB。 分別計算音節、母音和字元的正確率,正確率的計算有,正確率(accuracy)、插入錯誤(insertion)、刪除錯誤 (deletion)以及替換錯誤(substitution),並且考慮前 N 名辨識結果。其計算式如下:  accuracyP WIDSW=−−− (式 17) 其中,W 為辨識結果,總字元長度。I 為比較較正確結果多辨識出的字,屬於插入錯誤,D 為比較正確結 果少辨識到的字,屬於刪除錯誤。S 為比較正確結果,辨識錯誤的字,屬於替換錯誤。音節正確率為有百 分之八十三,字元辨識率約為百分之八十,分析如(表 2):  表 2. 公視新聞測試語料之正確率 ----------------------------------------- Syllable Results--------------------------------------","ACCURACY INSERTION DELETION SUBSTITUTION Syllable ,Top 1: 83.20% 2.98% 2.03% 11.79% Syllable ,Top 5: 87.50% 3.09% 2.13% 7.28% Syllable ,Top 10: 89.02% 3.20% 2.25% 5.53%  ----------------------------------------- Character Results--------------------------------------","ACCURACY INSERTION DELETION SUBSTITUTION Characters 80.38% 2.92% 1.94% 14.76%"]},{"title":"4.2 摘要效果評估","paragraphs":["利用資訊檢索方式來評估,與原本辨識結果做比較,看是否摘要後結果,能夠充分保留原新聞報導的 要旨。隨機選取二十組詞彙作為查詢,依 2.2 節所述之向量模式對測試語料做檢索。由於檢索資料庫數量 不大,對於各查詢詞彙所檢索到的文件並不多,因此只取出前十名分數最高的檢索結果。計算其 mean average precision (mAP)[13] 和 raw average precision (rAP)[13]: 11"]},{"title":"11 mAP","paragraphs":["q iN N ikqi"]},{"title":"k NNrank","paragraphs":["=="]},{"title":"= ∑∑","paragraphs":["ik; 1"]},{"title":"1 rAP","paragraphs":["qN i iq"]},{"title":"N NN","paragraphs":["="]},{"title":"= ∑","paragraphs":["(式 18) 其中, :查詢的問句數。 :對於 的查詢結果,共有幾篇相關文章。 :對於 的查詢結果, 排序第 篇相關文章。mAP 可以分析查詢結果,是否有正相關性,也就是前面的文章是相關的,而後面 的文章可能相關性較低,mAP 曲線若無跳動的情形,則表示評估查詢的效果好,反之亦然。rAP 則可以 判斷在第幾篇文件,文章對於查詢結果相關度的降低。由(圖 7)觀察得知,當摘要比例越高則所含的資訊越 高,也就是資訊壓縮越小則語意保留程度越高。但是,當我們做 30%的摘要時,所檢索的前四篇文件與摘 要 70%和 50%時的結果很相近。 q"]},{"title":"N","paragraphs":["i"]},{"title":"N","paragraphs":["i"]},{"title":"q","paragraphs":["ik"]},{"title":"rank","paragraphs":["i"]},{"title":"q k ","paragraphs":["另外,將測試音檔做人工的摘要記錄後,與自動摘要結果相對照,分別計算其正確率、插入錯誤、刪 除錯誤以及替換錯誤等,如(式 17)。同時,實 0 驗各種知識庫所代表的重要程度,以(C_L_W_S)分別代表 語音辨識信賴分數、語言學分數、詞重要性分數和語意相依法則分數,考慮各種情況如下圖所示:  0.4 0.5 0.6 0.7 0.8 0.9 1 12345678910 Number of Documents Retrieved A v e r ag e P r ec i s i o n ORG mAP ORG rAP SUM30% mAP SUM30% rAP SUM50% mAP SUM50% rAP SUM70% mAP SUM70% rAP  圖 7. 重要資訊檢索的結果  圖 8. 摘要之各分數重要性評估   由實驗結果(圖 8)可知,利用求取關鍵詞的作法(word significance score)效果最為顯著,其次為語意相依法 則、三連語言模型,最後是語音辨識信賴分數。","ALL 代表結合四種分數所得到的摘要結果,依據各種知識所代表的重要性程度,設定其權重分別為 C(0.1)、L(0.2)、W(0.4)和 S(0.3),評估正確率 accuracy 為百分之三十五。詳細的實驗結果如(圖 9)所示。由 (圖 9)得知,摘要錯誤較常發生在插入錯誤,其次為替換錯誤和刪除錯誤。由此可知摘要結果的好壞,主觀 因素影響較大,插入和替換錯誤較容易發生。"]},{"title":"4.3 串接效果評估","paragraphs":["串接語音的實驗可由(圖 10)表示,請十位受測者分別針對不同摘要比例評比。受測者先看過原始標準 報導,並聆聽報導內容之後。比較摘要後的文字結果和聆聽語音串結效果,是否能表達報導文意及合成語 音是否流暢,評比一到十分數,代表從劣到優的表現效果。  圖 9. 摘要結果正確率評估  圖 10. 摘要串接及合成結果評估 "]},{"title":"5. 結論及未來展望","paragraphs":["本論文提出新聞語料庫及語意相依法則於中文語音文件摘要,同時對語音串接單元計算頻譜上的特徵","參數,利用動態規劃搜尋方法,生成一個兼具語意壓縮和聽覺效果流暢的摘要結果。分析摘要語音文件的","聲學、語意和語法等特徵,結合語音辨識信賴分數、詞重要性分數、語言學分數、句法結構分數及語意相","依法則分數。摘要單元串結從頻譜上取五項特徵參數,頻譜中心、頻譜滑動、頻譜變遷、越零率以及梅爾","倒頻譜參數,決定最佳的語音串接。目前在八成的語音辨識率下,實驗證實系統可以做到良好的語意擷取","保留,以及流暢的摘要語音效果。 語音摘要的目的,旨在壓縮語音文件,取出具代表性內容,並且能流暢地將語音串接輸出。以此研究","為基礎展望未來,可藉由聯合各種方法,探討如何改善摘要效果: 1) 從摘要語音可分為文體規範式語音和自然口語式語音兩大類。其中,文體規範式語音是指語音","內容有事先經過設計,表達內容與書本或文章的格是相近,像是新聞報導。而自然口語式語音","則指語音內容無經過設計,表達內容是臨時思考應對,像是對話、訪談等。 2) 分析文章語意,進ㄧ步探討應用 Ontology 於摘要。 3) 以新聞語音為例,可將新聞分類並依照不同的新聞類別,抽取出具代表性的關鍵詞,或建立不","同新聞類別的句法結構模組,以輔助摘要生成。 4) 分析語音聲學上特性,如:音高、週期和能量等。 5) 藉由網際網路的幫助,可分析因為時間的推進,所產生的新詞、文章用法的表達,和各領域的","知識等。 "]},{"title":"誌謝","paragraphs":["感謝國科會支持本研究計畫,計畫編號 NSC90-2213-E-006-088。    "]},{"title":"參考文獻","paragraphs":["[1] Berlin Chen, Hsin-min Wang, Member, IEEE, and Lin-shan Lee, Fellow, IEEE, \"Discriminating Capabilities of Syllable-Based Features and Approaches of Utilizing Them for Voice Retrieval of Speech Information in Mandarin Chinese,\" IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 10, NO. 5, JULY 2002 [2] Julian Kupiec, Jan Pedersen and Francine Chen, \"A Trainable Document Summarizer\", Xerox Palo Alto Research Center [3] Kiyonori Ohtake, Kazuhide Yamamoto, Yuji Toma, Shiro Sado, Shigeru Masuyama,and Seiichi Nakagawa, \"NEWSCAST SPEECH SUMMARIZATION VIA SENTENCE SHORTENING BASED ON PROSODIC FEATURES\", Toyohashi University of Technology, Japan [4] Chiori Hori, Member, IEEE, and Sadaoki Furui, Fellow, IEEE, \"A New Approach to Automatic Speech Summarization,\" IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 5, NO. 3, SEPTEMBER 2003 [5] Furui, S.; Kikuchi, T.; Shinnaka, Y.; Hori, C., “Speech-to-Text and Speech-to-Speech Summarization of Spontaneous Speech,” Speech and Audio Processing, IEEE Transactions on , Volume: 12 , Issue: 4 , July 2004, pp. 401 – 408 [6] G. Tzanetakis and P. Cook, “Musical genre classification of audio signals,” IEEE Transactions on Speech and Audio Processing, vol. 10, No. 5, July 2002. [7] Biing-Hwang Juang, Fellow, IEEE, Wu Chou, Member, IEEE, and Chin-Hui Lee, Fellow, IEEE, \"Minimum Classification Error Rate Methods for Speech Recognition,\" IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 5, NO. 3, MAY 1997 [8] Christopher D. Manning and Hinrich Schutze, \"Foundations of Statistical Natural Language Processing\", The MIT Press, 1999 [9] Manhung Siu, Member, IEEE, and Mari Ostendorf, Senior Member, IEEE, \"Variable N-Grams and Extensions for Conversational Speech LanguageModeling\", IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 8, NO. 1, JANUARY 2000 [10] F. Jelinek and R.L. Mercer, “ Interpolated Estimation of Markov Source Parameters From Sparse Data,” Pattern Recognition in Practice, E.S. Gelsema and L.N. Kanal, Eds., North-Holland Pub. Co., Amsterdam, pp. 381-397, 1980 [11] http://rocling.iis.sinica.edu.tw/ [12] HowNet. http://www.keenage.com/ [13] M. Banko, V. Mittal and M. Witbrock, “Headline generation based on statistical translation, “ in Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, 2000, pp. 318-325. "]}]}