{"sections":[{"title":"","paragraphs":["Computational Linguistics and Chinese Language Processing Vol. 12, No. 1, March 2007, pp. 79-90 79 The Association for Computational Linguistics and Chinese Language Processing [Received August 11, 2006; Revised September 30, 2006; Accepted October 22, 2006]"]},{"title":"Emotional Recognition Using a Compensation Transformation in Speech Signal Cairong Zou* , Yan Zhao+ , Li Zhao + , Wenming Zhen+ , and Yongqiang Bao+ Abstract","paragraphs":["An effective method based on GMM is proposed in this paper for speech emotional recognition; a compensation transformation is introduced in the recognition stage to reduce the influence of variations in speech characteristics and noise. The extraction of emotional features includes the globe feature, time series structure feature, LPCC, MFCC and PLP. Five human emotions (happiness, angry, surprise, sadness and neutral) are investigated. The result shows that it can increase the recognition ratio more than normal GMM; the method in this paper is effective and robust. Key words: Speech Emotional Recognition (SER), GMM, Emotion Recognition, Compensation Transformation"]},{"title":"1. Introduction","paragraphs":["One of the natural goals for research on speech signals is recognizing emotions of humans [Chen 1987; Oppenheim 1976; Cowie 2001]; it has gained growing amounts of interest over the last 20 years. A study conducted by Shirasawa et al. showed that SER could be made by ICA and attain an 87% average recognition ratio [Shirasawa 1997; Shirasawa 1999] Many studies have been conducted to investigate neural networks for SER. Chang-Hyun Park tried to recognize sequentially inputted data using DRNN in 2003[Park et al. 2003], Muhammad, W. B. obtained about 79% recognition rate using GRNN [Bhatti et al. 2004]. Aishah Abdul Razak achieved an average recognition rate of 62.35% using combination MLP","[Razak et al. 2005]. Fuzzy rules are also introduced into SER such that an 84% rate has been achieved in recognizing anger and sadness [Austermann et al. 2005]. A number of studies in SER have  * Foshan University, Foshan, 528000, Guangdong, China + Research Center of Learning Science, Southeast University, Nanjing, 210096, China E-mail: zhaoli@seu.edu.cn   80 Cairong Zou et al. also been done with the development of GMM/HMM","[Rabiner 1989; Jiang et al. 2004; Lin et al. 2005]. However, in SER, the variations in speech characteristics, noise and individual differences always influence the recognition results. In addition, the methods above have always handled such problems in the preprocessing stage and have not been able to eliminate the influence effectively. Therefore, a valid solution has still not been proposed. In this paper a compensation transformation is introduced into an algorithm for GMM which operates in the recognition module. The experiments with five emotions (happiness, angry, neutral, surprise and sadness) show that the method in this paper is effective in emotional recognition."]},{"title":"2. Descriptions of Emotion and Selection of Emotion Speech Materials","paragraphs":["Usually, emotions are classified into two main categories: basic emotions and derived emotions. Basic emotions, generally, can be found in all mammals. Derived emotions mean derivations from basic emotions. One viewpoint is that the basic emotions are composed by the basic mood. Due to different research backgrounds, different researchers have expressed different definitions of basic emotions. Some of the major definitions [Ortony et al. 1990] of the basic emotions are shown in Table 1."]},{"title":"Table 1. Researches about basic emotions definition","paragraphs":["Researchers definitions Plutchik Acceptance, joy, anger, anticipation, disgust, fear, sadness, surprise Ekman/Friesen/ Ellsworth Anger, disgust, fear, joy, sadness, surprise James Fear, grief, love, rage Izard Anger, contempt, disgust, distress, fear, guilt, interest, joy, shame, surprise Oatley/Johnson -Laird Anger, disgust, anxiety, happiness, sadness Panksepp Expectancy, fear, rage, panic Weiner/Graham Happiness, sadness The common emotion classification which was proposed by Plutchik is shown in Figure 1e. In this paper, the authors only recognize five kinds of emotion. Anger Disgust Acceptance Fear Joy Anticipation Surprise Sadness Neutral"]},{"title":"Figure 1. Emotion wheel  ","paragraphs":["Emotional Recognition Using a Compensation Transformation in Speech Signal 81 This is a relatively conservative view of what emotion is so special attention has been paid to emotional dimension space theory. Three major dimensions (valence, arousal, and control) [Cowie 2001] are used to describe emotions.","a. Valence: The clearest common element of emotional states is that the person is materially influenced by feelings that are valenced, i.e., they are centrally concerned with positive or negative evaluations of people or things or events.","b. Arousal: It has been proven that emotional states involve dispositions to act in certain ways. A basic way of reflecting that theme turns out to be surprisingly useful. States are simply rated in terms of the associated activation level, i.e., the strength of the person’s disposition to take some action rather than none.","c. Control: Embodying in the initiative and the degree of control. For instance, contempt and fear are in different ends of the control dimension.","In this paper, two aspects have to be taken into consideration in the selection of emotional materials: 1. the sentence materials can ’t have any emotional tendency; 2. the materials should relate to five kinds of emotions (happiness, angry, surprise, sadness, and neutral). All recordings were carried out in a large, soundproof room with no echo interference using a high quality microphone, a SONY DAT recorder and a PC164 audio card at a sampling rate of l2KHZ with 16-bit resolution. Six speakers (three male and three female) who are good at acting spoke the sentences with happiness, anger, surprise and sadness, expressing each emotion three times. At the same time, the researchers made the speakers speak each sentence three times in a neutral way. In this way, 2430 sentences for experiments were compiled."]},{"title":"3. Feature Extraction","paragraphs":["The emotional features of speech signals are always represented as the change of speech rhythm [Shigenaga 1999; Muraka 1998]. For example, when a man is in a rage, his speech rate, volume and tone will all get higher. Some characteristics of phonemes can also reflect the change of emotions such as formant and the cross section of the vocal tract","[Muraka 1998; Zhao et al. 2001]. As the emotional information of speech signals is more or less related to the meaning of the sentences, the distributing rules and construction characteristics should be attained by analyzing the relationship between emotional speech and neutral speech to avoid the effect caused by the meaning of the sentences.","The global features used in this paper are duration, mean pitch, maximum pitch, average different rate of pitch, average amplitude power, amplitude power dynamic range, average frequency of formant, average different rate of formant, mean slope of the regression line of the peak value of the formant and the average peak value of formant [Zhao et al. 2001; Zhao   82 Cairong Zou et al. et al. 2000; Zhao et al. 2000]. The duration is the continuous time from start to end in each emotional sentence. It includes the silence, because these parts contribute to the emotion. Duration ratio of emotional speech and neutral speech was used as the characteristic parameters for recognition. The frequency of pitch was obtained by calculating cepstrum. Then the pitch-track was gained, and maximum pitch ( max0F ), average fundamental frequency ( 0F ), average different rate of pitch ( 0rateF ) of the envelopes of different emotional speech signals can all be extracted from it. 0rateF mentioned here, refers to the mean absolute value of the difference between each frame of speech signal ’s fundamental frequencies. The authors used the differences in value of the mean pitch, the maximum pitch and the ratio of 0rateF between the emotional and neutral speech as the characteristic parameters. In this paper, the average amplitude power ( A) and the dynamic range ( rangeA ) are to be taken into account. To avoid the influence of the silent and noisy parts of the speech, the authors only took the mean absolute value of the amplitude into account and all the absolute values must above a threshold. The difference of average amplitude power and the dynamic range between the emotional and neutral speech was used for parameters of recognition. Formant is an important parameter that reflects the characteristics of vocal track. Formant was attained as follows [Zhao et al. 2001]. At first, LPC method was applied to calculate 14-order coefficients of linear prediction. Then, the coefficients were used to estimate the track’s frequency of the formant by analyzing the frequency average ( 1F ), frequency-changing rate ( 1rateF ) of the first formant, the average and the average slope of recursive lines of the first four formants. The authors use the difference of 1F , the last two parameters and the ratio of 1rateF between the emotional and neutral speech as the characters in each frame.","The structural features of time series for the emotional sentences used in this paper is maximum value of the pitch in each vowel segment, amplitude power of the corresponding frame, maximum value of the amplitude energy in each vowel segment, pitch of the corresponding frame, duration of each vowel segment and mean value and rate of change of the first three formants. For these parameters, the ratio between the emotional and neutral speech was used as the recognition characters.","In addition to the above features, LPCC, PLP, MFCC are also taken into consideration for precise decision. Figure 2 is the module for feature extraction.        Emotional Recognition Using a Compensation Transformation in Speech Signal 83 "]},{"title":"Figure. 2 the module for feature extraction 4. Speech Emotion Recognition based on GMM","paragraphs":["GMM can be described as follow: {,,}iiiialm=S, (1) 1(|)()M ii ipxabxl == rr",", 1 1 M i i a = = , (2) 1 /21/2"]},{"title":"11 ()exp{()()} 2(2)||","paragraphs":["t","iiiiD i"]},{"title":"bxxxmm p","paragraphs":["-"]},{"title":"=--S-S rrr","paragraphs":[", (3) where"]},{"title":"xr ","paragraphs":["is the D-dimensional feature vector, ()ibxr (1,2,...)iM= is the density function of the member"]},{"title":"xr","paragraphs":[", (|)px lr is the probability density function of"]},{"title":"xr","paragraphs":[", and i"]},{"title":"a","paragraphs":["satisfies: 1 1 M i i a = = (1,2,...)iM="]},{"title":".","paragraphs":["The GMM probability function of a speech signal with T frames 12(,,,)TXxxx= vvv L can be denoted as: 1(|)(|)T t tPXpxll == v , (4) or Speech Signal Preprocessing Adding window, Frame Endpoint Detection Amplitude, duration, etc. pitch, format, etc. LPCC MFCC PLP LPC   84 Cairong Zou et al. 1(|)log(|)log(|)T t tSXPXpxlll === v",". (5)","According to the statistical characteristic of likelihood probability (LP) output by Gaussian Mixture Model, the likelihood probability with the best model is generally bigger than that of the other GMM, but due to the existence of variations in speech characteristics and noise, some frames’ LP shows a best model that is smaller than that of the others, so the decision may be incorrect. In order to reduce this error recognition rate, some transformation should be introduced to compensate for the likelihood probability, that is, raise the probability with the best model and reduce the probability with the other models. Therefore, a nonlinear compensation transformation is proposed in this paper to solve this problem."]},{"title":"5. Compensation Transformation for GMM","paragraphs":["The transformation must satisfy three conditions as follow:","1. The difference of the output probability in different time should be reduced, i.e. increase 1SD ;","11 ,1log(|)log(|) T ttk tk tk","Spxpxll = „","D=- vv ","2. The difference of the output probability in the same time with different emotion should be increased, i.e. increase 2SD ; 2 ,1log(|)log(|) M titj ij ij","Spxpxll = „","D=- vv  3. The relative value of the output probability should not be changed. Assuming that"]},{"title":"xr ","paragraphs":["is a feature vector, 0l is the best model corresponded to"]},{"title":"xr","paragraphs":[", and 1l is the other model that is mismatched. If the transformation is linear: [(|)]tifpx lv = (|)tiapxbl +v ","01[(|)][(|)]fpxfpxll-vv 01[(|)(|)]ttapxpxll=-vv",", (6) where ,abconst= . Here set 0a > :","01(|)(|)pxpxll‡vv 01[(|)][(|)]fpxfpxll‡vv",", (7)","01(|)(|)pxpxll£vv 01[(|)][(|)]fpxfpxll£vv",". (8)","From (7) ~ (8), it is obvious that the linear transformation cannot increase or reduce the LP of the output. The compensation could not be linear transformation, so a nonlinear compensation transformation is proposed; the detailed steps are described as follow:    Emotional Recognition Using a Compensation Transformation in Speech Signal 85","1. Compute the probability of the t-th feature vector, where N is the number of the emotions, and T is the number of the frames. (|)tipx lv (1,2,...)iN= , (1,2,...)tT= 2. Normalize (|)tipx lv . (|) (|) max(|)ti ti ti px Px pxl l l = v r v (9) 3. Compute the output LP. [(|)] (,) [(|)]","n ti","ti n ti Px Sx Pxb l l l = + v v v , (10) where 2~5n = , 1b > and b is always set close to 1. 4. Introduce the compensation: compute the average probability with K former frames. ,1,2,,(...,)tKtKtiSxxxl-+-+vvv , 1 1 (|) K tki k Sx K l+ == v (11) In general, K also has an influence on output probability, here set 2~5K = .","5. Take ,1,2,,(...,)tKtKtiSxxxl-+-+vvv as the compensation for (|)tiSx lr .","'(|)(|)titiSxSxll=+rr 12[(,,...,,)(|)]tititKtKtitiaSxxxSxdll-+-+ -rrrr",", (12) where [0,1)tia ̨, 12"]},{"title":"1(,,...,,)(|) 1","paragraphs":["tKtKtiti ti"]},{"title":"SxxxSx otherwisell d","paragraphs":["-+-+"]},{"title":">= - rrrr","paragraphs":[". 6. Calculate the joint probability for each model. 1(,)log'(|)T","iti tSXSxll == v (13)","7. Make the decision of which emotion X belongs to. If (,)max(,)ji iSXSXll= , then X belongs to jl . Assuming two emotions: 0l , 1l and two vectors: 21"]},{"title":", xx rr","paragraphs":[".Set 2T = . The output probability without transformation:","01020(,)ln(|)ln[(|)]Sxpxpxlll=+vvv , (14) 11121"]},{"title":"(,)ln(|)ln[(|)]Sxpxpxlll=+vvv","paragraphs":[". (15) ","1020ln(|)ln(|)PxPxll+vv > 1121ln(|)ln(|)PxPxll+vv"," 10201121(|)(|)(|)(|)PxPxPxPxllll-vvvv",">0","10201121(|)(|)(|)(|)nnnn","PxPxPxPxllll-vvvv",">0, (16)   86 Cairong Zou et al.","When 01(,)(,)SxSxll>vv , i"]},{"title":"xr","paragraphs":["(i=1, 2) belongs to 0l , otherwise belongs to 1l . The output probability with transformation:","10(|)Sx l = 1010","1,01,00 1010 '(|)'(|)","log([(0,)]) '(|)'(|) nn nnPxPx","S PxbPxb ll","dal ll","- +-","++ vv vv, (17)","20(|)Sx l = 2020","2,02,00 2020 '(|)'(|)","log([(1,)]) '(|)'(|) nn nnPxPx","S PxbPxb ll","dal ll","- +-","++ vv vv. (18) 11"]},{"title":"(,)Sx lv","paragraphs":["and 21(,)Sx lv are similar to (17)~(18). The decision rule is the same as the one without transformation.","01(|)(|)SXSXll- 1010","1,01,00 1010 '(|)'(|)","log([(0,)]) '(|)'(|) nn nnPxPx","S PxbPxb ll","dal ll","-","=+- ++ vv vv 2020","2,02,00 2020 '(|)'(|)","log([(1,)]) '(|)'(|) nn nnPxPx","S PxbPxb ll","dal ll","-","++- ++ vv vv 1111","1,11,11 1111 '(|)'(|)","log([(0,)]) '(|)'(|) nn nnPxPx","S PxbPxb ll","dal ll","-","-+- ++ vv vv 2121","2,12,11 2121 '(|)'(|)","log([(1,)]) '(|)'(|) nn nnPxPx","S PxbPxb ll","dal ll","-","-+- ++ vv vv, (19) Set 10201122aaaaconsta=====,"]},{"title":"( )","paragraphs":["titippxl= r ,"]},{"title":"()","paragraphs":["1 1 [(|)] , [(|)]","n ti","tiin ti Px","SSt Pxb l l l +","+=- + v v . 1. 1020 1pp==, (16) and (19) can be changed into (20) ~ (21): 1121 1PP < (20) 01(|)(|)SXSXll-=","101,0002,0 2 00101,02,02 1","log[]","1(1) aSaS aSS bb dd dd --","--+","++ ++  "]},{"title":"()()()()","paragraphs":["112121111111 11211121 log ppaSaS","pbpbpbpbddØ","-+-Œ ++++Œo 2","11211121]0aSSdd > (21)"]},{"title":"()()() ()()","paragraphs":["10002010112121111111 2 11211121","1","11 SaSppSS","a pbpbbpbpbb dd dd +","-+--++++++ Łł","2 1020100011211121()0aSSSSdddd+-> (22) s.t."]},{"title":"()","paragraphs":["210002010 102010002 1 0 11 aSaS aSS bb dd dd +","++> ++    Emotional Recognition Using a Compensation Transformation in Speech Signal 87"]},{"title":"()()()()","paragraphs":["2112121111111 11211121 11211121 0 ppaSaS","aSS","pbpbpbpbdd","dd+-+>","++++  where"]},{"title":"a","paragraphs":["is small enough to ignore the influence of the second and the third item in (22). 1121 2 1121","1 ()()(1) pp pbpbb -","+++ = 2","112111212111 2 1121 (1)(1)(1) (1)()() bppbppbpp bpbpb -+-+- +++ >0 (23) Compared to (20), it can be seen that the LP with transformation is increased. 2. 1021 1pp==, (16) and (19) can be changed into 2011 0pp-> (24)"]},{"title":"()()","paragraphs":["2202010100020","0110201000 2020 (|)(|)log","11paSaSp","SXSXaSS","bpbbpbdd","llddØ ø","-=+++Œ œ","++++Œ ßo  "]},{"title":"()()","paragraphs":["2110111211111 11211121","1111log]0 11aSpaSp aSS bpbbpbd d dd Ø","-+++>Œ","++++Œo (25)"]},{"title":"()()()()","paragraphs":["202010100020110111211111 201120111111pSaSpSpSp","a","bpbbpbbpbbpbdddd","-++--++++++++Łł","+ 2 001001111,02,01,12,1()aSSSSdddd----","- >0 (26) s.t."]},{"title":"()()","paragraphs":["2202010100020 10201000 2020"]},{"title":"0 11paSaSp aSS bpbbpbdd dd+++> ++++  ()()","paragraphs":["2110111211111 11211121 1111 ]0 11aSpaSp aSS bpbbpbd d","dd+++> ++++  The first and the second item in (26) 2011 1120","() (1)()()b","pp bpbpb - +++ . (27) Compared to (24), (27) has little effect in increasing or reducing probability, except according to the convention: If 1020(|)(|)PxPxll>vv",", then 1121(|)(|)PxPxll<vv",". So 20211,1dd==- , the first and third items in (26) are positive, the second item is far smaller than the first one. Even if the second and the fourth items were negative, the output probability with the best modal would still be bigger than the one with other modals. 10S is always bigger than 01S , and"]},{"title":"a","paragraphs":["is small enough to ignore the fourth item. When the LP of 1"]},{"title":"xr ","paragraphs":["with 0l and LP of 2"]},{"title":"xr ","paragraphs":["with 1l is big, the compensation transformation can enlarge the distance between these   88 Cairong Zou et al. two probabilities. 3. 1120 1pp==, the analysis is similar to Derivation 2."]},{"title":"6. Experiment Results","paragraphs":["In this paper, six people (three male and three female) have taken part in a recording test. They read 27 sentences using five kinds of emotion (happiness, angry, neutral, surprise and sadness), every sentence was read three times, and 2430 sentences were taken as the experiment materials.","GMM with compensation and GMM without compensation are compared first. In the first experiment, globe features and structural features of the time series were utilized. The result is shown in Table 2. In the second experiment, 12 LPCC, 12 MFCC, 16 PLP were utilized. The result is listed in Table 3. Set 3,Kn== 0.01iiaconst”="]},{"title":"Table 2. the result of the experiments between compensated and uncompensated emotion recognition (globe features and structural features %)","paragraphs":["Emotion Uncompensated GMM Compensated GMM Anger 77.6 86.2 Sadness 84.5 99.8 Happiness 73.4 80.0 Surprise 75.8 79.3 Neutral 71.6 77.1"]},{"title":"Table 3. the result of the experiments between compensated and uncompensated emotion recognition (LPCC, MFCC, PLP %)","paragraphs":["Emotion Uncompensated GMM Compensated GMM Anger 76.3 84.2 Sadness 82.1 97.8 Happiness 79.6 88.3 Surprise 77.8 82.1 Neutral 80.4 87.0","The experiments indicate that the compensation transformation can improve the recognition rate effectively. Angry recognition rate increased 8.2%, sadness recognition rate increased 15.5%, and happiness recognition rate increased 8.5%, surprise recognition rate increased 4%, and neutral recognition rate increased 6%. The selection of ,,tiKna also can improve recognition rate. Here, the authors only selected a set of parameters to explain the effectiveness and robustness of the method. Due to the compensation for GMM, the probability of the output has been stabilized and 2SD has been increased.","Table 4 shows another experiment which compared three methods: KNN, NN [7]","and compensated GMM (CGMM).   Emotional Recognition Using a Compensation Transformation in Speech Signal 89"]},{"title":"Table 4. KNN, NN, Compensated GMM (%)","paragraphs":["Emotion KNN NN CGMM Anger 76.0 82.3 86.2 Sadness 82.3 86.0 99.8 Happiness 70.5 71.4 80.0 Surprise 72.2 64.0 79.3 Neutral 78.9 70.6 77.1 Compared to KNN, the recognition rate of anger using CGMM increased 10.2%, sadness increased 17.5%, happiness increased 7.5%, and surprise increased 7.1%, while neutral decreased 1.7%. This decrease doesn ’t effect the improvement of the whole recognition rate. Compared to NN, the average recognition rate also has been increased about 9.7% using CGMM. The results indicate that CGMM also can improve some other methods to a certain degree."]},{"title":"7. Conclusion and Future Works","paragraphs":["In this paper, a method based on GMM with compensation transformation is proposed. In speech emotion recognition, the variations in speech characteristics and noise always influence the recognition results. The common method to solve this problem is conventional preprocessing. As the method in this paper deals with this problem in the recognition stage, the likelihood probability of the output with different models has been increased or decreased to reduce these influences. According to a simple analysis, this compensation transformation can reduce this impact effectively, and the examination results also proved it has better emotion recognition rates. However, the recognition rate of happiness and surprise is still not ideal, and the test materials are too few to further experiments. In further research, the authors will extend the experiment sentences first, then do some studies, such as adding more types of noise and the consideration of gender."]},{"title":"Reference","paragraphs":["Austermann, A., N. Esau, L. Kleinjohann, and B. Kleinjohann, “Fuzzy Emotion Recognition in Natural Speech Dialogue,” IEEE International Workshop on Robots and Human Interactive Communication, 2005, pp. 317-322.","Bhatti1, M. W., Y. Wang, and L. Guan, “A Neural Network Approach for Human Emotion Recognition in Speech,” IEEE Circuits and System, Proceedings of the 2004 International Symposium ISAS, 2004, vol. 2, pp. 181-184.","Chen, Y.-B., “Automatic Segmentation of Chinese Continuous Speech, ” In Proceedings of IEEE Asian Electronics Conference, 1987, pp. 163-168, Hong Kong, (1987, 09, 1-4).   90 Cairong Zou et al.","Cowie, R., E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S.Kollias, W. Fellenz, and J.G. Taylor, “Emotion Recognition in Human-Computer Interaction, ” IEEE Signal Processing Magazine, 18(1), 2001, pp. 32-80.","Jiang, D.-N., and L.-H. Cai, “Speech Emotion Classification with the Combination of Statistic Features and Temporal Features, ” IEEE International Conference on Multimedia and Expro (ICME), June 2004, vol.3, pp. 1967-1970.","Lin, Y. L., and G. Wei, “Speech Emotion Recognition Based on HMM and SVM,” In Proceedings of the Fourth International Conference on Machine Learning and Cybernetics, Guangzhou, August 2005, vol. 8, pp.18-21.","Muraka, S.,“Emotional Constituents in Text and Emotional Components in Speech,” Ph. D. Theis, Kyoto: Kyoto Institute of Technology, Japan, 1998.","Zhao, L., X. Qian, C. Zou, and Z. Wu, “A study on emotional recognition in speech signal,” Journal of Software, 12(7), 2001, pp. 1050-1055 (in Chinese).","Oppenheim, A. V., C.E. Kopec, and J.M.Tribolet, “Speech Analysis by Homomorphic Prediction,” IEEE Trans., Vol. ASSP-24, pp. 327-332, 1976.","Ortony, A., and T. J. Turner, “What's Basic About Basic Emotions?” Psychological Review, 1990, vol. 97, pp. 315-331.","Park, C.-H., and K.-B. Sim, “Emotion Recognition And Acoustic Analysis From Speech Signal,” IEEE Neural Networks, Proceedings of the International Joint Conference. vol. 4, 2003 July, pp. 2594-2598.","Rabiner, L. R., “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,” Processing of the IEEE, 1989, 77(2), pp. 257 - 286.","Razak, A.A., R. Komiya, and M. I. Z. Abidin, “Comparison Between Fuzzy and NN Method for Speech Emotion Recognition,” Third International Conference of Information Technology and Applications, 2005, ICITA 2005. vol. 1, 4-7 July 2005, pp. 297-302.","Shigenaga, M., “Features of Emotionally Uttered Speech Revealed by Discriminant Analysis(VI),” The preprint of the acoustical society of Japan, 2-p-18 (1999.9) (in Japan).","Shirasawa, T., and T. Yamamura, “Discriminating Emotion Intended in Speech, ” The Preprint of the Acoustical Society of Japan, HIP: 96-38(1997) (in Japanese).","Zhao, L., X. Qian, C. Zou, and Z. Wu, “A study on emotional feature analysis and recognition in speech signal,” Journal of China Institute of Communications, 21(10), 2000, pp. 18-25.","Zhao, L., X. Qian, C. Zou, and Z. Wu, “A study on emotional feature extract in speech signal,” Data Collection and Process, 15(1), 2000, pp. 120-123. "]}]}