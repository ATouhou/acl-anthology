{"sections":[{"title":"One-Sample Speech Recognition of Mandarin Monosyllables using Unsupervised Learning","paragraphs":["By Tze Fen Li Institute of Management, Ming Dao University, Chang-Hua, Taiwan, ROC and Shui-Ching Chang Department of Information Management, The Overseas Institute of Technology, Taichung, Taiwan, ROC Abstract In the speech recognition, a mandarin syllable wave is compressed into a matrix of linear predict coding cepstra (LPCC), i.e., a matrix of LPCC represents a mandarin syllable. We use the Bayes decision rule on the matrix to identify a mandarin syllable. Suppose that there are K different mandarin syllables, i.e., K classes. In the pattern classification problem, it is known that the Bayes decision rule, which separates K classes, gives a minimum probability of misclassification. In this study, a set of unknown syllables is used to learn all unknown parameters (means and variances) for each class. At the same time, in each class, we need one known sample (syllable) to identify its own means and variances among K classes. Finally, the Bayes decision rule classifies the set of unknown syllables and input unknown syllables. It is an one-sample speech recognition. This classifier can adapt itself to a better decision rule by making use of new unknown input syllables while the recognition system is put in use. In the speech experiment using unsupervised learning to find the unknown parameters, the digit recognition rate is improved by 22%. Key words and phrases: classification, dynamic processing algorithm, EM (estimate maximize) algorithm, empirical Bayes, maximum likelihood estimation, speech recognition. ————————————————— Corresponding author address: Tze Fen Li, Institute of Management, Ming Dao University, 369 Wen-Hua Road, Pee-Tow, Chang-Hua (52345), Taiwan, ROC. email address(Tze Fen Li): tfli@mdu.edu.tw 1. Introduction 1 A speech recognition system in general consists of feature extractor and classification of an utterance [1-5]. The function of feature extractor is to extract the important features from the speech waveform of an input speech syllable. Let x denote the measurement of the significant, characterizing features. This x will be called a feature value. The function performed by a classifier is to assign each input syllable to one of several possible syllable classes. The decision is made on the basis of feature measurements supplied by the feature extractor in a recognition system. Since the measurement x of a pattern may have a variation or noise, a classifier may classify an input syllable to a wrong class. The classification criterion is usually the minimum probability of misclassification [1]. In this study, a statistical classifier, called an empirical Bayes (EB) decision rule, is applied to solving K-class pattern problems: all parameters of the conditional density function f (x | ω) are unknown, where ω denotes one of K classes, and the prior probability of each class is unknown. A set of n unidentified input mandarin monosyllables is used to establish the decision rule, which is used to separate K classes. After learning the unknown parameters, the EB decision rule will make the probability of misclassification arbitrarily close to that of the Bayes rule when the number of unidentified patterns increases. The problem of learning from unidentified samples (called unsupervised learning or learning without a teacher) presents both theoretical and practical problems [6-8]. In fact, without any prior assumption, successful unsupervised learning is indeed unlikely. In our speech recognition using unsupervised learning, a syllable is denoted by a matrix of features. Since the matrix has 8x12 feature values, we use a dynamic processing algorithm to estimate the 96 feature parameters (means and variances). Our EB classifier, after unsupervised learning of the unknown parameters, can adapt itself to a better and more accurate decision rule by making use of the unidentified input syllables after the speech system is put in use. The results of a digit speech experiment are given to show the recognition rates provided by the decision rule. 2. Empirical Bayes Decision Rules for Classification Let X be the present observation which belongs to one of K classes ci, i = 1, 2, · · · , K. Consider the decision problem consisting of determining whether X belongs to ci. Let f (x | ω) be the conditional density function of X given ω, where ω denotes one of K classes and let θi, i = 1, 2, · · · , K, be the prior probability of ci with","∑K i=1 θi = 1. In this study, both the parameters of f (x | ω) and the θi are unknown. Let d be a decision rule. A simple loss model is used such that the loss is 1 when d makes a wrong decision and the loss is 0 when d makes a correct decision. Let θ = {(θ1, θ2, · · · , θK); θi > 0,","∑K i=1 θi = 1} be the prior probabilities. Let R(θ, d) denote the risk function (the probability of misclassification) of d. Let Γi, i = 1, 2, · · · , K, be K 2 regions separated by d in the domain of X, i.e., d decides ci when X ∈ Γi. Let ξi denote all parameters of the conditional density function in class ci, i = 1, ..., K. Then R(θ, d) = K ∑ i=1 ∫ Γc i θi f (x | ξi)dx (1)","where Γc i is the complement of Γi . Let D be the family of all decision rules which separate K pattern classes. For θ fixed, let the minimum probability of misclassification be denoted by","R(θ) = inf d∈D R(θ, d). (2) A decision rule dθ which satisfies (2) is called the Bayes decision rule with respect to the prior probability vector θ = (θ1, θ2, · · · , θK) and given by Ref.[1] dθ(x) = ci if θi f (x | ξi) > θj f (x | ξj) f or all j ̸= i. (3) In the empirical Bayes (EB) decision problem [9], the past observations (ωm, Xm), m = 1, 2, · · · , n, and the present observation (ω, X) are i.i.d., and all Xm are drawn from the same conditional densities, i.e., f (xm | ωm) with p(ωm = ci) = θi. The EB decision problem is to establish a decision rule based on the set of past observations Xn = (X1, X2, · · · , Xn). In a pattern recognition system with unsupervised learning, Xn is a set of unidentified input patterns. The decision rule can be constructed using Xn to select a decision rule tn(Xn) which determines whether the present observation X belongs to ci. Let ξ = (ξ1, ..., ξK). Then the risk of tn, conditioned on Xn = xn, is R(θ, tn(xn)) ≥ R(θ) and the overall risk of tn is Rn(θ, tn) = ∫ R(θ, tn(xn)) n ∏ m=1 p(xm | θ, ξ) dx1 · · · dxn (4) where p(xm | θ, ξ) is the marginal density of Xm with respect to the prior distribution of classes, i.e., p(xm | θ, ξ) =","∑K i=1 θif (xm | ξi). The EB approach has been recently used in many areas including classification [10,11], sequential estimation [12], reliability [13-15], multivariate analysis [16,17], linear models [18,19], nonparametric estimation [20,21] and some other estimation problems [22,23]. Let S = {(θ, ξ); θ = (θ1, ..., θK), ξ = (ξ1, ..., ξK)} (5) define a parameter space of prior probabilities θi and parameters ξi representing the i-th class, i = 1, ..., K. Let P be a probability distribution on the parameter space S. In this study, we want to find an EB decision rule which minimizes R̂n(P, tn) = ∫ Rn(θ, tn)dP (θ, ξ). (6) 3 Similar approaches to constructing EB decision rules can be found in the recent literature [11,15,24]. From (1) and (4), (6) can be written as R̂n(P, tn) =","∫ K ∑ i=1 ∫ Γc i,n [∫ f (x | ξi)θi n ∏ m=1 p(xm | θ, ξ)dP (θ, ξ) ] dx dx1 · · · dxn (7) where, in the domain of X, Γi,n, i = 1, 2, · · · , K, are K regions, separated by tn(Xn), i.e., tn(Xn) decides ci when X ∈ Γi,n and hence they depend on the past observations Xn. The EB decision rule which minimizes (7) can be found in Ref[24]. Since the unsupervised learning in this study is based on the following two theorems given in Ref[24], both theorems and their simple proofs are provided in this paper. Theorem 1 [24]. The EB decision rule t̂n with respect to P which minimizes the overall risk function (7) is given by t̂n(xn)(x) = ci if ∫ f (x | ξi) θi n ∏ m=1 p(xm | θ, ξ)dP (θ, ξ) > ∫ f (x | ξj) θj n ∏ m=1 p(xm | θ, ξ)dP (θ, ξ) (8) for all j ̸= i, i.e., Γi,n is defined by the definition of the inequality in (8). Proof. To minimize the overall risk (7) is to minimize the integrand K ∑ i=1 ∫ Γc i,n [∫ f (x|ξi)θi n ∏ m=1","p(xm|θ, ξ)dP (θ, ξ)] dx of (7) for each past observations xn. Let the past obervations xn be fixed and let i be fixed for i = 1, ..., k. Let gi(x) = ∫ f (x|ξi)θi n ∏ m=1 p(xm|θ, ξ)dP (θ, ξ). Then the integrand of (7) can be written as K ∑ i=1 ∫ Γc i,n gi(x)dx = ∫ Γc i,n gi(x)dx + ∑ j̸=i [∫ gj(x)dx − ∫ Γj,n gj(x)dx] = ∑ j̸=i ∫ gj(x)dx + ∑ j̸=i ∫ Γj,n","[gi(x) − gj(x)]dx (Γc i,n = ∑ j̸=i Γj,n) which is minimum since Γj,n ⊂ {x|gj(x) > gi(x)} for all j ̸= i by the definition of Γj,n. In applications, we let the parameters ξi, i = 1, ..., K, be bounded by a finite numbers Mi. Let ρ > 0 and δ > 0. Consider the subset S1 of the parameter space S defined by 4","S1 ={(n1ρ, n2ρ, ..., nKρ, nK+1δ, nK+2δ, ..., n2Kδ); integer ni > 0, i = 1, ..., K, K ∑ i=1 niρ = 1, |niδ| ≤ Mi, integer ni, i = K + 1, ..., 2K} (9) where (n1ρ, ..., nKρ) are prior probabilities and (nK+1δ, ..., n2Kδ) are the parameters of K classes. In order to simplify the conditional density of (θ, ξ), let P be a uniform distribution on S1 so that the conditional density can later be written as a recursive formula. The boundary for class i relative to another class j as separated by (8) can be represented by the equation E[f (x | ξi)θi | xn] = E[f (x | ξj)θj | xn] (10) where E[f (x | ξi)θi | xn] is the conditional expectation of f (x | ξi)θi given Xn = xn with the conditional probability function of (θ, ξ) given Xn = xn equal to h(θ, ξ | xn) =","∏n m=1 p(xm | θ, ξ)","∑ (θ′ ξ′",")∈S1","∏n","m=1 p(xm | θ′",", ξ′",") (11) The actual region for class i as determined by (8) is the intersection of the regions whose borders are given by (10), relative to all other classes. The main result in Ref[24] is that the estimates E[θi | Xn] converge almost sure (a.s.) to a point arbitrarily close to the true prior probability and E[ξi|Xn] will converge to a point arbitrarily close to the true parameter in the conditional density for the i-th class. Let λ = (θ1, ..., θK, ξ1, ..., ξK) in the parameter","space S. Let λo be the true parameter of λ. Lamma 1 (Kullback, 1973 [25]). Let","H(λo , λ) = ∫","ln p(x|λ)p(x|λo )dx.","Then the Kullback-Leibler information number H(λo , λo )−H(λo",", λ) ≥ 0 with equality if and only if p(x|λ) =","p(x|λo",") for all x, i.e., H(λo",", λ) has an absolutely maximum value at λ = λo",".","Let λ′","= (θ′",", ξ′ ) ∈ S1 such that H(λo",", λ′",") = maxλ∈S1H(λo",", λ). Since S1 has a finite number of points,","H(λo",", λ′ ) − H(λo",", λ) ≥ ε for some ε > 0 and for all λ ∈ S1. Since H(λo",", λ) is a smooth (differentiable)","function of λ ∈ S, the maximum point λ′ in S1 is arbitrarily close to the true parameter λo","in S if the increments δ and ρ are small.","Theorem 2 [24]. Let λo be the true parameter of λ. Let λ = (θ, ξ) in S. The conditional probability function h(λ|xn) given Xn = xn in (11) has the following property: for each λ ∈ S1, lim n→∞ h(λ | xn) = 0 if λ ̸= λ′ = 1 if λ = λ′ (12) 5","and hence E[λ | Xn] converges to λ′ with probability 1.","Proof. H(λo",", λ) has an absolutely maximum value at λ = λ′","on S1. Let λ ∈ S1 and λ ̸= λ′",". Consider 1 n ln","∏n m=1 p(Xm|λ)","∏n m=1 p(Xm|λ′",") = 1 n n ∑ m=1 ln p(Xm|λ) − 1 n n ∑ m=1","ln p(Xm|λ′ )","which converges almost sure to H(λo",", λ) − H(λo",", λ′",") < −ε by a theorem (the strong law of large numbers, Wilks, (1962) [26]), i.e., there exists a N > 0 such that for all n > N , 1 n ln","∏n m=1 p(Xm|λ)","∏n m=1 p(Xm|λ′",") < − ε 2 .","Hence, for all n > N , 1","n ln h(λ|Xn) < − ε 2 , i.e., for all n > N , ln h(λ|Xn) < −n ε","2 . This implies that","limn→∞ln h(λ|Xn) = −∞ and limn→∞h(λ|Xn) = 0 for λ ̸= λ′","almost sure. Obviousy, ∑","λ∈S1 h(λ|Xn) = 1","implies limn→∞h(λ′ |X n) = 1 almost sure. 3. Feature Extraction The measurements of features made on the speech waveform include energy, zero crossings. extrema count, formants, LPC cepstrum (LPCC) and the Mel frequency cepstrum coefficient (MFCC). The LPCC and MFCC are most commonly used for the features to represent a syllable. The LPC method provides a robust, reliable and accurate method for estimating the parameters that characterize the linear, time-varying system which is recently used to approximate the nonlinear, time-varying system of the speech wave. The MFCC method uses the bank of filters scaled according to the Mel scale to smooth the spectrum, performing a processing that is similar to that executed by the human ear. 3.1. Preprocessing Speech Signal In the real world, all signals contain noise. In our speech recognition system, the speech data must contain noise. We propose two simple methods to eliminate noise. One way is to use the sample variance of a fixed number of sequential sampled points of a syllable wave to detect the real speech signal, i.e., the sampled points with small variance does not contain real speech signal. Another way is to compute the sum of the absolute values of differences of two consecutive sampled points in a fixed number of sequential speech sampled points, i.e., the speech data with small absolute value does not contain real speech signal. In our speech recognition experiments, the latter provides slightly faster and more accurate speech recognition. 3.2. Linear Predict Coding Cepstrum (LPCC) For speech recognition, the most common features to be extracted from a speech signal are Mel-frequency cepstrum coefficient (MFCC) and linear predict coding cepstrum (LPCC). The MFCC was proved to be 6 better than the LPCC for recognition [27], but we have shown [28] that the LPCC has a slightly higher recognition rate. Since the MFCC has to compute the DFT and inverse DFT of a speech wave, the computational complexity is much heavier than that of the LPCC. The LPC coefficients can be easily obtained by Durbin’s recursive procedure [2,29,30] and their cepstra can be quickly found by another recursive equations [2,29,30]. The LPCC can provide a robust, reliable and accurate method for estimating the parameters that characterize the linear and time-varying system like speech signal [2,4,29-30]. Therefore, in this study, we use the LPCC as the feature of a mandarin syllable. The following is a brief discussion on the LPC method: It is assumed [2-4] that the sampled speech wave s(n) can be linearly predicted from the past p samples of s(n). Let ŝ(n) = p ∑ k=1 aks(n − k) (13) and let E be the squared difference between s(n) and ŝ(n) over N samples of s(n), i.e., E = N−1 ∑ n=0","[s(n) − ŝ(n)]2 . (14) The unknown ak, k = 1, ...p, are called the LPC coefficients and can be solved by the least square method. The most efficient method known for obtaining the LPC coefficients is Durbin’s recursive procedure [31]. Here in our speech experiment, p = 12, because the cepstra in the last few elements are almost zero. 3.3. Feature Extraction Our feature extraction from LPCC is quite simple. Let x(k) = (x(k)1,..., x(k)p), k = 1, .., n, be the LPCC vector for the k-th frame of a speech wave in the sequence of n vectors. Normally, if a speaker does not intentionally elongate pronunciation, a mandarin syllable has 30-70 vectors of LPCC. After 50 vectors of LPCC, the sequence does not contain significant features. Since an utterance of a syllable is composed two parts: stable part and feature part. In the feature part, the LPCC vectors have a dramatic change between two consecutive vectors, representing the unique characteristics of syllable utterance and in the stable part, the LPCC vectors do not change much and stay about the same. Even if the same speaker utters the same syllable, the duration of the stable part of the sequence of LPCC vectors changes every time with nonlinear expansion and contraction and hence the duration of the stable parts and the duration of the whole sequence of LPCC vectors are different every time. Therefore, the duration of stable parts is contracted such that the compressed speech waveforms have about the same length of the sequence of LPCC vectors. Li [32] proposed several simple techniques to contract the stable parts of the sequence of vectors. We state one simple technique for contraction as follows: 7 Let x(k) = (x(k)1, ..., x(k)p), k = 1, ..., n, be the k-th vector of a LPCC sequence with n vectors, which represents a mandarin syllable. Let the difference of two consecutive vectors be denoted by D(k) = p ∑ i=1 |x(k)i − x(k − 1)i|, k = 2, ..., n. (15) In order to accurately identify the syllable utterance, a compression process must first be performed to remove the stable and flat portion in the sequence of vectors. A LPCC vector x(k) is removed if its difference D(k)","from the previous vector x(k − 1) is too small. Let x′ (k), k = 1, ..., m(< n), be the new sequence of LPCC vectors after deletion. We think that the first part (about 40 vectors or less) of an utterance of a mandarin syllable contains main features which can most represent the syllable and the rest of the sequence contains the ”tail” sound, which has a variable length. If a speaker intentionally elongates pronunciation of a syllable, the speaker only increases the tail part of the sequence and the length of the feature part stays about the same. We partition the feature part (the first 40 vectors of the new sequence) into 6 equal segments since the feature part of LPCC vectors has a dramatic change and partition the tail part into 2 equal segments. If the whole length of the new sequence is less than 40, we neglect the tail sound and partition the new sequence into 8 equal segments. The average value of the LPCC in each segment is used as a feature value. Note that the average values of samples tend to have a normal distribution [26]. This compression produces 12x8 feature values for each mandarin syllable. 4. Stochastic Approximation Stochastic approximation [1,2,33,34] is an iterative algorithm for random environments, which is used for parameter estimation in pattern recognition. Its convergence is guaranteed under very general circumstances. Essentially, a stochastic approximation procedure [1,2,33,34] should satisfy: (1) the successive expression of the estimate of a parameter can be written as an estimate calculated from the old n patterns and the contribution of the new (n + 1)-st pattern and (2) the effect of the new pattern may diminish by using a decreasing sequence of coefficients. The best known of the stochastic approximation procedures are the Robbins-Monro procedure [1,33,34] and the Kiefer-Wolfowitz procedure [1,34]. For the unsupervised learning, (11) can be written in the recursive form h(λ|xn+1) = p(xn+1|λ)h(λ|xn)","∑ λ′ ∈S1 p(xn+1|λ′",")h(λ′","|xn) f or n = 0, 1, 2, ... (16) where h(λ|xn) = 1, if n = 0. Equ. (16) is different from the above two types of procedures. It does not have a regression function or an obvious decreasing sequence of coefficients, but it appears to be a weighted product of the estimates calculated from the old patterns and the contribution of the new pattern. In each 8 step of evaluation, (16) multiplies a new probability factor with the old conditional probability h(λ|xn) based on the new pattern xn+1. The convergence of (16) is guaranteed by Theorem 2. 5. A Dynamic Processing Algorithm As in Section 3, a mandarin syllable is represented by a 12x8 matrix of feature values, which tend to be normally distributed. Let xn = (x1, ..., xn) denote n unidentified syllables, where each xm, m =","1, ..., n, denotes a 12x8 matrix of feature values, which are used to learn the means μkij, variances σ2 kij, i = 1, ..., 12, j = 1, ..., 8, k = 1, ..., K, of normal distributions of 12x8 feature values and the prior probabilities θk (the probability for a syllable to appear) for K classes of syllables. For large number of classes, the stochastic approximation procedure in Section 4 is not able to estimate the means and variances, because the recursive procedure (16) needs tremendous size of computer memory. For simplicity, we let θk = 1/K, i.e., each syllable has an equal chance to be pronounced. Let λ denote all parameters, i.e., Kx12x8 means","and variances for K classes of syllables. Let λo be the true parameters. From Theorem 2 in Section 2, the","conditional probability h(λ|xn) has the maximum probability at λ = λo for large n, i.e., the numerator F (xn|λ) = n ∏ m=1 p(xm|λ) (17)","is maximum at λ = λo for large n, where x m, m = 1, ..., n, is the 12x8 matrix. Therefore, to search the true","parameter λo by the recursive equation (16) is to find the MLE of λ. To find the MLE of unknown parameters is a complicated multi-parameter optimization problem. First one has to evaluate the likelihood function F on a coarse grid to locate roughly the global maximum and then apply a numerical method (Gauss method, Newton-Raphson or some gradient-search iterative algorithm). Hence the direct approach tends to be computationally complex and time consuming. Here, we use a simple dynamic processing algorithm to find the MLE, which is similar to an EM [35,36] algorithm. 5.1. The Log Likelihood Function A syllable is denoted by a matrix of feature values Xij, i = 1, ..., 12, j = 1, ..., 8. For simplicity, we assume that the 12x8 random variables Xij are stochastically independent (as a matter of fact, they are not independent). The marginal density function of an unidentified syllable Xm with its matrix denoted by","xm = (xm ij ) in (17) can be written as p(xm|λ) = K ∑ k=1 θk ∏ ij","f (xm ij |μijk, σijk) (18) 9","where f (xm ij |μijk, σijk) is the conditional normal density of the feature value Xm","ij in the matrix if the syllable","Xm = (Xm ij ) belongs to the k-th class. The log likelihood function can be written as ln F (xn|λ) = n ∑ m=1 ln { K ∑ k=1 θk 12 ∏ i=1 8 ∏ j=1 1 √ 2πσkij e − 1 2 (","xm ij −μ","kij σ kij )2","} . (19) 5.2. A Dynamic Processing Algorithm From the log likelihood function (19), we present a simple dynamic processing algorithm to find the MLE of unknown parameters μijk and σijk. Our algorithm is an EM algorithm [35,36], more and less like the Viterbi algorithm [2-4]. We state the our dynamic processing algorithm as follows: 1. In the matrix, pick up an initial value of (μkij, σkij), k = 1, ..., K, for K classes. 2. For k = 1 and for each i = 1, ..., 12 and j = 1, ..., 8, pick up a point (μ̂1ij, σ̂1ij) such that ln F in (19) is maximum. 3. Continue step 2 for k = 2, ..., K. 4. If (19) continues increasing, go to step 2, otherwise, stop the dynamic processing and the final estimates (μ̂kij, σ̂kij) are the MLE of (μkij, σkij) for all K classes and are saved in a database. 5.3. Finding the Means and Variances for each Syllable by a Known Sample For each element (i, j) in the matrix, we have found the MLE (μ̂kij, σ̂kij) for each syllable. There are totally K matrices of MLE representing K different syllables, but we do not know which matrix of MLE belongs to the syllable ci, i = 1, ..., K. We have to use one known sample from each syllable to identify its own matrix of MLE. In this paper, we simply use the distance to select a matrix of MLE among K matrices for the known sample. 5.4. Classification by the Bayes Decision Rule After each syllable obtains its means and variances which are identified by a known sample of the syllable, the Bayes decision rule (3) with the estimated means and variances (MLE) classifies the set of all unidentified syllables. After simplification [32], the Bayes decision rule (3) can be reduced to l(ck) = ∑ ij ln(σ̂kij) + 1 2 ∑ ij (","xij − μ̂kij σ̂kij",")2 (20) where {xij} denotes the matrix of LPCC of an input unknown syllable. The matrix of LPCC of an unknown syllable is compared with each known syllable ck represented by (μ̂kij, σ̂kij). The Bayes rule (20) selects a syllable ck with the least value of l(ck) from K known syllables to be the input unknown syllable. 10 Note that new input unidentified syllables can update the estimated means and variances (MLE) which are closer to the true unknown means and variances, and hence the Bayes decision rule will become a more accurate classifier. 6. Speech Experiment on Classification of Digits Our speech recognition is implemented in a classroom. The data of 10 mandarin digits are created by 10 different male and female students, each pronouncing 10 digits (0-9) once. The mandarin pronunciation for 1 and 7 is almost the same. It is hard to classify these two syllables. 6.1. Speech Signal Processing. The speech signal of a mandarin monosyllable is sampled at 10k Hz. A Hamming window with a width of 25.6 ms is applied every 12.8 ms for our study. A Hamming window with 256 points is used to select the data points to be analyzed. In this study, the 12x8 unknown parameters of features representing a digit are estimated by unsupervised learning. After learning the parameters, there are 10 12x8 matrices of estimates representing 10 digits. For each digit, use one known sample to identify a 12x8 matrix of estimates to represent the digit. In our speech experiments, we use this database to produce the LPCC and obtain a 12x8 matrix of feature values for each syllable. There are totally 100 matrices of feature values. 6.2. To Learn Means and Variances using Unsupervised Learning The simple dynamic processing algorithm in Section 5 produces 10 matrices of MLE (estimated means and variances). After a known sample of each digit (0,1,...,9) picks up its own matrix of MLE, the 10 matrices are ranked in order from 0 to 9 as fellows: (μ̂kij, σ̂kij), i = 1, ..., 12, j = 1, ..., 8, for k = 0, ..., 9. One of 10 students pronounces 10 digits which are considered as 10 known samples (each for one digit) and the other 9 students pronounce 10 digits (90 samples), which are considered as unknown samples. The total 100 samples (10 known samples and 90 unknown samples) are used for finding the matrices of MLE of the means and variances for 10 digits. This experiment is implemented five times, each time for one of five different students whose 10 digit pronunciations are considered as known samples. Note that the only training samples are the only one sample for each digit pronounced by a student and note that the testing samples are the mixed 90 unknown samples of 10 digits pronounced by the other 9 students. Actually, the experiment is a speaker-independent speech recognition. The 10 training samples and the 90 testing samples (90 mixed unknown samples also used for unsupervised learning of parameters) are totally separated. 6.3. Speech Classification on the Mixed Samples 11 In this study, two different classifiers are used to classify 90 unknown mixed digital samples since 10 digital samples pronounced by one student are already known. (a). Bayes Decision Rule. The estimated means and variances of each digit obtained in (6.2) are placed into the Bayes decision rule (20). The Bayes decision rule classifies 90 mixed samples (except 10 known samples for 10 digits (0-9)). The recognition rates are listed in Table 1. (b). Distance Measure from 10 Known Samples The known sample of a digit (0-9) identifies 90 other mixed unknown samples using distance measure from the known sample, i.e., to classify an unknown sample, we select a known sample from 10 known samples which is the closest to the unknown sample to be the unknown sample. Its recognition rates are also listed in Table 1. From Table 1, the Bayes decision rule using unsupervised learning gives the higher recognition rate 79%, 22% more than the rate 57% given by the distance measure using one known sample. Table 1. Recognition rates for 10 digits given by the Bayes decision rule with unsupervised learning to classify 90 unknown samples as compared with the distance measure without unsupervised learning. ———————————————————————————————————– student 1 student 2 student 3 student 4 student 5 average ———————————————————————————————————– Bayes rule with 72 70 69 68 76 71.0 unsupervised learning .80 .78 .77 .76 .84 .79 ———————————————————————————————————- distance measure 55 51 39 54 58 54.4 .61 .57 .43 .60 .64 .57 ———————————————————————————————————- Discussions and Conclusion This paper is the first attempt to use an unsupervised learning for speech recognition. Actually, this paper presents an one-sample speech recognition. An unsupervised learning needs a trmendous amount of unknown samples to learn the unkown parameters of syllables. From Theorem 2, the estimates using unsupervised learning will converge the true parameters and hence, our classifier can adapt itself to a better decision rule by making the use of unknown input syllables for unsupervised learning and will become more and more accurate after the system is put in use. Theoretically, from Theorem 2, our one-sample speech 12 recognition rate will approach to the rate given by supervised learning classifiers if a syllable does not have too many unknown parameters. In our experiments, we only have 9 samples for each syllable (a total of 90 unknown samples after 90 samples are mixed) for unsupervised learning of 96 parameters for each syllable and hence we only obtain 79% accuracy, 22% more than the rate without unsupervised learning. Acknowledgments The authors are grateful to the editor and the referees for their valuable suggestions to improve this paper. References [1]. K. Fukunaga, Introduction to Statistical Pattern Recognition, New York: Academic Press, 1990. [2]. Sadaoki Furui, Digital Speech Processing, Synthesis and Recognition, Marcel Dekker, Inc., New York and Basel, 1989. [3]. L. Rabiner and B. H. Juang, Fundamentals of Speech Recognition, Prentice Hall, PTR, Englewood Cliffs, New Jersey, 1993. [4]. X. D. Huang, A. Acero, and H. W. Hon, Spoken Language Processing - A guide to theory, algorithm, and system development, Prentice Hall, PTR, Upper Saddle River, New Jersey, USA, 2001. [5]. L. Deroye, L. Gyorfi, and G. Lugosi, A Probabilistic Theory of Pattern Recognition, Elsevier, New York, 1996. [6]. R. L. Kasyap, C. C. Blayton, and K. S. Fu, Stochastic Approximation in Adaptation, Learning and Pattern Recognition Systems: Theory and Applications, J. M. Mendel and K. S. Fu. Eds., New York, Academic, 1970. [7]. T. Y. Young and T. W. Calvert, Classification, Estimation and Pattern Recognition, New York: Elsevier, 1974. [8]. A. G. Barto and P. Anandan, Pattern recognizing stochastic learning automata, IEEE Trans. Syst., Man, Cybern., Vol. SMC-15(May 1985) 360-375. [9]. H. Robbins, An empirical Bayes approach to statistics, Proc. Third Berkeley Symp. Math. Statist. Prob., Vol. 1, University of California Press, (1956), 157-163. [10]. Y. Lin, A note on margin-based loss function in classification, Statist. and Pro. Letters, 68(1)(2004), 73-81. [11]. T.F. Li and S.C. Chang, Classification on defective items using unidentified samples, Pattern Recognition, 38(2005), 51-58. [12]. R. J. Karunamuni, Empirical Bayes sequential estimation of the means, Sequential Anal., 11(1)(1992), 13 37-53. [13]. A. Sarhan, Non-parametric empirical Bayes procedure, Reliability Engineering and System, 80(2)(2003), 115-122. [14]. A. Sarhan, Empirical Bayes estimation in exponential reliability model, Applied Math. and Computation, 135(2)(2003), 319-332. [15]. T. F. Li, Bayes empirical Bayes approach to estimation of the failure rate in exponential distribution, Commu.-Stat. Meth., 31(9)(2002), 1457-1465. [16]. M. Ghosh, Empirical Bayes minimax estimators of matrix normal means, J. Multivariate Anal., 38(2)(1991), 306-318. [17]. S. D. Oman, Minimax hierarchical empirical Bayes estimation in multivariate regression, J. Multivariate Anal., 80(2)(2002), 285-301. [18]. R. Basu, J. K. Ghosh, and R. Mukerjee, Empirical Bayes prediction intervals in a normal regression model: higher order asymptotics, Statist. and Pro. Letters, 63(2)(2003), 197-203. [19]. L. Wei and J. Chen, Empirical Bayes estimation and its superiority for two-way classification model, Statist. and Prob. Letters, 63(2)(2003), 165-175. [20]. M. Pensky, Nonparametric empirical Bayes estimation of the matrix parameter of the Wishart distribution, J. Multivariate Anal., 69(2)(1999), 242-260. [21]. M. Pensky, A general approach to nonparametric empirical Bayes estimation, Statistics, 29(1)(1997), 61-80. [22]. S. Majumder, D. Gilliland, and J. Hannan, Bounds for robust maximum likelihood and posterior consistency in compound mixture state experiments, Statist. and Prob. Letters, 41(3)(1999), 215-227. [23]. Y. Ma, Empirical Bayes estimation for truncation parameters, J. Statistical Planning and Inference, 84(1)(2000), 111-120. [24]. T. F. Li and T. C. Yen, A Bayes Empirical Bayes decision rule for classification, Communications in Statistics-Theory and Methods, 34(2005), 1137-1149. [25]. S. Kullback, Information Theory and Statistics, Gloucester, MA: Peter Smith, 1973. [26]. S.S. Wilks, Mathematical Statistics, New York: John Wiley and Son, 1962. [27]. S. B. Davis and P. Mermelstein, Comparison of parametric representation for monosyllabic word recognition in continously spoken sentences, IEEE. Trans. Acoust., Speech, Signal Processing, 28(4)(1980), 357-366. [28]. T. F. Li, A note on Mel frequency cepstra in speech recognition, Department of Applied Mathematics, Chung Hsing University, Taichung, Taiwan, (2006). 14 [29]. J. Makhoul and J. Wolf, Linear Prediction and the Spectral Analysis of Speech, Bolt, Baranek, and Newman, Inc., Cambridge, Mass., Rep. 2304, 1972. [30]. J. Makhoul, Linear prediction: a tutorial review, Proc. IEEE, 63(4)(1975), 561-580. [31]. J. Tierney, A study of LPC analysis of speech in additive noise, IEEE Trans. Acoust. Speech Signal Process., 28(4)(1980), 389-397. [32]. T. F. Li, Speech recognition of mandarin monosyllables, Pattern Recognition, 36(2003), 2712-2721. [33]. H. Robbins and S. Monro, A stochastic approximation method, Ann. Math. Statist., 22(1951), 400-407. [34]. A. Abert and L. Gardner, Stochastic Approximation and Nonlinear Regression, Cambridge, MA, M.I.T., 1967. [35]. A. P. Dempster, N. M. Laird, and D. B. Rubin, Maximum likelihood from incomplete data via the EM algorithm, Ann. R. Stat. Soc. 39(1977), 1-35. [36]. C. F. J. Wu, On the convergence properties of the EM algorithm, Ann. Stat., 11(1983), 95-103. 15"]}]}