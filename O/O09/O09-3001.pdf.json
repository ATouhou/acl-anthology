{"sections":[{"title":"","paragraphs":["Computational Linguistics and Chinese Language Processing Vol. 14, No. 1, March 2009, pp. 1-18 1 © The Association for Computational Linguistics and Chinese Language Processing [Received June 6, 2008; Revised May 12, 2009; Accepted May 15, 2009]"]},{"title":"Fertility-based Source-Language-biased Inversion Transduction Grammar for Word Alignment Chung-Chi Huang","paragraphs":["∗"]},{"title":"and Jason S. Chang","paragraphs":["+"]},{"title":"Abstract","paragraphs":["We propose a version of Inversion Transduction Grammar (ITG) model with IBM-style notation of fertility to improve word-alignment performance. In our approach, binary context-free grammar rules of the source language, accompanied by orientation preferences of the target language and fertilities of words, are leveraged to construct a syntax-based statistical translation model. Our model, inherently possessing the characteristics of ITG restrictions and allowing for many consecutive words aligned to one and vice-versa, outperforms the Bracketing Transduction Grammar (BTG) model and GIZA++, a state-of-the-art word aligner, not only in alignment error rate (23% and 14% error reduction) but also in consistent phrase error rate (13% and 9% error reduction). Better performance in these two evaluation metrics suggests that, based on our word alignment result, more accurate phrase pairs may be acquired, leading to better machine translation quality. Keywords: Inversion Transduction Grammar, Syntax-based Statistical Translation Model, Word Alignment."]},{"title":"1. Introduction","paragraphs":["A statistical translation model is a model which detects word correspondences within sentence pairs, whether relying on lexical information or on syntactic aspects of the involved languages or both. In spite of the fact that methodologies vary, the intention is clear: to obtain better word alignment results so that a better translation model implies better performance in different linguistic applications. Among the methodologies are phrase-based (Och & Ney, 2004; Chiang, 2005; Liu et al., 2006) and syntax-based machine translation systems (Galley et al., 2004; Galley et al., 2006).  ∗ CLCLP, TIGP, Academia Sinica, Taipei, Taiwan + Department of Computer Science, NTHU, Hsinchu, Taiwan E-mail: {u901571; jason.jschang}@gmail.com   2 Chung-Chi Huang and Jason S. Chang","Since the pioneering work of Brown et al. (1988), a myriad of research projects have focused on the statistical translation model. These could be classified into two main categories: one paying little attention to the grammar of the languages (Vogel et al., 1996; Och & Ney, 2000; Toutanova et al., 2002) and the other explicitly utilizing languages’ structural or syntactic information (Wu, 1997; Yamada & Knight, 2001; Cherry & Lin, 2003; Gildea, 2004; Zhang & Gildea, 2005). With an increasing number of more accurate syntactic analyzers (e.g., part-of-speech tagger and Stanford parser) being developed and in view of the deficiency in modeling grammatical aspects of languages facing IBM-like models, the latter has received increasing attention.","Recently, in order to incorporate languages’ syntax, Yamada and Knight (2001) transformed source-language (SL) (e.g., English) parse trees into target-language (TL) (e.g., Japanese) strings, using operations of reordering, inserting, and translating on tree nodes. Instead of accepting monolingual (i.e., SL or TL) parse trees to do the transformation, Wu’s ITG model (1997) first associates production rules (e.g., S→NP VP) commonly shared by two languages with (straight or inverted) word orientation and, based on these synchronous rules, constructs bilingual parse trees at run time. This data-oriented parsing methodology is reported to outperform tree-to-string model (i.e., (Yamada & Knight, 2001)) concerning word-level alignment (Zhang & Gildea, 2004).","Even though the promising ITG is proposed, Wu (1997) conducts a word-aligning experiment leveraging a special case of ITG, minimal bracketing transduction grammar (BTG), in which languages’ grammars are assumed to be unavailable, constituent categories (e.g., NP and VP) are not differentiated (using only three symbols: one for lexical translation rules, another for straight binary production rules, the other for inverted), and the probabilities of the straight and inverted binary rules are all assigned constant. These imply that the choices of straight or inverted word orientations would be made solely based on the bonds of lexical translations rather than on the structural divergences of the involved languages and that the potential of the syntax-oriented ITG would not be fully explored.","More recently, Zhang and Gildea (2005) presented a lexicalized BTG model where orientation choices are also dependent on the head words of the structural constituents. They expect lexical pairs passed up from the bottom (i.e., leaf nodes) of the bilingual parse tree will make BTG models more knowledgeable in determining straight/inverted word order. Nonetheless, they found that lexical information at the lower levels of trees is more deterministic in word orientations than that at the higher levels.","To explore the power of ITG a little more (and inspired by Zhang et al. (2006), who suggest that binarized rules improve both speed and accuracy of a syntax-based machine translation system), in this paper, we describe a version of ITG model where the binary grammatical rules (e.g., S→NP VP) of the source language (e.g., English) are used as the   Fertility-based Source-Language-biased 3 Inversion Transduction Grammar for Word Alignment skeleton of our synchronous rules. Since the rules are biased toward the syntactic labels of the source language, our model is referred to as bITG model, short for biased ITG model. In our model, based on word-aligned sentence pairs, binary SL CFG rules are automatically annotated with the target language’s word orientations and the associated orientation probabilities are automatically computed via Maximum Likelihood Estimation (MLE).","For example, take the languages of English, Chinese, and Japanese. The higher probability of our binary bITG rule VP→[VP NP], where the square brackets denote the same ordering (straight) of the two right-hand-side constituents in both languages when expanding the left-hand-side symbol, indicates a similar VO construct exists in English (SVO language) and Chinese (SVO language). On the contrary, the different VO construct in English and Japanese (SOV language) is modeled through the high inverted probability of the binary bITG rule VP→<VP NP> where the pointed brackets denote that we expand the left-hand-side symbol into two right-hand-side symbols in reverse orientation in two languages. Notice that these two bITG rules originate from the same binary CFG rule (VP→VP NP) of the source language, English, only with different ordering tendencies on the TL (i.e., Chinese or Japanese) end.","In addition, we leverage IBM-style fertility probabilities of words to accommodate many-to-one or one-to-many word alignment links. In other words, in our model, many contiguous words in the source can be aligned to one word in the target and vice-versa. Originally, Wu’s BTG model (1997) only allowed for a maximum of one-to-one word correspondences, which may affect the performance on word alignments and the accuracy of the bilingual parse trees. This one-to-one mapping restriction is especially not suitable for a language pair involving a language without clear word delimiters since the tokenization (or segmentation) of sentences of that language (e.g., Chinese) prior to word alignment is independent of words of another (e.g., English), resulting in tokens being under- or over-segmented for the corresponding words and, subsequently, abundant many-to-one/one-to-many word alignments.","The paper is organized as follows. Sections 2 and 3 describe our model in detail. Section 4 shows empirical results. Discussions are made before the conclusion in Section 6."]},{"title":"2. Method","paragraphs":["In this section, we begin with an example of how bITG rules and fertilities of words are utilized to assist in word-aligning sentence pairs. Thereafter, a more formal description of our model will be discussed.    4 Chung-Chi Huang and Jason S. Chang"]},{"title":"2.1 An Example   Figure 1. An example sentence pair and its bilingual parse tree","paragraphs":["Once a sentence pair and the part-of-speech (POS) information of the SL sentence are fed into our model, it synchronously parses the sentence pair using unary lexical translation rules (e.g., JJ→positive/積極 where / denotes word correspondence in two languages) and binary SL CFG rules attached with orientation preferences in the target language (e.g., VP→[VP NP]). Also, the leaves of the bilingual parse tree are the word alignment results for this sentence pair.","During bilingual parsing, the model assigns probabilities to substring pairs of the bitext after each of them is associated with possible syntactic labels on the source side. For example, take the sentence pair and its parse in Figure 1, where spaces in the Chinese sentence are used to distinguish the boundaries of segments, ε stands for NULL, and * denotes the inverted orientation of the node’s two children on the target. The substring pair (positive role, 積極 作 用) associated with linguistic symbol NP will be assigned a probability. In this particular parse, the probability is the product of probabilities of the straight binary bITG rule, NP→[JJ NN],"]},{"title":"S","paragraphs":["English sentence: These factors will continue to play a positive role after its return. English POS tags: DT NNS MD VB TO VB DT JJ NN IN PRP$ NN Chinese sentence: 香港 回歸 後 這些 條件 將會 繼續 發揮 積極 作用   Fertility-based Source-Language-biased 5 Inversion Transduction Grammar for Word Alignment and the lexical rules of bITG, JJ→positive/積極, and NN→role/作用. In our model, the higher probability of rule NP→[JJ NN] than the probability of the corresponding inverted rule NP→<JJ NN> does not merely instruct the model to align the two right-hand-side counterparts (i.e., JJ and NN) of two languages in a straight fashion more, but also implies English and Chinese exhibit similar word-order regularity regarding the syntactic constituents.","On the other hand, in the example sentence pair, the beginning half, “These factors will continue to play a positive role,” is translated into the back of the Chinese sentence whereas the ending half, “after its return,” is translated into the beginning. Inverted rules (e.g., S→<S PP>) are designed to capture such systematic differences in the languages’ grammars.","What is more, since only monolingual information is exploited to segment Chinese sentences, it is likely that the word alignments will not be constrained to one-to-one, one-to-zero, and zero-to-one mappings. For instance, 香港 is often segmented as a word in Chinese but needs to be aligned to two words (Hong and Kong) in English, a case of two-to-one mapping. Therefore, we incorporate notion of fertility into our model.","As for the example of “Hong Kong” aligned to “香港”, three possible word-aligning scenarios concerning fertility will be considered at runtime parsing: zero fertility of Hong and singular fertilities of Kong and 香港 where Hong is aligned to NULL but Kong is aligned to 香港; zero fertility of Kong and singular fertilities of Hong and 香港 where Kong is aligned to NULL but Hong is aligned to 香港; singular fertilities of Hong and Kong and dual fertility of 香港 where both Hong and Kong are aligned to 香港.","Taking into account the probabilities of lexical translations, binary grammatical rewrite rules, and fertilities of words, our model manages to find a better parse tree that applies more appropriate synchronous rules to match the structural divergences and more suitable lexical mapping relations (one-to-one, one-to-two, et al.) in two languages. Better parses are more likely to yield better word alignment results.","We actually estimate the probabilities of bITG rules, consisting of unary lexical translation rules and binary SL CFG rules with word orientation on the TL, and those of the fertilities of words from a parallel corpus and an SL CFG. We will discuss the training algorithm in more detail in Section 3."]},{"title":"2.2 Formal Description","paragraphs":["We now formally describe our statistical translation model. To be comparable to previous work, the English-French notation is used throughout this paper. E and F denote the source and target language, respectively, and ie stands for the i-th word in sentence e in language E and jf for the j-th word in sentence f in F. Given"]},{"title":"( )","paragraphs":[",ef ="]},{"title":"( )","paragraphs":["11,mneef f\"\" and the POS tag sequence of e , τ , our model aims   6 Chung-Chi Huang and Jason S. Chang","to construct the most probable bilingual parse tree * tB , satisfying"]},{"title":"(){}","paragraphs":["arg max Pr , , t t B Befτ , with the by-product of word-level correspondences. Intuitively, the probability of a bilingual parse tree tB provided with e, f, and τ is modeled as the product of probabilities associated with grammatical rewrite rules and lexical information:"]},{"title":"()()( )","paragraphs":["Pr , , Pr , , Pr , ,tBef ef efτ ττ=×DA (1) where, by inspecting the parse tree tB , D, and A represent the set of its production rules with syntactic labels on the right hand side (e.g., NP→JJ NN) and the set of rules with word alignments on the right (e.g., JJ→positive/積極), respectively.","For simplicity, we use kα to denote internal nodes (NP, JJ, etc) of the tree tB , whereas we use kβ to denote leaf nodes (e.g., these/這些, positive/積極). Tree nodes in tB can be divided into three groups according to the number of children they are connected to: the first, denoted by set 2N , consists of nodes with two children; the second, denoted by set 1N , is made up of nodes with one child; the last, denoted by set 0N , comprises nodes without a child. For notation convenience, each 2kα ∈ N has two children represented by 2kα and","21kα + , and each 1kα ∈ N has one child kβ .","In our model, the probability of constructing tB is the product of the probabilities of two sources: the first estimating the probabilities of the applied binary bITG rules; the second estimating those of the unary lexical translation rules and the fertilities of words in the tree. Assuming each applied rule is independent of one another, we rewrite the grammatical-related term in Equation (1) as"]},{"title":"() a b( )","paragraphs":["1","221Pr , , P k kkkef λ","ατααα+ ∈≅→"]},{"title":"∏ 2ND","paragraphs":["(2) where"]},{"title":"a b","paragraphs":["can be straight"]},{"title":"[ ]","paragraphs":["or inverted . On the other hand, the lexical-related term in Equation (1) is decomposed into three factors, as shown in Equation (3): one for the product of probabilities of lexical translation rules given τ , another for the product of fertility probabilities of words in e, and the other for the product of fertility probabilities of words in f."]},{"title":"() ()() ( )","paragraphs":["222 1 11Pr , , P P Pi j","k kk mn","e f ijef λλλ","αταβτ φ φ","∈==≅→×Φ=×Φ="]},{"title":"∏∏∏ NA","paragraphs":["(3) In Equation (3),"]},{"title":"Φ","paragraphs":["is the random variable for fertilities of words, and ie"]},{"title":"φ","paragraphs":["and jf"]},{"title":"φ","paragraphs":["denote fertilities of i"]},{"title":"e","paragraphs":["and j"]},{"title":"f","paragraphs":[", respectively. From Equations (1) to (3), we estimate the probability of a parse tree via      Fertility-based Source-Language-biased 7 Inversion Transduction Grammar for Word Alignment"]},{"title":"() a b( )","paragraphs":["1","221Pr , , P k kkktBef λ","ατααα+ ∈≅ →×"]},{"title":"∏ 2N  ()()","paragraphs":["22 1 1PPii k kk m ee i λλ α","αβτ φ ∈=→× Φ=×"]},{"title":"∏∏ N  ( )","paragraphs":["2 1 P j j n f f j","λ φ = Φ="]},{"title":"∏","paragraphs":["(4) in which the sum of the weight 1λ and 2λ is one."]},{"title":"2.3 Runtime Parsing","paragraphs":["In this subsection, we depict a CYK-like parsing algorithm for obtaining the most likely bilingual parse tree given the sentence pair"]},{"title":"( )","paragraphs":[",ef ="]},{"title":"( )","paragraphs":["11,mneef f\"\", the pre-determined POS tag sequence,"]},{"title":"( )","paragraphs":["1,,mtt\" , of sentence e, and the grammar G in E (i.e., SL grammar). Notice that our model is a data-driven one as is Wu (1997). In other words, it synchronously parses sentence pair via bITG rules without a monolingual (SL or TL) parse tree. Figure 2 shows the run-time parsing algorithm. Parsing Algorithm","//Initial Step For 1 ,1im jn≤≤ ≤ ≤ (1)"]},{"title":"( ) ( ) ( )","paragraphs":["222 ,1,, 1, PP1P1ii jiij eti ij j ftefλλλ","δ −−= →×Φ=×Φ= (2) For every in iLtGE→∈ (3)"]},{"title":"()( ) ( )","paragraphs":["222 ,1,, 1, PP1P1i jij eLi i j j fLefλλλ","δ −−= →×Φ=×Φ=","For 1 , 0im jn≤≤ ≤ ≤ (4)"]},{"title":"()()","paragraphs":["22 ,1,,, PP0ii ii eti ijj teλλ","δε− =→×Φ= (5) For every in iLtGE→∈ (6)"]},{"title":"()( )","paragraphs":["22 ,1,,, PP0iieLi i j j Leλ λδε− = →×Φ= For 0 ,1 , syntactic labels on endim jnL E≤≤ ≤ ≤ ∈ (7)"]},{"title":"( ) ( )","paragraphs":["22 ,,, 1, PP0jjLii j j fLfλλ","δε− = →×Φ=","","//Recurrent Step For any possible (s,t,u,v) //1, ,1,s tm uvn≤≤ ≤ ≤ For any possible grammatical label p If (ts≥ and vu≥ ) and not ( ts= and vu= )   8 Chung-Chi Huang and Jason S. Chang (8)"]},{"title":"[]() ()","paragraphs":["1","1 , syntax labels on   ,, ,, , ,, , ,,,, ,, , , , ,,, P , P"]},{"title":"max","paragraphs":["qr E ss t uu v qss uu rs tu v pstuv qssuv rstuu pqr pqr λ λ δδ δ","δδ∈ ′≤≤ ′≤≤ ′′ ′′ ′ ′′′ →× × = →× ×"]},{"title":"⎧ ⎫⎪ ⎪ ⎨ ⎬ ⎪ ⎪⎩⎭ ","paragraphs":["//for backtracking "]},{"title":"[]() ()","paragraphs":["1","1 , syntax labels on   ,, ,, , ,, , ,,,, ,, , , , ,,, P , b P"]},{"title":"arg max","paragraphs":["qr E ss t uu v qssuu rstuv pstuv qssuv rstuu pqr pqr λ λ δδ","δδ∈ ′≤≤ ′≤≤ ′′ ′′ ′ ′′′ →× × = →× ×"]},{"title":"⎧ ⎫⎪ ⎪ ⎨ ⎬ ⎪ ⎪⎩⎭","paragraphs":["(9) Backtrack()"]},{"title":"Figure 2. Run-time parsing.","paragraphs":["During a parse of a sentence pair in our model, a table of ,,,,pstuvδ , the best probability for parsing substring pair"]},{"title":"()","paragraphs":["11,s tu veef f++\"\" attached with a syntactic symbol p on E side, is constructed.","In Step (1) of Figure 2, we compute the probability of a one-to-one word correspondence ijef with ei’s pre-determined POS tag it , according to the probability of the unary bITG rule iijtef→ and the probabilities of fertilities of ie and jf (fertilities are 1s for one-to-one mapping). Since the POS tag it can be derived from some possible phrasal constituents in G (Step (2)) (e.g., NN can be derived from NP), we also compute their associated probabilities (Step (3)). Similarly, in Steps (4) to (7), we calculate the probabilities of the one-to-zero and zero-to-one word correspondences limited to the scope of the sentence pair.","Afterwards, relying on the work done previously, word correspondences and parsing results of longer substring pairs would unveil themselves in a bottom-up manner. In Step (8), s’ divides the substring 1s tee+ \" , labeled as p, into two parts, 1's see+ \" and '1s tee+ \" , with q as a possible grammatical symbol of the first part and r as a possible symbol of the second, while u’ divides the substring 1uvf f+ \" into 1'uuf f+ \" and '1uvf f+ \" . As the substring 1's see+ \" can be aligned to 1'uuf f+ \" or '1uvf f+ \" , both straight and inverted orientation of the SL CFG rules “p→q r ” ought to be considered. Note that the computation in Step (8) does not properly deal with the cases of many-to-one or one-to-many word-level alignments. For many-to-one alignments, ,,, 1,pstu uδ − should further incorporate the parsing candidate:"]},{"title":"()() []() () ()()","paragraphs":["21","22 ,syntax labels on , , 1, 1, ,1,,1,","PmaxP P1P 1u","uu qr E qss u u rs tu u f ff ts p qr ts","λλ λλδ δ ∈","+− +−⎧⎫","⎪⎪","Φ=− × → × ×⎨⎬","Φ= Φ=−−⎪⎪⎩⎭   Fertility-based Source-Language-biased 9 Inversion Transduction Grammar for Word Alignment where ,1,,1,rs tu uδ +− needs to be constructed from many-to-one or one-to-one word mapping relation since words 1s tee+ \" are all aligned to uf . A similar principal applies to one-to-many mapping (i.e., the calculation of ,1,,,ps suvδ − ).","Finally, using the standard CYK backtracking technique, we can find the most probable bilingual parse tree of the sentence pair with word alignment results. The integration of fertilities of words into the model aims to improve the parsing and the word-aligning quality."]},{"title":"2.4 Pruning","paragraphs":["Although the complexity of the described algorithm is polynomial-time (proportion to 33","mn ), the execution time grows rapidly with the increase in the variety of syntactic labels, from three structural labels (Wu, 1997) to the grammatical categories of the source language’s syntax in our model. As a result, pruning techniques are essential to reduce the time spent on parsing.","We adopt pruning in the following two manners. The first pruning technique is, for a given SL substring 1 tsee+ \" and a given TL substring’s length, to only keep parse trees whose probabilities fall within the best N σ× , where N is the number of possible parses for a SL substring 1 tsee+ \" and a length of the TL substring, and σ is a real number between 0 and 1. In other words, we remove inferior parse trees that are not in the set of the best N σ× ones. Since N varies from case to case (depending on the SL substring and the length of TL substring), only the more probable trees within the ratio (i.e., σ ) of N will remain.","The second pruning technique is related to the ratio of the length of the SL and TL substring. ,,,,pstuvδ will not be calculated if ts vu− −","is smaller than ratioθ or larger than 1 ratioθ where 01ratioθ≤ ≤ , since few words will be aligned to more than 1 ratioθ words in another language.","By applying the aforementioned pruning techniques, the time spent on parsing each sentence pair can be reduced by more than half. Empirically, pruning unlikely parses has little affect on the word alignment quality but reduces computational overhead significantly."]},{"title":"3. Probability Estimation","paragraphs":["In this section, we describe how to estimate the probabilities of our unary bITG rules (e.g., JJ→positive/積極) and binary bITG rules (e.g., VP→[VP NP]) which denote the association of bilingual lexical words and model the structural divergences of the two languages, respectively. Figure 3 shows the probabilistic estimation procedure.      10 Chung-Chi Huang and Jason S. Chang (1) =HWA (2)"]},{"title":"()( )","paragraphs":["22 22 11 11For,,,,, ,,,,,,ij ij ij ijr e f L rhs rel r e f L rhs rel′′","∈∈","′′′′′HH have not been considered (3)"]},{"title":"()","paragraphs":["21If 1ii′=− (4) For every in LLLGE′′′→∈ (5)"]},{"title":"( )","paragraphs":["212If 1jjjδ′+ ≤≤+ (6)"]},{"title":"( ){ }","paragraphs":["2 2 11,, ,, ,ji ijre f L LL Straight′′ ′′ ′=∪HH (7)"]},{"title":"( )","paragraphs":["212If 1jjjδ′′+ ≤≤+ (8)"]},{"title":"( ){ }","paragraphs":["22 11,, ,, ,ij ijr e f L L L Inverted′","′ ′′ ′=∪HH","(9) similar strategy applies when 211ii′ = − Incorporate words aligned to null, each of which is denoted using 6-tuple representation, in both languages into H."]},{"title":"( )","paragraphs":["22 11For , , , , ,ij ijre f Lrhsrel ∈ H "]},{"title":"()","paragraphs":["If is not a lexical pairrhs (10)"]},{"title":"[]()()()","paragraphs":["12 12","count *,*,*, , , ; P LR R Straight LRR→= H H  (11)"]},{"title":"()()()","paragraphs":["12 12","count *,*,*, , , ; P LR R Inverted LRR→= H H  Else (12)"]},{"title":"()()()","paragraphs":["count *,*,*, , ,* ; P ij ij Le f Lef→= H H  (13)"]},{"title":"() ( )","paragraphs":["Based on and , estimate P and P via MLEi je fΦΦWA C"]},{"title":"Figure 3. The procedure of probabilistic estimation.","paragraphs":["In Step (1) of our training procedure, an existing word-aligning strategy or tool (e.g., GIZA++) is employed to obtain the word alignments (i.e., WA) of a parallel corpus C. WA comprises elements of the form"]},{"title":"( )","paragraphs":["22 11,, ,, ,ij ijre f Lrhsrel , which represents that the substring","pair"]},{"title":"( )","paragraphs":["121 2,iij jeef f\"\" in sentence pair r has L rhs→ as the production rule leading to the bilingual structure and has rel (either straight or inverted) as the cross-language word-order relation of the constituents of rhs . rhs denotes either a sequence of syntactic labels or a terminating bilingual word pair. Following this format, the example parses of (positive,積極)JJ and (after its return,香港 回歸 後)PP in Figure 1 would be denoted by the 6-tuple (193, 89","89,ef,JJ,positive/ 積極,don’t_care) and (193, 12 3","10 1,ef,PP,IN NP,Inverted) respectively, where 193 is the record number of this sentence pair. Then, we recursively select two sections of a sentence pair, which have not yet been   Fertility-based Source-Language-biased 11 Inversion Transduction Grammar for Word Alignment","paired up, from H (Step (2)). If the SL substring of the first section (i.e., 2 1i ie ) is adjacent to","that of the second (i.e., 2 1i ie","′","′ ) on the right (Step (3)), based on word alignment result (Step (5) and Step(7)), a new straight-ordered (Step (6)) or inverted-ordered (Step (8)) section representing these two will be added into H. Specifically, once the SL substrings are related to some possible binary SL CFG rules, the right-hand-side constituents of these rules will be associated with an orientation on the TL end based on word alignment links. Since our model is a synchronous bilingual parsing one, without a monolingual parse tree, it enumerates all possible syntactic symbols to derive L and L′ in Step (4). Note that, in Steps (5) and (7), δ , a small positive integer, is utilized to tolerate aligning errors introduced by the automatic word aligner or explicitness issue 1","during translation from one language to another, when determining cross-language straight/inverted word order phenomenon.","From Step (10) to Step (12), in which |W| stands for the number of entries in set W and count(p;Q) for the frequency of p in set Q, we estimate probabilities of bITG rules via Maximum Likelihood Estimation. In our model, the probabilities of lexical translation rules (e.g., JJ→positive/積極) and binary bITG rules (e.g., VP→[VP NP]) are estimated from the same source (i.e., H). Alternative probabilistic estimation of these two kinds of rules can be adopted. For example, the probabilities of lexical translation rules can be derived from pure word alignment set WA while those of binary bITG rules can be derived from set H without word-level alignment links. We employ the former estimation approach and, in experiments, it yields satisfying results (see Section 4), suggesting word-order tendencies of the two languages are properly modeled.","Finally, fertility probabilities related to words in both languages are also calculated (Step (13))."]},{"title":"4. Experiments","paragraphs":["In experiments, we trained our model on a large English-Chinese parallel corpus. We examined word alignments produced by our bITG model using the evaluation metrics proposed by Och and Ney (2000). For comparison, we also trained GIZA++, a state-of-the-art word-aligning system, on the same parallel corpus."]},{"title":"4.1 Training Proposed Model","paragraphs":["We used the news portion of Hong Kong Parallel Text2","(HKPT) distributed by Linguistic Data Consortium as our sentence-aligned corpus C, which consisted of 739,919 English-Chinese  1 Some translations may be omitted for conciseness, or some of the function words in one language may have no counterparts in another. 2 LDC2004T08   12 Chung-Chi Huang and Jason S. Chang sentence pairs. The average length was 24.4 words for English and 21.5 words for Chinese.","In our model, English sentences were considered to be the source while Chinese sentences were the target. SL sentences were POS tagged and TL sentences were segmented prior to word alignment. During training (as described in Section 3), we employed a GIZA++ run with default settings to obtain the word alignment set WA and our binary SL CFG G was based upon PTB section 233","production rules distributed by Andrew B. Clegg."]},{"title":"4.2 Evaluation","paragraphs":["To evaluate our statistical translation model, 114 sentence pairs were chosen randomly from the news portion of HKPT as our testing data set. For the sake of execution time, we only selected sentence pairs whose SL and TL length did not exceed 15. Sentence pairs satisfying such a length constraint covered approximately 40% of the sentence pairs in the news portion of HKPT and were expected to be better word aligned via GIZA++.","We examined the word-aligning performance using the metrics of alignment error rate (AER) proposed by Och and Ney (2000), in which the quality of a word alignment result A produced by an automatic system is evaluated by: precision ∩ = AP A , recall ∩ = AS S and"]},{"title":"()","paragraphs":[",; 1AER ∩ +∩ =− + AS AP SPA AS . In AER, S (sure) denotes the set whose alignments are not ambiguous and P (possible) denotes the set consisting of alignments that might or might not exist"]},{"title":"( )","paragraphs":["⊆SP. Thus, human annotations may contain many-to-one, one-to-many, or even many-to-many word alignments. Table 1 shows the experimental results of GIZA++, the BTG model (Wu, 1997), and our fertility-based SL-biased ITG model."]},{"title":"Table 1. Results of test data of different systems","paragraphs":["P R AER F E to F .891 .385 .459 .537 F to E .882 .533 .333 .664 Refined .879 .635 .261 .737 BTG .844 .610 .290 .708","bITG w/o fertility .866 .638 .263 .735","bITG w/ fertility .878 .692 .224 .774  3 http://textmining.cryst.bbk.ac.uk/acl05/   Fertility-based Source-Language-biased 13 Inversion Transduction Grammar for Word Alignment","In this table4",", P, R, and F stand for precision, recall, and F-measure5",", respectively. The performance of the E-to-F alignments (E stands for English and F for Chinese), the F-to-E alignments, and the refined alignments (proposed by Och and Ney (2000)) from both E-to-F and F-to-E directions of GIZA++ are shown in first three rows, along with that of BTG, which also trained on the word-aligning output of GIZA++. The results of our translation model without or with the capability of making many-to-one/one-to-many links are listed in the last two rows.","Compared with the BTG model that does not distinguish the constituent categories and makes the orientation choices merely on lexical evidence (without the information of languages’ grammars), our model without fertility probability which allows for at most one-to-one alignment, as the BTG model does, achieved 9% reduction in the alignment error rate. This indicates that the binary SL CFG rules encoding with TL ordering preference in our model do capture the linguistic information of the languages such as word-order regularities or grammar and do impose more realistic and accurate reordering constraints on word alignment in the language pairs.","Furthermore, in comparison to the refined alignments of both word-aligning directions, our model with the concept of fertility (allowing for many-to-one/one-to-many links), which is quite similar to the refined approach accommodating many-to-many word mappings, increased the recall by 9% while maintaining high precision and achieved 14% alignment error reduction overall (increased F-measure by 5%).","As suggested by Table 1, it is safe to say that the proposed model yields more accurate bilingual parse trees, thus better word alignment quality, by introducing binary CFG rules of a language (i.e., the source language) and fertility notation of IBM models into ITG model."]},{"title":"5. Discussion","paragraphs":["In this section, we examine how the learnt similarities (straight) and differences (inverted) in word orders of two languages aid the word-aligning process of our model by means of the adjacency feature and cohesion constraint, mentioned in Cherry and Lin (2003). Subsequently, to evaluate the possibility of better machine translation quality by providing our model’s output (i.e., word correspondences), we adopt the recently-proposed metric, consistent phrase error rate (CPER) by Ayan and Dorr (2006).   4 S","P is 85.56% in human-annotated test data. 5 Calculated using the formula"]},{"title":"()","paragraphs":["2PR P+R×× .   14 Chung-Chi Huang and Jason S. Chang"]},{"title":"5.1 Straight/Inverted Orientation","paragraphs":["Table 2 shows the accuracy of adjacent alignments made by our model, and the accuracy achieved by the refined approach is shown for comparison. If compared against the gold standard in the sure set (i.e., S in Section 4), our model with bITG rules relatively increased the accuracy by more than 3%, suggesting the similar (or straight) word orientations of the binary syntactic constituents (e.g., JJ and NN) in the languages are better captured in our model than in GIZA++. Note that alignments must have orders before an adjacency feature exists (see Cherry and Lin (2003)) in them. Therefore, an ordering, depending on the position of the English word in the sentence, was imposed to examine the feature."]},{"title":"Table 2. Examination of adjacent links ","paragraphs":["Compared tosure links","Compared to possible links Refined .835 .869","bITG w/ fertility .863 .881","Additionally, we examined whether the inverted binary bITG rules captured the diversities of the two grammars and helped to make correct crossing (or reverse) alignment links or not. For that purpose, we first acquired the dependency relations of the source (i.e., English) sentences via a Stanford parser, and computed the percentage of links violating the cohesion constraint (see Cherry and Lin (2003)). The ratios of having crossing dependencies in the mapped Chinese dependency trees6","are summarized in Table 3. As suggested by Table 3, our model reduced sixteen percent of the links violating the cohesion constraint (compared to the refined approach)."]},{"title":"Table 3. Percentage of links violating cohesion constraint","paragraphs":["Percentage Refined .044","bITG w/ fertility .037","The above statistics indicate that the probabilities related to straight and inverted word orders of bITG rules in our model not only impose a more suitable alignment constraint but properly model the systematic similarities and differences in two languages’ grammars.   6 Chinese dependency trees are mapped from English dependency trees based on word correspondences.   Fertility-based Source-Language-biased 15 Inversion Transduction Grammar for Word Alignment"]},{"title":"5.2 CPER","paragraphs":["According to Ayan and Dorr (2006), the intrinsic evaluation metric of AER (Och and Ney, 2000) examines only the quality of word-level alignments and correlates poorly with the MT-community metric—BLEU score. As a result, we exploited consistent phrase error rate (CPER) to evaluate words alignments in the context of machine translation. CPER is reported to better correlate with translation quality (the smaller the CPER is, the better the translation quality) in that it evaluates phrase-level alignments and in that phrase-level alignments (bilingual phrase pairs) constitute the key essences of a MT system. In Ayan and Dorr (2006), precision (P), recall (R), and CPER are computed via:","P, RA GAG AG PP PP PP","∩∩ ==, and 2PR CPER 1 PR× × =− + where AP and GP stand for two sets of phrases generated by an automatic alignment A and manual alignment G, respectively. In Table 4, the proposed fertility-based source-language-based ITG model yielded the lowest CPER. This indicates that MT systems, accepting our word alignment output, are more likely to lead to better translation performance."]},{"title":"Table 4. Reports on CPER","paragraphs":["P R CPER E to F .479 .383 .574 F to E .544 .518 .470 Refined .573 .606 .411 BTG .569 .569 .431","bITG w/o fertility .598 .597 .402","bITG w/ fertility .624 .626 .375"]},{"title":"6. Conclusion and Future Work","paragraphs":["To combine the strengths of the competing models, a thought-provoking fusion of IBM-style fertility with syntax-based ITG model is described. In our model, the orientation probabilities of the binary SL-based ITG rules are automatically estimated based on a word-aligned parallel corpus and are devised to better capture structural divergences of the involved languages. The proposed bITG model with fertility reduces AER by 14% and 23%, and reduces CPER by 9% and 13% compared to GIZA++ and Wu’s BTG (1997), respectively. Lower CPER suggests MT systems chained after our statistical translation model are likely to yield better translation quality. In this paper, the performance of ITG models trained on large-scale bitexts is shown   16 Chung-Chi Huang and Jason S. Chang for the first time with quite encouraging results.","As for future work, we would like to explore methods (e.g. (Brown, 1992)) for partitioning long sentences into shorter ones so that the time spent on bilingual parsing in our model can be reduced. We also like to see whether word-aligning quality can be further improved if our bITG rules are lexicalized, especially when lexical contents play an important role in determining word orders of the languages."]},{"title":"References","paragraphs":["Ayan, N. F. & Dorr, B. J. (2006). Going beyond AER: an extensive analysis of word alignments and their impact on MT. In Proceedings of ACL-2006, 9-16.","Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., Mercer, R. L., & Mohanty, S. (1992). Dividing and conquering long sentence in a translation system. In Proceedings of the Workshop on Speech and Natural Language, 267-271.","Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2), 263-311.","Cherry, C. & Lin, D. (2003). A probability model to improve word alignment. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, 88-95.","Chiang, D. (2005). A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd","Annual Meeting of the Association for Computational Linguistics, 263-270.","Clegg, A. B. & Shepherd, A. (2005). Evaluating and integrating Treebank parsers on a biomedical corpus. In Association for Computational Linguistics Workshop on software 2005.","Galley, M., Hopkins, M., Knight, K., & Marcu, D. (2004). What’s in a translation rule? In Proceedings of HLT/NAACL-2004, 273-280.","Galley, M., Graehl, J., Knight, K., Marcu, D., DeNeefe, S., Wang, W. et al. (2006). Scable inference and training of context-rich syntactic translation models. In Proceedings of the 44th","Annual Conference of the Association for Computational Linguistics, 961-968.","Gildea, D. (2004). Dependencies vs. constituents for tree-based alignment. In Proceedings of the EMNLP, 214-221.","Liu, Y., Liu, Q., & Lin, S. (2006). Tree-to-string alignment template for statistical machine translation. In Proceedings of the 44th","Annual Conference of the Association for Computational Linguistics, 609-616.","Och, F. J. & Ney, H. (2000). Improved statistical alignment models. In Proceedings of the 38th"," Annual Conference of ACL-2000, 440-447.","Och, F. J. & Ney, H. (2004). The alignment template approach to statistical machine translation. Computational Linguistics, 30(4), 417-449.   Fertility-based Source-Language-biased 17 Inversion Transduction Grammar for Word Alignment","Toutanova, K., Ilhan, H. T., & Manning, C. D. (2002). Extentions to HMM-based statistical word alignment models. In Proceedings of the Conference on Empirical Methods in Natural Processing Language, 87-94.","Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics, 836-841.","Wu, D. (1997). Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3), 377-403.","Yamada, K. & Knight, K. (2001). A syntax-based statistical translation model. In Proceedings of the 39th","Annual Conference of ACL-2001, 523-530.","Zens, R. & Ney, H. (2003). A comparative study on reordering constraints in statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 144-151.","Zhang, H. & Gildea, D. (2004). Syntax-based alignment: supervised or unsupervised? In Proceedings of the 20th","International Conference on Computational Linguistics, 418-424.","Zhang, H. & Gildea, D. (2005). Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings of the 43rd","Annual Meeting of the ACL, 475-482.","Zhang, H., Huang, L., Gildea, D., & Knight, K. (2006). Synchronous binarization for machine translation. In Proceedings of the NAACL-HLT, 256-263.   18 Chung-Chi Huang and Jason S. Chang "]}]}