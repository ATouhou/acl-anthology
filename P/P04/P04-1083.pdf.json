{"sections":[{"title":"Statistical Machine Translation by Parsing I. Dan Melamed Computer Science Department New York University New York, NY, U.S.A. 10003-6806 lastname @cs.nyu.edu Abstract","paragraphs":["In an ordinary syntactic parser, the input is a string, and the grammar ranges over strings. This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. Such algorithms can infer the synchronous structures hidden in parallel texts. It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statistical machine translation system."]},{"title":"1 Introduction","paragraphs":["A parser is an algorithm for inferring the structure of its input, guided by a grammar that dictates what structures are possible or probable. In an ordinary parser, the input is a string, and the grammar ranges over strings. This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. Such inference algorithms can perform various kinds of analysis on parallel texts, also known as multitexts.","Figure 1 shows some of the ways in which ordinary parsing can be generalized. A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these structures.1","When a parser’s input can have fewer dimensions than the parser’s grammar, we call it a translator. When a parser’s grammar can have fewer dimensions than the parser’s input, we call it a synchronizer. The corresponding processes are called translation and synchronization. To our knowledge, synchronization has never been explored as a class of algorithms. Neither has the relationship between parsing and word alignment. The relationship between translation and ordinary parsing was noted a long time 1","A suitable set of ordinary parsers can also infer the syntactic structure of each component, but cannot infer the correspondence relation between these structures. translation synchronization synchronous parsing 1 parsing 32 2 3 1 ... ... ordinary I = dimensionality of input D = dimensionality of grammar","synchronization (I >= D) parsingsynchronous (D=I) word alignment translation (D >= I) ordinary parsing (D=I=1)","generalized parsing (any D; any I) Figure 1: Generalizations of ordinary parsing. ago (Aho & Ullman, 1969), but here we articulate it in more detail: ordinary parsing is a special case of synchronous parsing, which is a special case of translation. This paper offers an informal guided tour of the generalized parsing algorithms in Figure 1. It culminates with a recipe for using these algorithms to train and apply a syntax-aware statistical machine translation (SMT) system."]},{"title":"2 Multitext Grammars and Multitrees","paragraphs":["The algorithms in this paper can be adapted for any synchronous grammar formalism. The vehicle for the present guided tour shall be multitext grammar (MTG), which is a generalization of context-free grammar to the synchronous case (Melamed, 2003). We shall limit our attention to MTGs in Generalized Chomsky Normal Form (GCNF) (Melamed et al., 2004). This normal form allows simpler algorithm descriptions than the normal forms used by Wu (1997) and Melamed (2003).","In GCNF, every production is either a terminal production or a nonterminal production. A nonterminal production might look like this:         A D(2) B E   (1) There are nonterminals on the left-hand side (LHS) and in parentheses on the right-hand side (RHS). Each row of the production describes rewriting in a different component text of a multitext. In each row, a role template describes the relative order and contiguity of the RHS nonterminals. E.g., in the top row, [1,2] indicates that the first nonterminal (A) precedes the second (B). In the bottom row, [1,2,1] indicates that the firstnonterminal both precedes and follows the second, i.e. D is discontinuous. Discontinuous nonterminals are annotated with the number of their contiguous segments, as in   ",". The","(“join”) operator rearranges the nonterminals in each component according to their role template. The nonterminals on the RHS are written in columns called links. Links express translational equivalence. Some nonterminals might have no translation in some components, indicated by (), as in the 2nd row. Terminal productions have exactly one “acti ve” component, in which there is exactly one terminal on the RHS. The other components are inactive. E.g.,","   "," (2)","The semantics of","are the usual semantics of rewriting systems, i.e., that the expression on the LHS can be rewritten as the expression on the RHS. However, all the nonterminals in the same link must be rewritten simultaneously. In this manner, MTGs generate tuples of parse trees that are isomorphic up to reordering of sibling nodes and deletion. Figure 2 shows two representations of a tree that might be generated by an MTG in GCNF for the imperative sentence pair Wash the dishes / Pasudu moy . The tree exhibits both deletion and inversion in translation. We shall refer to such multidimensional trees as multitrees.","The different classes of generalized parsing algorithms in this paper differ only in their grammars and in their logics. They are all compatible with the same parsing semirings and search strategies. Therefore, we shall describe these algorithms in terms of their underlying logics and grammars, abstracting away the semirings and search strategies, in order to elucidate how the different classes of algorithms are related to each other. Logical descriptions of inference algorithms involve inference rules:  "," means that  can be inferred from  and ",". An item that appears in an inference rule stands for the proposition that the item is in the parse chart. A production rule that appears in an inference rule stands for the proposition that the production is in the grammar. Such specifications are nondeter-       ","  ","Wash "," "," ","  moy "," ","     ","the   ","  ","  ","dishes ","  ","  Pasudu  Figure 2: Above: A tree generated by a 2-MTG in English and (transliterated) Russian. Every internal node is annotated with the linear order of its children, in every component where there are two children. Below: A graphical representation of the same tree. Rectangles are 2D constituents. dishesthe Wash moy Pasudu S NP NV WASH D DISH PAS MIT V NNP S ministic: they do not indicate the order in which a parser should attempt inferences. A deterministic parsing strategy can always be chosen later, to suit the application. We presume that readers are familiar with declarative descriptions of inference algorithms, as well as with semiring parsing (Goodman, 1999)."]},{"title":"3 A Synchronous CKY Parser","paragraphs":["Figure 3 shows Logic C. Parser C is any parser based on Logic C. As in Melamed (2003)’s Parser A, Parser C’s items consist of a  -dimensional label vector  "," and a  -dimensional d-span vector   .2","The items con-","tain d-spans, rather than ordinary spans, because","2","Superscripts and subscripts indicate the range of dimen-","sions of a vector. E.g.,","","is a vector spanning dimensions 1","through",". See Melamed (2003) for definitionsof cardinality,","d-span, and the operators","and",". Parser C needs to know all the boundaries of each item, not just the outermost boundaries. Some (but not all) dimensions of an item can be inactive, de-noted ",", and have an empty d-span (). The input to Parser C is a tuple of  parallel texts,","with lengths       . The notation  ","   ","","indicates that the Goal item must span the input from the left of the firstword to the right of the last word in each component   "," ",". Thus, the Goal item must be contiguous in all dimensions.","Parser C begins with an empty chart. The only inferences that can firein this state are those with no antecedent items (though they can have antecedent production rules). In Logic C,  ","","is the value that the grammar assigns to the terminal production   . The range of this value depends on the","semiring used. A Scan inference can firefor the","th","word  ","in component","for every terminal pro-","duction in the grammar where","","","appears in the","","th component. Each Scan consequent has exactly","one active d-span, and that d-span always has the","form     ","because such items always span one word, so the distance between the item’s boundaries is always one.","The Compose inference in Logic C is the same as in Melamed’s Parser A, using slightly different notation: In Logic C, the function","","    "," represents the value that the grammar assigns to the nonterminal production     ","","",". Parser C can compose two items if their labels appear on the RHS of a production rule in the grammar, and if the contiguity and relative order of their intervals is consistent with the role templates of that production rule. Item Form:","       ","Goal:     ","      Inference Rules Scan component d,   ","  :","     ","      ","  ","   ","  ","          ","      ","   ","","      ","      Compose: ","    ","     ","","              ","    ","     Figure 3: Logic C (“C” for CKY) These constraints are enforced by the d-span operators","and",".","Parser C is conceptually simpler than the synchronous parsers of Wu (1997), Alshawi et al. (2000), and Melamed (2003), because it uses only one kind of item, and it never composes terminals. The inference rules of Logic C are the multidimensional generalizations of inference rules with the same names in ordinary CKY parsers. For example, given a suitable grammar and the input (imperative) sentence pair Wash the dishes / Pasudu moy, Parser C might make the 9 inferences in Figure 4 to infer the multitree in Figure 2. Note that there is one inference per internal node of the multitree.","Goodman (1999) shows how a parsing logic can be combined with various semirings to compute different kinds of information about the input. Depending on the chosen semiring, a parsing logic can compute the single most probable derivation and/or its probability, the","most probable derivations and/or their total probability, all possible derivations and/or their total probability, the number of possible derivations, etc. All the parsing semirings catalogued by Goodman apply the same way to synchronous parsing, and to all the other classes of algorithms discussed in this paper.","The class of synchronous parsers includes some algorithms for word alignment. A translation lexicon (weighted or not) can be viewed as a degenerate MTG (not in GCNF) where every production has a link of terminals on the RHS. Under such an MTG, the logic of word alignment is the one in Melamed (2003)’s Parser A, but without Compose inferences. The only other difference is that, instead of a single item, the Goal of word alignment is any set of items that covers all dimensions of the input. This logic can be used with the expectation semiring (Eisner, 2002) to find the maximum likelihood estimates of the parameters of a word-to-word translation model.","An important application of Parser C is parameter estimation for probabilistic MTGs (PMTGs). Eisner (2002) has claimed that parsing under an expectation semiring is equivalent to the Inside-Outside algorithm for PCFGs. If so, then there is a straightforward generalization for PMTGs. Parameter estimation is beyond the scope of this paper, however. The next section assumes that we have an MTG, probabilistic or not, as required by the semiring."]},{"title":"4 Translation","paragraphs":["A ","-MTG can guide a synchronous parser to infer the hidden structure of a ","-component multitext. Now suppose that we have a ","-MTG and an","input multitext with only components,  .      ","      "," "," ","","  ","","     ","  ","","     ","","  ",""," ","  ","","       ","         ","   ","","     "," ","       ","  "," "," ","","  ","","   ","  ","  "," "," ","  ","  ","  ","  ",""," ","   ",""," ","  ","  ",""," ",""," "," ","      ","   ","  ","   "," ","   ","",""," ","","      ","  ","","   "," ","   ","",""," ","","   ","        "," "," ",""," ","","  ","","","   ","    "," ","","  ",""," ","  ","  ","  ","   ","        ","  "," "," ","    ","","  ","  "," ","    ","","  ","  "," ","  ","",""," ",""," "," ","         "," "," ",""," "," ","","  ","",""," ",""," Figure 4: Possible sequence of inferences of Parser C on input Wash the dishes / Pasudu moy. When some of the component texts are missing, we can ask the parser to infer a ","-dimensional multitree that includes the missing components. The resulting multitree will cover the","input components/dimensions among its  dimensions. It will also express the "," ","output components/dimensions, along with their syntactic structures. Item Form:","     ","","","Goal:","     ","      Inference Rules","Scan component  ","   :","     ","      ","  ","    ","  ","             ","     ","    ","     ","","      ","        ","Load component",","," ","","  :","","       ","      ","     ","   ","               ","   ","    ","             Compose:  ","      ","  ","","   "," ","  "," ","                ","           Figure 5: Logic CT (“T” for Translation)","Figure 5 shows Logic CT, which is a generalization of Logic C. Translator CT is any parser based on Logic CT. The items of Translator CT have a","-dimensional label vector, as usual. However, their d-span vectors are only","-dimensional, because it is not necessary to constrain absolute word positions in the output dimensions. Instead, we need only constrain the cardinality of the output nonterminals, which is accomplished by the role templates ",""," ","in the","term. Translator CT scans only the input components. Terminal productions with active output components are simply loaded from the grammar, and their LHSs are added to the chart without d-span information. Composition proceeds as before, except that there are no constraints on the role templates in the output dimensions – the role templates in  ","","","","are free variables.","In summary, Logic CT differs from Logic C as","follows:"," Items store no position information (d-spans) for the output components."," For the output components, the Scan inferences are replaced by Load inferences, which are not constrained by the input."," The Compose inference does not constrain the d-spans of the output components. (Though it still constrains their cardinality.) We have constructed a translator from a synchronous parser merely by relaxing some constraints on the output dimensions. Logic C is just Logic CT for the special case where"," ",". The relationship between the two classes of algorithms is easier to see from their declarative logics than it would be from their procedural pseudocode or equations.","Like Parser C, Translator CT can Compose items that have no dimensions in common. If one of the items is active only in the input dimension(s), and the other only in the output dimension(s), then the inference is, de facto, a translation. The possible translations are determined by consulting the grammar. Thus, in addition to its usual function of evaluating syntactic structures, the grammar simultaneously functions as a translation model.","Logic CT can be coupled with any parsing semiring. For example, under a boolean semiring, this logic will succeed on an","-dimensional input if and only if it can infer a ","-dimensional multitree whose root is the goal item. Such a tree would contain a ","  ","-dimensional translation of the input. Thus, under a boolean semiring, Translator CT can determine whether a translation of the input exists.","Under an inside-probability semiring, Translator CT can compute the total probability of all multitrees containing the input and its translations in the","","output components. All these derivation trees, along with their probabilities, can be efficientlyrepresented as a packed parse forest, rooted at the goal item. Unfortunately, findingthe most probable output string still requires summing probabilities over an exponential number of trees. This problem was shown to be NP-hard in the one-dimensional case (Sima’an, 1996). We have no reason to believe that it is any easier when  ","",". The Viterbi-derivation semiring would be the most often used with Translator CT in practice. Given a ","-PMTG, Translator CT can use this semiring to find the single most probable ","-dimensional multitree that covers the  -dimensional input. The multitree inferred by the translator will have the words of both the input and the output components in its leaves. For example, given a suitable grammar and the input Pasudu moy, Translator CT could infer the multitree in Figure 2. The set of inferences would be exactly the same as those listed in Figure 4, except that the items would have no d-spans in the English component.","In practice, we usually want the output as a string tuple, rather than as a multitree. Under the various derivation semirings (Goodman, 1999), Translator CT can store the output role templates     ","in each internal node of the tree. The intended order-ing of the terminals in each output dimension can be assembled from these templates by a linear-time linearization post-process that traverses the finished multitree in postorder.","To the best of our knowledge, Logic CT is the first published translation logic to be compatible with all of the semirings catalogued by Goodman (1999), among others. It is also the first to simultaneously accommodate multiple input components and multiple output components. When a source document is available in multiple languages, a translator can benefit from the disambiguating information in each. Translator CT can take advantage of such information without making the strong independence assumptions of Och & Ney (2001). When output is desired in multiple languages, Translator CT offers all the putative benefitsof the interlingual approach to MT, including greater efficiency and greater consistency across output components. Indeed, the language of multitrees can be viewed as an interlingua."]},{"title":"5 Synchronization","paragraphs":["We have explored inference of","-dimensional multitrees under a  -dimensional grammar, where    . Now we generalize along the other axis of Figure 1(a). Multitext synchronization is most often used to infer","-dimensional multitrees without the benefit of an","-dimensional grammar. One application is inducing a parser in one language from a parser in another (Lü et al., 2002). The application that is most relevant to this paper is bootstrapping an  -dimensional grammar. In theory, it is possible to induce a PMTG from multitext in an unsupervised manner. A more reliable way is to start from a corpus of multitrees — a multitreebank.3","We are not aware of any multitreebanks at this time. The most straightforward way to create one is to parse some multitext using a synchronous parser, such as Parser C. However, if the goal is to bootstrap an","-PMTG, then there is no","-PMTG that can evaluate the","terms in the parser’s logic. Our solu-tion is to orchestrate lower-dimensional knowledge sources to evaluate the","terms. Then, we can use the same parsing logic to synchronize multitext into a multitreebank.","To illustrate, we describe a relatively simple synchronizer, using the Viterbi-derivation semiring.4 Under this semiring, a synchronizer computes the single most probable multitree for a given multitext. 3 In contrast, a parallel treebank might contain no informa-","tion about translational equivalence. 4 The inside-probability semiring would be required for","maximum-likelihood synchronization. ya kota kormil I fed the cat Figure 6: Synchronization. Only one synchronous dependency structure (dashed arrows) is compatible with the monolingual structure (solid arrows) and word alignment (shaded cells). If we have no suitable PMTG, then we can use other criteria to search for trees that have high probability. We shall consider the common synchronization scenario where a lexicalized monolingual grammar is available for at least one component.5","Also, given a tokenized set of","-tuples of parallel sentences, it is always possible to estimate a word-to-word translation model  ","         (e.g., Och & Ney, 2003).6","A word-to-word translation model and a lexicalized monolingual grammar are sufficient to drive a synchronizer. For example, in Figure 6 a monolingual grammar has allowed only one dependency structure on the English side, and a word-to-word translation model has allowed only one word alignment. The syntactic structures of all dimensions of a multitree are isomorphic up to reordering of sibling nodes and deletion. So, given a fixed correspondence between the tree leaves (i.e. words) across components, choosing the optimal structure for one component is tantamount to choosing the optimal synchronous structure for all components.7 Ignoring the nonterminal labels, only one dependency structure is compatible with these constraints – the one indicated by dashed arrows. Bootstrapping a PMTG from a lower-dimensional PMTG and a word-to-word translation model is similar in spirit to the way that regular grammars can help to estimate CFGs (Lari & Young, 1990), and the way that simple translation models can help to bootstrap more sophis