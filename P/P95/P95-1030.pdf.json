{"sections":[{"title":"New Techniques for Context Modeling","paragraphs":["Eric Sven Ristad and Robert G. Thomas","Department of Computer Science Princeton University"]},{"title":"{ristad, rgt }~cs. princeton, edu","paragraphs":["Abstract We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic. Together these techniques result in language models that have few states, even fewer parameters, and low message entropies. 1 Introduction Current approaches to automatic speech and handwriting transcription demand a strong language model with a small number of states and an even smaller number of parameters. If the model entropy is high, then transcription results are abysmal. If there are too many states, then transcription be-comes computationally infeasible. And if there are too many parameters; then \"overfitting\" occurs and predictive performance degrades.","In this paper we introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic. Together these techniques result in language models that have few states, even fewer parameters, and low message entropies. For example, our techniques achieve a message entropy of 1.97 bits/char on the Brown corpus using only 89,325 parameters. By modestly increasing the number of model parameters in a principled manner, our techniques are able to further reduce the message entropy of the Brown Corpus to 1.91 bits/char. 1 In contrast, the character 4-gram model requires 250 times as many parameters in order to achieve a message entropy of only 2.47 bits/char. Given the logarithmic nature of codelengths, a savings of 0.5 bits/char is quite significant. The fact that our model performs significantly better using vastly fewer parameters argues","1The only change to our model selection procedure is to replace the incremental cost formula ALe(w, ~', a) with a constant cost of 2 bits/extension. This small change reduces the test message entropy from 1.97 to 1.91 bits/char but it also quadruples the number of model parameters and triples the total codelength. that it is a much better probability model of natural language text.","Our first two techniques - nonmonolonic contexts and exlension modeling - are generalizations of the traditional context model (Cleary and Witten 1984; Rissanen 1983,1986). Our third technique - the divergence heuristic - is an incremental model selection criterion based directly on Rissanen's (1978) minimum description length (MDL) principle. The MDL principle states that the best model is the simplest model that provides a compact description of the observed data.","In the traditional context model, every prefix and every suffix of a context is also a context. Three consequences follow from this property. The first consequence is that the context dictionary is un-necessarily large because most of these contexts are redundant. The second consequence is to attenuate the benefits of context blending, because most contexts are equivalent to their maximal proper suffixes. The third consequence is that the length of the longest candidate context can increase by at most one symbol at each time step, which impairs the model's ability to model complex sources. In a nonmonotonic model, this constraint is relaxed to allow compact dictionaries, discontinuous backoff, and arbitrary context switching.","The traditional context model maps every history to a unique context. All symbols are predicted using that context, and those predictions are estimated using the same set of histories. In contrast, an extension model maps every history to a sel of contexts, one for each symbol in the alphabet. Each symbol is predicted in its own context, and the model's current predictions need not be estimated using the same set of histories. This is a form of parameter tying that increases the accuracy of the model's predictions while reducing the number of free parameters in the model.","As a result of these two generalizations, nonmonotonic extension models can outperform their equivalent context models using significantly fewer parameters. For example, an order 3 n-gram (ie., the 4-gram) requires more than 51 times as many con-"]},{"title":"220","paragraphs":["texts and 787 times as many parameters as the order 3 nonmonotonic extension model, yet already performs worse on the Brown corpus by 0.08 bits/char.","Our third contribution is the divergence heuristic, which adds a more specific context to the model only when it reduces the codelength of the past data more than it increases the codelength of the model. In contrast, the traditional selection heuristic adds a more specific context to the model only if it's entropy is less than the entropy of the more general context (Rissanen 1983,1986). The traditional minimum entropy heuristic is a special case of the more effective and more powerful divergence heuristic. The divergence heuristic allows our models to generalize from the training corpus to the testing corpus, even for nonstationary sources such as the Brown corpus.","The remainder of our article is organized into three sections. In section 2, we formally define the class of extension models and present a heuristic model selection algorithm for that model class based on the divergence criterion. Next, in section 3, we demonstrate the efficacy of our techniques on the Brown Corpus, an eclectic collection of English prose containing approximately one million words of text. Section 4 discusses possible improvements to the model class. 2 Extension Model Class This section consists of four parts. In 2.1, we formally define the class of extension models and prove that they satisfy the axioms of probability. In 2.2, we show to estimate the parameters of an extension model using Moffat's (1990) \"method C.\" In 2.3, we provide codelength formulas for our model class, based on efficient enumerative codes. These codelength formulas will be used to match the complexity of the model to the complexity of the data. In 2.4, we present a heuristic model selection algorithm that adds parameters to an extension model only when they reduce the codelength of the data more than they increase the codelength of the model. 2.1","Model Class Definition Formally, an"]},{"title":"extension model","paragraphs":["¢ : (E, D, E, A) consists of a finite alphabet E, [E[ = m, a dictionary D of contexts, D C E*, a set of available context extensions E, E C D x E, and a probability function I : E ---* [0, 1]. For every context w in"]},{"title":"D, E(w)","paragraphs":["is the set of symbols available in the context w and A(~rlw ) is the conditional probability of the symbol c~ in the context w. Note that )--]o~ A(c~[w) < 1 for all contexts w in the dictionary D.","The probability /5(h1¢ ) of a string h given the model ¢, h • E', is calculated as a chain of conditional probabilities (1) /5(h{¢) --\""]},{"title":"~(hnlhl...hn_l,¢)~(hl...h,~_ll¢)","paragraphs":["(1) while the conditional probability ih(elh, ¢) of a single symbol ~r after the history h is defined as (2). { ~(~rlh ) if (h,a) ~ E /3(a]h, ¢) -"]},{"title":"5(h)~(a]h2h3...h,,","paragraphs":["¢) otherwise"]},{"title":"(2)","paragraphs":["The expansion factor 6(h) ensures that/5(.]h, ¢) is a probability function if/5(-Ih2.., h,~, ¢) is a probability function."]},{"title":"1 - )~(E(h)[h)","paragraphs":["(3)"]},{"title":"6(h)- 1- ~(E(h)Ih2...h,~,¢)","paragraphs":["Note that"]},{"title":"E(h)","paragraphs":["represents a set of symbols, and so by a slight abuse of notation"]},{"title":")~(E(h)Ih )","paragraphs":["denotes ~]~eE(h) A(a[h), ie., the sum of A(alh ) over all ~ in"]},{"title":"E(h).","paragraphs":["Examplel. Let E:{0,1},D: {e,\"0\" },E(e) - {0, 1}, E(\"0\") -= {0}. Suppose"]},{"title":"A(010 = 1⁄2,","paragraphs":["X(lle)","= 1⁄2, and A(01\"0\" ) = 3 Then 6(\"0\") = 1/1 _ 1 ~. ~-y","and i6(11\"0\",¢ ) : 5(\"0\") (l[e) -","1","The fundamental difference between a context model and an extension model lies in the inputs to the context selection rule, not its outputs. The traditional context model includes a selection rule s : E* --~ D whose only input is the history. In contrast, an extension model includes a selection rule s : E* x E --+ D whose inputs include the past history and the symbol to be predicted. This distinction is preserved even if we generalize the selection rule to select a set of candidate contexts. Under such a generalization, the context model would map every history to a set of candidate contexts, ie., s : E* ---* 2 D , while an extension model would map every history and symbol to a set of candidate contexts, ie., s : E* x E --* 2 D.","Our extension selection rule s : E* x E --+ D is defined implicitly by the set E of extensions currently in the model. The recursion in (2) says that each symbol should be predicted in its longest candidate context, while the expansion factor 6(h) says that longer contexts in the model should be trusted more than shorter contexts when combining the predictions from different contexts.","An extension model ¢ is"]},{"title":"valid","paragraphs":["iff it satisfies the following constraints: a. eC DAE(c) :E c. Vw • D [E(w) : E =¢, ~oez(~o) A(~iw) : 1]"]},{"title":"(4)","paragraphs":["These constraints suffice to ensure that the model ¢ defines a probability function. Constraint (4a) states that every symbol has the empty string as a context. This guarantees that every symbol will always have at least one context in every history and that the recursion in (2) will terminate. Constraint (45) states that the sum of the probabilities of the extensions"]},{"title":"E(w)","paragraphs":["available in in a given context w cannot sum 221 to more than unity. The third constraint (4c) states that the sum of the probabilities of the extensions"]},{"title":"E(w)","paragraphs":["must sum exactly to unity when every symbol is available in that context (ie., when"]},{"title":"E(w) : E).","paragraphs":["Lemma 2.1 VyEE* Vcr62E [ fi(~]lY) : 1 :~/](EIqy) = 1 ] Proof. By the definition of"]},{"title":"6(~ry).","paragraphs":["Theorem 1"]},{"title":"If an exlension model ¢ is valid, then","paragraphs":["vn ]S,es,, = 1. Proof. By induction on n. For the base case, n : 1 and the statement is true by the definition of validity (constraints 4a and 4c). The induction step is true by lemma 2.1 and definition (1). [] 2.2 Parameter Estimation Let us now estimate the conditional probabilities A(.[-) required for an extension model. Traditionally, these conditional probabilities are estimated using string frequencies obtained from a training corpus. Let c(c~[w) be the number of times that the symbol followed the string w in the training corpus, and let"]},{"title":"c(w)","paragraphs":["be the sum ~es"]},{"title":"c(crlw)","paragraphs":["of all its conditional frequencies.","Following Moffat (1990), we first partition the conditional event space E in a given context w into two subevents: the symbols"]},{"title":"q(w)","paragraphs":["that have previously occurred in context w and those that q(w) that have not. Formally,"]},{"title":"q(w) - {(r :","paragraphs":["c(,r[w) > 0} and ~(w) - E -"]},{"title":"q(w).","paragraphs":["We estimate"]},{"title":")~c(q(w)lw ) as e(w)/(c(w) + #(w))","paragraphs":["and"]},{"title":")~c(4(w)[w) as #(w)/(c(w)+ #(w))","paragraphs":["where #(w) is the total weight assigned to the novel events q(w) in the context w. Currently, we calculate #(w) as min([q(w)l, Iq(w)[) so that highly variable contexts receive more flattening, but no novel symbol in ~(w) receives more than unity weight. Next,"]},{"title":")~c(alq(w ), w)","paragraphs":["is estimated as"]},{"title":"c(alw)/c(w )","paragraphs":["for the previously seen symbols c~ e"]},{"title":"q(w)","paragraphs":["and"]},{"title":"Ac((r]4(w), w)","paragraphs":["is estimated uniformly as 1/[4(w)[ for the novel symbols ~r • 4(w). Combining these estimates, we obtain our overall estimate (5)."]},{"title":"c( lw) c(w) + #(w)","paragraphs":["if c~ •"]},{"title":"q(w)","paragraphs":["Ae (alw) = #(w) otherwise + O) Unlike Moffat, our estimate (5) does not use escape probabilities or any other form of context blending. All novel events 4(w) in the context w are assigned uniform probability. This is suboptimal but simpler.","We note that our frequencies are incorrect when used in an extension model that contains contexts that are proper suffixes of each other. In such a situation, the shorter context is only used when the longer context was"]},{"title":"not","paragraphs":["used. Let y and"]},{"title":"xy","paragraphs":["be two distinct contexts in a model ¢. Then the context y will never be used when the history is E*xy. Therefore, our estimate of A(.ly ) should be conditioned on the fact that the longer context"]},{"title":"xy","paragraphs":["did not occur. The interaction between candidate contexts can be-come quite complex, and we consider this problem in other work (Ristad and Thomas, 1995).","Parameter estimation is only a small part of the overall model estimation problem. Not only do we have to estimate the parameters for a model, we have to find the right parameters to use! To do this, we proceed in two steps. First, in section 2.3, we use the minimum description length (MDL) principle to quantify the total merit of a model with respect to a training corpus. Next, in section 2.4, we use our MDL codelengths to derive a practical model selection algorithm with which to find a good model in the vast class of all extension models. 2.3","Codelength Formulas The goal of this section is to establish the proper tension between model complexity and data complexity, in the fundamental units of information. Although the MDL framework obliges us to propose particular encodings for the model and the data, our goal is not to actually encode the data or the model.","Given an extension model ¢ and a text corpus T, ITI = t, we define the total codelength"]},{"title":"L(T,¢I(I))","paragraphs":["relative to the model class ~ using a 2-part code."]},{"title":"L(T,","paragraphs":["¢[(I)) : L(¢I~ ) +"]},{"title":"L(TI¢ , ~)","paragraphs":["Since conditioning on the model class (I) is always understood, we will henceforth suppress it in our notation.","Firstly, we will encode the text T using the probability model ¢ and an arithmetic code, obtaining the following codelength. L(T[¢) = - logif(Tl¢ ) Next, we encode the model ¢ in three parts: the context dictionary as"]},{"title":"L(D),","paragraphs":["the extensions as"]},{"title":"L(EID),","paragraphs":["and the conditional frequencies c(.[-) as"]},{"title":"L(e[D, E).","paragraphs":["The dictionary D of contexts forms a suffix tree containing ni vertices with branching factor i. The","m tree contains"]},{"title":"n = )--~i=l ni","paragraphs":["internal vertices and no leaf vertices. There are (no + nl + ... +"]},{"title":"nm - 1)!/no!nl!...nm!","paragraphs":["such trees (Knuth, 1986:587). Accordingly, this tree may be encoded with an enumerative code using"]},{"title":"L(D)","paragraphs":["bits. LID):"]},{"title":"Lz(n)+log( n+m-lm_l )","paragraphs":["+log"]},{"title":"(no + nl + ...+ nm -","paragraphs":["1)!"]},{"title":"no!nl!...nm!","paragraphs":["rn-1"]},{"title":"i +Lz<([[DJ[,n)","paragraphs":["i=l + log ( n + I LDJl 1"]},{"title":"JLDJI-7 )","paragraphs":["\\"]},{"title":"222","paragraphs":["where [DJ is the set of all contexts in D that are proper suffixes of another context in D. The first term encodes the number n of internal vertices using the Elias code. The second term encodes the counts {nl, n2,...,"]},{"title":"am}.","paragraphs":["Given the frequencies of these internal vertices, we may calculate the number no of leaf vertices as no = 1 + n2 + 2n3 + 3n4 +... + (m - 1)am. The third term encodes the actual tree (with-out labels) using an enumerative code. The fourth term assigns labels (ie., symbols from E) to the edges in the tree. At this point the decoder knows all contexts which are not proper suffixes of other contexts, ie., D - LD]. The fourth term encodes the magnitude of [D] as an integer bounded by the number n of internal vertices in the suffix tree. The fifth term identifies the contexts [DJ as interior vertices in the tree that are proper suffices of another context in D.","Now we encode the symbols available in each context. Let"]},{"title":"mi","paragraphs":["be the number of contexts that have exactly i extensions, ie., mi - J{w:"]},{"title":"JE(w)l","paragraphs":["= i}l. 7\"n","Observe that ~i=1"]},{"title":"mi = IDI. () E m","paragraphs":["-F rni log i i--1 The first term represents the encoding of {mi } while the second term represents the encoding"]},{"title":"IE(w)l","paragraphs":["for each w in D. The third term represents the encoding of E(w) as a subset of E for each w in D.","Finally, we encode the frequencies c(~rlw) used to estimate the model parameters wED"]},{"title":"+ g ,o, ( C(°) + ) IE(w)l","paragraphs":["where [y] consists of all contexts that have y as their maximal proper suffix, ie., all contexts that y immediately dominates, and [y] is the maximal proper suffix of y in D, ie., the unique context that immediately dominates y. The first term encodes ITI with an Elias code and the second term recursively partitions"]},{"title":"c(w)","paragraphs":["into c([w]) for every context w. The third term partitions the context frequency"]},{"title":"c(w)","paragraphs":["into the available extensions"]},{"title":"c(E(w)lw )","paragraphs":["and the \"unallocated frequency\" c(E-"]},{"title":"E(w)lw) = c(w) - c(E(w)[w)","paragraphs":["in the context w. 2.4 Model Selection The final component of our contribution is a model selection algorithm for the extension model class ~. Our algorithm repeatedly refines the accuracy of our model in increasingly long contexts. Adding a new parameter to the model will decrease the codelength of the data and increase the codelength of the model. Accordingly, we add a new parameter to the model only if doing so will decrease the total codelength of the data"]},{"title":"and","paragraphs":["the model.","The incremental cost and benefit of adding a single parameter to a given context cannot be accurately approximated in isolation from any other parameters that might be added to that context. Accordingly, the incremental cost of adding the"]},{"title":"set E'","paragraphs":["of extensions to the context w is defined as (6) while the incremental benefit is defined as (7). ALe(w, E') - L(¢ U ({w} × E')) - L(¢) (6)"]},{"title":"ALT(W,","paragraphs":["E') - L(TI¢ ) -"]},{"title":"L(T[¢","paragraphs":["U ({w} x E')) (7) Keeping only significant terms that are monotonically nondecreasing, we approximate the incremental cost ALe(w, E') as"]},{"title":"loglDl+log IS'l + log c(Lwj) + log ( c(w)ls, i + C 'I )","paragraphs":["The first term represents the incremental increase in the size of the context dictionary D. The second term represents the cost of encoding the candidate extensions"]},{"title":"E(w)","paragraphs":["= E ~. The third term represents (an upper bound on) the cost of encoding"]},{"title":"c(w).","paragraphs":["The fourth term represents the cost of encoding"]},{"title":"c(.Iw )","paragraphs":["for"]},{"title":"E(w).","paragraphs":["Only the second and fourth terms are signficant.","Let us now consider the incremental benefit of adding the extensions E' to a given context w. The addition of a single parameter (w, ~r) to the model ¢ will immediately change A(alw), by definition of the model class. Any change to A(.Iw ) will also change the expansion factor"]},{"title":"5(w)","paragraphs":["in that context, which may in turn change the conditional probabilities"]},{"title":"~(E-E(w)lw, ¢)","paragraphs":["of symbols not available in that context. Thus the incremental benefit of adding the extensions E' to the context w may be calculated as"]},{"title":"ALT(w,E')","paragraphs":["-- c(E - E'lw)log 1 - A(E'Iw) 1 - ~(S'l~, ¢)"]},{"title":"+ ~ c('/Iwll°g~(~,lw,¢ )","paragraphs":["a' E Fd The first term represents the incremental benefit (in bits) for evaluating E - E' in the context w using the more accurate expansion factor 5(w). The second term represents the incremental benefit (in bits) of using the direct estimate"]},{"title":"A(a'lw )","paragraphs":["instead of the model probability/5(cr'lw, ¢) in the context w. Note that A(a'lw) may be more or less than/~(cr'lw , ¢). Now the incremental cost and benefit of adding"]},{"title":"a single","paragraphs":["extension"]},{"title":"(w, cr)","paragraphs":["to a model that already contains the extensions (w, El/ may be defined as follows. ALe(w, E', a) -- ALe(w, E' U {a}) - ALe(w, E')"]},{"title":"223","paragraphs":["ALT(w, ~', a) -"]},{"title":"ALT(w,","paragraphs":["~' U {a}) -"]},{"title":"ALT(W, ~')","paragraphs":["Let us now use these incremental cost/benefit formulas to design a simple heuristic estimation algorithm for the extension model. The algorithm consists of two subroutines."]},{"title":"Refine(D,E,n)","paragraphs":["augments the model with all individually profitable extensions of contexts of length n. It rests on the assump-tion that adding a new context does not change the model's performance in the shorter contexts. Extend(w) determines all profitable extensions of the candidate context w, if any exist. Since it is not feasible to evaluate the incremental profit of every subset of E, Extend(w) uses a greedy heuristic that repeatedly augments the set of profitable extensions of w by the single most profitable extension until it is not longer profitable to do so."]},{"title":"Refine( D,E,n)","paragraphs":["1. D, := {};E, := {}; 2. Cn := {w: w e Cn-1 ~']~ A c(w) > Cmi.} ; 3. if (( n > nm~=) V (ICnl = 0)) then return;"]},{"title":"4. for w E Cn","paragraphs":["5. ~' := Extend(w); 6. if ISI > o then D. :-- Dn U {w};"]},{"title":"En(w)","paragraphs":[":= S;"]},{"title":"7. D:=DUDn;E:=EUEn; 8. Refine( D,E,n -F","paragraphs":["1); Cn is the set of candidate contexts of length n,","obtained from the training corpus."]},{"title":"Dn","paragraphs":["is the set of profitable contexts of length n, while En is the set of profitable extensions of those contexts. Extend(w) 1. S :: {}; 2. o\" := argmaxoe~."]},{"title":"{AL(w,","paragraphs":["{at})} 3. while (AL(w,S,~) > O) 4. S := S U {a}; S. o\" := argrnax.e]g_ s {AL(w, ,S', ¢r)} 6. return(S);","The loop in lines 3-5 repeatedly finds the single most profitable symbol a with which to augment the set S of profitable extensions. The incremental profit AL(...) is the incremental benefit"]},{"title":"ALT(...)","paragraphs":["minus the incremental cost ALe(...).","Our breadth-first search considers shorter contexts before longer ones, and consequently the decision to add a profitable context y may significantly decrease the benefit of a more profitable context"]},{"title":"xy,","paragraphs":["particularly when"]},{"title":"c(xy) ~ c(y).","paragraphs":["For example, consider a source with two hidden states. In the first state, the source generates the alphabet E = {0, 1,2} uniformly. In the second state, the source generates the string \"012\" with certainty. With appropriate state transition probabilities, the source generates strings where c(0) ~ c(1) ~ e(2),"]},{"title":"c(211)/c(1 ) >> c(21e)/c(c),","paragraphs":["and"]},{"title":"c(2101)/c(01 ) > c(211)/c(1 ).","paragraphs":["In such a situation, the best context model includes the contexts \"0\" and \"01\" along with the empty context c. However, the divergence heuristic will first determine that the context \"1\" is profitable relative to the empty context, and add it to the model. Now the profitability of the better context °'01\" is reduced, and the divergence heuristic may therefore not include it in the model. This problem is best solved with a best first search. Our current implementation uses a breadth first search to limit the computational complexity of model selection.","Finally, we note that our parameter estimation techniques and model selection criteria are comparable in computational complexity to Rissanen's context models (1983, 1986). For that reason, extension models should be amendable to efficient online implementation. 3 Empirical Results By means of the following experiments, we hope to demonstrate the utility of our context modeling techniques. All results are based on the Brown corpus, an eclectic collection of English prose drawn from 500 sources across 15 genres (Francis and Kucera, 1982). The irregular and nonstationary nature of this corpus poses an exacting test for statistical language models. We use the first 90% of each file in the corpus to estimate our models, and then use the remaining 10% of each file in the corpus to evaluate the models. Each file contains approximately 2000 words. Due to limited computational resources, we set nmax = 10, Cmin -~- 8, and restrict our our alphabet size to 70 (ie., all printing ascii characters, ignoring case distinction).","Our results are summarized in the following table. Message entropy (in bits/symbol) is for the testing corpus only, as per traditional model validation methodology. The nonmonotonic extension model (NEM) outperforms all other models for all orders using vastly fewer parameters. Its performance all the more impressive when we consider that no context blending or escaping is performed, even for novel events.","We note that the test message entropy of the n-gram model class is minimized by the 5-gram at 2.38 bits/char. This result for the 5-gram is not honest because knowledge of the test set was used to select the optimal model order. Jelinek and Mercer (1980) have shown to interpolate n-grams of different order using mixing parameters that are conditioned on the history. Such interpolated Markov sources are considerably more powerful than traditional n-grams but contain even more parameters.","The best reported results on the Brown Corpus are 1.75 bits/char using a large interpolated trigram word model whose parameters are estimated using over 600,000,000 words of proprietary training data (Brown et.al., 1992). The use of proprietary training data means that these results are not independently repeatable. In contrast, our results were obtained using only 900,000 words of generally available training data and may be independently verified by any-"]},{"title":"224","paragraphs":["Model NEM NCM MCM1 MCM2 n-gram"]},{"title":"Parameters Entropy","paragraphs":["89,325 1.97 687,276 2.19 88,945,904 2.43 88,945,904 3.12 506,352,021,176,052 3.74 Table 1: Results for the nonmonotonic extension model (NEM), the nonmonotonic context model (NCM), Rissanen's (1983,1986) monotonic context models (MCM1, MCM2) and the n-gram model. All models are order 7. The rightmost column contains test message entropy in bits/symbol. NEM outperforms all other model classes for all orders using significantly fewer parameters. It is possible to reduce the test message entropy of the NEM and NCM to 1.91 and 1.99, respectively, by quadrupling the number of model parameters. one with the inclination to do so. The amount of training data is known to be a significant factor in model performance. Given a sufficiently rich dictionary of words and a sufficiently large training corpus, a model of word sequences is likely to outperform an otherwise equivalent model of character sequences. For these three reasons - repeatability, training corpus size, and the advantage of word models over character models - the results reported by Brown et.al (1992) are not directly comparable to those reported here.","Section 3.1 compares the statistical efficiency of the various context model classes. Next, section 3.2 anecodatally examines the complex interac-tions among the parameters of an extension model. 3.1","Model Class Comparison Given the tremendous risk of overfitting, the most important property of a model class is arguably its statistical efficiency. Informally, statistical efficiency measures the effectiveness of individual parameters in a given model class. A high efficiency indicates that our model class provides a good description of the data. Conversely, a low efficiency indicates that the model class does not adequately describe the observed data.","In this section, we compare the statistical efficiency of three model classes: context models, extension models, and fixed-length Markov processes (ie., n-grams). Our model class comparison is based on three criteria of statistical efficiency: total codelength, bits/parameter on the test message, and bits/order on the test message. The context and extension models are all of order 9, and were estimated using the true incremental benefit and a range of fixed incremental costs (between 5 and 25 bits/extension for the extension model and between 25 and 150 bits/context for the context model).","According to the first criteria of statistical efficiency, the best model is the one that achieves the smallest total codelength"]},{"title":"L(T,","paragraphs":["¢) of the training corpus T and model ¢ using the fewest parameters. This criteria measures the statistical efficiency of a model class according to the MDL framework, where we would like each parameter to be as cheap as possible and do as much work as possible. Figure 1 graphs the number of model parameters required to achieve a given total codelength for the training corpus and model. The extension model class is the overwhelming winner. ......... N. um ,be. r, of Param,et?rs.. vs: Codele, ng~ ........ 3,5 \"l t ..... M-... 2,3,4 ngrarn • exle~lslon model .-~ E - - ~-- context mod~ ....'\" • \"\"\" [ ..'\"\"\" i • '\"\" I '\"'\"\"'\"'\"\" i 15 .m\" ~- Btm ~ 10000 100000 1000000 I(XX)OO(X) Number of parameters Figure 1: The relationship between the number of model parameters and the total codelength"]},{"title":"L(T,","paragraphs":["¢) of the training corpus T and the model ¢. By this criteria of statistical efficiency, the extension models completely dominate context models and n-grams.","According to the second criteria of statistical efficiency, the best model is the one that achieves the lowest test message entropy using the fewest parameters. This criteria measures the statistical efficiency of a model class according to traditional model validation methodology, tempered by a healthy concern for overfitting. Figure 2 graphs the number of model parameters required to achieve a given test message entropy for each of the three model classes. Again, the extension model class is the clear winner. (This is particularly striking when the number of parameters is plotted on a linear scale.) For example, one of our extension models saves 0.98 bits/char over the trigram while using less than 1/3 as many parameters. Given the logarithmic nature of codelength and the scarcity of training data, this is a significant improvement.","According to the third criteria of statistical efficiency, the best model is one that achieves the lowest test message entropy for a given model order. This criteria is widely used in the language modeling community, in part because model order is typi-"]},{"title":"225 \"C\" & i","paragraphs":["'s 3.0. . ....... , ........ i ........ = . i e~slon mo~l 2.8 22 27. =~°~\"~ ..... 3,4 gram \"'\"'\",........ \"'... 2.6","'.... \"'x 2.4 2.2 2.0 L8 10 4 10 s 10 6 10 7","Number of Parameters vs. Message Entropy ........ , ........ i ........ = ........ N~r~ Pa~em to ~ Figure 2: The relationship between the number of model parameters and test message entropy. The most striking fact about this graph is the tremendous efficiency of the extension model. cally -- although not necessarily -- related to both the number of model parameters and the amount of computation required to estimate the model. Figure 3 compares model order to test message entropy for each of the three model classes. As the order of the models increases from 0 (ie., unigram) to 10, we naturally expect the test message entropy to approach a lower bound, which is itself bounded below by the true source entropy. By this criteria, the extension model class is better than the context model class, and both are significantly better than the n-gram. 4.4 4.2 4.0 3.8 3.6 3.4 3.2 3.0 2.8 2.6 2.4 2.2 2.0 1.8 Model Order vs. Message Entropy ..... • .. ...... . --:-- :~Z~m=~ ~ 2 4 6 8 10 Model Order Figure 3: The relationship between model order and test message entropy. The extension model class is the clear winner by this criteria as well. 3.2 Anecdotes It is also worthwhile to interpret the parameters of the extension model estimated from the Brown Corpus, to better understand the interaction between our model class and our heuristic model selection algorithm. According to the divergence heuristic, the decision to add an extension (w, ~) is made relative to that context's maximal proper suffix LwJ in D as well as any other extensions in the context w. An extension"]},{"title":"(w, ~)","paragraphs":["will be added only if the direct estimate of its conditional probability is significantly different from its conditional probability in its maximal proper suffix after scaling by the expansion factor in the context w, ie., if A(alw ) is significantly different than"]},{"title":"6(w)~(c~ I","paragraphs":["LwJ). This is illusrated by the three contexts and six","extensions shown immediately below, where"]},{"title":"+E(w)","paragraphs":["includes all symbols in"]},{"title":"E(w)","paragraphs":["that are more likely in w than they were in [wJ and"]},{"title":"-E(w)","paragraphs":["includes all symbols in"]},{"title":"E(w)","paragraphs":["that are less likely in w than they were in"]},{"title":"L J.","paragraphs":["W","\"blish\" \"ouestablish\" \"euestablish\""]},{"title":"+E(w) -E(w)","paragraphs":["e,i,m U m e The substring"]},{"title":"blish","paragraphs":["is most often followed by the characters 'e', 5', and 'm', corresponding to the relatively frequent word forms"]},{"title":"publish{ ed, er, ing}","paragraphs":["and"]},{"title":"establish{ ed, ing, ment}.","paragraphs":["Accordingly, the context \"blish\" has three positive extensions {e,i,m}, of which e has by far the greatest probability. The context \"blish\" is the maximal proper suffix of two other contexts in the model, \"ouestablish\" and \"euestablish\".","The substring o"]},{"title":"establish","paragraphs":["occurs most frequently in the gerund"]},{"title":"to establish,","paragraphs":["which is nearly always followed by a space. Accordingly, the context \"ouestablish\" has a single positive extension \"u\". The substring o"]},{"title":"establish","paragraphs":["is also found before the characters 'm', 'e', and 'i' in sequences such as"]},{"title":"to establishments, {who, ratio, also} established,","paragraphs":["and"]},{"title":"{ to, into, also} establishing.","paragraphs":["Accordingly, the context \"ouestablish\" does not have any negative extensions. In contrast, the substring e"]},{"title":"establish","paragraphs":["is overwhelmingly followed by the character 'm', rarely followed by 'e', and never followed by either 'i' or space. For this reason, the context \"euestablish\" has a single positive extension {m} corresponding to the great frequency of the string"]},{"title":"the establishment.","paragraphs":["This context also has single negative extension {e}, corresponding to the fact that the character 'e' is still possible in the context \"euestablish\" but considerably less likely than in that context's maximal proper suffix \"blish\".","Since 'i' is reasonably likely in the context \"blish\" but completely unlikely in the context \"euestablish\", we may well wonder why the model 226 does not include the negative extension 'i' in addition to 'e' or even instead of 'e'. This puzzle is explained by the expansion factor as follows. Since the substring e"]},{"title":"establish","paragraphs":["is only followed by 'm' and 'e', the expansion factor ~(\"e,,establish\") is essentially zero after 'm' and 'e' are added to that context, and therefore ~(~- {m, e}l \"euestablish\") is also essentially zero. Thus, 'i' and space are both assigned nearly zero probability in the context \"e, ,establish\", simply because 'm' and 'e' get nearly all the probability in that context."]},{"title":"4 Conclusion","paragraphs":["In ongoing work, we are investigating extension mixture models as well as improved model selection algorithms. An extension mixture model is an extension model whose ~(~lw) parameters are estimated by linearly interpolating the empirical probability estimates for all extensions that dominate w with respect to c~, ie., all extensions whose symbol is and whose context is a suffix of w. Extension mixing allows us to remove the uniform flattening of zero frequency symbols in our parameter estimates (5). Preliminary results are promising. The idea of context mixing is due to Jelinek and Mercer (1980).","Our results highlight the fundamental tension between model complexity and data complexity. If the model complexity does not match the data complexity, then both the total codelength of the past observations"]},{"title":"and","paragraphs":["the predictive error increase. In other words, simply increasing the number of parameters in the model does not necessarily increase predictive power of the model. Therefore, it is necessary to have a a fine-grained model along with a heuristic model selection algorithm to guide the expansion of the model in a principled manner. Acknowledgements. Thanks to Andrew Appel, Carl de Marken, and Dafna Scheinvald for their critique. The paper has benefited from discussions with the participants of DCC95. Both authors are partially supported by Young Investigator Award IRI-0258517 to the first author from the National Science Foundation. The second author is additionally supported by a tuition award from the Princeton University Research Board. The research was partially supported by NSF SGER IRI-9217208. FRANCIS, W. N., AND KUCERA, H."]},{"title":"Frequency analysis of English usage: lexicon and grammar.","paragraphs":["Houghton Mifflin, Boston, 1982.","JELINEK, F., AND MERCER, a. L. Interpolated estimation of Markov source parameters from sparse data. In"]},{"title":"Pattern Recognition in Practice","paragraphs":["(Amsterdam, May 21-23 1980), E. S. Gelsema and L. N. Kanal, Eds., North Holland, pp. 381-397. KNUTH, D. E."]},{"title":"The","paragraphs":["Art"]},{"title":"of Computer Programming,","paragraphs":["1 ed., vol. 1. Addison-Wesley, Reading, MA, 1968.","MOFFAT, A. Implementing the PPM data compre-sion scheme."]},{"title":"IEEE Trans. Communications 38,","paragraphs":["11 (1990), 1917-1921.","RISSANEN, J. Modeling by shortest data description."]},{"title":"Automatica 14","paragraphs":["(1978), 465-471. RISSANEN, J. A universal data compression system."]},{"title":"IEEE Trans. Information Theory IT-29,","paragraphs":["5 (1983), 656-664.","RISSANEN, J. Complexity of strings in the class of Markov sources."]},{"title":"IEEE Trans. Information The- ory IT-32,","paragraphs":["4 (1986), 526-532.","RISTAD, E. S., AND THOMAS, R. G. Context models in the MDL framework. In"]},{"title":"Proceedings of 5th Data Compression Conference","paragraphs":["(Los Alamitos, CA, March 28-30 1995), J. Storer and M. Cohn, Eds., IEEE Computer Society Press, pp. 62-71."]},{"title":"References","paragraphs":["BROWN, P., PIETRA, V. D., PIETRA, S. D., LAI, J., AND MERCER, R. An estimate of an upper bound for the entropy of English."]},{"title":"Computational Linguistics 18","paragraphs":["(1992), 31-40.","CLEARY, J., AND WITTEN, I. Data compression using adaptive coding and partial string matching."]},{"title":"IEEE Trans. Comm. COM-32,","paragraphs":["4 (1984), 396-402."]},{"title":"227","paragraphs":[]}]}
