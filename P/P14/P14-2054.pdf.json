{
  "sections": [
    {
      "title": "",
      "paragraphs": [
        "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 327–332, Baltimore, Maryland, USA, June 23-25 2014. c⃝2014 Association for Computational Linguistics"
      ]
    },
    {
      "title": "Polynomial Time Joint Structural Inference for Sentence CompressionXian Qian and Yang LiuThe University of Texas at Dallas800 W. Campbell Rd., Richardson, TX, USA{qx,yangl}@hlt.utdallas.eduAbstract",
      "paragraphs": [
        "We propose two polynomial time inference algorithms to compress sentences under bigram and dependency-factored objectives. The first algorithm is exact and requires O(n6",
        ") running time. It extends Eisner’s cubic time parsing algorithm by using virtual dependency arcs to link deleted words. Two signatures are added to each span, indicating the number of deleted words and the rightmost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n4",
        ") running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach."
      ]
    },
    {
      "title": "1 Introduction",
      "paragraphs": [
        "Sentence compression aims to shorten a sentence by removing uninformative words to reduce read-ing time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparser1",
        ",",
        "1",
        "http://sourceforge.net/projects/mstparser/ Warrensaystheeconomycontinuesthesteadyimprovement ROOT Warrensays steady theeconomycontinuesthe improvement ROOT Figure 1: The compressed sentence is not a subtree of the original sentence. Words in gray are removed. and found that 2561 of 5379 sentences (47.6%) do not satisfy the subtree deletion model.",
        "Methods beyond the subtree model are also explored. Trevor et al. proposed synchronous tree substitution grammar (Cohn and Lapata, 2009), which allows local distortion of the tree topology and can thus naturally capture structural mismatches. (Genest and Lapalme, 2012; Thadani and McKeown, 2013) proposed the joint compression model, which simultaneously considers the n-gram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case.",
        "In this paper, we propose a new exact decoding algorithm for the joint model using dynamic programming. Our method extends Eisner’s cubic time parsing algorithm by adding signatures to each span, which indicate the number of deleted words and the rightmost kept word within the span, resulting in O(n6",
        ") time complexity and O(n4",
        ") space complexity. We further propose a faster approximate algorithm based on Lagrangian relaxation, which has T O(n4",
        ") running time and O(n3",
        ") space complexity (T is the iteration number in the subgradient decent algorithm). Experiments on the popular Edinburgh dataset show that 327"
      ]
    },
    {
      "title": "x x ...x x ...x ...",
      "paragraphs": [
        "0(root) 2 i i+1 j"
      ]
    },
    {
      "title": "x x",
      "paragraphs": [
        "1 n"
      ]
    },
    {
      "title": "w",
      "paragraphs": [
        "0idep"
      ]
    },
    {
      "title": "w",
      "paragraphs": [
        "i2dep"
      ]
    },
    {
      "title": "w",
      "paragraphs": [
        "ijdep"
      ]
    },
    {
      "title": "w",
      "paragraphs": [
        "ii+1dep"
      ]
    },
    {
      "title": "w",
      "paragraphs": [
        "2ibgr"
      ]
    },
    {
      "title": "w",
      "paragraphs": [
        "ii+1bgr"
      ]
    },
    {
      "title": "w",
      "paragraphs": [
        "i+1jbgr",
        "Figure 2: Graph illustration for the objective func-",
        "tion. In this example, words x2, xi, xi+1, xj are",
        "kept, others are deleted. The value of the ob-",
        "jective function is wtok",
        "2 + wtok",
        "i + wtok",
        "i+1 + wtok",
        "j +",
        "wdep",
        "0i +wdep",
        "i2 +wdep",
        "ii+1 +wdep",
        "ij +wbgr",
        "2i +wbgr",
        "ii+1 +wbgr",
        "i+1j. the proposed approach is 10 times faster than a high-performance commercial ILP solver."
      ]
    },
    {
      "title": "2 Task Definition",
      "paragraphs": [
        "We define the sentence compression task as: given a sentence composed of n words, x = x1, . . . , xn, and a length L ≤ n, we need to remove (n − L) words from x, so that the sum of the weights of the dependency tree and word bigrams of the remaining part is maximized. Formally, we solve the following optimization problem: max z,y ∑",
        "i wtok",
        "i zi + ∑",
        "i,j wdep",
        "ij zizjyij (1)",
        "+ ∑",
        "i<j wbgr",
        "ij zizj ∏",
        "i<k<j(1 − zk) s.t. z is binary , ∑",
        "i zi = L y is a projective parse tree over the subgraph: {xi|zi = 1} where z is a binary vector, zi indicates xi is kept or not. y is a square matrix denoting the projective dependency parse tree over the remaining words, yij indicates if xi is the head of xj (note that each word has exactly one head). wtok",
        "i is the informativeness of xi, wbgr",
        "ij is the score of bigram xixj in an n-gram model, wdep",
        "is the score of dependency arc xi → xj in an arc-factored dependency parsing model. Hence, the first part of the objective function is the total score of the kept words, the second and third parts are the scores of the parse tree and bigrams of the compressed sentence, zizj ∏",
        "i<k<j(1 − zk) = 1 indicates both xi and xj are kept, and are adjacent after compression. A graph illustration of the objective function is shown in Figure 2. Warrensays steady theeconomycontinuesthe improvement ROOT Figure 3: Connect deleted words using virtual arcs."
      ]
    },
    {
      "title": "3 Proposed Method3.1 Eisner’s Cubic Time Parsing Algorithm",
      "paragraphs": [
        "Throughout the paper, we assume that all the parse trees are projective. Our method is a generaliza-tion of Eisner’s dynamic programming algorithm (Eisner, 1996), where two types of structures are used in each iteration, incomplete spans and complete spans. A span is a subtree over a number of consecutive words, with the leftmost or the rightmost word as its root. An incomplete span denoted as Ii",
        "j is a subtree inside a single arc xi → xj, with root xi. A complete span is denoted as Ci",
        "j, where xi is the root of the subtree, and xj is the furthest descendant of xi.",
        "Eisner’s algorithm searches the optimal tree in a bottom up order. In each step, it merges two adjacent spans into a larger one. There are two rules for merging spans: one merges two complete spans into an incomplete span, the other merges an incomplete span and a complete span into a large complete span. 3.2",
        "Exact O(n6 ) Time Algorithm First we consider an easy case, where the bigram scores wbgr",
        "ij in the objective function are ignored. The scores of unigrams wtok",
        "i can be transfered to the dependency arcs, so that we can remove all linear terms wtok",
        "i zi from the objective function. That is:",
        "∑",
        "i wtok",
        "i zi + ∑",
        "i,j wdep",
        "ij zizjyij",
        "= ∑",
        "i,j (wdep",
        "ij + wtok",
        "j )zizjyij This can be easily verifed. If zj = 0, then in both equations, all terms having zj are zero; If zj = 1, i.e., xj is kept, since it has exactly one head word xk in the compressed sentence, the sum of the terms having zj is wtok",
        "j + wdep",
        "kj for both equations.",
        "Therefore, we only need to consider the scores of arcs. For any compressed sentence, we could augment its dependency tree by adding a virtual 328 ii+1 ii+1"
      ]
    },
    {
      "title": "+ =",
      "paragraphs": [
        "ij r+1j"
      ]
    },
    {
      "title": "+ =",
      "paragraphs": [
        "ir ij i+1j"
      ]
    },
    {
      "title": "+ =",
      "paragraphs": [
        "ii+1 ... ... ij rj"
      ]
    },
    {
      "title": "+ =",
      "paragraphs": [
        "ir Case1 Case2 Case3 Case4 Figure 4: Merging rules for dependency-factored sentence compression. Incomplete spans and complete spans are represented by trapezoids and triangles respectively. arc i − 1 → i for each deleted word xi. If the first word x1 is deleted, we connect it to the root of the parse tree x0, as shown in Figure 3. In this way, we derive a full parse tree of the original sentence. This is a one-to-one mapping. We can reversely get the the compressed parse tree by removing all virtual arcs from the full parse tree. We restrict the score of all the virtual arcs to be zero, so that scores of the two parse trees are equivalent.",
        "Now the problem is to search the optimal full parse tree with n − L virtual arcs.",
        "We modify Eisner’s algorithm by adding a signature to each span indicating the number of virtual arcs within the span. Let Ii",
        "j(k) and Ci",
        "j(k) denote the incomplete and complete spans with k virtual arcs respectively. When merging two spans, there are 4 cases, as shown in Figure 4.",
        "• Case 1 Link two complete spans by a virtual",
        "arc : Ii i+1(1) = Ci",
        "i (0) + Ci+1",
        "i+1 (0). The two complete spans must be single words, as the length of the virtual arc is 1.",
        "• Case 2 Link two complete spans by a non-",
        "virtual arc: Ii",
        "j(k) = Ci",
        "r(k′",
        ") + Cj",
        "r+1(k′′",
        "), k′",
        "+",
        "k′′",
        "= k.",
        "• Case 3 Merge an incomplete span and a complete span. The incomplete span is covered by a virtual arc: Ii",
        "j(j − i) = Ii",
        "i+1(1) + Ci+1 j (j − i − 1). The number of the virtual arcs within Ci+1",
        "j must be j − i − 1, since the descendants of the modifier of a virtual arc xj must be removed.",
        "• Case 4 Merge an incomplete span and a com-",
        "plete span. The incomplete span is covered",
        "by a non-virtual arc: Ci",
        "j(k) = Ii",
        "r(k′",
        ") +",
        "Cr",
        "j (k′′ ), k′",
        "+ k′′",
        "= k.",
        "The score of the new span is the sum of the two spans. For case 2, the weight of the dependency arc i → j, wdep",
        "ij is also added to the final score. The root node is allowed to have two modifiers: one is the modifier in the compressed sentence, the other is the first word if it is removed.",
        "For each combination, the algorithm enumerates the number of virtual arcs in the left and right spans, and the split position (e.g., k′",
        ", k′′",
        ", r in case 2), thus it takes O(n3",
        ") running time. The overall time complexity is O(n5",
        ") and the space complexity is O(n3",
        ").",
        "Next, we consider the bigram scores. The following proposition is obvious. Proposition 1. For any right-headed span Ii",
        "j or Ci j, i > j, words xi, xj must be kept. Proof. Suppose xj is removed, there must be a virtual arc j − 1 → j which is a conflict with the fact that xj is the leftmost word. As xj is a descendant of xi, xi must be kept.",
        "When merging two spans, a new bigram is created, which connects the rightmost kept words in the left span and the leftmost kept word in the right span. According to the proposition above, if the right span is right-headed, its leftmost word is kept. If the right span is left-headed, there are two cases: its leftmost word is kept, or no word in the span is kept. In any case, we only need to consider the leftmost word in the right span.",
        "Let Ii",
        "j(k, p) and Ci",
        "j(k, p) denote the single and complete span with k virtual arcs and the rightmost kept word xp. According to the proposition above, we have, for any right-headed span p = i.",
        "We slightly modify the two merging rules above, and obtain:",
        "• Case 2’ Link two complete spans by a",
        "non-virtual arc: Ii j(k, j) = Ci",
        "r(k′",
        ", p) + Cj r+1(k′′",
        ", j), k′",
        "+ k′′",
        "= k. The score of the new span is the sum of the two spans plus wdep ij + wbgr",
        "p,r+1. 329",
        "• Case 4’ Merge an incomplete span and a",
        "complete span. The incomplete span is cov-",
        "ered by a non-virtual arc. For left-headed",
        "spans, the rule is Ci",
        "j(k, q) = Ii",
        "r(k′",
        ", p) +",
        "Cr",
        "j (k′′",
        ", q), k′",
        "+ k′′",
        "= k, and the score of",
        "the new span is the sum of the two span-",
        "s plus wbgr",
        "pr ; for right-headed spans, the rule",
        "is Ci j(k, i) = Ii",
        "r(k′",
        ", i) + Cr",
        "j (k′′",
        ", r), and the",
        "score of the new span is the sum of the two",
        "spans.",
        "The modified algorithm requires O(n6",
        ") running",
        "time and O(n4",
        ") space complexity. 3.3",
        "Approximate O(n4 ) Time Algorithm In this section, we propose an approximate algorithm where the length constraint ∑",
        "i zi = L is relaxed by Lagrangian Relaxation. The relaxed version of Problem (1) is min λ max",
        "z,y ∑",
        "i wtok",
        "i zi + ∑",
        "i,j wdep",
        "ij zizjyij (2)",
        "+ ∑",
        "i<j wbgr",
        "ij zizj ∏",
        "i<k<j(1 − zk)",
        "+λ(∑ i zi − L) s.t. z is binary y is a projective parse tree over the subgraph: {xi|zi = 1} Fixing λ, the optimal z, y can be found using a simpler version of the algorithm above. We drop the signature of the virtual arc number from each span, and thus obtain an O(n4",
        ") time algorithm. Space complexity is O(n3",
        "). Fixing z, y, the dual variable is updated by",
        "λ = λ + α(L − ∑ i zi) where α > 0 is the learning rate. In this paper, our choice of α is the same as (Rush et al., 2010)."
      ]
    },
    {
      "title": "4 Experiments4.1 Data and Settings",
      "paragraphs": [
        "We evaluate our method on the data set from (Clarke and Lapata, 2008). It includes 82 newswire articles with manually produced compression for each sentence. We use the same partitions as (Martins and Smith, 2009), i.e., 1,188 sentences for training and 441 for testing.",
        "Our model is discriminative – the scores of the unigrams, bigrams and dependency arcs are the linear functions of features, that is, wtok",
        "i = vT",
        "f (xi), where f is the feature vector of xi, and v is the weight vector of features. The learning task is to estimate the feature weight vector based on the manually compressed sentences.",
        "We run a second order dependency parser trained on the English Penn Treebank corpus to generate the parse trees of the compressed sentences. Then we augment these parse trees by adding virtual arcs and get the full parse trees of their corresponding original sentences. In this way, the annoation is transformed into a set of sentences with their augmented parse trees. The learning task is similar to training a parser. We run a CRF based POS tagger to generate POS related features.",
        "We adopt the compression evaluation metric as used in (Martins and Smith, 2009) that measures the macro F-measure for the retained unigrams (Fugr), and the one used in (Clarke and Lapata, 2008) that calculates the F1 score of the grammatical relations labeled by RASP (Briscoe and Carroll, 2002).",
        "We compare our method with other 4 state-of-the-art systems. The first is linear chain CRFs, where the compression task is casted as a binary sequence labeling problem. It usually achieves high unigram F1 score but low grammatical rela-tion F1 score since it only considers the local interdependence between adjacent words. The second is the subtree deletion model (Berg-Kirkpatrick et al., 2011) which is solved by integer linear programming (ILP)2",
        ". The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. The last one jointly infers tree structures alongside bigrams using ILP (Thadani and McKeown, 2013). For fair comparison, systems were restricted to produce compressions that matched their average gold compression rate if possible. 4.2 Features Three types of features are used to learn our model: unigram features, bigram features and dependency features, as shown in Table 1. We also use the in-between features proposed by (McDonald et 2 We use Gurobi as the ILP solver in the paper.",
        "http://www.gurobi.com/ 330 Features for unigram xi wi−2, wi−1, wi, wi+1, wi+2 ti−2, ti−1, ti, ti+1, ti+2 witi wi−1wi, wiwi+1 ti−2ti−1, ti−1ti, titi+1, ti+1ti+2 ti−2ti−1ti, ti−1titi+1, titi+1ti+2 whether wi is a stopword Features for selected bigram xixj distance between the two words: j − i wiwj, wi−1wj, wi+1wj, wiwj−1, wiwj+1 titj, ti−1tj, ti+1tj, titj−1, titj+1 Concatenation of the templates above {titktj|i < k < j} Dependency Features for arc xh → xm distance between the head and modifier h − m dependency type direction of the dependency arc (left/right) whwm, wh−1wm, wh+1wm, whwm−1, whwm+1 thtm, th−1tm, th+1tm, thtm−1, thtm+1 th−1thtm−1tm, thth+1tm−1tm th−1thtmtm+1, thth+1tmtm+1 Concatenation of the templates above {thtktm|xk lies between xh and xm} Table 1: Feature templates. wi denotes the word form of token xi and ti denotes the POS tag of xi. al., 2005), which were shown to be very effective for dependency parsing. 4.3 Results We show the comparison results in Table 2. As expected, the joint models (ours and TM13) consistently outperform the subtree deletion model, since the joint models do not suffer from the subtree restriction. They also outperform McDonald’s, demonstrating the effectiveness of considering the grammar structure for compression. It is not surprising that CRFs achieve high unigram F scores but low syntactic F scores as they do not System C Rate Funi RASP Sec. Ours(Approx) 0.68 0.802 0.598 0.056 Ours(Exact) 0.68 0.805 0.599 0.610 Subtree 0.68 0.761 0.575 0.022 TM13 0.68 0.804 0.599 0.592 McDonald06 0.71 0.776 0.561 0.010 CRFs 0.73 0.790 0.501 0.002 Table 2: Comparison results under various quality metrics, including unigram F1 score (Funi), syntactic F1 score (RASP), and compression speed (seconds per sentence). C Rate is the compression ratio of the system generated output. For fair comparison, systems were restricted to produce compressions that matched their average gold compression rate if possible. consider the fluency of the compressed sentence.",
        "Compared with TM13’s system, our model with exact decoding is not significantly faster due to the high order of the time complexity. On the other hand, our approximate approach is much more efficient, about 10 times faster than TM13’ system, and achieves competitive accuracy with the exact approach. Note that it is worth pointing out that the exact approach can output compressed sentences of all lengths, whereas the approximate method can only output one sentence at a specific compression rate."
      ]
    },
    {
      "title": "5 Conclusion",
      "paragraphs": [
        "In this paper, we proposed two polynomial time decoding algorithms using joint inference for sentence compression. The first one is an exact dynamic programming algorithm, and requires O(n6",
        ") running time. This one does not show significant advantage in speed over ILP. The second one is an approximation of the first algorithm. It adopts Lagrangian relaxation to eliminate the compression ratio constraint, yielding lower time complexity T O(n4",
        "). In practice it achieves nearly the same accuracy as the exact one, but is much faster.3",
        "The main assumption of our method is that the dependency parse tree is projective, which is not true for some other languages. In that case, our method is invalid, but (Thadani and McKeown, 2013) still works. In the future, we will study the non-projective cases based on the recent parsing techniques for 1-endpoint-crossing trees (Pitler et al., 2013)."
      ]
    },
    {
      "title": "Acknowledgments",
      "paragraphs": [
        "We thank three anonymous reviewers for their valuable comments. This work is partly supported by NSF award IIS-0845484 and DARPA under Contract No. FA8750-13-2-0041. Any opinion-s expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies."
      ]
    },
    {
      "title": "References",
      "paragraphs": [
        "Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of ACL-HLT, pages 481–490, June. 3 Our code is available at http://code.google.com/p/sent-",
        "compress/ 331",
        "T. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text.",
        "Yllias Chali and Sadid A. Hasan. 2012. On the effectiveness of using sentence compression models for query-focused multi-document summarization. In Proceedings of COLING, pages 457–474.",
        "James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. J. Artif. Intell. Res. (JAIR), 31:399–429. Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. J. Artif. Int. Res., 34(1):637–674, April.",
        "Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: an exploration. In Proceedings of COLING.",
        "Pierre-Etienne Genest and Guy Lapalme. 2012. Fully abstractive approach to guided summarization. In Proceedings of the ACL, pages 354–358.",
        "Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression. In Proceedings of EMNLP, October.",
        "Fei Liu and Yang Liu. 2009. From extractive to abstractive meeting summaries: Can it be done by sentence compression? In Proceedings of ACL-IJCNLP 2009, pages 261–264, August.",
        "André F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages 1–9.",
        "Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL. Ryan McDonald. 2006. Discriminative Sentence Compression with Soft Syntactic Constraints. In Proceedings of EACL, April.",
        "Hajime Morita, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2013. Subtree extractive summarization via submodular maximization. In Proceedings of ACL, pages 1023–1032, August.",
        "Emily Pitler, Sampath Kannan, and Mitchell Marcus. 2013. Finding optimal 1-endpoint-crossing trees. In Transactions of the Association for Computational Linguistics, 2013 Volume 1.",
        "Xian Qian and Yang Liu. 2013. Fast joint compression and summarization via graph cuts. In Proceedings of EMNLP, pages 1492–1502, October.",
        "Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposi-tion and linear programming relaxations for natural language processing. In Proceedings of EMNLP.",
        "Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Proceedings of the CoNLL, August. 332"
      ]
    }
  ]
}
