{
  "sections": [
    {
      "title": "",
      "paragraphs": [
        "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759–764, Baltimore, Maryland, USA, June 23-25 2014. c⃝2014 Association for Computational Linguistics"
      ]
    },
    {
      "title": "EM Decipherment for Large Vocabularies Malte Nuhn and Hermann Ney Human Language Technology and Pattern Recognition Computer Science Department, RWTH Aachen University, Aachen, Germany <surname>@cs.rwth-aachen.de Abstract",
      "paragraphs": [
        "This paper addresses the problem of EM-based decipherment for large vocabularies. Here, decipherment is essentially a tagging problem: Every cipher token is tagged with some plaintext type. As with other tagging problems, this one can be treated as a Hidden Markov Model (HMM), only here, the vocabularies are large, so the usual O(N V 2",
        ") exact EM approach is infeasible. When faced with this situation, many people turn to sampling. However, we propose to use a type of approximate EM and show that it works well. The basic idea is to collect fractional counts only over a small subset of links in the forward-backward lattice. The subset is different for each iteration of EM. One option is to use beam search to do the subsetting. The second method restricts the successor words that are looked at, for each hypothesis. It does this by consulting pre-computed tables of likely n-grams and likely substitutions."
      ]
    },
    {
      "title": "1 Introduction",
      "paragraphs": [
        "The decipherment of probabilistic substitution ciphers (ciphers in which each plaintext token can be substituted by any cipher token, following a distribution p(f |e), cf. Table 2) can be seen as an important step towards decipherment for MT. This problem has not been studied explicitly be-fore. Scaling to larger vocabularies for probabilistic substitution ciphers decipherment is a difficult problem: The algorithms for 1:1 or homophonic substitution ciphers are not applicable, and standard algorithms like EM training become in-tractable when vocabulary sizes go beyond a few hundred words. In this paper we present an efficient EM based training procedure for probabilistic substitution ciphers which provides high decipherment accuracies while having low computational requirements. The proposed approach allows using high order n-gram language models, and is scalable to large vocabulary sizes. We show improvements in decipherment accuracy in a variety of experiments (including MT) while being computationally more efficient than previous published work on EM-based decipherment."
      ]
    },
    {
      "title": "2 Related Work",
      "paragraphs": [
        "Several methods exist for deciphering 1:1 substitution ciphers: Ravi and Knight (2008) solve 1:1 substitution ciphers by formulating the decipherment problem as an integer linear program. Corlett and Penn (2010) solve the same problem using A∗",
        "search. Nuhn et al. (2013) present a beam search approach that scales to large vocabulary and high order language models. Even though being successful, these algorithms are not applicable to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for machine translation.",
        "EM training for probabilistic ciphers was first covered in Ravi and Knight (2011). Nuhn et al. (2012) have given an approximation to exact EM training using context vectors, allowing to training models even for larger vocabulary sizes. Ravi (2013) report results on the OPUS subtitle corpus using an elaborate hash sampling technique, based on n-gram language models and context vectors, that is computationally very efficient.",
        "Conventional beam search is a well studied topic: Huang et al. (1992) present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, Young and Young (1994) present an HMM toolkit, including pruned forward-backward EM training. Pal et al. (2006) use beam search for training of CRFs. 759 Method Publications Complexity EM Full (Knight et al., 2006), (Ravi and Knight, 2011) O(N V n",
        ") EM Fixed Candidates (Nuhn et al., 2012) O(N ) EM Beam This Work O(N V ) EM Lookahead This Work O(N ) Table 1: Different approximations to exact EM training for decipherment. N is the cipher sequence length, V the size of the target vocabulary, and n the order of the language model.",
        "The main contribution of this work is the preselection beam search that—to the best of our knowledge—was not known in literature before, and serves as an important step to applying EM training to the large vocabulary decipherment problem. Table 1 gives an overview of the EM based methods. More details are given in Section 3.2."
      ]
    },
    {
      "title": "3 Probabilistic Substitution Ciphers",
      "paragraphs": [
        "We define probabilistic substitutions ciphers using the following generative story for ciphertext sequences f N",
        "1 :",
        "1. Stochastically generate a plaintext sequence eN 1 according to a bigram1",
        "language model.",
        "2. For each plaintext token en choose a substitution fn with probability P (fn|en, θ).",
        "This generative story corresponds to the model",
        "p(eN",
        "1 , f N 1 , θ) = p(eN",
        "1 ) · p(f N",
        "1 |eN",
        "1 , θ) , (1) with the zero-order membership model",
        "p(f N 1 |eN",
        "1 , θ) = N ∏ n=1 plex(fn|en, θ) (2) with parameters p(f |e, θ) ≡ θf|e and normaliza-tion constraints ∀e ∑",
        "f θf|e = 1, and first-order",
        "plaintext sequence model",
        "P (eN 1 ) = N ∏ n=1 pLM (en|en−1) . (3) Thus, the probabilistic substitution cipher can be seen as a Hidden Markov Model. Table 2 gives an overview over the model. We want to find those parameters θ that maximize the marginal distribution p(f N",
        "1 |θ):",
        "θ = arg max θ′    ∑ [eN 1 ]",
        "p(f N",
        "1 , eN",
        "1 |θ′",
        ")    (4) 1 This can be generalized to n-gram language models. After we obtained the parameters θ we can obtain eN",
        "1 as the Viterbi decoding",
        "arg maxeN 1 {",
        "p(eN 1 |f N",
        "1 , θ)} . 3.1 Exact EM training",
        "In the decipherment setting, we are given the ob-",
        "served ciphertext f N",
        "1 and the model p(f N",
        "1 |eN",
        "1 , θ)",
        "that explains how the observed ciphertext has been",
        "generated given a latent plaintext eN",
        "1 . Marginaliz-ing the unknown eN",
        "1 , we would like to obtain the maximum likelihood estimate of θ as specified in Equation 4. We iteratively compute the maximum likelihood estimate by applying the EM algorithm (Dempster et al., 1977): θ̃f|e = ∑ n:fn=f",
        "pn(e|f N 1 , θ) ∑ f ∑ n:fn=f",
        "pn(e|f N 1 , θ) (5) with",
        "pn(e|f N 1 , θ) = ∑ [eN 1 :en=e]",
        "p(eN 1 |f N",
        "1 , θ) (6) being the posterior probability of observing the plaintext symbol e at position n given the ciphertext sequence f N",
        "1 and the current parameters θ. pn(e|f N",
        "1 , θ) can be efficiently computed using the forward-backward algorithm. 3.2 Approximations to EM-Training The computational complexity of EM training stems from the sum",
        "∑ [eN 1 :en=e] contained in the posterior pn(e|f N",
        "1 , θ). However, we can approximate this sum (and hope that the EM training procedure is still working) by only evaluating the dominating terms, i.e. we only evaluate the sum for sequences eN",
        "1 that have the largest contributions to",
        "∑",
        "[eN",
        "1 :en=e]. Note that due to this approxi-",
        "mation, the new parameter estimates in Equation 5",
        "can become zero. This is a critical issue, since",
        "pairs (e, f ) with p(f |e) = 0 cannot recover from 760",
        "Sequence of cipher tokens : f N 1 = f1, . . . , fN",
        "Sequence of plaintext tokens : eN 1 = e1, . . . , eN",
        "Joint probability : p(f N 1 , eN",
        "1 |θ) = p(eN",
        "1 ) · p(f N",
        "1 |eN",
        "1 , θ)",
        "Language model : p(eN 1 ) = N∏ n=1 pLM (en|en−1)",
        "Membership probabilities : p(f N 1 |eN",
        "1 , θ) = N∏ n=1 plex(fn|en, θ) Paramater Set : θ = {θf|e}, p(f |e, θ) = θf|e Normalization : ∀e : ∑ f θf|e = 1",
        "Probability of cipher sequence : p(f N 1 |θ) = ∑ [eN 1 ]",
        "p(f N 1 , eN",
        "1 |θ) Table 2: Definition of the probabilistic substitution cipher model. In contrast to simple or homophonic substitution ciphers, each plaintext token can be substituted by multiple cipher text tokens. The parameter θf|e represents the probability of substituting token e with token f . acquiring zero probability in some early iteration. In order to allow the lexicon to recover from these zeros, we use a smoothed lexicon p̂lex(f |e) = λplex(f |e) + (1 − λ)/|Vf | with λ = 0.9 when conducting the E-Step. 3.2.1 Beam Search Instead of evaluating the sum for terms with the exact largest contributions, we restrict ourselves to terms that are likely to have a large contribution to the sum, dropping any guarantees about the actual contribution of these terms.",
        "Beam search is a well known algorithm related to this idea: We build up sequences ec",
        "1 with grow-ing cardinality c. For each cardinality, only a set of the B most promising hypotheses is kept. Then for each active hypothesis of cardinality c, all possible extensions with substitutions fc+1 → ec+1 are explored. Then in turn only the best B out of the resulting B · Ve many hypotheses are kept and the algorithm continues with the next cardinality. Reaching the full cardinality N , the algorithm explored B · N · Ve many hypotheses, resulting in a complexity of O(BN Ve).",
        "Even though EM training using beam search works well, it still suffers from exploring all Ve possible extensions for each active hypothesis, and thus scaling linearly with the vocabulary size. Due to that, standard beam search EM training is too slow to be used in the decipherment setting. 3.2.2 Preselection Search Instead of evaluating all substitutions fc+1 → ec+1 ∈ Ve, this algorithm only expands a fixed number of candidates: For a hypothesis ending in a language model state σ, we only look at BLM many successor words ec+1 with the highest LM probability pLM (ec+1|σ) and at Blex many successor words ec+1 with the highest lexical probability plex(fc+1|ec+1). Altogether, for each hypothesis we only look at (BLM + Blex) many successor states. Then, just like in the standard beam search approach, we prune all explored new hypotheses and continue with the pruned set of B many hypotheses. Thus, for a cipher of length N we only explore N · B · (BLM + Blex) many hypotheses.2",
        "Intuitively speaking, our approach solves the EM training problem for decipherment using large vocabularies by focusing only on those substitutions that either seem likely due to the language model (”What word is likely to follow the current partial decipherment?”) or due to the lexicon model (”Based on my knowledge about the current cipher token, what is the most likely substitution?”).",
        "In order to efficiently find the maximizing e for pLM (e|σ) and plex(f |e), we build a lookup table that contains for each language model state σ the BLM best successor words e, and a separate lookup table that contains for each source word f the Blex highest scoring tokens e. The language model lookup table remains constant during all it-erations, while the lexicon lookup table needs to be updated between each iteration.",
        "Note that the size of the LM lookup table scales linearly with the number of language model states. Thus the memory requirements for the lookup ta-",
        "2",
        "We always use B = 100, Blex = 5, and BLM = 50. 761"
      ]
    },
    {
      "title": "f",
      "paragraphs": [
        "1"
      ]
    },
    {
      "title": "f",
      "paragraphs": [
        "2"
      ]
    },
    {
      "title": "f",
      "paragraphs": [
        "3"
      ]
    },
    {
      "title": "f",
      "paragraphs": [
        "4"
      ]
    },
    {
      "title": "f",
      "paragraphs": [
        "5"
      ]
    },
    {
      "title": "e",
      "paragraphs": [
        "5"
      ]
    },
    {
      "title": "e",
      "paragraphs": [
        "4"
      ]
    },
    {
      "title": "e",
      "paragraphs": [
        "3"
      ]
    },
    {
      "title": "e",
      "paragraphs": [
        "2"
      ]
    },
    {
      "title": "e",
      "paragraphs": [
        "1"
      ]
    },
    {
      "title": "Beam Search Preselection SearchFull Search f",
      "paragraphs": [
        "6"
      ]
    },
    {
      "title": "... start V oc ab Sentence",
      "paragraphs": [
        "Figure 1: Illustration of the search space explored by full search, beam search, and preselection search. Full search keeps all possible hypotheses at cardinality c and explores all possible substitutions at (c+1). Beam search only keeps the B most promising hypotheses and then selects the best new hypotheses for cardinality (c + 1) from all possible substitutions. Preselection search keeps only the B best hypotheses for every cardinality c and only looks at the (Blex + BLM ) most promising substitutions for cardinality (c + 1) based on the current lexicon (Blex dashed lines) and language model (BLM solid lines). Name Lang. Sent. Words Voc. VERBMOBIL English 27,862 294,902 3,723 OPUS Spanish 13,181 39,185 562 English 19,770 61,835 411 Table 3: Statistics of the copora used in this paper: The VERBMOBIL corpus is used to conduct experiments on simple substitution ciphers, while the OPUS corpus is used in our Machine Translation experiments. ble do not form a practical problem of our approach. Figure 1 illustrates full search, beam search, and our proposed method."
      ]
    },
    {
      "title": "4 Experimental Evaluation",
      "paragraphs": [
        "We first show experiments for data in which the underlying model is an actual 1:1 substitution cipher. In this case, we report the word accuracy of the final decipherment. We then show experiments for a simple machine translation task. Here we report translation quality in BLEU. The corpora used in this paper are shown in Table 3. 4.1 Simple Substitution Ciphers In this set of experiments, we compare the exact EM training to the approximations presented in this paper. We use the English side of the German-English VERBMOBIL corpus (Wahlster, 2000) to construct a word substitution cipher, by substituting every word type with a unique number. In order to have a non-parallel setup, we train language Vocab LM Method Acc.[%] Time[h] 200 2 exact 97.19 224.88 200 2 beam 98.87 9.04 200 2 presel. 98.50 4.14 500 2 beam 92.12 24.27 500 2 presel. 92.16 4.70 3 661 3 beam 91.16 302.81 3 661 3 presel. 90.92 19.68 3 661 4 presel. 92.14 23.72 Table 4: Results for simple substitution ciphers based on the VERBMOBIL corpus using exact, beam, and preselection EM. Exact EM is not tractable for vocabulary sizes above 200. models of order 2, 3 and 4 on the first half of the corpus and use the second half as ciphertext. Table 4 shows the results of our experiments.",
        "Since exact EM is not tractable for vocabulary sizes beyond 200 words, we train word classes on the whole corpus and map the words to classes (consistent along the first and second half of the corpus). By doing this, we create new simple substitution ciphers with smaller vocabularies of size 200 and 500. For the smallest setup, we can directly compare all three EM variants. We also in-clude experiments on the original corpus with vocabulary size of 3661. When comparing exact EM training with beam- and preselection EM training, the first thing we notice is that it takes about 20 times longer to run the exact EM training than training with beam EM, and about 50 times longer than the preselection EM training. Interestingly, 762 Model Method BLEU [%] Runtime",
        "2-gram Exact EM(Ravi and Knight, 2011) 15.3 850.0h whole segment lm Exact EM(Ravi and Knight, 2011) 19.3 850.0h 2-gram Preselection EM (This work) 15.7 1.8h 3-gram Preselection EM (This work) 19.5 1.9h Table 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on the Spanish/English OPUS corpus using only non-parallel corpora for training. the accuracy of the approximations to exact EM training is better than that of the exact EM training. Even though this needs further investigation, it is clear that the pruned versions of EM training find sparser distributions plex(f |e): This is desir-able in this set of experiments, and could be the reason for improved performance.",
        "For larger vocabularies, exact EM training is not tractable anymore. We thus constrain ourselves to running experiments with beam and preselection EM training only. Here we can see that the runtime of the preselection search is roughly the same as when running on a smaller vocabulary, while the beam search runtime scales almost linearly with the vocabulary size. For the full vocabulary of 3661 words, preselection EM using a 4-gram LM needs less than 7% of the time of beam EM with a 3-gram LM and performs by 1% better in symbol accuracy.",
        "To summarize: Beam search EM is an order of magnitude faster than exact EM training while even increasing decipherment accuracy. Our new preselection search method is in turn or-ders of magnitudes faster than beam search EM while even being able to outperform exact EM and beam EM by using higher order language models. We were thus able to scale the EM decipherment to larger vocabularies of several thousand words. The runtime behavior is also consistent with the computational complexity discussed in Section 3.2. 4.2 Machine Translation We show that our algorithm is directly applicable to the decipherment problem for machine translation. We use the same simplified translation model as presented by Ravi and Knight (2011). Because this translation model allows insertions and deletions, hypotheses of different cardinalities coexist during search. We extend our search approach such that pruning is done for each cardinality separately. Other than that, we use the same preselection search procedure as used for the simple substitution cipher task.",
        "We run experiments on the opus corpus as presented in (Tiedemann, 2009). Table 5 shows previously published results using EM together with the results of our new method:",
        "(Ravi and Knight, 2011) is the only publication that reports results using exact EM training and only n-gram language models on the target side: It has an estimated runtime of 850h. All other published results (using EM training and Bayesian inference) use context vectors as an additional source of information: This might be an explana-tion why Nuhn et al. (2012) and Ravi (2013) are able to outperform exact EM training as reported by Ravi and Knight (2011). (Ravi, 2013) reports the most efficient method so far: It only consumes about 3h of computation time. However, as mentioned before, those results are not directly comparable to our work, since they use additional context information on the target side.",
        "Our algorithm clearly outperforms the exact EM training in run time, and even slighlty improves performance in BLEU. Similar to the simple substitution case, the improved performance might be caused by inferring a sparser distribution plex(f |e). However, this requires further investigation."
      ]
    },
    {
      "title": "5 Conclusion",
      "paragraphs": [
        "We have shown a conceptually consistent and easy to implement EM based training method for decipherment that outperforms exact and beam search EM training for simple substitution ciphers and decipherment for machine translation, while reducing training time to a fraction of exact and beam EM. We also point out that the preselection method presented in this paper is not restricted to word based translation models and can also be applied to phrase based translation models. 763"
      ]
    },
    {
      "title": "References",
      "paragraphs": [
        "Eric Corlett and Gerald Penn. 2010. An exact A* method for deciphering letter-substitution ciphers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040–1047, Uppsala, Sweden, July. The Association for Computer Linguistics.",
        "Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B, 39.",
        "Xuedong Huang, Fileno Alleva, Hsiao wuen Hon, Mei yuh Hwang, and Ronald Rosenfeld. 1992. The sphinx-ii speech recognition system: An overview. Computer, Speech and Language, 7:137–148.",
        "Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised Analysis for Decipherment Problems. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06, pages 499–506, Stroudsburg, PA, USA. Association for Computational Linguistics.",
        "Malte Nuhn, Arne Mauser, and Hermann Ney. 2012. Deciphering foreign language by combining language models and context vectors. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 156–164, Jeju, Republic of Korea, July. Association for Computational Linguistics.",
        "Malte Nuhn, Julian Schamper, and Hermann Ney. 2013. Beam search for solving substitution ciphers. In Annual Meeting of the Assoc. for Computational Linguistics, pages 1569–1576, Sofia, Bulgaria, August.",
        "Chris Pal, Charles Sutton, and Andrew McCallum. 2006. Sparse forward-backward using minimum divergence beams for fast training of conditional random fields. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP).",
        "Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order n-gram models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 812–819, Honolulu, Hawaii. Association for Computational Linguistics.",
        "Sujith Ravi and Kevin Knight. 2011. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 12–21, Portland, Oregon, USA, June. Association for Computational Linguistics.",
        "Sujith Ravi. 2013. Scalable decipherment for machine translation via hash sampling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 362–371, Sofia, Bulgaria, August. Association for Computational Linguistics.",
        "Jörg Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia, Borovets, Bulgaria.",
        "Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of speech-to-speech translations. Springer-Verlag, Berlin.",
        "S.J. Young and Sj Young. 1994. The htk hidden markov model toolkit: Design and philosophy. Entropic Cambridge Research Laboratory, Ltd, 2:2– 44. 764"
      ]
    }
  ]
}
