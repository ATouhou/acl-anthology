{"sections":[{"title":"","paragraphs":["Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 25–32, Vancouver, October 2005. c⃝2005 Association for Computational Linguistics"]},{"title":"On Coreference Resolution Performance Metrics Xiaoqiang Luo 1101 Kitchawan Road, Room 23-121 IBM T.J. Wastson Research Center Yorktown Heights, NY 10598, U.S.A. xiaoluo@us.ibm.com Abstract","paragraphs":["The paper proposes a Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value."]},{"title":"1 Introduction","paragraphs":["A working definition of coreference resolution is partitioning the noun phrases we are interested in into equivalence classes, each of which refers to a physical entity. We adopt the terminologies used in the Automatic Content Extraction (ACE) task (NIST, 2003a) and call each individual phrase a mention and equivalence class an entity. For example, in the following text segment, (1): “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a district doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” mentions are underlined, “American Medical Association”, “its” and “group” refer to the same organization (object) and they form an entity. Similarly, “the heir apparent” and “president-elect” refer to the same person and they form another entity. It is worth pointing out that the entity definition here is different from what used in the Message Understanding Conference (MUC) task (MUC, 1995; MUC, 1998) – ACE entity is called coreference chain or equivalence class in MUC, and ACE mention is called entity in MUC.","An important problem in coreference resolution is how to evaluate a system’s performance. A good performance metric should have the following two properties:"," Discriminativity: This refers to the ability to differentiate a good system from a bad one. While this criterion sounds trivial, not all performance metrics used in the past possess this property."," Interpretability: A good metric should be easy to interpret. That is, there should be an intuitive sense of how good a system is when a metric suggests that a certain percentage of coreference results are correct. For example, when a metric reports ","or above correct for a system, we would expect that the vast majority of mentions are in right entities or coreference chains.","A widely-used metric is the link-based F-measure (Vilain et al., 1995) adopted in the MUC task. It is computed by first counting the number of common links between the reference (or “truth”) and the system output (or “response”); the link precision is the number of common links divided by the number of links in the system output, and the link recall is the number of common links divided by the number of links in the reference. There are known problems associated with the link-based F-measure. First, it ignores single-mention entities since no link can be found in these entities; Second, and more importantly, it fails to distinguish system outputs with different qualities: the link-based F-measure intrinsically favors systems producing fewer entities, and may result"]},{"title":"25","paragraphs":["in higher F-measures for worse systems. We will revisit these issues in Section 3.","To counter these shortcomings, Bagga and Baldwin (1998) proposed a B-cubed metric, which first computes a precision and recall for each individual mention, and then takes the weighted sum of these individual precisions and recalls as the final metric. While the B-cubed metric fixes some of the shortcomings of the MUC F-measure, it has its own problems: for example, the mention precision/recall is computed by comparing entities containing the mention and therefore an entity can be used more than once. The implication of this drawback will be revisited in Section 3.","In the ACE task, a value-based metric called ACE-value (NIST, 2003b) is used. The ACE-value is computed by counting the number of false-alarm, the number of miss, and the number of mistaken entities. Each error is associated with a cost factor that depends on things such as entity type (e.g., “LOCATION”, “PER-SON”), and mention level (e.g., “NAME,” “NOMINAL,” and “PRONOUN”). The total cost is the sum of the three costs, which is then normalized against the cost of a nominal system that does not output any entity. The ACE-value is finally computed by subtracting the normalized cost from",". A perfect coreference system will get a  ","ACE-value while a system outputs no entities will get a","ACE-value. A system outputting many erroneous entities could even get negative ACE-value. The ACE-value is computed by aligning entities and thus avoids the problems of the MUC F-measure. The ACE-value is, however, hard to interpret: a system with    ACE-value does not mean that   ","of system entities or mentions are correct, but that the cost of the system, relative to the one outputting no entity, is ",".","In this paper, we aim to develop an evaluation metric that is able to measure the quality of a coreference system – that is, an intuitively better system would get a higher score than a worse system, and is easy to interpret. To this end, we observe that coreference systems are to recognize entities and propose a metric called Constrained Entity-Aligned F-Measure (CEAF). At the core of the metric is the optimal one-to-one map between subsets of reference and system entities: system entities and reference entities are aligned by maximizing the total entity similarity under the constraint that a reference entity is aligned with at most one system entity, and vice versa. Once the total similarity is defined, it is straightforward to compute recall, precision and F-measure. The constraint imposed in the entity alignment makes it impossible to “cheat” the metric: a system outputting too many entities will be penalized in precision while a system outputting two few entities will be penalized in recall. It also has the property that a perfect system gets an F-measure","while a system outputting no entity or no common mentions gets an F-measure",". The proposed CEAF has a clear meaning: for mention-based CEAF, it reflects the percentage of mentions that are in the correct entities; For entity-based CEAF, it reflects the percentage of correctly recognized entities.","The rest of the paper is organized as follows. In Section 2, the Constrained Entity-Alignment F-Measure is presented in detail: the constraint entity alignment can be represented by a bipartite graph and the optimal alignment can be found by the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957). We also present two entity-pair similarity measures that can be used in CEAF: one is the absolute number of common mentions between two entities, and the other is a “local” mention F-measure between two entities. The two measures lead to the mention-based and entity-based CEAF, respectively. In Section 3, we compare the proposed metric with the MUC link-based metric and ACE-value on both artificial and real data, and point out the problems of the MUC F-measure."]},{"title":"2 Constrained Entity-Alignment F-Measure","paragraphs":["Some notations are needed before we present the pro-","posed metric and the algorithm to compute the metric. Let reference entities in a document","be","   ","and system entities be       To simplify typesetting, we will omit the dependency on  when it is clear from context, and write "," as ","and","  as  . Let","           and let  and  ","","be any subsets with entities. That is, ","  and",""," ",". Let     be the set of one-to-one entity maps from","  to   , and   be the set of all possible one-to-one maps between","the size-","subsets of and  . Or ","     ",""," ","","   "," ","      The requirement of one-to-one map means that for any       , and any  and  ,","we have that ","implies that ","  "," , and  ","  "," implies that","",". Clearly, there are","one-to-one maps from  to   (or     "," ), and","  ","",". Let  ","be a “similarity” measure between two en-","tities and",". "," takes non-negative value: zero"]},{"title":"26","paragraphs":["value means that","and","have nothing in common. For","example, "," could be the number of common men-","tions shared by and",", and "," the number of men-","tions in entity",". For any  , the total similarity ","","for a map","is the sum of similarities between the aligned entity pairs:"," ","","  ","   "," . Given a document",", and its reference entities  and system entities ",", we can find the best alignment maximizing the total similarity:","  ","","  ","  ","","     "," (1) Let    and   ","      denote the reference and","system entity subsets where ","is attained, respectively.","Then the maximum total similarity is     ","    ","  "," (2) If we insist that "," whenever","or","is","empty, then the non-negativity requirement of   makes it unnecessary to consider the possibility of map-ping one entity to an empty entity since the one-to-one map maximizing "," must be in ",". Since we can compute the entity self-similarity  "," and "," for any  and ","(i.e., using the identity map), we are now ready to define the precision, recall and F-measure as follows:","          (3)","          (4)    "," ","(5)","The optimal alignment  involves only","    ","","reference and system entities, and entities not aligned do not get credit. Thus the F-measure (5) penalizes a coreference system that proposes too many (i.e., lower precision) or too few entities (i.e., lower recall), which is a desired property.","In the above discussion, it is assumed that the similarity measure ","","is computed for all entity pair","",". In practice, computation of ","","can be","avoided if it is clear that","and","have nothing in common","(e.g., if no mention in","and","overlaps, then    ). These entity pairs are not linked and they will not be considered when searching for the optimal alignment. Consequently the optimal alignment could involve less than","reference and system entities. This can speed up considerably the F-measure computation when the majority of entity pairs have zero similarity. Nevertheless, summing over","entity pairs in the general formulae (2) does not change the optimal total similarity between  and ","and hence the F-measure.","In formulae (3)-(5), there is only one document in the test corpus. Extension to corpus with multiple test documents is trivial: just accumulate statistics on the per-document basis for both denominators and numerators in (3) and (4), and find the ratio of the two.","So far, we have tacitly kept abstract the similarity measure ","","for entity pair","and",". We will defer the","discussion of this metric to Section 2.2. Instead, we first","present the algorithm computing the F-measure (3)-(5). 2.1 Computing Optimal Alignment and F-measure A naive implementation of (1) would enumerate all the possible one-to-one maps (or alignments) between size- ","(recall that","    ) subsets of  and","size-subsets of",", and find the best alignment max-","imizing the similarity. Since this requires computing","the similarities between"," entity pairs and there are  ","  ","","possible one-to-one maps, the complex-","ity of this implementation is","   ","","",". This","is not satisfactory even for a document with a moderate","number of entities: it will have about"," million opera-","tions for ","","","",", a document with only","reference and","system entities.","Fortunately, the entity alignment problem under the constraint that an entity can be aligned at most once is the classical maximum bipartite matching problem and there exists an algorithm (Kuhn, 1955; Munkres, 1957) (henceforth Kuhn-Munkres Algorithm) that can find the optimal solution in polynomial time. Casting the entity alignment problem as the maximum bipartite matching is trivial: each entity in  and  is a vertex and the node pair "," , where  ,  , is connected by an edge with the weight ","",". Thus the problem (1) is exactly the maximum bipartite matching. With the Kuhn-Munkres algorithm, the procedure to compute the F-measure (5) can be described as Algorithm 1. Algorithm 1 Computing the F-measure (5). Input: reference entities:  ; system entities: ","Output: optimal alignment","","; F-measure (5).","1:Initialize:  ","; ","  ",".","2:For to  ","3: For to   4: Compute "," .","5:[  , ","   ]=KM {","} .","6: ",";","",".","7:     ;     ;  .","8:return and",". The input to the algorithm are reference entities  and system entities  . The algorithm returns the best one-to-"]},{"title":"27","paragraphs":["one map","","and F-measure in equation (5). Loop from line 2 to 4 computes the similarity between all the possible reference and system entity pairs. The complexity of this loop is"," ","",". Line 5 calls the Kuhn-Munkres","algorithm, which takes as input the entity-pair scores"," "," and outputs the best map","","and the corre-","sponding total similarity ","   . The worst case (i.e., when all entries in  ","are non-zeros) complexity","of the Kuhn-Algorithm is"," ","  ",". Line 6 computes “self-similarity”   and    needed in the F-measure computation at Line 7.","The core of the F-measure computation is the Kuhn-Munkres algorithm at line 5. The algorithm is initially discovered by Kuhn (1955) and Munkres (1957) to solve the matching (a.k.a assignment) problem for square matrices. Since then, it has been extended to rectangular matrices (Bourgeois and Lassalle, 1971) and parallelized (Balas et al., 1991). A recent review can be found in (Gupta and Ying, 1999), which also details the techniques of fast implementation. A short description of the algorithm is included in Appendix for the sake of completeness. 2.2 Entity Similarity Metric In this section we consider the entity similarity metric  "," defined on an entity pair "," . It is desirable that ","","is large when","and","are “close” and small","when","and","are very different. Some straight-forward","choices could be   ","","if","","otherwise (6)    "," if",""," otherwise (7)","(6) insists that two entity are the same if all the mentions","are the same, while (7) goes to the other extreme: two","entities are the same if they share at least one common","mention.","(6) does not offer a good granularity of similarity: For","example, if",", and one system response","is  ",", and the other system response","",""," , then clearly","","is more similar to","than","",", yet    ","","","",". For the same reason, (7) lacks of the desired discriminativity as well.","From the above argument, it is clear that we want to have a metric that can measure the degree to which two entities are similar, not a binary decision. One natural choice is measuring how many common mentions two entities share, and this can be measured by the absolute number or relative number:  "," (8)    ","  ","(9)","Metric (8) simply counts the number of common men-","tions shared by","and",", while (9) is the mention F-","measure between","and",", a relative number measuring","how similar and","are. For the abovementioned exam-","ple,   ","  ","    ","        ","    ","  ","     thus both metrics give the desired ranking   ","  "," ","   ,"," ","  "," ","   . If "," is adopted in Algorithm 1, ","","","","is the num-","ber of total common mentions corresponding to the best","one-to-one map","","while the denominators of (3) and (4)","are the number of proposed mentions and the number","of system mentions, respectively. The F-measure in (5)","can be interpreted as the ratio of mentions that are in the","“right” entities. Similarly, if ","","is adopted in Algorithm 1, the denominators of (3) and (4) are the number of proposed entities and the number of system entities, respectively, and the F-measure in (5) can be understood as the ratio of correct entities. Therefore, (5) is called mention-based CEAF and entity-based CEAF when (8) and (9) are used, respectively.",""," "," and"," ","","are two reasonable entity similarity measures, but by no means the only choices. At mention level, partial credit could be assigned to two mentions with different but overlapping spans; or when mention type is available, weights defined on the type confusion matrix can be incorporated. At entity level, entity attributes, if avaiable, can be weighted in the similarity measure as well. For example, ACE data defines three entity classes: NAME, NOMINAL and PRONOUN. Different weights can be assigned to the three classes.","No matter what entity similarity measure is used, it is crucial to have the constraint that the document-level similarity between reference entities and system entities is calculated over the best one-to-one map. We will see examples in Section 3 that misleading results could be produced without the alignment constraint.","Another observation is that the same evaluation paradigm can be used in any scenario that needs to measure the “closeness” between a set of system and reference objects, provided that a similarity between two objects is defined. For example, the 2004 ACE tasks include detecting and recognizing relations in text documents. A relation instance can be treated as an object and the same evaluation paradigm can be applied."]},{"title":"3 Comparison with Other Metrics","paragraphs":["In this section, we compare the proposed F-measure with the MUC link-based F-measure (and its variation B-cube F-measure) and the more recent ACE-value. The"]},{"title":"28","paragraphs":["1 2 3 4 5 6 7 8 9 A B C (4) system response (c) 1 2 3 4 5 6 7 8 9 A B C (1) truth 1 2 3 4 5 6 7 8 9 A B C (2) system response (a) 1 2 3 4 5 6 7 8 9 A B C (3) system response (b) 1 2 3 4 5 6 7 8 9 A B C (5) system response (d) Figure 1: Example entities: (1)truth; (2)system response (a); (3)system response (b); (4)system response (c); (5)system response (d) proposed metric has fixed problems associated with the MUC and B-cube F-measure, and has better interpretability than the ACE-value.","3.1 Comparison with the MUC F-measure and B-cube Metric on Artificial Data We use the example in Figure 1 to compare the MUC link-based F-measure, B-cube, and the proposed mention- and entity-based CEAF. In Figure 1, mentions are represented in circles and mentions in an entity are connected by arrows. Intuitively, if each mention is treated equally, the system response (a) is better than the system response (b) since the latter mixes two big entities,   and  ",", while","the former mixes a small entity with one big en-","tity ","",". System response (b) is clearly better than system response (c) since the latter puts all the mentions into a single entity while (b) has correctly separated the entity","from the rest. The system response (d) is the worst: the system does not link any mentions and outputs","single-mention entities.","Table 1 summarizes various F-measures for system response (a) to (d): the first column contains the indices of the system responses found in Figure 1; the second and third columns are the MUC F-measure and B-cubic F-measure respectively; the last two columns are the proposed CEAF F-measures, using the entity similarity metric"," "," and"," ","",", respectively.","As shown in Table 1, the MUC link-based F-measure fails to distinguish the system response (a) and the system response (b) as the two are assigned the same F-measure. The system response (c) represents a trivial output: all mentions are put in the same entity. Yet the MUC metric will lead to a  recall (  out of  reference links are System CEAF response MUC B-cube"," ","     (a) 0.947 0.865 0.833 0.733 (b) 0.947 0.737 0.583 0.667 (c) 0.900 0.545 0.417 0.294 (d) – 0.400 0.250 0.178 Table 1: Comparison of coreference evaluation metrics","correct) and a   precision ( ","out of system links are correct), which gives rise to a   ","F-measure. It is striking that a “bad” system response gets such a high F-measure. Another problem with the MUC link-based metric is that it is not able to handle single-mention entities, as there is no link for a single mention entity. That is why the entry for system response (d) in Table 1 is empty.","B-cube F-measure ranks the four system responses in Table 1 as desired. This is because B-cube metric (Bagga and Baldwin, 1998) is computed based on mentions (as opposed to links in the MUC F-measure). But B-cube uses the same entity “intersecting” procedure found in computing the MUC F-measure (Vilain et al., 1995), and it sometimes can give counter-intuitive results. To see this, let us take a look at recall and precision for system response (c) and (d) for B-cube metric. Notice that all the reference entities are found after intersecting with the system responsce (c):   "," . Therefore, B-cube recall is  (the corresponding precision is    ","","  ","     "," ","). This is counter-intuitive because the set of reference entities is not a sub-set of the proposed entities, thus the system response should not have gotten a ","recall. The same problem exists for the system response (d): it gets a  B-cube precision (the corresponding B-cube recall is        ","    ","    ","  ","",", but clearly not all","the entities in the system response (d) are correct! These","numebrs are summarized in Table 2, where columns with"," and","represent recall and precision, respectively.","System B-cube CEAF","response R P","-R","-P","-R","-P","(c) 1.0 0.375 0.417 0.417 0.196 0.588","(d) 0.25 1.0 0.250 0.250 0.444 0.111 Table 2: Example of counter-intuitive B-cube recall or precision: system repsonse (c) gets  recall (column R) while system repsonse (d) gets ","precision (column P). The problem is fixed in both CEAF metrics.","The counter-intuitive results associated with the MUC and B-cube F-measures are rooted in the procedure of “intersecting” the reference and system entities, which allows an entity to be used more than once! We will come back to this after discussing the CEAF numbers.","From Table 1, we see that both mention-based ( col-"]},{"title":"29","paragraphs":["umn under  "," ) CEAF and entity-based ("," ","",") CEAF are able to rank the four systems properly: system (a) to (d) are increasingly worse. To see how the CEAF numbers are computed, let us take the system response (a) as an example: first, the best one-one entity map is determined. In this case, the best map is: the reference entity "," is aligned to the system entity   , the reference entity   is aligned to the system ","","and the","reference entity is unaligned. The number","of common mentions is therefore","which results","in a mention-based ( ","",") recall","","and precision   . Since","     ","  , and            ,    ","","   ","(c.f. equation (4) and (3)), and the entity-based F-","measure (c.f. equation (9)) is therefore   ","   ","","","","   ","  CEAF for other system responses are computed similarly.","CEAF recall and precision breakdown for system (c) and (d) are listed in column 4 through 7 of Table 1. As can be seen, neither mention-based nor entity-based CEAF has the abovementioned problem associated with the B-cube metric, and the recall and precision numbers are more or less compatible with our intuition: for instance, for system (c), based on","","-CEAF number, we can say that about ","mentions are in the right entity, and","based on the","-CEAF recall and precision, we can state","that about     of “true” entities are recovered (recall) and about    ","of the proposed entities are correct.","A comparison of the procedures of computing the MUC F-measure/B-cube and CEAF reveals that the crucial difference is that the MUC and B-cube F-measure allow an entity to be used multiple times while CEAF insists that entity map be one-to-one. So an entity will never get double credit. Take the system repsonse (c) as an example, intersecting three reference entity in turn with the reference entities produces the same set of reference entities, which leads to ","recall. In the intersection step, the system entity is effectively used three times. In contrast, the system entity is aligned to only one reference entity when computing CEAF. 3.2 Comparisons On Real Data 3.2.1 MUC F-measure and CEAF","We have seen the different behaviors of the MUC F-measure, B-cube F-measure and CEAF on the artificial data. We now compare the MUC F-measure, CEAF, and ACE-value metrics on real data (compasion between the MUC and B-cube F-measure can be found in (Bagga and Baldwin, 1998)). Comparsion between the MUC F-measure and CEAF is done on the MUC6 coreference test set, while comparison between the CEAF and ACE-value is done on the 2004 ACE data. The setup reflects the fact that the official MUC scorer and ACE scorer run on their own data format and are not easily portable to the other data set. All the experiments in this section are done on true mentions. Penalty #sys-ent MUC-F","-CEAF -0.6 561 .851 0.750 -0.8 538 .854 0.756 -0.9 529 .853 0.753 -1 515 .853 0.753 -1.1 506 .856 0.764 -1.2 483 .857 0.768 -1.4 448 .863 0.761 -1.5 425 .862 0.749 -1.6 411 .864 0.740 -1.7 403 .865 0.741 -10 113 .902 0.445 Table 3: MUC F-measure and mention-based CEAF on the official MUC6 test set. The first column contains the penalty value in decreasing order. The second column contains the number of system-proposed entities. The column under MUC-F is the MUC F-measure while","","- CEAF is the mention-based CEAF.","The coreference system is similar to the one used in (Luo et al., 2004). Results in Table 3 are produced by a system trained on the MUC6 training data and tested on the","official MUC6 test documents. The test set contains","reference entities. The coreference system uses a penalty parameter to balance miss and false alarm errors: the smaller the parameter, the fewer entities will be generated. We vary the parameter from","","to",", listed in the first column of Table 3, and compare the system performance measured by the MUC F-measure and the proposed mention-based CEAF.","As can be seen, the mention-based CEAF has a clear maximum when the number of proposed entities is close to the truth: at the penlaty value","",", the system produces","entities, very close to",", and the","","-CEAF achieves the maximum",". In contrast, the MUC F-measure increases almost monotonically as the system proposes fewer and fewer entities. In fact, the best system according to the MUC F-measure is the one proposing only","entities. This demonstrates a fundamental flaw of the MUC F-measure: the metric intrinsically favors a system producing fewer entities and therefore lacks of discriminativity. 3.2.2 ACE-Value and CEAF","Now let us turn to ACE-value. Results in Table 4 are produced by a system trained on the ACE 2002 and 2004 training data and tested on a separate test set, which contains ","","reference entities. Both ACE-value and the","mention-based CEAF penalizes systems over-producing","or under-producing entities: ACE-value is maximum"]},{"title":"30","paragraphs":["Penalty #sys-ent ACE-value(%)","-CEAF 0.6 1221 88.5 0.726 0.4 1172 89.1 0.749 0.2 1145 89.4 0.755 0 1105 89.7 0.766 -0.2 1050 89.7 0.775 -0.4 1015 89.7 0.780 -0.6 990 89.5 0.782 -0.8 930 88.6 0.794 -1 891 86.9 0.780 -1.2 865 86.7 0.778 -1.4 834 85.6 0.769 -1.6 790 83.8 0.761 Table 4: Comparison of ACE-value and mention-based CEAF. The first column contains the penalty value in decreasing order. The second column contains the number of system-proposed entities. ACE-values are in percentage. The number of reference entities is   .","when the penalty value is ","and CEAF is maximum","when the penalty value is  . However, the optimal","CEAF system produces  ","entities while the optimal","ACE-value system produces ","","entities. Judging from the number of entities, the optimal CEAF system is closer to the “truth” than the counterpart of ACE-value. This is not very surprising since ACE-value is a weighted metric while CEAF treats each mention and entity equally. As such, the two metrics have very weak correlation.","While we can make a statement such as “the system with penalty","","puts about    ","mentions in right entities”, it is hard to interpret the ACE-value numbers.","Another difference is that CEAF is symmetric1",", but ACE-Value is not. Symmetry is a desirable property. For example, when comparing inter-annotator agreement, a symmetric metric is independent of the order of two sets of input documents, while an asymmetric metric such as ACE-Value needs to state the input order along with the metric value."]},{"title":"4 Conclusions","paragraphs":["A coreference performance metric – CEAF – is proposed in this paper. The CEAF metric is computed based on the best one-to-one map between reference entities and system entities. Finding the best one-to-one map is a maximum bipartite matching problem and can be solved by the Kuhn-Munkres algorithm. Two example entity-pair similarity measures (i.e., "," and ","",") are proposed, resulting one mention-based CEAF and one entity-based CEAF, respectively. It has been shown that the proposed CEAF metric has fixed problems associated with the MUC link-based F-measure and B-cube F-measure. 1 This was pointed out by Nanda Kambhatla. The proposed metric also has better interpretability than ACE-value."]},{"title":"Acknowledgments","paragraphs":["This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No. N66001-99-2-8916. The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.","The author would like to thank three reviewers and my colleagues, Hongyan Jing and Salim Roukos, for sugges-tions of improving the paper."]},{"title":"References","paragraphs":["Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation (LREC’98), pages 563–566.","Egon Balas, Donald Miller, Joseph Pekny, and Paolo Toth. 1991. A parallel shortest augmenting path algorithm for the assignment problem. Journal of the ACM (JACM), 38(4).","Francois Bourgeois and Jean-Claude Lassalle. 1971. An extension of the munkres algorithm for the assignment problem to rectangular matrices. Communications of the ACM, 14(12).","R. Fletcher. 1987. Practical Methods of Optimization. John Wiley and Sons.","Anshul Gupta and Lexing Ying. 1999. Algorithms for finding maximum matchings in bipartite graphs. Technical Report RC 21576 (97320), IBM T.J. Watson Research Center, October.","H.W. Kuhn. 1955. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(83).","Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In Proc. of ACL.","MUC-6. 1995. Proceedings of the Sixth Message Understanding Conference(MUC-6), San Francisco, CA. Morgan Kaufmann.","MUC-7. 1998. Proceedings of the Seventh Message Understanding Conference(MUC-7).","J. Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of SIAM, 5:32–38."]},{"title":"31","paragraphs":["NIST. 2003a. The ACE evaluation plan. www.nist.gov/speech/tests/ace/index.htm.","NIST. 2003b. Proceedings of ACE’03 workshop. Book-let, Alexandria, VA, September.","M. Vilain, J. Burger, J. Aberdeen, D. Connolly, , and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In In Proc. of MUC6, pages 45–52."]},{"title":"Appendix: Kuhn-Munkres Algorithm","paragraphs":["Let index the reference entities ","and index the system entities  , and ","","be the similarity between the"," reference entity and the","system entity. Alge-","braically, the maximum bipartite matching can be stated","as an integer programming problem:  "," "," (10) subject to: "," (11) ","  ","(12)"," (13)","If",", the","","reference entity and the","","system","entity are aligned. Constraint (11) (or (12)) implies that a","reference (or system) entity cannot be aligned more than","once with a system (or reference) entity. Observe that the coefficients of (11) and (12) are uni-","modular. Thus, Constraint (13) can be replaced by","","","","(14)","The dual (cf. pp. 219 of (Fletcher, 1987)) to the optimization problem (10) with constraints (11),(12) and (14) is: ","    ","","       ","(15)","","","  ","","","","","(16)","","  ","(17)","","  ","(18)","The dual has the same optimal objective value as the pri-","mal.","It can be shown that the optimal conditions for the dual","problem (and hence the maximum similarity match) are:","","","",""," if ","","is aligned (19)","","","","if","is free (i.e., not aligned) (20)","","","","if","is free. (21)","The Kuhn-Munkres algorithm starts with an empty","match and an initial feasible set of  ","and","","",", and","iteratively increases the cardinality of the match while satisfying the optimal conditions (19)-(21). Notice that conceptually, a matching problem with a rectangular matrix ","","can always reduce to a square one by padding zeros (this is not necessary in practice, see, for instance (Bourgeois and Lassalle, 1971)). For this reason, we state the Kuhn-Munkres algorithm for the case where     (or ","","",") in Algorithm 2. The proof","of correctness is omitted due to space limit.","Note that  "," on line 9 stands for the augment-","ing (i.e., a free node followed by an aligned node, fol-","lowed by a free node, ...) path from","to","in the corre-","sponding bipartite graph.","",""," is understood as edge “exclusive-or:” if an edge "," is in","and on the","path   "," , it will be removed from","; if the edge is","in either","or ",""," , it will be added. Algorithm 2 Kuhn-Munkres Algorithm Input: similarity matrix:  ","Output: best match","","and similarity",".","1:Initialize:",", ","","; , ",";",".","2:For to  3: If","is not free, Continue; EndIF. 4:",",","; 5: While true 6:   ","   "," "," 7: If     8: pick    ","9: If","is free","10:","","",""," ; break 11: Else 12: Find","such that ","","",".","13: ",".","14: Goto line 6.","15: EndIf","16: Else  ","17:","","","","     ","","","","","","  18:   ","","","","","","","     ","","","","",""," ","19:","   ","for","",".","20:","","  for",".","21:   . Goto line 9.","22: EndIf","23: EndWhile","24:EndFor","25:     ","","",""," .","26:Return and","."]},{"title":"32","paragraphs":[]}]}