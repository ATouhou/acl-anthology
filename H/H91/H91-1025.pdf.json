{"sections":[{"title":"A Statistical Approach to Sense Disambiguation in Machine Translation Peter F. Brown, Stepheu A. Della. Pietra, Vince,,t d. Della Pietra, Robert L. Mercer • -' S IBM Resea,rch Division, ql horns,: J. Wa, tson Resea,rch Center Yorktown Heights, NY 10598 ABSTRACT","paragraphs":["We describe ~ statisticM technique for assigning senses to words. An instance of ~ word is assigned ;~ sense by asking a question about the context in which the word ~tppears. The qttestlou is constructed to ha, re high mutua,1 i~fformation with the word's translations. INTRODUCTION","An a,lluring a,spect of the staMstica,1 a,pproa,ch to ins,chine tra,nsla,tion rejuvena.ted by Brown, et al., [_1] is the systems.tic framework it provides for a.tta.ck-ing the problem of lexicM dis~tmbigua.tion. For example, the system they describe tra,ns]a.tes th.e French sentence Je vais prendre la ddeision a,s [ will make the decision, thereby correctly interpreting prendre a.s make, The staMstica.l tra.nslation model, which supplies English. tra,nsla,tions of French words, prefers the more common tra.nslation take, but the trigram la.n-gu.age mode] recognizes tha.t the three-word sequence make the decision is much more proba])le tha.n take the decision.","The system is not a.lwa,ys so successful. It incorrectly renders Je vats prendre ma propre ddcision a.s 1 will take my own decision. Here, the la.nguage model does not realize tha, t take my own decision is improbable beca,use take a,nd decision no longer fall within a. single trigram.","Errors such a.s this a,re common because otlr sta,- tistical models o.ly capture loca,l phenomena,; if l, he context necessa,ry to determine ~ transla, tion fa,lls outside the scope of our models, the word is likely to be tra,nsla,ted incorrectly. However, if the re]evant co.- text is encoded locally, the word should be tra, nsla, ted correctly. We ca,n a,chieve this within the traditionM p,~radigm of a.na,lysis - tra,nsfer - synthesis by incorpora,ting into the ana,lysis pha,se a, sense--disa, mbigu~tion compo,ent that assigns sense la, bels to French words. ]if prendre is labeled with one sense in the context of ddcisiou but wil.h a, different sense in other contexts, then the tra,nsla,tion model will learn from training data tha,t the first sense usua,lly tra.nslates to make, where.a,s the other sense usua,lly tra.nslates to take.","In this paper, we describe a. sta, tistica,1 procedure for constructing a. sense-disambiguation eomponent that label words so as to elucida.te their translations. STATISTICAL TRANSLATION","As described by Brown, et al. []], in the sta.tistica.1 a.l)proa.ch to transla, tion, one chooses for tile tra,nsla,- tion of a. French sentence .F, tha.t English sentence E which ha.s the greatest l)robability, Pr(EIF), a.ccordi,g to a, model of th.e tra, ns]ation process. By Ba.yes'"]},{"title":"r,,le, Pr(EI ~') = Pr(E) Pr(FIE )/Pr(.F). Since the","paragraphs":["(lenomina.tor does not del)end on E, the sentence for which Pr(EIF ) is grea, test is also the sentence for which the product Pr(E) Pr(FIE ) is grea~test. The first term in this product is a~ sta, tisticM cha.ra.cteriza-tion of the, English ]a.nguage a, nd the second term is a, statistical cha.ra.cteriza,timt of the process by which English sentences are tra.nslated into French. We can compute neither of these probabilities precisely. Rather, in statistical tra.nslat, iou, we employ a. language model P,,od~l(E) which 1)rovide, s a,n estima.te of Pr (E) and a, lrav, slatiov, model which provides a,n estimate of"]},{"title":"t'r ( Vl/~:).","paragraphs":["146","The performance of the system depends on the extent to which these statistical models approximate the actual probabilities. A useful gauge of this is tile cross entropy 1"]},{"title":"H(EIF)-= - ~","paragraphs":["Pr(E,F) log"]},{"title":"PmoZ~,(EI F) (1)","paragraphs":["E,F which measures the average uncertainty that the model has about the English translation E of a French sentence F. A better model has less uncertainty and thus a lower cross entropy.","A shortcoming of the architecture described above is that it requires the statistical models to deal directly with English and French sentences. Clearly the probability distributions Pr(E) and Pr(FIE ) over sentences are immensely complicated. On the other hand, in practice the statistical models must be relatively simple in order that their parameters can be reliably estimated from a manageable amount of training data. This usually means that they are restricted to the modeling of local linguistic phenonrena. As a. result, the estimates Pmodcz(E) and Pmodd(F I E) will be inaccurate.","This difficulty can be addressed by integrating statistical models into the traditional machine translation architecture of analysis-transfer-synthesis. The resulting system employs","1. An analysis component which encodes a French sentence F into an intermediate structure F<","2. A statistical transfer component which translates F t a corresponding intermediate English structure E'. This component incorporates a language model, a translation model, and a decoder as before, but here these components deal with the intermediate structures rather than the sentences directly.","3. A synthesis component which reconstructs an English sentence E from E t. For statistical modeling we require that the synthesis transformation E ~ ~ E be invertible. Typically, analysis and synthesis will involve a sequence of successive transformations in which F p is incrementally","tin this equation and in the remainder of the paper, we use bold face letters (e.g. E) for random variables and roman letters (e.g. E) for the values of random variables. constructed from F, or E is incrementally recovered from E I.","'File purpose of analysis and synthesis is to facilitate the task of statistical transfer. This will be the case if the probability distribution Pr (E ~, F ~) is easier to model then the original distribution Pr (E, F). In practice this nleans that E' and F' should encode global linguistic facts about E and F in a local form.","The utility of tile analysis and synthesis transformatious can be measured in terms of cross-entropy. Thus transfotma.tions F -+ F' and t~/ ---+ E are useful if we Call construct models ' P ,~od~t( F I E') and P',,,oa+,(E') such that H(E' I r') < H(EIF )."]},{"title":"SENSE DISAMBIGUATION","paragraphs":["In this paper we present a statistical method for automatically constructing analysis and synthesis transformations which perform cross-lingual word-sense labeling. The goal of such transformations is to label the words of a French sentence so as to ehlcidate their English. trauslations, and, conversely, to label the words of an English sentence so as to elucidate their French translations. For exa.mple, in some contexts the French verb prendre translates as to take, but in other contexts it translates as to make. A sense disambiguation transformation, by examining the contexts, might label occurrences of prendre that likely mean to take with one lal)el, and other occurrences of prendre with another label. Then the uncertainty in the translation of prendre given the label would be less than the uncertainty in the translation of prendre without the label. All, hough tile label does not provide any infof mation that is not already present in the context, it encodes this information locally. Thus a local statistical model for the transfer of labeled sentences should be more accurate than one for the transfer of unlal)eled ones.","While the translation o:f a word depends on many woMs in its context, we can often obtain information by looking at only a single word. For example, in the sentence .Ic vats prendre ma propre ddeision (I will 'make my own decisiou), tile verb prendre should be translated as make because its object is ddcision. If we replace ddcision by voiture then prendre should be translated as take: Je vais prendre ma propre voiture (l will take my own car). Thus we can reduce the uncertainity in the translation of prendre by asking a question about its object, which is often the first noun 147 to its right, and we might assign a sense to"]},{"title":"prendre","paragraphs":["based upon the answer to this question. In"]},{"title":"It doute que Ins ndtres gagnent (He doubts that we will win),","paragraphs":["the word"]},{"title":"il","paragraphs":["should be translated as"]},{"title":"he.","paragraphs":["On the other hand, if we replace"]},{"title":"doute","paragraphs":["by"]},{"title":"faut","paragraphs":["then"]},{"title":"il","paragraphs":["should be translated as"]},{"title":"it: It faut que les nStres gagnent (It is necessary that we win).","paragraphs":["Here, we might assign a sense label to"]},{"title":"il","paragraphs":["by asking a,bout the identity of the first verb to its right.","These examples motivate a. sense-labeling scheme in which the la.bel of a word is determined by a question aJ)out an"]},{"title":"informant","paragraphs":["word in its context. In the first example, the informant of"]},{"title":"prendre","paragraphs":["is the first noun to the right; in. the second example, the infof mant of"]},{"title":"ilis","paragraphs":["the first verb to the right. If we want to assign n senses to a word then we can consider a question with n answers.","We can fit this scheme into the fl:amework of the previous section a.s follows:"]},{"title":"The Intermediate Structures.","paragraphs":["The intermediate structures E' and F r consist of sequences of words labeled by their senses. Thus F' is a sentence over the expanded vocabulary whose 'words' f' are pairs (f,l) where f is a word in the original French vocabulary and 1 is its sense label. Similarly, E ¢ is a sentence over the expanded vocabulary whose words e t are pairs (e, l) where e is a.n English word and l is its sense label."]},{"title":"The analysis and synthesis transformations.","paragraphs":["For each French word and each English word we choose"]},{"title":"an informant site,","paragraphs":["such as"]},{"title":"first noun to the left,","paragraphs":["and an n-ary question about the va,lue of the informant at that site. The analysis transformation F ~ U and the inverse synthesis transfof marion E ~ E ~ map a sentence to the intermediate structure in which each word is labeled by a sense determined by the question a])out its informant. The synthesis transformation E ~ ~ E maps a labeled sentence to a sentence in which the labels have been removed."]},{"title":"The probability models.","paragraphs":["We use the translation model that was discussed in [l] for both e;~oaet(F'lE') and for"]},{"title":"P,nodd(FIE).","paragraphs":["We use a trigram language model. [1] for P,,~oa~a(E) and In order to construct these tra.nsformations we need to choose for each English and French word a.n informant and a question. As suggested in the previous section, a criterion for doing this is that of minimizing the (:ross entropy H(E' I F'). In the remainder of the l)aper we present an algorithm for doing this. THE TRANSLATION MODEL","We begin by reviewing our statistical model for the translation of a sentence from one language to another [1]. In statistical French to English translation system. we need to model transformations from English sentences E to French sentences F, or from intermediate English structures E' to intermediate French structures F t. ltowever, it is clarifying to consider transformations from an arbitrary"]},{"title":"source language","paragraphs":["to an arbitrary"]},{"title":"target language. Review of the Model","paragraphs":["The l)urpose of a translation model is to compute the prol)al)i]ity"]},{"title":"P,,odet(T [ S)","paragraphs":["of transforming a source sentence S into a. target sentence T. For our simple mode], we assume that each word of S independent]y I)rodnces zero or mote words from the target vocabulary and that these words are then ordered to produce T. We use the term"]},{"title":"alignment","paragraphs":["to refer to an associa-tion between words in T and words in S. The probability"]},{"title":"P,,oda(T I S)","paragraphs":["is the sum of the probabilities of all possible alignnmnts A between S and T"]},{"title":"I S) = e,,oa t(T, A is).","paragraphs":["A"]},{"title":"(2)","paragraphs":["The joint probal)ility"]},{"title":"P,,odft(7', A I S)","paragraphs":["of T and a patticula.r a.]ignmeut is given by"]},{"title":"1',,,od¢,(7', A IS) = (a) H P(tl\"~A(t)) II P(iZA(s) ls)-Pdi.'toTtio'(T, A I S).","paragraphs":["t6T s6.5' llere .iA(t) is tile word of ,5' aligned with t in the alignmen t A, a.nd"]},{"title":"fi.A (s)","paragraphs":["is the number of words of T aligned with s ill A. Tile"]},{"title":"distortion model Pdistortlon","paragraphs":["describes tile ordering of tile words of T. We will not give it explicitly. The parameters in (3) are","I. The l)robabilities p(n ] s) that a word s in the source language generates n target words; 2. \"File prol)abilities"]},{"title":"p(t I s)","paragraphs":["that s generates the word t; 3. The pa.ra,meters of the distortion model. 148 We determine values for these parameters using"]},{"title":"maximv.m likelihood training.","paragraphs":["Thus we collect a large"]},{"title":"bilingual corpus","paragraphs":["consisting of pairs of sentences (S, T) which are translations of one another, and we seek parameter va.lues that maximize the likelihood of this"]},{"title":"training data","paragraphs":["as computed by the model. This is equivalent to minimizing the cross entropy If(T IS) = - ~"]},{"title":"Pt~,i,,(S,T)","paragraphs":["log"]},{"title":"P,,,od,t(TI S)","paragraphs":["(4) S,T where"]},{"title":"Ptr~.i,~(S,T)","paragraphs":["is the empirical distribution obtained by counting the number of times that the pair (S, T) occurs in the training corpus. The Viterbi Approximation","The sum over alignments in (2) is too expensive to compute directly since the number of alignments increases exponentially with sentence length. It is useful to approximate this sum by the single term corresponding to the alignment, A(S,T), with greatest probability. We refer to this approximation as the"]},{"title":"Viterbi approzimation","paragraphs":["and to A(S,T) as the"]},{"title":"Viterbi alignment.","paragraphs":["Let c(s,t) be the expected number of times that s is aligned with t in the Viterbi alignnmnt of a pair of sentences drawn at random from the training data.. Let"]},{"title":"c(s, n)","paragraphs":["be the expected number of times that s is aligned with n words. Then"]},{"title":"c(s,t) = ~ P,~o,~(S,T)c(s,t l J(S,T) )","paragraphs":["S,T"]},{"title":"e(s,n) = ~ Pt,~i,(S,T)c(s, n I A(S,T) )","paragraphs":["(5) S,T where c(.s,t I A) is the number of times that s is aligned with t in the alignment A, and"]},{"title":"c(s, n I A)","paragraphs":["is the number of times that s generates n target words in A. It can be shown [2] that these counts are also averages"]},{"title":"with respect to the model c(s, t) = ~ P,,,oda(S, T) c(s,","paragraphs":["t I A(,5', T) ) S,T"]},{"title":"~(s,~) = ~ P.,o~,(S,T)e(s,,~ I A(S,T)). (6)","paragraphs":["S,T","By normalizing the counts c(s,t) and c(s,n) we obtain probability distributions"]},{"title":"p(s, t)","paragraphs":["and"]},{"title":"p(s, n) 2 1 1 p(~, t) _ It(S, t) p(~, ,,) _ I ~(s, ,3. (7)","paragraphs":["7ZOT77~ ~OT77Z ~In these equations and in the remainder of the paper, we The conditional distributions"]},{"title":"p(t I s)","paragraphs":["and"]},{"title":"p(n Is)","paragraphs":["are the Viterbi approximation estimates [or the parameters of the model. The marginals satisfy"]},{"title":"~p(.%,,) = ,,,(,,) ~p(,,t) : ~,(t)","paragraphs":["~qt"]},{"title":"~.p(s,t)- _2_~(~),,,(~) (8)","paragraphs":["t where u(s) and u(t) are the unigram distributions of s and t and Fz(s) = ~ p(n I s)n is the average number of target words aligned with s. These formulae reflect the fact that in any alignment each target word is aligned with exactly one source word. CROSS ENTROPY ]n this section we express the cross entropies"]},{"title":"H","paragraphs":["( S I T ) and ][(S ~ I Tt) in terms of the information between source and target words. In the Viterbi approximation the cross entropy"]},{"title":"H(T","paragraphs":["IS) is given by"]},{"title":"H(T I s) : Lr { H(t I s) + H(n t ~) } (9)","paragraphs":["where"]},{"title":"LT","paragraphs":["is the average length of the target sentences in the training data, and"]},{"title":"lt(t","paragraphs":["I s) and"]},{"title":"It(n","paragraphs":["I s) are the conditional entropies for the probability distributions"]},{"title":"1,(s, t) and p(.., ~): H(t Is) = -~p(s,t) log p(tls)","paragraphs":[",%t"]},{"title":",\"(,, I~) : - ~p(,,,,~) log v(.,l~). (10)","paragraphs":[".$ t~. We wa.nt a similar exl)ression for the cross entropy"]},{"title":"I[(S","paragraphs":["IT). Since"]},{"title":"l ,,,oa~,(~, T) P,,,o~.dT I S) P,,~o~z(S),","paragraphs":["this cross entropy depends on both the translation model, ]',,,oact(T I S), and the language model, P,,.oact(S). We now show that with a suitable additional approxitn ation"]},{"title":"H(S I T) : Lr { H(n I+) - ~(+,t) } + H(S) (~1)","paragraphs":["use the generic symbol ~ to denote ~ normalizing fa.ctor that norgn com, er!s counts to probabilities. We let the actua.1 value of .ol I,e implicit from the context. Thus, for example, in the left ha.nd equation of (7), the normalizing factor is norm = ~,,, c(s, t) which equals tile a,verage length of target sentences. In the right hand equation of (7), the normalizing fa.ctor is the average ]engt.h of source sentences. 149 where H(S) is the cross entropy of"]},{"title":"P,+od+t(S)","paragraphs":["and I(s, t) is tire mutual information between t and s for the probability distribution"]},{"title":"p(s, t).","paragraphs":["The additional approximation that we require is HiT) ,~"]},{"title":"LTHit) =- --LT ~p(t)log pi t)","paragraphs":["t"]},{"title":"(12)","paragraphs":["where"]},{"title":"p(t)","paragraphs":["is the marginal of"]},{"title":"p(s,t).","paragraphs":["This amounts to approximating"]},{"title":"Pmod¢l(T)","paragraphs":["by the unigram distribution that is closest to it in cross entropy. Granting this, formula (11) is a consequence of (9) and of the identities"]},{"title":"HCS IT) = Hi T I S) - HCT) + I/iS), HCt,) = HCt I +) + I(+, t). (13) Target Questions","paragraphs":["For sensing target sentences, a question about an informant is a f, nction ~ from the target vocabulary into the set of possible senses. If the informant of t is z, then t is assigned the sense 5(z). We want to choose the function fi(z) to minimize the cross entropy"]},{"title":"It(S","paragraphs":["IT'). Front formula (34), we see that this is equivale:,t to maximizing the conditional mutual i,formation I(s, t' I t) between s and t'"]},{"title":"p(s,~(z) I t) (15) ICs, t' I t ) = ~_,pC.s,x","paragraphs":["[ t)log"]},{"title":"pCs","paragraphs":["1 t)P(+(.+) t 0 where"]},{"title":"p(s, t, x)","paragraphs":["is the probability distribution obtained by counting the number of times in the Viterbi alignments that s is aligned with t and the value of the informa, t of t is x,","Next consider H(S' I T'). Let S ~ S' and T T' be sense labeling transformations of the type discussed in Section 2. Assume that these transformations"]},{"title":"preserve Viterbi alignments;","paragraphs":["that is, if the words s and t are aligned in the Viterbi alignment for ($, T), then their sensed versions s ~ and t' are aligned in the Viterbi alignment for"]},{"title":"(SI,T').","paragraphs":["It follows that the word translation probabilities obtained from the Viterbi align ntents satisfy"]},{"title":"p(s,t) = Zt'etP(S,t') =","paragraphs":["~,'oP('S',t) where the sums range over tire sensed versions t' of t and the sensed versions s' of ~. By applying (11) to the cross entropies HCS I T),"]},{"title":"It(S I","paragraphs":["T'), and H(S'I T), it is not hard. to verify that"]},{"title":"HCSIT') = HCSI T)- LT~PCO/Cs, t'I¢)","paragraphs":["t"]},{"title":"HCS'IT) = HiS IT)- (:14) L~ ~ ~(,){:(t, +' I s)+ .rCn, ~', Is)}.","paragraphs":["$ Here I(s, t' I t) is the conditional mutual information given a target word t between its translations s and its sensed versions t'; I(t, s' [ s) is the conditional mutual information given a source word s between its translations t a.nd its sensed versions s'; and I(n,s' I s) is the conditional mutual information given .s between n and its sensed versions s'."]},{"title":"pC.+, ~, ~) - p Cs, +, +) - 1 ~ P,ro+..CS, T) e(s, +, + I ACS, T))","paragraphs":["71\"()7+71~ S,T"]},{"title":"~ v0, t,:~). (16)","paragraphs":["?~Or77% x:e(z)=c","An exhaustive search for the best ~ requires a computation that is exponential in the number of values of x and is not practical. In previous work [3] we found a good ~ usi,g the flip-flop algorithm [4], which is only al)l)licable if the number of senses is restricted to two. Since then, we have developed a different Mgorithm that can be used to find 5 for any number of senses. The algorithm uses the technique of"]},{"title":"alternating minimization,","paragraphs":["and is similar to the k-means algorithm for determining pattern clusters and to the generalized Lloyd algorithm for designing vector quantitizers. A discussion of alternating minimization, together with refcrences, can be found in Chou [5].","The algorithm is ba,sed on tile fact that, up to a constant independent of 5, the mutual information l(s,t t I t) can be expressed as an infimum over conditional probal)ility distributions q(s I c),"]},{"title":"l(s, t' If) = (17)","paragraphs":["i.f ~"]},{"title":"pix)D(pis","paragraphs":["I x,t) ; q(s I 5(x)) + constant q :r"]},{"title":"SELECTING QUESTIONS","paragraphs":["We now present an algorithm for finding good in-formants and questions for sensing. where"]},{"title":"Di~(+) ; q(+)) =- ~V(s)log p(') (18) + q(s)","paragraphs":["150 The best value of the information is thus a.n infimiim over both the choice for 2. and the choice for the q. This suggests the following iterative procedure for obtaining a good 2: 1. For given q, find the best"]},{"title":"E:","paragraphs":["E(x)"]},{"title":"=","paragraphs":["argmin,D(p(s"]},{"title":"(","paragraphs":["x,t) ; g(s"]},{"title":"(","paragraphs":["c)). 2. For this"]},{"title":"El","paragraphs":["find the best 3:"]},{"title":"3.","paragraphs":["Iterate steps (1) a.nd (2) ilntil no fnrther increase in I(s,"]},{"title":"t' I t)","paragraphs":["results."]},{"title":"Source Questions","paragraphs":["For sensing source sentences, a, question a.bont an informant is a Iunction"]},{"title":"2:","paragraphs":["from the source voca1)iila.ry int'o the set of possible senses. We want to chose 2. to minimize the entropy"]},{"title":"H(S1 I T).","paragraphs":["From"]},{"title":"( 14)","paragraphs":["this is equivalent to ~na.ximizing the sum I(t,st"]},{"title":"I","paragraphs":["s)"]},{"title":"+ T(","paragraphs":["n"]},{"title":",","paragraphs":["s'"]},{"title":"I","paragraphs":["s"]},{"title":").","paragraphs":["In analogy to (18), and we can again find a good 2 by alternating minimiza.tion."]},{"title":"CONCLUSION","paragraphs":["In this paper we presented a general framework for integrating analysis and synthesis with statistical translation, and within this framework we invcstigated cross-lingnal sense labeling. We gave an algorithm for antoinatically constructing a simple labeling transformation that assigns a sense to a word by asking a question about a single word of the context. In a companion paper"]},{"title":"[3]","paragraphs":["we present results of translation experiments using a sense-labeling cvnlponent that employs a similar algorithn~. We are currently studying the auton~atic construction of more complex transformations which utilize more detailed contextual informa tion."]},{"title":"((A","paragraphs":["stat,istic:xl a.pproa.ch to madline transla.tion,)) Compufnlio1rrc.1 Ling.zl.istics, vol. 16, pp. 79-85, ;June 1990. [2]"]},{"title":"P.","paragraphs":["Brown,"]},{"title":"S.","paragraphs":["Dellal'ietra,"]},{"title":"V.","paragraphs":["Uella.Pietra., and"]},{"title":"It.","paragraphs":["Mercer, \"Initial estimates of word tra.nsla.tion prol)a.Bilities.\" In prepa,ra.tion. [3]"]},{"title":"P.","paragraphs":["Brown,"]},{"title":"S.","paragraphs":["Della.Pietra.,"]},{"title":"V.","paragraphs":["DellaPietra, and"]},{"title":"R.","paragraphs":["Mercer, \"Word sense disainbigua.tion wing statistica.1 metl~ods,\" in proceeding.̂"]},{"title":"29th","paragraphs":["Annual h4eeting of the ~'ssociatioltjor Comp~itationnl Ling~rislics, (Berkeley, CA), June 1991. [4]"]},{"title":"A.","paragraphs":["Na.das,"]},{"title":"D.","paragraphs":["Na.hamoo,"]},{"title":"M.","paragraphs":["Picheny, a.nd J. Powell, \"An iterat,ive \"flip-flop\"') a.pproximation of the most inIorma.tive split in the construction of decision trees,\" in Proceedings of the"]},{"title":"IEEE","paragraphs":["Inlernnlionir.1 Con,jerence on Acoustics, Speech and Signal Processing, (Toronto, Cana.da.), May 1991.","[5] 1'. Chon, Applicntions of I~?,jormatio~ Theory to Pnttcrn Recognition and the Design of Decision 'I?ree.s and Trellises."]},{"title":"PhD","paragraphs":["t,hesis, Sta.nford Universit,y, .Inne 1988."]},{"title":"REFERENCES","paragraphs":["[I]"]},{"title":"P.","paragraphs":["Brown,"]},{"title":"J.","paragraphs":["Cocke,"]},{"title":"S.","paragraphs":["DellaPietra,"]},{"title":"V.","paragraphs":["DellaPietra, F. Jelinek,"]},{"title":".J.","paragraphs":["Lafferty,"]},{"title":"R.","paragraphs":["Mercer, and"]},{"title":"P.","paragraphs":["Roossin,"]}]}